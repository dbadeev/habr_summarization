# Суммаризация статей Habr
---
## Постановка задачи

Цель проекта - построение автоматической [абстрактивной суммаризации](https://habr.com/ru/articles/514540/#:~:text=%D0%90%D0%B1%D1%81%D1%82%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F%20%D1%81%D1%83%D0%BC%D0%BC%D0%B0%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F) статей из Habr. <br>
Для сообщений пользователей из тестового набора, необходимо предсказать с максимально возможным результатом, является ли тональность твита положительной, отрицательной или нейтральной.<br><br>
При этом, необходимо: <br>
1. **Подготовка данных**  _Для получения эмбеддигов, обработать исходные сообщения, используя различные подходы:<br> just tokenization -- stemming -- lemmatization -- stemming+misspellings -- lemmatization+misspellings -- any other ideas of preprocessing || 0 or 1, if word exists -- word count -- TFIDF_<br>

2. **Определение сходства**  _Среди датасетов, полученных в результате использования различных подходов к предварительной обработке данных, с помощью косинусного сходства, найти 10 наиболее похожих пар твитов_<br>

3. **Машинное обучение**  _Провести анализ тональности сообщений, используя разные алгоритмы машинного обучения и наборы данных, полученных в результате использования различных подходов к предварительной обработке. Результат оценки предсказания (_accuracy_)  на тестовом датасете должен иметь минимальную точность 0.832_<br>

4. _Bonus_: <br>
    * _Использовать иные методы для векторного представления слов (например, word2vec)_
    * _Результат оценки предсказания (accuracy)  на тестовом датасете должен иметь минимальную точность 0.873_
   
**_Замечания:_**
При необходимости, при подготовке данных  можно использовать различные способы очистки данных, включая исключение незначащих стоп-слов.


## Начало Работы

### Копирование
Для копирования файлов Проекта на локальный компьютер в папку *<your_dir_on_local_computer>* выполните:

```
    $ git clone git@github.com:dbadeev/tweets.git <your_dir_on_local_computer>
```

### Описание файлов
* *tweets.pdf* - текст задания
* *requirements.txt* - список библиотек, необходимых для работы
* Папка *data*
  - *processedNegative.csv* - файл с твитами негативной тональности
  - *processedNeutral.csv*  - файл с твитами нейтральной тональности
  - *processedPositive.csv*  - файл с твитами позитивной тональности
* *tweets.ipynb* - ноутбук проекта  
* *text_cleaninig.py* - утилиты "чистки" текста твитов
* *text_processing.py* - утилиты векторизации текста твитов
* *w2v_ml.py* - утилиты векторизации с помощью Word2Vec и предварительно обученных моделей представления векторов слов
* *cosine_similarity.py* - утилиты вычисления косинусного сходства векторов представлений слов
* *machine_learning.py* - утилиты нахождения оптимальных параметров различных моделей машинного обучения для подсчета accuracy с помощью GridSearch
* Папка *res*
  - *cos_sim.csv* - файл с 10 наиболее схожими парами твитов среди датасетов, полученных в результате использования различных подходов к предварительной обработке данных 
  - *df_df_prep.csv*  - файл с твитами, полученными в результате использования различных подходов к предварительной обработке данных 
<br>

## Запуск
В файле *tweets.ipynb* приведена пошаговая реализация проекта с пояснениями и промежуточными результатами. 

Мы собрали несколько тысяч статей с Habr, для 100 из них предстоит построить автоматическую абстрактивную суммаризацию. 
### Что нужно сделать
 - Построить суммаризации для 100 статей из файла `test_articles_clear_100.json`
 - Суммаризации оформить в нужном формате и отправить на проверку
 

### Как оценивается качество
- Качество будет оцениваться моделью, обученной на метрику **SEAHORSE Q6 Concise** 
    - Статья https://arxiv.org/abs/2305.13194
    - Данные и метрики от авторов https://github.com/google-research-datasets/seahorse
- Про обучение модели качества:
        - Мы взяли RU и EN данные из статьи SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation https://arxiv.org/abs/2305.13194
        - Обучили модель-метрику предсказывать Q6 Concise из датасета статьи (как самую высокоуровневую метрику из предложенных)
        - 10% данных ушло на dev/test
        - 90% данных ушло на обучение модели (мы обучили 2 модели, открытую и закрытую, каждая из моделей обучена на 60% данных от полного датасета)

- На вход метрике-модели текст подается в следующем формате: `"Текст статьи:\n" + Text + "\n\n" + "Краткое содержание:\n" + Summary as metric_input`

### Критерий успешного прохождения
 - Получить качество суммаризаций не хуже (с учетом статзначимости), чем у выбранного нами бейзлайна по оценке **закрытой** моделью. 
 - Качество бейзлайна по оценке открытой моделью `=0.557`
 - <i>Примечание.</i> Почему используется закрытая модель? Так как тестовые данные и модель для оценки качества открыты, то можно переобучиться и обмануть открытую модель оценки качества методом перебора.

### Что в архиве
- `test_articles_clear_100.json` - статьи, которые нужно суммаризовать.
- `train_data.json` - примеры различных суммаризаций с оценкой открытой моделью, которые можно использовать для дообучения при необхдимости.
- `example.ipynb` - jupyter ноутбук с примерами чтения данных и того, как записывать решение, то есть выходного формата. Также приведен пример запроса в API для замера качества суммаризаций во время разработки решения. 

### Формат отправки выполненного задания
- `json lines` - в каждой строке json вида `{'id': AAA, 'summary': "BBB"}` (есть в примере)
