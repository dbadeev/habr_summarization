{"Text":"«За сотню»: две «вертушки» для винила от Pro-Ject и Technics / Habr              \n\n29-10-2021\nВ прошлый раз мы поговорили о проигрывателях от Yamaha, Denon и TEAC в сегменте «до 60 тысяч рублей», а потом продвинулись дальше с обзором чуть более дорогих систем от Denon и Pro-Ject. Сегодня переходим планку в сто тысяч за вертушку и смотрим, что готовы предложить за эту сумму бренды Pro-Ject и Technics. Проигрыватель Pro-Ject X2 — Audiomania.ru [ 105 000 ] Pro-Ject X2 — это одна из наиболее свежих разработок инженеров этого бренда. Проигрыватель производят в Европе, поэтому в качестве используемых материалов и реализации выбранных ими решений сомневаться не приходится. Шасси выполнено из MDF и имеет внушительную толщину — 50 мм. При общих габаритах системы [460 х 150 х 340 мм] это — серьезный показатель, как и ее существенный вес — 10 кг. Два килограмма из десяти приходятся на 300-мм опорный акриловый диск толщиной 30 мм. Он установлен на скрытый субдиск, опирающийся на подшипник из стали и бронзы со специальными вставками. Опорный диск здесь приводит в движение пассиковый привод, а для двигателя производитель использует специальную систему стабилизиации и электронного управления скоростью вращения SpeedBox. Она поддерживает три скорости [33 / 45 / 78 об/мин]. Из недостатков — у Pro-Ject X2 нет автостопа, однако наши специалисты рекомендуют установить полуавтоматический подъемник Pro-Ject Q UP [ 3 500 ]. Он поднимет тонарм по мере завершения проигрывания пластинки. Тонарм проигрывателя Pro-Ject X2 — Audiomania.ru Тонарм представляет собой 230-мм трубку из алюминия и углепластика. В него интегрирован шелл для установки MM- и MC-звукоснимателей. Кстати, в последнем случае массивный опорный диск и шасси позволяют минимизировать потенциальные детонации. Стоит заметить, что российский вариант комплектации в большинстве случаев поступает на прилавки магазинов без картриджа, поэтому мы рекомендуем присмотреться к головкам от Ortofon , пусть и далеко не всегда бюджетным. Из других нюансов — есть RCA (Phono) и соответсвующий кабель Connect It E. Присутствует акриловая пылезащитная крышка и специальные опоры для шасси с резиновой вставкой. При необходимости они поддаются регулировке, а их нижняя поверхность оклеена фетром. В целом проигрыватель идет в двух вариантах отделки — шпон натурального дерева и многослойный лак. Сравнение с младшей моделью бренда [X1] есть тут . В нем вы найдете не только крупные снимки X2, но и подробный рассказ эксперта Stereo.ru о впечатлениях от прослушивания этой системы. Проигрыватель Technics SL-1200 MK7 — Audiomania.ru [ 119 990 ] Technics SL-1200 MK7 обладает сопоставимыми габаритами [453 x 169 x 353 мм] и весом [9,6 кг], плюс — включает несколько доработанных решений, использованных в других вертушках бренда. Например, двигатель с высоким пусковым моментом. Он предоставляет возможность плавной регулировки скорости, «подтормаживания» в ручном режиме и вращения в обе стороны. Привод — прямой, есть питч-контроль и электронная регулировка скорости [33 / 45 / 78 об/мин]. 332-мм опорный диск [1,8 кг] выполнен из алюминия, как и шасси проигрывателя. Верхняя плоскость диска обеспечена резиновым покрытием, которое служит в роли мата. Тонарм для этой модели инженеры выбрали классический. Это — 230-мм статически сбалансированная s-образная трубка. В силу гораздо большего профессионального уклона — по сравнению с Pro-Ject X2 — он «заточен» для стабильной работы в весьма специфических условиях. Из других нюансов — у вертушки есть система подсветка Target Light и возможность переключать цвет подсветки стробоскопа и органов управления. Опоры проигрывателя можно регулировать. Обзоры других проигрывателей винила в нашем хабраблоге: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 . Дополнительное чтение по теме у нас в «Мире Hi-Fi»: «Между винилом и кассетой»: история тефифона «Камера, мотор, музыка!»: как режиссеры используют винил в кино От левитирующих пластинок до звука из коробки — 6 гаджетов для винила От игр до тайных сообщений: обсуждаем «пасхалки» в виниловых релизах Почему вернулся винил, и как с этим связаны стриминговые сервисы Винил вместо почтовой марки: необычный раритет Как развивалось домашнее аудио: эра винила\n","metric":0.47802734380000000414,"metric_input":"Текст статьи:\n«За сотню»: две «вертушки» для винила от Pro-Ject и Technics / Habr              \n\n29-10-2021\nВ прошлый раз мы поговорили о проигрывателях от Yamaha, Denon и TEAC в сегменте «до 60 тысяч рублей», а потом продвинулись дальше с обзором чуть более дорогих систем от Denon и Pro-Ject. Сегодня переходим планку в сто тысяч за вертушку и смотрим, что готовы предложить за эту сумму бренды Pro-Ject и Technics. Проигрыватель Pro-Ject X2 — Audiomania.ru [ 105 000 ] Pro-Ject X2 — это одна из наиболее свежих разработок инженеров этого бренда. Проигрыватель производят в Европе, поэтому в качестве используемых материалов и реализации выбранных ими решений сомневаться не приходится. Шасси выполнено из MDF и имеет внушительную толщину — 50 мм. При общих габаритах системы [460 х 150 х 340 мм] это — серьезный показатель, как и ее существенный вес — 10 кг. Два килограмма из десяти приходятся на 300-мм опорный акриловый диск толщиной 30 мм. Он установлен на скрытый субдиск, опирающийся на подшипник из стали и бронзы со специальными вставками. Опорный диск здесь приводит в движение пассиковый привод, а для двигателя производитель использует специальную систему стабилизиации и электронного управления скоростью вращения SpeedBox. Она поддерживает три скорости [33 / 45 / 78 об/мин]. Из недостатков — у Pro-Ject X2 нет автостопа, однако наши специалисты рекомендуют установить полуавтоматический подъемник Pro-Ject Q UP [ 3 500 ]. Он поднимет тонарм по мере завершения проигрывания пластинки. Тонарм проигрывателя Pro-Ject X2 — Audiomania.ru Тонарм представляет собой 230-мм трубку из алюминия и углепластика. В него интегрирован шелл для установки MM- и MC-звукоснимателей. Кстати, в последнем случае массивный опорный диск и шасси позволяют минимизировать потенциальные детонации. Стоит заметить, что российский вариант комплектации в большинстве случаев поступает на прилавки магазинов без картриджа, поэтому мы рекомендуем присмотреться к головкам от Ortofon , пусть и далеко не всегда бюджетным. Из других нюансов — есть RCA (Phono) и соответсвующий кабель Connect It E. Присутствует акриловая пылезащитная крышка и специальные опоры для шасси с резиновой вставкой. При необходимости они поддаются регулировке, а их нижняя поверхность оклеена фетром. В целом проигрыватель идет в двух вариантах отделки — шпон натурального дерева и многослойный лак. Сравнение с младшей моделью бренда [X1] есть тут . В нем вы найдете не только крупные снимки X2, но и подробный рассказ эксперта Stereo.ru о впечатлениях от прослушивания этой системы. Проигрыватель Technics SL-1200 MK7 — Audiomania.ru [ 119 990 ] Technics SL-1200 MK7 обладает сопоставимыми габаритами [453 x 169 x 353 мм] и весом [9,6 кг], плюс — включает несколько доработанных решений, использованных в других вертушках бренда. Например, двигатель с высоким пусковым моментом. Он предоставляет возможность плавной регулировки скорости, «подтормаживания» в ручном режиме и вращения в обе стороны. Привод — прямой, есть питч-контроль и электронная регулировка скорости [33 / 45 / 78 об/мин]. 332-мм опорный диск [1,8 кг] выполнен из алюминия, как и шасси проигрывателя. Верхняя плоскость диска обеспечена резиновым покрытием, которое служит в роли мата. Тонарм для этой модели инженеры выбрали классический. Это — 230-мм статически сбалансированная s-образная трубка. В силу гораздо большего профессионального уклона — по сравнению с Pro-Ject X2 — он «заточен» для стабильной работы в весьма специфических условиях. Из других нюансов — у вертушки есть система подсветка Target Light и возможность переключать цвет подсветки стробоскопа и органов управления. Опоры проигрывателя можно регулировать. Обзоры других проигрывателей винила в нашем хабраблоге: 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 . Дополнительное чтение по теме у нас в «Мире Hi-Fi»: «Между винилом и кассетой»: история тефифона «Камера, мотор, музыка!»: как режиссеры используют винил в кино От левитирующих пластинок до звука из коробки — 6 гаджетов для винила От игр до тайных сообщений: обсуждаем «пасхалки» в виниловых релизах Почему вернулся винил, и как с этим связаны стриминговые сервисы Винил вместо почтовой марки: необычный раритет Как развивалось домашнее аудио: эра винила\n\n\nКраткое содержание:\nВ статье рассматриваются два проигрывателя винила от Pro-Ject и Technics, которые стоят дороже 100 тысяч рублей. Pro-Ject X2 имеет массивное шасси и акриловый опорный диск, а его тонарм интегрирован с шеллом для установки звукоснимателей. Technics SL-1200 MK7 имеет прямой привод и регулируемую скорость вращения.","summary":"В статье рассматриваются два проигрывателя винила от Pro-Ject и Technics, которые стоят дороже 100 тысяч рублей. Pro-Ject X2 имеет массивное шасси и акриловый опорный диск, а его тонарм интегрирован с шеллом для установки звукоснимателей. Technics SL-1200 MK7 имеет прямой привод и регулируемую скорость вращения."}
{"Text":"8 вредныхсоветов по «ускорению» сайта / Habr             \n\n01-04-2016\nВремя идет, интернет становится быстрее, а устройства пользователей — мобильнее. Практики, которые были актуальными 5-10 лет назад, устаревают, и на смену им приходят новые. За 9 лет существования WEBO Group (WEBO Site Speedup, webo.in, webopulsar.ru, Айри.рф) мы проанализировали, увеличили и промониторили скорость сотен тысяч сайтов. С годами браузеры становились быстрее, а сайты — более тяжелыми. И каждый год подходы по ускорению сайтов немного но менялись. Я рассмотрел наиболее часто встречаемые проблемы скорости сайта и наиболее эффективные методы их решения на сегодняшний день. Дисклаймер: все советы вредные, применять их не нужно! 1. Отключите сжатие (gzip) В далеком 2008 году исследование показало, как именно gzip-сжатие ускоряет сайты. Но сейчас пропускные способности каналов существенно превышают заявленные 1500 Кб, а мобильные процессоры находятся на уровне компьютеров 10-летней давности. Поэтому накладные издержки на разархивацию (обратный gzip) превосходят выигрыш от экономии времени на передачу данных. Срочно отключите gzip на сайтах! 2. Чередуйте стили и скрипты Современные браузеры умеют загружать файлы в несколько потоков, а потом применять их к HTML-документу. Для оптимизации времени загрузки можно чередовать загрузку стилей и скриптов, например, так: <link rel=stylesheet media=all href=\"/media/css/admin/styles.css\"> <script type=\"text/javascript\" src=\"/media/js/admin/header.pkg.js\"></script> <link rel=stylesheet media=all href=\"/media/css/admin/icons.css\"> <script type=\"text/javascript\" src=\"/media/js/admin/app.pkg.js\"></script> ... Как это будет работать: пока браузер загружает второй файл (скриптов) он сможет отрендерить первый файл (стилей), и далее по списку. Вы сможете сэкономить время загрузки всех файлов за счет рендеринга (парсинга) предыдущих! 3. Загружайте счетчики в самом верху Сложилась следующая практика — вставлять счетчики и сторонние виджеты внизу страницы, якобы для предотвращения блокировки загрузки. На самом деле, предварительная загрузка сторонних виджетов с самого начала документа позволяет распараллелить загрузку и использовать канал пользователя наиболее рациональным образом. Обращение к домену отличному от домена сайта вызывает дополнительный DNS-запрос. В этом время браузер «простаивает», а мог бы загружать ценную информацию с вашего домена. Вставляйте все сторонние скрипты в самое начало документа — так вы максимально распараллелите загрузку данных. 4. Создайте для каждого ресурса отдельный поддомен Все браузеры имеют ограничение в 10 параллельных запросов к одному домену (не серверу). Чтобы максимально утилизировать канал передачи данных, необходимо распараллелить запросы. Вы можете разбить их на группы по 10, а можете каждый стиль, скрипт, шрифт, изображение загружать со своего поддомена. Это несколько сложно в настройке, и придется дорабатывать систему управления сайтом, но результат превзойдет все ожидания. Здесь — подробная инструкция и обоснование использования множества доменов. 5. Не оптимизируйте изображения Любая оптимизация изображений изменяет их структуру и бинарное представление. Для JPEG-файлов это может привести к нарушению исходных таблиц Хаффмана (снижению качества изображения), для PNG-файлов — к различных «сторонним» эффектам (например, потери прозрачности или искажению цветовой палитры на некоторых устройствах). Если вы хотите быть уверены в качестве изображений — никогда их не оптимизируйте. А лучше всего — используйте BMP-формат на сайтах: он гарантирует передачу всей информации без потерь. 6. Используйте таблицы в верстке Стандарты CSS создают все предпосылки для легкой и семантической верстки страниц. Но реализация стандарта в браузерах не всегда поддерживает эти предпосылки. И достаточно часто специальные возможности CSS3 (например, градиенты или тени) приводят к существенному замедлению отображения страницы. Также существуют проблемы с большим количеством вложенных блоков, имеющих относительное (или плавающее) позиционирование и сложными селекторами (пруфлинк 1, пруфлинк 2, пруфлинк 3). Только таблицы смогут обеспечить космическую скорость и стабильность отображения страницы на всех устройствах. Это давно забытая практика верстки, но настало время дать ей второе дыхание. Вы задумывались, почему даже сегодня в e-mail рассылках повсеместно используются таблицы? За ними будущее! 7. Избегайте оптимизации стилей или скриптов Оптимизация CSS или JavaScript файлов дает не более 15% выигрыша в размере, но при этом может привести к трудно отслеживаемым проблемам в верстке или выполнении (например, при изменении порядка селекторов или удалении «неиспользуемых» переменных из скриптов). Отказ от минимизации текстовых файлов позволит вам также отлаживать все проблемы разработки прямо на самом сайте, не восстанавливая исходные, не минимизированные версии файлов. Если вдруг для объединения и сжатия файлов вы используете какие-то модули системы управления — отключите их. Модули создают динамический код для объединения файлов, и код этот дополнительно нагружает хостинг и тратит его ресурсы. 8. Отключите кэширование файлов Как показала практика оптимизации сайтов за последние 10 лет, кэширование файлов в браузере — сложноуправляемый и неоднозначный механизм. Никогда нельзя быть полностью уверенным, что браузер показывает именно те файлы, которые загружены на хостинг. Если вы хотите, чтобы у пользователей всегда показывался именно ваш сайт (а не его старая версия), то нужно отключить кэширование статических файлов. Это делается в два этапа: нужно отключить безусловное кэширование (заголовки Expires и Cache-Control) и условное (заголовки ETag и Last-Modified). Отключение кэширование также позволит выдавать поисковым роботам наиболее актуальную версию вашего сайта, а не какое-то старье (подробнее об отключении кэширования).\n","metric":0.62158203130000000414,"metric_input":"Текст статьи:\n8 вредныхсоветов по «ускорению» сайта / Habr             \n\n01-04-2016\nВремя идет, интернет становится быстрее, а устройства пользователей — мобильнее. Практики, которые были актуальными 5-10 лет назад, устаревают, и на смену им приходят новые. За 9 лет существования WEBO Group (WEBO Site Speedup, webo.in, webopulsar.ru, Айри.рф) мы проанализировали, увеличили и промониторили скорость сотен тысяч сайтов. С годами браузеры становились быстрее, а сайты — более тяжелыми. И каждый год подходы по ускорению сайтов немного но менялись. Я рассмотрел наиболее часто встречаемые проблемы скорости сайта и наиболее эффективные методы их решения на сегодняшний день. Дисклаймер: все советы вредные, применять их не нужно! 1. Отключите сжатие (gzip) В далеком 2008 году исследование показало, как именно gzip-сжатие ускоряет сайты. Но сейчас пропускные способности каналов существенно превышают заявленные 1500 Кб, а мобильные процессоры находятся на уровне компьютеров 10-летней давности. Поэтому накладные издержки на разархивацию (обратный gzip) превосходят выигрыш от экономии времени на передачу данных. Срочно отключите gzip на сайтах! 2. Чередуйте стили и скрипты Современные браузеры умеют загружать файлы в несколько потоков, а потом применять их к HTML-документу. Для оптимизации времени загрузки можно чередовать загрузку стилей и скриптов, например, так: <link rel=stylesheet media=all href=\"/media/css/admin/styles.css\"> <script type=\"text/javascript\" src=\"/media/js/admin/header.pkg.js\"></script> <link rel=stylesheet media=all href=\"/media/css/admin/icons.css\"> <script type=\"text/javascript\" src=\"/media/js/admin/app.pkg.js\"></script> ... Как это будет работать: пока браузер загружает второй файл (скриптов) он сможет отрендерить первый файл (стилей), и далее по списку. Вы сможете сэкономить время загрузки всех файлов за счет рендеринга (парсинга) предыдущих! 3. Загружайте счетчики в самом верху Сложилась следующая практика — вставлять счетчики и сторонние виджеты внизу страницы, якобы для предотвращения блокировки загрузки. На самом деле, предварительная загрузка сторонних виджетов с самого начала документа позволяет распараллелить загрузку и использовать канал пользователя наиболее рациональным образом. Обращение к домену отличному от домена сайта вызывает дополнительный DNS-запрос. В этом время браузер «простаивает», а мог бы загружать ценную информацию с вашего домена. Вставляйте все сторонние скрипты в самое начало документа — так вы максимально распараллелите загрузку данных. 4. Создайте для каждого ресурса отдельный поддомен Все браузеры имеют ограничение в 10 параллельных запросов к одному домену (не серверу). Чтобы максимально утилизировать канал передачи данных, необходимо распараллелить запросы. Вы можете разбить их на группы по 10, а можете каждый стиль, скрипт, шрифт, изображение загружать со своего поддомена. Это несколько сложно в настройке, и придется дорабатывать систему управления сайтом, но результат превзойдет все ожидания. Здесь — подробная инструкция и обоснование использования множества доменов. 5. Не оптимизируйте изображения Любая оптимизация изображений изменяет их структуру и бинарное представление. Для JPEG-файлов это может привести к нарушению исходных таблиц Хаффмана (снижению качества изображения), для PNG-файлов — к различных «сторонним» эффектам (например, потери прозрачности или искажению цветовой палитры на некоторых устройствах). Если вы хотите быть уверены в качестве изображений — никогда их не оптимизируйте. А лучше всего — используйте BMP-формат на сайтах: он гарантирует передачу всей информации без потерь. 6. Используйте таблицы в верстке Стандарты CSS создают все предпосылки для легкой и семантической верстки страниц. Но реализация стандарта в браузерах не всегда поддерживает эти предпосылки. И достаточно часто специальные возможности CSS3 (например, градиенты или тени) приводят к существенному замедлению отображения страницы. Также существуют проблемы с большим количеством вложенных блоков, имеющих относительное (или плавающее) позиционирование и сложными селекторами (пруфлинк 1, пруфлинк 2, пруфлинк 3). Только таблицы смогут обеспечить космическую скорость и стабильность отображения страницы на всех устройствах. Это давно забытая практика верстки, но настало время дать ей второе дыхание. Вы задумывались, почему даже сегодня в e-mail рассылках повсеместно используются таблицы? За ними будущее! 7. Избегайте оптимизации стилей или скриптов Оптимизация CSS или JavaScript файлов дает не более 15% выигрыша в размере, но при этом может привести к трудно отслеживаемым проблемам в верстке или выполнении (например, при изменении порядка селекторов или удалении «неиспользуемых» переменных из скриптов). Отказ от минимизации текстовых файлов позволит вам также отлаживать все проблемы разработки прямо на самом сайте, не восстанавливая исходные, не минимизированные версии файлов. Если вдруг для объединения и сжатия файлов вы используете какие-то модули системы управления — отключите их. Модули создают динамический код для объединения файлов, и код этот дополнительно нагружает хостинг и тратит его ресурсы. 8. Отключите кэширование файлов Как показала практика оптимизации сайтов за последние 10 лет, кэширование файлов в браузере — сложноуправляемый и неоднозначный механизм. Никогда нельзя быть полностью уверенным, что браузер показывает именно те файлы, которые загружены на хостинг. Если вы хотите, чтобы у пользователей всегда показывался именно ваш сайт (а не его старая версия), то нужно отключить кэширование статических файлов. Это делается в два этапа: нужно отключить безусловное кэширование (заголовки Expires и Cache-Control) и условное (заголовки ETag и Last-Modified). Отключение кэширование также позволит выдавать поисковым роботам наиболее актуальную версию вашего сайта, а не какое-то старье (подробнее об отключении кэширования).\n\n\nКраткое содержание:\nВ данной статье рассматриваются 8 вредных советов по \"ускорению\" сайтов, которые могут привести к негативным последствиям. Вот их краткое содержание:\n\n1. Отключение сжатия (gzip) - в прошлом это было эффективно, но сейчас не актуально из-за улучшения скорости интернета.\n2. Чередование стилей и скриптов - позволяет ускорить загрузку, так как браузер может загружать файлы параллельно.\n3. Загрузка счетчиков в начале страницы - позволяет распределить нагрузку и ускорить загрузку.\n4. Использование отдельных поддоменов для ресурсов - позволяет разделить нагрузку и ускорить загрузку.\n5. Не оптимизация изображений - это может привести к потере качества на некоторых устройствах.\n6. Использование таблиц в верстке - это позволяет обеспечить стабильность отображения на всех устройствах.\n7. Отказ от оптимизации файлов - это может привести к проблемам в разработке и верстке.\n8. Отключение кэширования файлов - позволяет выдавать актуальную версию сайта и предотвращает его застарение.","summary":"В данной статье рассматриваются 8 вредных советов по \"ускорению\" сайтов, которые могут привести к негативным последствиям. Вот их краткое содержание:\n\n1. Отключение сжатия (gzip) - в прошлом это было эффективно, но сейчас не актуально из-за улучшения скорости интернета.\n2. Чередование стилей и скриптов - позволяет ускорить загрузку, так как браузер может загружать файлы параллельно.\n3. Загрузка счетчиков в начале страницы - позволяет распределить нагрузку и ускорить загрузку.\n4. Использование отдельных поддоменов для ресурсов - позволяет разделить нагрузку и ускорить загрузку.\n5. Не оптимизация изображений - это может привести к потере качества на некоторых устройствах.\n6. Использование таблиц в верстке - это позволяет обеспечить стабильность отображения на всех устройствах.\n7. Отказ от оптимизации файлов - это может привести к проблемам в разработке и верстке.\n8. Отключение кэширования файлов - позволяет выдавать актуальную версию сайта и предотвращает его застарение."}
{"Text":"Настройка ipv6 в Proxmox и заметки о lvm / Habr           \n\n10-11-2012\nВ многих местах вижу вопросы о том, как настроить ipv6 для контейнеров в proxmox, а так-же, почему не работает онлайн бекап. Сам уже довольно давно пользуюсь proxmox для виртуализации openvz (разделение сервисов, тесты, дев. окружения) ну и все это крутится на hetzner. О том, как установить и настроить proxmox — писать не буду, на Хабре в частности, да и в интернете в целом — этой информации полно. Не смотря на то, что лучший источник такой информации — официальный вики. Но что в вики нет, так это того, как же правильно ставить proxmox на дебиан. LVM В этом моменте есть маленький нюансик: инсталлер на си-диске размечает жесткий диск автоматически, создавая физический lvm том и 2 логических на нем. Причем, 2 логических тома не заполняют весь физический том на 100%, а оставляют свободным где-то 10-20 процентов. Это делается для того, чтоб правильно работал онлайн бекап (т.е. бекап без остановки работы виртуальных машин). Дело в том, что онлайн бекап происходит методом снепшота рабочего тома. Для этого создается временный логический том как раз в неразмеченном пространстве. А два тома инсталлер на си-диске создает для того, чтоб 2й использовать как хранилище бекапов, ибо нельзя бекапить текущий том на текущий, это какая-то рекурсия выходит. Поэтому, при установке Дебиан нужно учесть, что должно быть 2 логических lvm тома (1 для самих контейнеров, 2й для бекапов), которые заполняют физический lvm том где-то на 80-90%. Как создавать lvm тома и как это делать из рекавери окружение hetzner есть или в документации к дебиану, или в вики hetzner, есть даже на русском языке. Внимание. Тут есть еще один момент, связанный с регрессией в lvm. Во время снепшота — очень сильно проседает запись на логический том, который бекапится. Где-то я видел тикет, но не могу вспомнить где. Если у вас есть информация про этот момент — прошу поделиться. IP V6 Про ipv6 есть так-же масса вопросов. Многие вообще не используют ipv6 в proxmox просто потому, что не могут его настроить, что, в свете паники на рынке ipv4, весьма удручает. А делается-то это довольно легко. Ну, во первых, настроим ipv4 (еще раз напомню, как это делать) нужно создать бридж на физический интерфейс Поля ип адреса, маски и шлюза заполняете так, как выдал вам hetzner. Если вы заказали подсеть — вводите туда данные подсети. Если, как я, заказали несколько отдельных ip адресов — введите туда основной ip, который получили при заказе сервера. После чего — перезагрузить сервер или сеть. Далее, при создании контейнера — выбираете в качестве сетевого интерфейса veth network device и созданный вами бридж. Теперь важный момент для hetzner. Необходимо для каждого дополнительного ip адреса заказать виртуальный mac адрес. Нажимаете на иконочку около ip-адреса — и увидите ваш mac адрес. Скопируйте его и вставьте в поле MAC в окошке редактирования сетевого интерфейса вашей виртуальной машины (не Host MAC address, а просто MAC) Сохраняем, перезагружаем виртуальную машину. Теперь, немного поправим сетку физической машины. Создайте файл /etc/network/if-up.d/ipv6 Сделайте его исполняемым. Поместите туда такое содержимое #!/bin/sh ifconfig vmbr0 add 2a01:ffff:ffff:ffff:xxxx::1/80 ip -6 route add 2a01:ffff:ffff:ffff::1 dev vmbr0 ip -6 route add default via 2a01:ffff:ffff:ffff::1 dev vmbr0 Где ffff — заменить на ваши данные, а xxxx — придумайте свою под-подсеть из выданной вам подсети (ну либо можете использовать всю выданную вам) 80 — маска моей под-подсети. (если будете ставить 64 — учтите, что 2a01:ffff:ffff:ffff::1 — это шлюз, поэтому, для ip адреса используйте 2a01:ffff:ffff:ffff::2) Ну и назначение роутов. Тут понятно. Передергиваем сервер или сеть и проверяем ifconfig Заходим в виртуальную машину через консоль и vzctl enter 100 (или ваш номер). Редактируем /etc/network/interfaces (это для debian, как там в других системах — я не помню) auto eth0 iface eth0 inet static address 5.5.5.5 netmask 255.255.255.224 gateway 5.5.5.1 iface eth0 inet6 static address 2a01:ffff:ffff:ffff:xxxx::1/80 up ip -6 address add 2a01:ffff:ffff:ffff:xxxx::2 dev eth0 up ip -6 address add 2a01:ffff:ffff:ffff:xxxx::3 dev eth0 up ip -6 route add 2a01:ffff:ffff:ffff::/64 dev eth0 up ip -6 route add default via 2a01:ffff:ffff:ffff::1 dev eth0 тут — в блоке ipv4: address, netmask, gateway — данные вашего дополнительного ipv4 адреса или подсети в блоке ipv6: ffff — данные выданной вам сети хххх — подсеть для данной вирт. машины (внимание, тут хххх не равно хххх из предыдущего шага) строчки up ip -6 address add 2a01:ffff:ffff:ffff:xxxx::3 dev eth0 повторяйте столько, сколько ipv6 адресов вам нужно для этой вирт. машины (ну, естественно, увеличивая последнюю циферку на 1 в 16-ричной системе счесление). Я, например, каждому сайту назначаю индивидуальный ipv6 адрес. Последние 2 строки — это шлюзы: внутрисеть и по умолчанию для всего, что вне. Перегружаем машину или сеть. Пингуем ipv6.google.com. Спасибо за внимание.\n","metric":0.5087890625,"metric_input":"Текст статьи:\nНастройка ipv6 в Proxmox и заметки о lvm / Habr           \n\n10-11-2012\nВ многих местах вижу вопросы о том, как настроить ipv6 для контейнеров в proxmox, а так-же, почему не работает онлайн бекап. Сам уже довольно давно пользуюсь proxmox для виртуализации openvz (разделение сервисов, тесты, дев. окружения) ну и все это крутится на hetzner. О том, как установить и настроить proxmox — писать не буду, на Хабре в частности, да и в интернете в целом — этой информации полно. Не смотря на то, что лучший источник такой информации — официальный вики. Но что в вики нет, так это того, как же правильно ставить proxmox на дебиан. LVM В этом моменте есть маленький нюансик: инсталлер на си-диске размечает жесткий диск автоматически, создавая физический lvm том и 2 логических на нем. Причем, 2 логических тома не заполняют весь физический том на 100%, а оставляют свободным где-то 10-20 процентов. Это делается для того, чтоб правильно работал онлайн бекап (т.е. бекап без остановки работы виртуальных машин). Дело в том, что онлайн бекап происходит методом снепшота рабочего тома. Для этого создается временный логический том как раз в неразмеченном пространстве. А два тома инсталлер на си-диске создает для того, чтоб 2й использовать как хранилище бекапов, ибо нельзя бекапить текущий том на текущий, это какая-то рекурсия выходит. Поэтому, при установке Дебиан нужно учесть, что должно быть 2 логических lvm тома (1 для самих контейнеров, 2й для бекапов), которые заполняют физический lvm том где-то на 80-90%. Как создавать lvm тома и как это делать из рекавери окружение hetzner есть или в документации к дебиану, или в вики hetzner, есть даже на русском языке. Внимание. Тут есть еще один момент, связанный с регрессией в lvm. Во время снепшота — очень сильно проседает запись на логический том, который бекапится. Где-то я видел тикет, но не могу вспомнить где. Если у вас есть информация про этот момент — прошу поделиться. IP V6 Про ipv6 есть так-же масса вопросов. Многие вообще не используют ipv6 в proxmox просто потому, что не могут его настроить, что, в свете паники на рынке ipv4, весьма удручает. А делается-то это довольно легко. Ну, во первых, настроим ipv4 (еще раз напомню, как это делать) нужно создать бридж на физический интерфейс Поля ип адреса, маски и шлюза заполняете так, как выдал вам hetzner. Если вы заказали подсеть — вводите туда данные подсети. Если, как я, заказали несколько отдельных ip адресов — введите туда основной ip, который получили при заказе сервера. После чего — перезагрузить сервер или сеть. Далее, при создании контейнера — выбираете в качестве сетевого интерфейса veth network device и созданный вами бридж. Теперь важный момент для hetzner. Необходимо для каждого дополнительного ip адреса заказать виртуальный mac адрес. Нажимаете на иконочку около ip-адреса — и увидите ваш mac адрес. Скопируйте его и вставьте в поле MAC в окошке редактирования сетевого интерфейса вашей виртуальной машины (не Host MAC address, а просто MAC) Сохраняем, перезагружаем виртуальную машину. Теперь, немного поправим сетку физической машины. Создайте файл /etc/network/if-up.d/ipv6 Сделайте его исполняемым. Поместите туда такое содержимое #!/bin/sh ifconfig vmbr0 add 2a01:ffff:ffff:ffff:xxxx::1/80 ip -6 route add 2a01:ffff:ffff:ffff::1 dev vmbr0 ip -6 route add default via 2a01:ffff:ffff:ffff::1 dev vmbr0 Где ffff — заменить на ваши данные, а xxxx — придумайте свою под-подсеть из выданной вам подсети (ну либо можете использовать всю выданную вам) 80 — маска моей под-подсети. (если будете ставить 64 — учтите, что 2a01:ffff:ffff:ffff::1 — это шлюз, поэтому, для ip адреса используйте 2a01:ffff:ffff:ffff::2) Ну и назначение роутов. Тут понятно. Передергиваем сервер или сеть и проверяем ifconfig Заходим в виртуальную машину через консоль и vzctl enter 100 (или ваш номер). Редактируем /etc/network/interfaces (это для debian, как там в других системах — я не помню) auto eth0 iface eth0 inet static address 5.5.5.5 netmask 255.255.255.224 gateway 5.5.5.1 iface eth0 inet6 static address 2a01:ffff:ffff:ffff:xxxx::1/80 up ip -6 address add 2a01:ffff:ffff:ffff:xxxx::2 dev eth0 up ip -6 address add 2a01:ffff:ffff:ffff:xxxx::3 dev eth0 up ip -6 route add 2a01:ffff:ffff:ffff::/64 dev eth0 up ip -6 route add default via 2a01:ffff:ffff:ffff::1 dev eth0 тут — в блоке ipv4: address, netmask, gateway — данные вашего дополнительного ipv4 адреса или подсети в блоке ipv6: ffff — данные выданной вам сети хххх — подсеть для данной вирт. машины (внимание, тут хххх не равно хххх из предыдущего шага) строчки up ip -6 address add 2a01:ffff:ffff:ffff:xxxx::3 dev eth0 повторяйте столько, сколько ipv6 адресов вам нужно для этой вирт. машины (ну, естественно, увеличивая последнюю циферку на 1 в 16-ричной системе счесление). Я, например, каждому сайту назначаю индивидуальный ipv6 адрес. Последние 2 строки — это шлюзы: внутрисеть и по умолчанию для всего, что вне. Перегружаем машину или сеть. Пингуем ipv6.google.com. Спасибо за внимание.\n\n\nКраткое содержание:\nВ статье рассматривается настройка ipv6 в Proxmox и заметки о lvm. Описывается, как правильно устанавливать Proxmox на Debian, и какие логические тома необходимо создавать для корректной работы онлайн бекапа. Также рассматривается настройка ipv6, включая создание бриджа и настройку сетевого интерфейса виртуальной машины.","summary":"В статье рассматривается настройка ipv6 в Proxmox и заметки о lvm. Описывается, как правильно устанавливать Proxmox на Debian, и какие логические тома необходимо создавать для корректной работы онлайн бекапа. Также рассматривается настройка ipv6, включая создание бриджа и настройку сетевого интерфейса виртуальной машины."}
{"Text":"Глеб Помыкалов: Rust еще долго будет оставаться самым любимым языком / Habr            \n\n22-11-2022\nДва года подряд мы делаем конференцию для rust-разработчиков RustCon. И твёрдо убеждены в том, что это один из самых перспективных, хоть и самых сложных языков. Завоюет ли он мир, в каких проектах он не нужен и с каких языков проще всего на него перейти? Обо всем этом мы решили спросить у Глеба Помыкалова, который пишет на расте уже пять лет. Может быть, после интервью вы тоже влюбитесь в Rust. Расскажи, пожалуйста, где ты работаешь, кем и какое отношение имеешь к Rust. Я работаю engineering менеджером в компании Workato. Это стартап уже поздней стадии, мы занимаемся автоматизацией для корпораций. Позволяем не техническим людям строить сложные сценарии, интегрирующие разные сервисы, используемые в компаниях. Моя команда работает в основном с Rust. Как используется Rust в вашей компании, в какой степени и планируется ли расширение направления? Мы фокусируемся на создании микросервисов с высокими требованиями по производительности и масштабируемости. Пока что мы единственная команда, которая использует Rust в компании, но другие команды тоже начинают интересоваться. Сколько человек в твоей команде? Сейчас у нас 6 человек, основной рост пришелся на последний месяц. Недавно Линус Торвальдс принял в состав ветки ядра Linux 6.1 изменения, которые касаются использования Rust в качестве второго языка для разработки драйверов и модулей ядра. Как это повлияет на перспективы языка в целом? Я бы не стал говорить, что это решение исключительно Линуса Торвальдса, хотя он, конечно, важная фигура в мире Linux. Это была длительная работа, в которую были вовлечены и те, кто писал поддержку в линуксе, и команда самого раста, которая адаптировала и оптимизировала нужные фичи. То, что Линус с этим согласился, это здорово, он убедился в том что технология действительно достойна. В свое время C++ не удостоился такой чести. C, хоть и является низкоуровневой основой операционных систем, все же слишком опасный и враждебный. Постоянно думать о том, чтобы не отстрелить себе ногу, — это просто неэффективно и опасно. Ну и стоит ли говорить, что ошибки случаются постоянно. Rust, по сути, — это единственная технология, которая достигла зрелости и может эти проблемы решить. Причем решить достаточно хорошо: с хорошим перфомансом и возможностью писать безопасно. По большому счету, адаптация Rust — это вопрос дальнейшего роста. Другие ОС тоже смотрят в эту сторону. В конечном итоге, они получат более высокую скорость разработки и качество кода. В каких проектах Rust наиболее актуален, а для каких он не подходит? Всякий ли проект можно перевести на этот язык и стоит ли это делать? Вопрос не в технологии, а в той экосистеме, которая сложилась вокруг. Rust довольно тяжело изучать. Далеко не каждый захочет инвестировать в него свои силы и пробьется через те сложности, которые неизбежно возникнут в первые недели и месяцы использования языка. Я помню свой первый заход — было очень тяжело, особенно с tokio 0.1. В большинстве мейнстримовых языков нет таких проблем, они более интуитивны. Для Rust же нужно построить интуицию заново. Наверное, кому-то это дается легче, но все же язык сложен для изучения. Поэтому ключевой вопрос — не то, что Rust может, а то, какую вы решаете задачу и найдете ли вы под нее подходящих программистов. Те, кто приходит в язык сегодня, — это очень мотивированные люди с глубоким взглядом на вещи. Они хотят понимать как все устроено, как достигается перфоманс. Им более интересны глубокие, низкоуровневые вещи, чем, например, красивый интерфейс или сложные бизнес-процессы. До недавнего времени в России не было вакансии rust-разработчиков. Что с международным рынком и как отличается отношение к языку в России и в Европе? По моему ощущению, российский рынок более консервативен в выборе технологий и процессов. Драйвером роста вакансий долго был блокчейн. Там есть требование к скорости и надежность, а так же отсутствует старый код. Поэтому Rust выглядит оптимальным выбор для этих задач. Сейчас язык заходит всё больше в традиционные ниши. Microsoft, Amazon, Cloudflare и другие видят преимущества от использования Rust. Не могу сказать, что это быстрое движение. Проблема со сложностью в изучении имеет свои последствия: пишущих на языке не так много, переобучить довольно сложно. Преимущества все понимают, но все-таки сейчас это не массовая история. С какого языка проще всего переучиться на Rust? С каким языком у него есть что-то общее? В любом современном языке есть что-то общее с другими. Rust объединяет довольно полярные концепции. Кто бы ни пришел, ему будет сложно. Строгая система типов схожа с Haskell. Сам язык компилируемый, без рантайма и в этом похож на C. Многое в стандартной библиотеке взято из функциональных языков. Другие же части довольно эргономичны и создают некую схожесть с популярными интерпретируемыми языками — такими как Ruby, Python, Javascript. К этому добавим концепции языка, которые вообще не имеют аналогов среди популярных технологий. Не так много людей имеют одновременный опыт по всем этим направлениям. Думаю, основной вопрос не столько в технологиях, сколько в складе ума. Разработка в Rust — это совсем не инкрементальное написание строк кода. Это, в первую очередь, формирование правильных и строгих структур данных. Сам код второстепенен в большинстве случаев. Тут архитектура не просто какой-то набор идей или документов, а часть процесса разработки и часть кода. Попытка пренебречь, проигнорировать это — прямой путь к провалу. Такой код на Rust не взлетит. Из этого вытекает и другая особенность языка. В большинстве языков код со временем ухудшается, обрастает долгом. В Rust наоборот — чем больше кодовая база, тем проще писать новый код. Тем, кому такой подход близок, изучить язык будет проще. Станет ли он самым популярным языком и что может ему помешать? Самым популярным — нет. Есть много примеров, когда очень качественные с инженерной точки зрения изобретения не были популярны. Потому что популярность складывается из комплекса причин. Техническая структура и те возможности, которые дает язык, — это лишь часть этого комплекса. Самым популярным будет то, что сможет освоить большая масса людей. А вот в рейтинге самых любимых языков Rust уже который год удерживает первое место. Думаю, это правильно, и сохранится еще надолго. 2 декабря будем публично признаваться в любви к Rust на конференции RustCon, присоединяйтесь.\n","metric":0.52392578130000000414,"metric_input":"Текст статьи:\nГлеб Помыкалов: Rust еще долго будет оставаться самым любимым языком / Habr            \n\n22-11-2022\nДва года подряд мы делаем конференцию для rust-разработчиков RustCon. И твёрдо убеждены в том, что это один из самых перспективных, хоть и самых сложных языков. Завоюет ли он мир, в каких проектах он не нужен и с каких языков проще всего на него перейти? Обо всем этом мы решили спросить у Глеба Помыкалова, который пишет на расте уже пять лет. Может быть, после интервью вы тоже влюбитесь в Rust. Расскажи, пожалуйста, где ты работаешь, кем и какое отношение имеешь к Rust. Я работаю engineering менеджером в компании Workato. Это стартап уже поздней стадии, мы занимаемся автоматизацией для корпораций. Позволяем не техническим людям строить сложные сценарии, интегрирующие разные сервисы, используемые в компаниях. Моя команда работает в основном с Rust. Как используется Rust в вашей компании, в какой степени и планируется ли расширение направления? Мы фокусируемся на создании микросервисов с высокими требованиями по производительности и масштабируемости. Пока что мы единственная команда, которая использует Rust в компании, но другие команды тоже начинают интересоваться. Сколько человек в твоей команде? Сейчас у нас 6 человек, основной рост пришелся на последний месяц. Недавно Линус Торвальдс принял в состав ветки ядра Linux 6.1 изменения, которые касаются использования Rust в качестве второго языка для разработки драйверов и модулей ядра. Как это повлияет на перспективы языка в целом? Я бы не стал говорить, что это решение исключительно Линуса Торвальдса, хотя он, конечно, важная фигура в мире Linux. Это была длительная работа, в которую были вовлечены и те, кто писал поддержку в линуксе, и команда самого раста, которая адаптировала и оптимизировала нужные фичи. То, что Линус с этим согласился, это здорово, он убедился в том что технология действительно достойна. В свое время C++ не удостоился такой чести. C, хоть и является низкоуровневой основой операционных систем, все же слишком опасный и враждебный. Постоянно думать о том, чтобы не отстрелить себе ногу, — это просто неэффективно и опасно. Ну и стоит ли говорить, что ошибки случаются постоянно. Rust, по сути, — это единственная технология, которая достигла зрелости и может эти проблемы решить. Причем решить достаточно хорошо: с хорошим перфомансом и возможностью писать безопасно. По большому счету, адаптация Rust — это вопрос дальнейшего роста. Другие ОС тоже смотрят в эту сторону. В конечном итоге, они получат более высокую скорость разработки и качество кода. В каких проектах Rust наиболее актуален, а для каких он не подходит? Всякий ли проект можно перевести на этот язык и стоит ли это делать? Вопрос не в технологии, а в той экосистеме, которая сложилась вокруг. Rust довольно тяжело изучать. Далеко не каждый захочет инвестировать в него свои силы и пробьется через те сложности, которые неизбежно возникнут в первые недели и месяцы использования языка. Я помню свой первый заход — было очень тяжело, особенно с tokio 0.1. В большинстве мейнстримовых языков нет таких проблем, они более интуитивны. Для Rust же нужно построить интуицию заново. Наверное, кому-то это дается легче, но все же язык сложен для изучения. Поэтому ключевой вопрос — не то, что Rust может, а то, какую вы решаете задачу и найдете ли вы под нее подходящих программистов. Те, кто приходит в язык сегодня, — это очень мотивированные люди с глубоким взглядом на вещи. Они хотят понимать как все устроено, как достигается перфоманс. Им более интересны глубокие, низкоуровневые вещи, чем, например, красивый интерфейс или сложные бизнес-процессы. До недавнего времени в России не было вакансии rust-разработчиков. Что с международным рынком и как отличается отношение к языку в России и в Европе? По моему ощущению, российский рынок более консервативен в выборе технологий и процессов. Драйвером роста вакансий долго был блокчейн. Там есть требование к скорости и надежность, а так же отсутствует старый код. Поэтому Rust выглядит оптимальным выбор для этих задач. Сейчас язык заходит всё больше в традиционные ниши. Microsoft, Amazon, Cloudflare и другие видят преимущества от использования Rust. Не могу сказать, что это быстрое движение. Проблема со сложностью в изучении имеет свои последствия: пишущих на языке не так много, переобучить довольно сложно. Преимущества все понимают, но все-таки сейчас это не массовая история. С какого языка проще всего переучиться на Rust? С каким языком у него есть что-то общее? В любом современном языке есть что-то общее с другими. Rust объединяет довольно полярные концепции. Кто бы ни пришел, ему будет сложно. Строгая система типов схожа с Haskell. Сам язык компилируемый, без рантайма и в этом похож на C. Многое в стандартной библиотеке взято из функциональных языков. Другие же части довольно эргономичны и создают некую схожесть с популярными интерпретируемыми языками — такими как Ruby, Python, Javascript. К этому добавим концепции языка, которые вообще не имеют аналогов среди популярных технологий. Не так много людей имеют одновременный опыт по всем этим направлениям. Думаю, основной вопрос не столько в технологиях, сколько в складе ума. Разработка в Rust — это совсем не инкрементальное написание строк кода. Это, в первую очередь, формирование правильных и строгих структур данных. Сам код второстепенен в большинстве случаев. Тут архитектура не просто какой-то набор идей или документов, а часть процесса разработки и часть кода. Попытка пренебречь, проигнорировать это — прямой путь к провалу. Такой код на Rust не взлетит. Из этого вытекает и другая особенность языка. В большинстве языков код со временем ухудшается, обрастает долгом. В Rust наоборот — чем больше кодовая база, тем проще писать новый код. Тем, кому такой подход близок, изучить язык будет проще. Станет ли он самым популярным языком и что может ему помешать? Самым популярным — нет. Есть много примеров, когда очень качественные с инженерной точки зрения изобретения не были популярны. Потому что популярность складывается из комплекса причин. Техническая структура и те возможности, которые дает язык, — это лишь часть этого комплекса. Самым популярным будет то, что сможет освоить большая масса людей. А вот в рейтинге самых любимых языков Rust уже который год удерживает первое место. Думаю, это правильно, и сохранится еще надолго. 2 декабря будем публично признаваться в любви к Rust на конференции RustCon, присоединяйтесь.\n\n\nКраткое содержание:\nСтатья обсуждает перспективы языка Rust и его использование в разработке микросервисов. Автор, Глеб Помыкалов, работает в компании Workato и использует Rust в своей команде. Он отмечает, что Rust используется для разработки драйверов и модулей ядра Linux, что может повлиять на его перспективы. Автор также обсуждает сложности изучения языка и его преимущества, такие как производительность и безопасность. Он также сравнивает Rust с другими языками, такими как C и Haskell, и отмечает, что сложность изучения может быть препятствием для широкого использования. Однако автор считает, что Rust уже стал одним из самых любимых языков программирования благодаря своим техническим характеристикам и возможностям.","summary":"Статья обсуждает перспективы языка Rust и его использование в разработке микросервисов. Автор, Глеб Помыкалов, работает в компании Workato и использует Rust в своей команде. Он отмечает, что Rust используется для разработки драйверов и модулей ядра Linux, что может повлиять на его перспективы. Автор также обсуждает сложности изучения языка и его преимущества, такие как производительность и безопасность. Он также сравнивает Rust с другими языками, такими как C и Haskell, и отмечает, что сложность изучения может быть препятствием для широкого использования. Однако автор считает, что Rust уже стал одним из самых любимых языков программирования благодаря своим техническим характеристикам и возможностям."}
{"Text":"Выключаем камеру: теряется ли вовлеченность во время рабочих созвонов, когда вместо человека черный квадрат / Хабр                                                           \n\n16-06-2022\nПандемия закончилась, а удаленка осталась. Почти все, кто работает удаленно, участвуют в онлайн-созвонах. Коллеги говорили, что иногда созвонов становится слишком много и на них собирают всех подряд, поэтому некоторые отключают видео и пытаются работать вполглаза. Я и сама часто так делаю. Но обычно организаторы не любят смотреть на полдюжины черных квадратиков, которые на каждый второй вопрос говорят: «А-а-а, извините, повторите еще раз, пожалуйста». Что говорят исследования Чтобы как-то отвечать на справедливые упреки, я поискала доказательства того, что люди, которые сидят на встречах с выключенным видео, менее вовлечены или хуже коммуницируют. Глобальное исследование, проведенное Национальным бюро экономических исследований, показало, что общее количество видеовстреч после пандемии увеличилось. Продолжительность встреч стала короче, что привело к общему сокращению проведенного на встречах времени на 11,5%. Судя по цифрам, мы должны уставать от встреч меньше, чем в офисе. Видимо, есть еще что-то, кроме проведенного на них времени. Усталость от встреч в зависимости от включенной камеры Ученые под руководством Kristen M. Shockley провели четырехнедельный эксперимент. Случайным образом выбрали 103 удаленных сотрудника в крупной американской медицинской компании. Участников попросили во время созвонов держать камеру включенной или выключенной. 56,3% участников эксперимента — женщины, 43,7% — мужчины, средний возраст сотрудников — 41 год. В конце каждого рабочего дня они заполняли анкету, где отвечали на вопросы про уровень усталости. До начала эксперимента примерно 40—50% этих сотрудников включали камеру во время созвонов. В среднем сотрудники проводили на встречах 3,06 часа в день, за это время они успевали поучаствовать в 4—5 созвонах. Ученые проверяли несколько предположений. Устают ли люди больше в те дни, когда включают камеру? Эта гипотеза подтвердилась. Люди отмечали в опросах больший уровень усталости в те дни, когда использовали видео. Предполагается, что это из-за необходимости презентовать себя, делать умное лицо и невербально реагировать на реплики коллег. Участники более вовлечены, когда камера включена? С одной стороны, мы уже выявили, что люди сильнее устают от камеры, а уставшие люди обычно менее вовлечены. С другой стороны, может, есть какой-то эффект сцены, который повышает вовлеченность при включенной камере? Оказалось, что влияние камеры на вовлеченность довольно незначительное и все-таки негативное. Правда, как я поняла, вовлеченность измеряли по анкете, которую заполнял сам участник. Кажется более логичным, если бы вовлеченность оценивали другие участники встречи Есть ли различия в зависимости от пола или опыта работы в организации? Было предположение, что сильнее от включенной камеры будут уставать женщины и новые сотрудники, потому что для них важна самопрезентация. Они больше волнуются о том, как выглядят, и поэтому быстрее устают. Предположение оказалось верным: женщины и новые сотрудники от включенной камеры уставали сильнее. Получается, из-за этой специфической усталости их вовлеченность будет снижаться больше, чем у других сотрудников. Результаты исследования усталости в зависимости от включенной или выключенной камеры у мужчин и женщин Зависимость от включенной или выключенной камеры у старых и новых сотрудников Это интересные результаты, и они противоречат моему собственному опыту. Мне всегда кажется, что люди с выключенной камерой меньше вовлечены в беседу, чем те, у кого она включена. И я встречала много руководителей, которые думают так же, поэтому просят всех сотрудников включать камеру. Возможно, это просто когнитивное искажение, и многие — я в том числе — не замечают случаев, когда человек активно вовлечен во встречу, но при этом не включает камеру. Все-таки невербальные сигналы и выражение лица довольно важны и работают на каком-то менее рациональном уровне. Усталость от встреч в зависимости от пола Отличия мужчин от женщин на зум-встречах подтверждаются и другими исследованиями. Например, в исследовании Géraldine Fauville опросили более 10 000 людей. А — интенсивность усталости от онлайн-созвонов у мужчин и женщин. В — среднее количество встреч у мужчин и женщин в день. С — продолжительность встреч. D — промежутки между встречами В этом исследовании женщины показали на 13,8% больший уровень усталости от Зума, чем мужчины. Возможно, дело в том, что встречи, в которых они участвовали, были чуть длиннее, а промежутки между ними — меньше. Ученые построили модель, которая предсказывала различия уровня усталости у мужчин и женщин, если бы частота и продолжительность встреч была одинаковой, — гендерный эффект все равно сохранился. Интересно, что возраст отрицательно коррелирует с усталостью. То есть более взрослые люди меньше говорят об усталости от Зума, чем молодые. Включенная камера и успеваемость у студентов Другой эксперимент о включении камеры провели среди студентов одного из американских вузов. Группу курса по вводной психологии из 150 человек попросили прослушать небольшую лекцию длиной около получаса и после нее пройти тест с вопросами по материалам лекции. При этом часть студентов видела только слайды лекции, вторая часть — слайды и других студентов, а третья — слайды, себя и видео других студентов. В результате те студенты, которых, казалось бы, сильнее всего отвлекали от лекции видео себя и других студентов, сдали тесты лучше всех, а те, кто видел только слайды, справились хуже двух других групп. Возможно, включенные камеры повышают результаты, потому что работают как глаза на стене. Студентам стыдно залипать в телефон, когда их видят другие студенты или хотя бы когда они видят, что другие студенты внимательно слушают. Интересно, лучше или хуже ответили бы на тесты о содержании встречи работники из первого исследования в зависимости от включенной камеры? Несколько выводов по итогам исследований Женщинам можно меньше беспокоиться о том, насколько профессионально и убедительно они выглядят на видеовстречах. Мужчины не парятся — и им отлично. Если в конце дня накопилась усталость, есть шанс сохранить остатки сил, выключая видео на встречах. Если в команде вертикальная структура и кто-то в конце встречи будет сдавать тест, есть вероятность, что включение видео поможет пройти его лучше. Если структура горизонтальная и люди собираются на встречи, чтобы работу работать, возможно, лучше не настаивать на том, чтобы все включали видео. Человек сам решит, в какой момент внимательно слушать, а в какой одновременно отвечать на письмо, читать ленту и обедать.\n","metric":0.59521484380000000414,"metric_input":"Текст статьи:\nВыключаем камеру: теряется ли вовлеченность во время рабочих созвонов, когда вместо человека черный квадрат / Хабр                                                           \n\n16-06-2022\nПандемия закончилась, а удаленка осталась. Почти все, кто работает удаленно, участвуют в онлайн-созвонах. Коллеги говорили, что иногда созвонов становится слишком много и на них собирают всех подряд, поэтому некоторые отключают видео и пытаются работать вполглаза. Я и сама часто так делаю. Но обычно организаторы не любят смотреть на полдюжины черных квадратиков, которые на каждый второй вопрос говорят: «А-а-а, извините, повторите еще раз, пожалуйста». Что говорят исследования Чтобы как-то отвечать на справедливые упреки, я поискала доказательства того, что люди, которые сидят на встречах с выключенным видео, менее вовлечены или хуже коммуницируют. Глобальное исследование, проведенное Национальным бюро экономических исследований, показало, что общее количество видеовстреч после пандемии увеличилось. Продолжительность встреч стала короче, что привело к общему сокращению проведенного на встречах времени на 11,5%. Судя по цифрам, мы должны уставать от встреч меньше, чем в офисе. Видимо, есть еще что-то, кроме проведенного на них времени. Усталость от встреч в зависимости от включенной камеры Ученые под руководством Kristen M. Shockley провели четырехнедельный эксперимент. Случайным образом выбрали 103 удаленных сотрудника в крупной американской медицинской компании. Участников попросили во время созвонов держать камеру включенной или выключенной. 56,3% участников эксперимента — женщины, 43,7% — мужчины, средний возраст сотрудников — 41 год. В конце каждого рабочего дня они заполняли анкету, где отвечали на вопросы про уровень усталости. До начала эксперимента примерно 40—50% этих сотрудников включали камеру во время созвонов. В среднем сотрудники проводили на встречах 3,06 часа в день, за это время они успевали поучаствовать в 4—5 созвонах. Ученые проверяли несколько предположений. Устают ли люди больше в те дни, когда включают камеру? Эта гипотеза подтвердилась. Люди отмечали в опросах больший уровень усталости в те дни, когда использовали видео. Предполагается, что это из-за необходимости презентовать себя, делать умное лицо и невербально реагировать на реплики коллег. Участники более вовлечены, когда камера включена? С одной стороны, мы уже выявили, что люди сильнее устают от камеры, а уставшие люди обычно менее вовлечены. С другой стороны, может, есть какой-то эффект сцены, который повышает вовлеченность при включенной камере? Оказалось, что влияние камеры на вовлеченность довольно незначительное и все-таки негативное. Правда, как я поняла, вовлеченность измеряли по анкете, которую заполнял сам участник. Кажется более логичным, если бы вовлеченность оценивали другие участники встречи Есть ли различия в зависимости от пола или опыта работы в организации? Было предположение, что сильнее от включенной камеры будут уставать женщины и новые сотрудники, потому что для них важна самопрезентация. Они больше волнуются о том, как выглядят, и поэтому быстрее устают. Предположение оказалось верным: женщины и новые сотрудники от включенной камеры уставали сильнее. Получается, из-за этой специфической усталости их вовлеченность будет снижаться больше, чем у других сотрудников. Результаты исследования усталости в зависимости от включенной или выключенной камеры у мужчин и женщин Зависимость от включенной или выключенной камеры у старых и новых сотрудников Это интересные результаты, и они противоречат моему собственному опыту. Мне всегда кажется, что люди с выключенной камерой меньше вовлечены в беседу, чем те, у кого она включена. И я встречала много руководителей, которые думают так же, поэтому просят всех сотрудников включать камеру. Возможно, это просто когнитивное искажение, и многие — я в том числе — не замечают случаев, когда человек активно вовлечен во встречу, но при этом не включает камеру. Все-таки невербальные сигналы и выражение лица довольно важны и работают на каком-то менее рациональном уровне. Усталость от встреч в зависимости от пола Отличия мужчин от женщин на зум-встречах подтверждаются и другими исследованиями. Например, в исследовании Géraldine Fauville опросили более 10 000 людей. А — интенсивность усталости от онлайн-созвонов у мужчин и женщин. В — среднее количество встреч у мужчин и женщин в день. С — продолжительность встреч. D — промежутки между встречами В этом исследовании женщины показали на 13,8% больший уровень усталости от Зума, чем мужчины. Возможно, дело в том, что встречи, в которых они участвовали, были чуть длиннее, а промежутки между ними — меньше. Ученые построили модель, которая предсказывала различия уровня усталости у мужчин и женщин, если бы частота и продолжительность встреч была одинаковой, — гендерный эффект все равно сохранился. Интересно, что возраст отрицательно коррелирует с усталостью. То есть более взрослые люди меньше говорят об усталости от Зума, чем молодые. Включенная камера и успеваемость у студентов Другой эксперимент о включении камеры провели среди студентов одного из американских вузов. Группу курса по вводной психологии из 150 человек попросили прослушать небольшую лекцию длиной около получаса и после нее пройти тест с вопросами по материалам лекции. При этом часть студентов видела только слайды лекции, вторая часть — слайды и других студентов, а третья — слайды, себя и видео других студентов. В результате те студенты, которых, казалось бы, сильнее всего отвлекали от лекции видео себя и других студентов, сдали тесты лучше всех, а те, кто видел только слайды, справились хуже двух других групп. Возможно, включенные камеры повышают результаты, потому что работают как глаза на стене. Студентам стыдно залипать в телефон, когда их видят другие студенты или хотя бы когда они видят, что другие студенты внимательно слушают. Интересно, лучше или хуже ответили бы на тесты о содержании встречи работники из первого исследования в зависимости от включенной камеры? Несколько выводов по итогам исследований Женщинам можно меньше беспокоиться о том, насколько профессионально и убедительно они выглядят на видеовстречах. Мужчины не парятся — и им отлично. Если в конце дня накопилась усталость, есть шанс сохранить остатки сил, выключая видео на встречах. Если в команде вертикальная структура и кто-то в конце встречи будет сдавать тест, есть вероятность, что включение видео поможет пройти его лучше. Если структура горизонтальная и люди собираются на встречи, чтобы работу работать, возможно, лучше не настаивать на том, чтобы все включали видео. Человек сам решит, в какой момент внимательно слушать, а в какой одновременно отвечать на письмо, читать ленту и обедать.\n\n\nКраткое содержание:\nВ статье рассматривается влияние включения и выключения камеры на вовлеченность участников рабочих созвонов. Исследования показывают, что люди, которые сидят на встречах с выключенным видео, менее вовлечены или хуже коммуницируют. Также было выявлено, что женщины и новые сотрудники устают от включенной камеры сильнее, чем другие сотрудники. Результаты исследования противоречат личному опыту автора, который считает, что люди с выключенной камерой менее вовлечены в беседу.","summary":"В статье рассматривается влияние включения и выключения камеры на вовлеченность участников рабочих созвонов. Исследования показывают, что люди, которые сидят на встречах с выключенным видео, менее вовлечены или хуже коммуницируют. Также было выявлено, что женщины и новые сотрудники устают от включенной камеры сильнее, чем другие сотрудники. Результаты исследования противоречат личному опыту автора, который считает, что люди с выключенной камерой менее вовлечены в беседу."}
{"Text":"Теория, игра и практика Канбан — Certified Kanban/Lean Training / Habr               \n\n10-03-2016\nСовсем недавно я начал работать в компании ScrumTrek. Все новички у нас в рамках ввода в строй в обязательном порядке проходят обучение на открытых тренингах компании. И так устроен мой мозг, что хорошо усвоить материал у меня получается только тогда, когда удается внятно донести основные его тезисы другим. Бедная моя жена, коллеги и друзья – сколько им всего пришлось выслушать от меня! Кстати, Ицхак Пинтосевич называет это универсальной методикой системы сверхобучения 3-П. Итого, по мере прохождения тренингов я буду рассказывать вам про полученные знания глазами участника. Конечно, для компании ScrumTrek это реклама. Хотя лично я надеюсь, что выбранный формат поможет с ответом на вопрос: нужно ли это обучение вам или вашим коллегам? Непривычно разношерстный состав участников тренинга Certified Lean/Kanban Training в компании ScrumTrek привлек мое внимание только спустя неделю, когда я начал разбирать свои записи. На обучении были представители не только традиционного сегмента: ИТ-компании и банки – но также промышленной компании и государственного ведомства. А кто сами участники? Разработчик, аналитик, менеджеры проектов, менеджер продуктов, руководители отделов разработки, руководитель отдела кадров, генеральный директор компании. Все мы признались тренеру в малом практическом опыте в Канбан. При этом многие давно и активно применяют Скрам, но сейчас хотят масштабировать процесс с команды разработки на внешние подразделения. Канбан более универсальный, нежели Скрам? Как иначе объяснить такую разношерстную аудиторию тренинга. При этом Канбан сложнее в освоении? Ведь разработчики приходят к нему, имея за плечами опыт Скрама. Но порог вхождения у Канбан ниже? Ведь его начинают применять вместе с разработчиками и другие подразделения компаний. Теория Канбан – это процесс разработки программного обеспечения? Канбан внедрить относительно просто: достаточно нарисовать поток работ на доске, ограничить WIP, измерять lead time – и никакой работы с людьми не требуется? Если на один из этих вопросов вы ответили или хотели ответить утвердительно, то теорию вам стоит подтянуть. На самом деле Канбан – это метод улучшения сервиса (процессов), который основывается на 9 ценностях и 3 принципах. Почему это так важно? Возьмем, к примеру, самую неочевидную ценность – понимание. Она говорит о необходимости понять текущие процессы прежде, чем вносить в них изменения. В этом смысле Канбан дистанцируется от реализаций культа Карго. Например, зачастую Скрам-команды прилежно, но слепо (без понимания) исполняют формальные регламенты: есть владелец продукта и Скрам-мастер, прилежно проводятся встречи, есть спринты. Но при этом нарушают основополагающие принципы эмпиризма: прозрачность (к примеру, изолируются от «неприятных» заинтересованных лиц, не приглашая их на демонстрации, избирательно относятся к обратной связи), инспекция и адаптация (к примеру, работающий продукт есть в конце каждого спринта, правда, потребителю он почему-то не нужен, а команда ничего с этим не делает). Следуя этой ценности тренер и давал нам теорию, постоянно задавая нам вопросы на засыпку (понимание). Например, такая задачка. 2 из 3 полос дороги перекрыты на ремонт, а тут еще и авария происходит. Быстро формируется пробка, среднее время проезда которой составляет 1 час. В это время ремонт дороги до места аварии завершается, поэтому появляются 2 дополнительные полосы подъезда к пробке. Каким будет среднее время ожидания в пробке теперь? А если справа есть широкая обочина, а сама история происходит в России? Во сколько раз в среднем увеличивается скорость движения машин после проезда аварийного участка? Или вот пример. Тренер показывает нам видео «ONE PIECE FLOW versus BATCH PRODUCTION», которое убедительно показывает выигрыш в скорости работы малыми порциями. И, кажется, нам всем и все понятно! Но вот очередной вопрос тренера на засыпку: «Выходит так, что можно уволить много народу, если делать работу малыми порциями?» — возвращает нас к необходимости более глубокого осмысления. Правильный ответ: конечно, нет. При работе большими порциями часть сотрудников вынужденно простаивает, поэтому в большинстве компаний их начинают перекидывать туда-обратно между несколькими проектами. Иными словами, конечно же, Канбан не может сделать ту же самую работу быстрее, но он позволяет делать быстрее весь проект в целом, как набор работ, взаимосвязанных в едином потоке. Конечно, WIP limits, Classes of Services, Cost of Delay, Explicit Policies и прочие базовые инструменты Канбан в лекции также были освещены. Но выше я постарался привести самое интересное из теоретической части – примеры того, как тренер мучал нас для глубокого понимания закона Литтла, а также закона Бернулли и теории ограничений Голдратта. getKanban game В целом весь тренинг оказался очень практическим. Теория (лекция) заняла незначительную его часть. Большую часть времени мы играли в игру getKanban. Опишу, как это было, и какие уроки мы вынесли. Мы разбились на команды по 6 человек: Кабаны (и почему не Канбаны?), Газмяс и Ромашка. Каждой команде ведущий выдал на стол Канбан-доску, разложил на ней бумажки работ, как будто бы идет уже 9 день работы, и поставил задачу: «Ваша компания разрабатывает веб-приложение и зарабатывает на подписчиках. Новые фичи приносят новых подписчиков. Больше подписчиков, больше прибыль. Ваша цель в максимизации прибыли. Ваша задача в оптимизации потока работы». В течение игры мы отмечали показатели нашей работы на контрольной диаграмме (Run Chart), диаграмме распределения времени выполнения (Lead Time Distribution Chart) и кумулятивной диаграмме (Cumulative Flow Diagram), а также считали прибыль от накопленных подписок. По CFD видно, что команда Газмяс к концу игры научилась системно прокатывать любые работы всего за один день! Собственно, это и стало причиной победы Газмяс в общем зачете трех команд. Время за игрой пролетело незаметно! После игры тренер помог нам закрепить наши уроки. Итак, чему мы научились? Игра позволила прочувствовать механику потоковой работы, опасность накапливания очереди и инерцию ее рассасывания. После тренинга по пути в метро я смотрел на пробку на Садовом кольце, но видел поток. Автомобиль скорой помощи с включенным проблесковым маяком и сиреной ускоренно двигался по Expedite lane. Редиски-водители нарушали движения по полосам, чем увеличивали WIP, помогая пробке расти. Неплохо меня вставило, неправда ли? Мы увидели, как уменьшение ограничения на количество незавершенной работы увеличивает скорость ее выполнения. Хорошо помню тот момент, когда первый раз Cycle time стал равным одному дню. Ощущение: «Невероятно, вот оно!» Но оставалось легкое недоверие: «Быть может, нам просто повезло в этот раз, это ведь кости?» — как минимум, снижать WIP limits еще больше мы испугались. Но потом успех повторился еще 2 раза подряд. Думаю, что именно в этот момент мы действительно усвоили полученные ранее теоретические знания по Канбану. На самом деле перечень уроков более длинный. Выше я привел только самые важные уроки (откровения), которые возможно получить только на практике, в данном случае в рамках игры. Практика На второй день мы разбирали реальные ситуации в компаниях, где мы работаем, выстраивая для них Канбан-системы. По разумеющимся причинам приводить детальное описание разобранных случаев не буду. Заключение Судя по составу участников, тренинг полезен не только командам на старте внедрения Канбан в процесс разработки, но и компаниям, которые хотят выстроить прозрачный сквозной процесс. Итого, что дает тренинг в крупную клетку? Теорию с вопросами на понимание. Игру на то, чтобы прочувствовать теорию на практике. Разбор реальных примеров ваших компаний, чтобы применить полученные знания в деле. Ни убавить, ни добавить?\n","metric":0.58154296880000000414,"metric_input":"Текст статьи:\nТеория, игра и практика Канбан — Certified Kanban/Lean Training / Habr               \n\n10-03-2016\nСовсем недавно я начал работать в компании ScrumTrek. Все новички у нас в рамках ввода в строй в обязательном порядке проходят обучение на открытых тренингах компании. И так устроен мой мозг, что хорошо усвоить материал у меня получается только тогда, когда удается внятно донести основные его тезисы другим. Бедная моя жена, коллеги и друзья – сколько им всего пришлось выслушать от меня! Кстати, Ицхак Пинтосевич называет это универсальной методикой системы сверхобучения 3-П. Итого, по мере прохождения тренингов я буду рассказывать вам про полученные знания глазами участника. Конечно, для компании ScrumTrek это реклама. Хотя лично я надеюсь, что выбранный формат поможет с ответом на вопрос: нужно ли это обучение вам или вашим коллегам? Непривычно разношерстный состав участников тренинга Certified Lean/Kanban Training в компании ScrumTrek привлек мое внимание только спустя неделю, когда я начал разбирать свои записи. На обучении были представители не только традиционного сегмента: ИТ-компании и банки – но также промышленной компании и государственного ведомства. А кто сами участники? Разработчик, аналитик, менеджеры проектов, менеджер продуктов, руководители отделов разработки, руководитель отдела кадров, генеральный директор компании. Все мы признались тренеру в малом практическом опыте в Канбан. При этом многие давно и активно применяют Скрам, но сейчас хотят масштабировать процесс с команды разработки на внешние подразделения. Канбан более универсальный, нежели Скрам? Как иначе объяснить такую разношерстную аудиторию тренинга. При этом Канбан сложнее в освоении? Ведь разработчики приходят к нему, имея за плечами опыт Скрама. Но порог вхождения у Канбан ниже? Ведь его начинают применять вместе с разработчиками и другие подразделения компаний. Теория Канбан – это процесс разработки программного обеспечения? Канбан внедрить относительно просто: достаточно нарисовать поток работ на доске, ограничить WIP, измерять lead time – и никакой работы с людьми не требуется? Если на один из этих вопросов вы ответили или хотели ответить утвердительно, то теорию вам стоит подтянуть. На самом деле Канбан – это метод улучшения сервиса (процессов), который основывается на 9 ценностях и 3 принципах. Почему это так важно? Возьмем, к примеру, самую неочевидную ценность – понимание. Она говорит о необходимости понять текущие процессы прежде, чем вносить в них изменения. В этом смысле Канбан дистанцируется от реализаций культа Карго. Например, зачастую Скрам-команды прилежно, но слепо (без понимания) исполняют формальные регламенты: есть владелец продукта и Скрам-мастер, прилежно проводятся встречи, есть спринты. Но при этом нарушают основополагающие принципы эмпиризма: прозрачность (к примеру, изолируются от «неприятных» заинтересованных лиц, не приглашая их на демонстрации, избирательно относятся к обратной связи), инспекция и адаптация (к примеру, работающий продукт есть в конце каждого спринта, правда, потребителю он почему-то не нужен, а команда ничего с этим не делает). Следуя этой ценности тренер и давал нам теорию, постоянно задавая нам вопросы на засыпку (понимание). Например, такая задачка. 2 из 3 полос дороги перекрыты на ремонт, а тут еще и авария происходит. Быстро формируется пробка, среднее время проезда которой составляет 1 час. В это время ремонт дороги до места аварии завершается, поэтому появляются 2 дополнительные полосы подъезда к пробке. Каким будет среднее время ожидания в пробке теперь? А если справа есть широкая обочина, а сама история происходит в России? Во сколько раз в среднем увеличивается скорость движения машин после проезда аварийного участка? Или вот пример. Тренер показывает нам видео «ONE PIECE FLOW versus BATCH PRODUCTION», которое убедительно показывает выигрыш в скорости работы малыми порциями. И, кажется, нам всем и все понятно! Но вот очередной вопрос тренера на засыпку: «Выходит так, что можно уволить много народу, если делать работу малыми порциями?» — возвращает нас к необходимости более глубокого осмысления. Правильный ответ: конечно, нет. При работе большими порциями часть сотрудников вынужденно простаивает, поэтому в большинстве компаний их начинают перекидывать туда-обратно между несколькими проектами. Иными словами, конечно же, Канбан не может сделать ту же самую работу быстрее, но он позволяет делать быстрее весь проект в целом, как набор работ, взаимосвязанных в едином потоке. Конечно, WIP limits, Classes of Services, Cost of Delay, Explicit Policies и прочие базовые инструменты Канбан в лекции также были освещены. Но выше я постарался привести самое интересное из теоретической части – примеры того, как тренер мучал нас для глубокого понимания закона Литтла, а также закона Бернулли и теории ограничений Голдратта. getKanban game В целом весь тренинг оказался очень практическим. Теория (лекция) заняла незначительную его часть. Большую часть времени мы играли в игру getKanban. Опишу, как это было, и какие уроки мы вынесли. Мы разбились на команды по 6 человек: Кабаны (и почему не Канбаны?), Газмяс и Ромашка. Каждой команде ведущий выдал на стол Канбан-доску, разложил на ней бумажки работ, как будто бы идет уже 9 день работы, и поставил задачу: «Ваша компания разрабатывает веб-приложение и зарабатывает на подписчиках. Новые фичи приносят новых подписчиков. Больше подписчиков, больше прибыль. Ваша цель в максимизации прибыли. Ваша задача в оптимизации потока работы». В течение игры мы отмечали показатели нашей работы на контрольной диаграмме (Run Chart), диаграмме распределения времени выполнения (Lead Time Distribution Chart) и кумулятивной диаграмме (Cumulative Flow Diagram), а также считали прибыль от накопленных подписок. По CFD видно, что команда Газмяс к концу игры научилась системно прокатывать любые работы всего за один день! Собственно, это и стало причиной победы Газмяс в общем зачете трех команд. Время за игрой пролетело незаметно! После игры тренер помог нам закрепить наши уроки. Итак, чему мы научились? Игра позволила прочувствовать механику потоковой работы, опасность накапливания очереди и инерцию ее рассасывания. После тренинга по пути в метро я смотрел на пробку на Садовом кольце, но видел поток. Автомобиль скорой помощи с включенным проблесковым маяком и сиреной ускоренно двигался по Expedite lane. Редиски-водители нарушали движения по полосам, чем увеличивали WIP, помогая пробке расти. Неплохо меня вставило, неправда ли? Мы увидели, как уменьшение ограничения на количество незавершенной работы увеличивает скорость ее выполнения. Хорошо помню тот момент, когда первый раз Cycle time стал равным одному дню. Ощущение: «Невероятно, вот оно!» Но оставалось легкое недоверие: «Быть может, нам просто повезло в этот раз, это ведь кости?» — как минимум, снижать WIP limits еще больше мы испугались. Но потом успех повторился еще 2 раза подряд. Думаю, что именно в этот момент мы действительно усвоили полученные ранее теоретические знания по Канбану. На самом деле перечень уроков более длинный. Выше я привел только самые важные уроки (откровения), которые возможно получить только на практике, в данном случае в рамках игры. Практика На второй день мы разбирали реальные ситуации в компаниях, где мы работаем, выстраивая для них Канбан-системы. По разумеющимся причинам приводить детальное описание разобранных случаев не буду. Заключение Судя по составу участников, тренинг полезен не только командам на старте внедрения Канбан в процесс разработки, но и компаниям, которые хотят выстроить прозрачный сквозной процесс. Итого, что дает тренинг в крупную клетку? Теорию с вопросами на понимание. Игру на то, чтобы прочувствовать теорию на практике. Разбор реальных примеров ваших компаний, чтобы применить полученные знания в деле. Ни убавить, ни добавить?\n\n\nКраткое содержание:\nСтатья рассказывает о новом тренинге Certified Kanban/Lean Training, который проводится в компании ScrumTrek. Участники тренинга - представители разных отраслей, включая ИТ-компании, банки и промышленные предприятия.\n\nТренинг состоит из теоретической части и практических занятий. Теоретическая часть включает в себя лекцию о методе Канбан и его основных принципах. Однако основное внимание уделяется практическим занятиям, где участники играют в игру getKanban, которая позволяет им на практике освоить принципы Канбана.\n\nПосле игры участники анализируют свои результаты и обсуждают полученные уроки. Таким образом, тренинг позволяет участникам не только получить теоретические знания, но и применить их на практике.","summary":"Статья рассказывает о новом тренинге Certified Kanban/Lean Training, который проводится в компании ScrumTrek. Участники тренинга - представители разных отраслей, включая ИТ-компании, банки и промышленные предприятия.\n\nТренинг состоит из теоретической части и практических занятий. Теоретическая часть включает в себя лекцию о методе Канбан и его основных принципах. Однако основное внимание уделяется практическим занятиям, где участники играют в игру getKanban, которая позволяет им на практике освоить принципы Канбана.\n\nПосле игры участники анализируют свои результаты и обсуждают полученные уроки. Таким образом, тренинг позволяет участникам не только получить теоретические знания, но и применить их на практике."}
{"Text":"Linux-2017: самые перспективные дистрибутивы / Habr             \n\n20-01-2017\nНастал новый год, а значит – пришло время посмотреть в будущее, найти самые лучшие, самые перспективные дистрибутивы Linux. Дистрибутивы Linux часто бывают ориентированы на конкретные задачи. Поэтому не получится просто составить список операционных систем и сказать: «они – самые лучшие». Здесь выделены несколько областей использования Linux и выбраны те дистрибутивы, у которых есть все шансы стать первыми в своей нише в 2017-м. Лучший дистрибутив для системных администраторов: Parrot Linux Parrot Linux основана на Debian и предлагает практически все мыслимые средства для тестирования систем на проникновение У любого администратора всегда полно работы. Без хорошего набора инструментов его дни – это постоянное испытание на прочность, непрерывная гонка. Однако, существует множество дистрибутивов Linux, готовых прийти на помощь. Один из них – Parrot Linux. Уверен, он приобретёт серьёзную популярность в 2017-м. Этот дистрибутив основан на Debian, он предлагает огромное количество средств для испытания защищённости систем от несанкционированного доступа. Тут, кроме того, можно найти инструменты из сферы криптографии и компьютерной криминалистики, средства для работы с облачными службами и пакеты для обеспечения анонимности. Есть здесь и кое-что для разработчиков, и даже программы для организации времени. Всё это (на самом деле, там – море инструментов) работает на базе стабильной, проверенной временем системы. В результате получился дистрибутив, отлично подходящий для специалистов по информационной безопасности и сетевых администраторов. Сейчас Parrot Linux занимает 57-ю позицию в рейтинге Distrowatch. Полагаю, к концу года можно будет наблюдать значительное продвижение этого дистрибутива к вершине рейтинга. Здесь можно почитать о Parrot Linux подробнее. Лучший легковесный дистрибутив: LXLE LXLE – это отличная комбинация скромных требований к ресурсам и больших возможностей Без сомнений, LXLE станет легковесным дистрибутивом, который в 2017-м году выберут многие. Почему? Потому что всё в нём очень просто. LXLE сочетает в себе достойные возможности и скромные системные требования. Другими словами, это дистрибутив, который занимает мало места, но позволяет полноценно работать на компьютере. В LXLE можно найти всё необходимое, характерное для релизов Linux, рассчитанных на настольные ПК. Система вполне подойдёт для дома, под её управлением смогут работать не самые новые компьютеры (не говоря уже о вполне актуальных системах). LXLE основана на Ubuntu 16.04 (как результат – долговременная техподдержка обеспечена), здесь применяется менеджер рабочего стола LXDE, который, за более чем десять лет существования, знаком многим, да и устроен несложно. После установки LXDE у вас под рукой окажется множество стандартных средств, вроде LibreOffice и Gimp. Единственно, надо будет самостоятельно установить современный браузер. Сейчас LXLE занимает 16-ю позицию в списке Distrowatch. Всё указывает на то, что к середине года она вполне может войти в первую десятку. Подробнее о LXLE читайте здесь. Лучший настольный дистрибутив: Elementary OS Симпатичный внешний вид – не единственное достоинство Elementary OS Loki. Эта ОС стабильна, удобна, её создатели внимательны к деталям Возможно, я отношусь к этому дистрибутиву предвзято, но я уверен, что Elementary OS Loki сделает невозможное и сбросит с первого места Linux Mint, получив в 2017-м завидное звание «лучший настольный дистрибутив». Если так и будет, то Elementary OS совершит весьма впечатляющий подвиг, учитывая то, что Linux Mint постоянно побеждает в рейтинге Distrowatch. Сейчас Elementary OS находится на шестом месте (а Linux Mint продолжает царствовать на первом). Возможно ли, что Elementary OS сможет свергнуть с престола Mint? Похоже, что да. Loki не только зарекомендовала себя как один из самых красивых дистрибутивов Linux, эта система, кроме того, стабильна, удобна, понятна. Кому-то может показаться, что рабочий стол Elementary OS слишком похож на Mac. Однако, такое сравнение – это плюс системы, так как оно говорит о потенциальном удобстве работы пользователя. Безусловно, настройка внешнего вида Loki не так ограничена, как настойка OS X, поэтому из неё можно сделать именно то, что вам нужно. Вот мой материал об Elementary OS. Лучший дистрибутив для тех, кто знает, что к чему: Gentoo Gentoo требует от пользователя некоторой квалификации, однако, усилия тех, кто выберет этот дистрибутив, будут вознаграждены: они получат именно то, что им нужно. Эта категория дистрибутивов подойдёт тому, кто хочет, блеснуть своими знаниями Linux. Такие ОС предназначены для того, кто разбирается в Linux лучше, чем другие, и ищет дистрибутив, который можно остро заточить под собственные нужды. Когда от Linux ждут именно этого, на ум приходит лишь Gentoo. Gentoo – это дистрибутив Linux, основанный на компиляции пакетов из исходных кодов под конкретное аппаратное обеспечение. После установки ОС от пользователя требуется самостоятельная сборка всего, что ему нужно. Это не только требует достаточно высокого уровня понимания Linux, но и терпения, и времени. В итоге, однако, можно получить именно то, что нужно, и ничего лишнего. Gentoo – проект не новый, существует он уже много лет. Но если, кроме прочего, вам хочется доказать себе и окружающим, что вы – знаток Linux, без Gentoo не обойтись. Лучший дистрибутив для интернета вещей: Snappy Ubuntu Core За счёт использования snap-пакетов, Snappy Ubuntu Core упрощает установку ПО и избавляет от головной боли при обновлениях. В результате перед нами ОС, отлично подходящая для IoT Сейчас мы говорим об устройствах малых форм-факторов. Интернет вещей, или IoT – это область, в которой встроенной ОС Linux просто нет равных. Имеется множество дистрибутивов, которые подойдут для IoT. Однако, уверен, 2017-й станет годом Snappy Ubuntu Core. Благодаря использованию snap-пакетов, установка и обновление ПО в Snappy Ubuntu Core выполняется просто и быстро. Не надо беспокоиться о зависимостях или о перерывах в работе из-за обновлений. В результате перед нами – идеальная программная платформа для IoT. Ubuntu Snappy Core уже можно обнаружить в платах для компьютерных энтузиастов (вроде Rasberry Pi), а ещё – в дронах Erle-Copter и в шлюзах Dell Edge Gateways, в Nextcloud Box и в LimeSDR. Лучший серверный дистрибутив для малых и средних организаций: CentOS CentOS – это надёжная серверная платформа Нет ничего удивительного в том, что CentOS остаётся желанным гостем на серверах малых и средних организаций. У такого положения вещей есть одна очень важная причина: CentOS основана на исходном коде Red Hat Enterprise Linux (RHEL). Из-за этого, выбирая CentOS, вы можете не сомневаться в том, что перед вами – надёжная, проверенная серверная платформа. Главное отличие между RHEL и CentOS (помимо разных маркетинговых стратегий) – поддержка. Пользователи RHEL обеспечены официальной техподдержкой Red Hat. Однако, с 2004-го года, «техподдержка» CentOS – это огромное сообщество энтузиастов. В результате, если вы – владелец малого или среднего бизнеса, который собирается перевести свои сервера на платформу с открытым исходным кодом, вам, в первую очередь, стоит взглянуть на CentOS. Лучший корпоративный серверный дистрибутив: RHEL Red Hat прекрасно соответствует нуждам корпоративных клиентов Снова можно сказать, что здесь нет ничего удивительного. Например, SUSE прикладывает значительные усилия для захвата корпоративного рынка ОС, однажды эти усилия приведут её к вершинам славы, но случится подобное, к сожалению, не в этом году. В 2017-м первую позицию среди самых популярных корпоративных дистрибутивов удержит Red Hat Enterprise Linux (RHEL). По данным Gartner, RHEL принадлежит 67% рынка Linux-дистрибутивов для крупных организаций, при этом подписка на RHEL приносит компании Red Hat около 75% доходов. У такого положения дел много причин. Так, Red Hat предлагает корпоративным клиентам именно то, что им нужно, но, кроме этого, компания прикладывает огромные усилия к развитию множества проектов с открытым исходным кодом. Red Hat знает – что такое Linux, и что такое – корпоративный сектор. Red Hat доверяет немало компаний из списка Fortune 500 (например, ING, Sprint, Bayer Business Services, Atos, Amadeus, Etrade). Дистрибутив RHEL вывел множество разработчиков на новый уровень в областях безопасности, интеграции, управления, в сфере работы с облачными системами. Думаю, кроме того, Red Hat в этом году вложит немало сил в сферу IoT. Хотя, не удивлюсь, если к концу 2017-го SUSE откусит ещё немного от рыночной доли Red Hat. Выбор за вами Один из самых замечательных аспектов платформы Linux заключается в том, что, в конечном счёте, выбор остаётся за пользователем. Выбирать можно из сотен систем, многие из которых будут отлично соответствовать самым изысканным требованиям. Однако, если вы воспользуетесь одним из вышеописанных дистрибутивов, уверен, он вас не разочарует. А чем и для чего пользуетесь вы? Что из мира Linux можете порекомендовать нашим читателям?\n","metric":0.62451171880000000414,"metric_input":"Текст статьи:\nLinux-2017: самые перспективные дистрибутивы / Habr             \n\n20-01-2017\nНастал новый год, а значит – пришло время посмотреть в будущее, найти самые лучшие, самые перспективные дистрибутивы Linux. Дистрибутивы Linux часто бывают ориентированы на конкретные задачи. Поэтому не получится просто составить список операционных систем и сказать: «они – самые лучшие». Здесь выделены несколько областей использования Linux и выбраны те дистрибутивы, у которых есть все шансы стать первыми в своей нише в 2017-м. Лучший дистрибутив для системных администраторов: Parrot Linux Parrot Linux основана на Debian и предлагает практически все мыслимые средства для тестирования систем на проникновение У любого администратора всегда полно работы. Без хорошего набора инструментов его дни – это постоянное испытание на прочность, непрерывная гонка. Однако, существует множество дистрибутивов Linux, готовых прийти на помощь. Один из них – Parrot Linux. Уверен, он приобретёт серьёзную популярность в 2017-м. Этот дистрибутив основан на Debian, он предлагает огромное количество средств для испытания защищённости систем от несанкционированного доступа. Тут, кроме того, можно найти инструменты из сферы криптографии и компьютерной криминалистики, средства для работы с облачными службами и пакеты для обеспечения анонимности. Есть здесь и кое-что для разработчиков, и даже программы для организации времени. Всё это (на самом деле, там – море инструментов) работает на базе стабильной, проверенной временем системы. В результате получился дистрибутив, отлично подходящий для специалистов по информационной безопасности и сетевых администраторов. Сейчас Parrot Linux занимает 57-ю позицию в рейтинге Distrowatch. Полагаю, к концу года можно будет наблюдать значительное продвижение этого дистрибутива к вершине рейтинга. Здесь можно почитать о Parrot Linux подробнее. Лучший легковесный дистрибутив: LXLE LXLE – это отличная комбинация скромных требований к ресурсам и больших возможностей Без сомнений, LXLE станет легковесным дистрибутивом, который в 2017-м году выберут многие. Почему? Потому что всё в нём очень просто. LXLE сочетает в себе достойные возможности и скромные системные требования. Другими словами, это дистрибутив, который занимает мало места, но позволяет полноценно работать на компьютере. В LXLE можно найти всё необходимое, характерное для релизов Linux, рассчитанных на настольные ПК. Система вполне подойдёт для дома, под её управлением смогут работать не самые новые компьютеры (не говоря уже о вполне актуальных системах). LXLE основана на Ubuntu 16.04 (как результат – долговременная техподдержка обеспечена), здесь применяется менеджер рабочего стола LXDE, который, за более чем десять лет существования, знаком многим, да и устроен несложно. После установки LXDE у вас под рукой окажется множество стандартных средств, вроде LibreOffice и Gimp. Единственно, надо будет самостоятельно установить современный браузер. Сейчас LXLE занимает 16-ю позицию в списке Distrowatch. Всё указывает на то, что к середине года она вполне может войти в первую десятку. Подробнее о LXLE читайте здесь. Лучший настольный дистрибутив: Elementary OS Симпатичный внешний вид – не единственное достоинство Elementary OS Loki. Эта ОС стабильна, удобна, её создатели внимательны к деталям Возможно, я отношусь к этому дистрибутиву предвзято, но я уверен, что Elementary OS Loki сделает невозможное и сбросит с первого места Linux Mint, получив в 2017-м завидное звание «лучший настольный дистрибутив». Если так и будет, то Elementary OS совершит весьма впечатляющий подвиг, учитывая то, что Linux Mint постоянно побеждает в рейтинге Distrowatch. Сейчас Elementary OS находится на шестом месте (а Linux Mint продолжает царствовать на первом). Возможно ли, что Elementary OS сможет свергнуть с престола Mint? Похоже, что да. Loki не только зарекомендовала себя как один из самых красивых дистрибутивов Linux, эта система, кроме того, стабильна, удобна, понятна. Кому-то может показаться, что рабочий стол Elementary OS слишком похож на Mac. Однако, такое сравнение – это плюс системы, так как оно говорит о потенциальном удобстве работы пользователя. Безусловно, настройка внешнего вида Loki не так ограничена, как настойка OS X, поэтому из неё можно сделать именно то, что вам нужно. Вот мой материал об Elementary OS. Лучший дистрибутив для тех, кто знает, что к чему: Gentoo Gentoo требует от пользователя некоторой квалификации, однако, усилия тех, кто выберет этот дистрибутив, будут вознаграждены: они получат именно то, что им нужно. Эта категория дистрибутивов подойдёт тому, кто хочет, блеснуть своими знаниями Linux. Такие ОС предназначены для того, кто разбирается в Linux лучше, чем другие, и ищет дистрибутив, который можно остро заточить под собственные нужды. Когда от Linux ждут именно этого, на ум приходит лишь Gentoo. Gentoo – это дистрибутив Linux, основанный на компиляции пакетов из исходных кодов под конкретное аппаратное обеспечение. После установки ОС от пользователя требуется самостоятельная сборка всего, что ему нужно. Это не только требует достаточно высокого уровня понимания Linux, но и терпения, и времени. В итоге, однако, можно получить именно то, что нужно, и ничего лишнего. Gentoo – проект не новый, существует он уже много лет. Но если, кроме прочего, вам хочется доказать себе и окружающим, что вы – знаток Linux, без Gentoo не обойтись. Лучший дистрибутив для интернета вещей: Snappy Ubuntu Core За счёт использования snap-пакетов, Snappy Ubuntu Core упрощает установку ПО и избавляет от головной боли при обновлениях. В результате перед нами ОС, отлично подходящая для IoT Сейчас мы говорим об устройствах малых форм-факторов. Интернет вещей, или IoT – это область, в которой встроенной ОС Linux просто нет равных. Имеется множество дистрибутивов, которые подойдут для IoT. Однако, уверен, 2017-й станет годом Snappy Ubuntu Core. Благодаря использованию snap-пакетов, установка и обновление ПО в Snappy Ubuntu Core выполняется просто и быстро. Не надо беспокоиться о зависимостях или о перерывах в работе из-за обновлений. В результате перед нами – идеальная программная платформа для IoT. Ubuntu Snappy Core уже можно обнаружить в платах для компьютерных энтузиастов (вроде Rasberry Pi), а ещё – в дронах Erle-Copter и в шлюзах Dell Edge Gateways, в Nextcloud Box и в LimeSDR. Лучший серверный дистрибутив для малых и средних организаций: CentOS CentOS – это надёжная серверная платформа Нет ничего удивительного в том, что CentOS остаётся желанным гостем на серверах малых и средних организаций. У такого положения вещей есть одна очень важная причина: CentOS основана на исходном коде Red Hat Enterprise Linux (RHEL). Из-за этого, выбирая CentOS, вы можете не сомневаться в том, что перед вами – надёжная, проверенная серверная платформа. Главное отличие между RHEL и CentOS (помимо разных маркетинговых стратегий) – поддержка. Пользователи RHEL обеспечены официальной техподдержкой Red Hat. Однако, с 2004-го года, «техподдержка» CentOS – это огромное сообщество энтузиастов. В результате, если вы – владелец малого или среднего бизнеса, который собирается перевести свои сервера на платформу с открытым исходным кодом, вам, в первую очередь, стоит взглянуть на CentOS. Лучший корпоративный серверный дистрибутив: RHEL Red Hat прекрасно соответствует нуждам корпоративных клиентов Снова можно сказать, что здесь нет ничего удивительного. Например, SUSE прикладывает значительные усилия для захвата корпоративного рынка ОС, однажды эти усилия приведут её к вершинам славы, но случится подобное, к сожалению, не в этом году. В 2017-м первую позицию среди самых популярных корпоративных дистрибутивов удержит Red Hat Enterprise Linux (RHEL). По данным Gartner, RHEL принадлежит 67% рынка Linux-дистрибутивов для крупных организаций, при этом подписка на RHEL приносит компании Red Hat около 75% доходов. У такого положения дел много причин. Так, Red Hat предлагает корпоративным клиентам именно то, что им нужно, но, кроме этого, компания прикладывает огромные усилия к развитию множества проектов с открытым исходным кодом. Red Hat знает – что такое Linux, и что такое – корпоративный сектор. Red Hat доверяет немало компаний из списка Fortune 500 (например, ING, Sprint, Bayer Business Services, Atos, Amadeus, Etrade). Дистрибутив RHEL вывел множество разработчиков на новый уровень в областях безопасности, интеграции, управления, в сфере работы с облачными системами. Думаю, кроме того, Red Hat в этом году вложит немало сил в сферу IoT. Хотя, не удивлюсь, если к концу 2017-го SUSE откусит ещё немного от рыночной доли Red Hat. Выбор за вами Один из самых замечательных аспектов платформы Linux заключается в том, что, в конечном счёте, выбор остаётся за пользователем. Выбирать можно из сотен систем, многие из которых будут отлично соответствовать самым изысканным требованиям. Однако, если вы воспользуетесь одним из вышеописанных дистрибутивов, уверен, он вас не разочарует. А чем и для чего пользуетесь вы? Что из мира Linux можете порекомендовать нашим читателям?\n\n\nКраткое содержание:\nВ статье рассматриваются перспективные дистрибутивы Linux для различных областей использования, такие как системный администратор, разработка, интернет вещей и корпоративные серверы.\n\nСреди них:\n\n- Parrot Linux - для системного администратора, с большим количеством инструментов для тестирования систем на проникновение и обеспечения безопасности.\n- LXLE - легковесный дистрибутив для настольных компьютеров, с минимальными требованиями к ресурсам и широким функционалом.\n- Elementary OS - настольный дистрибутив с красивым внешним видом и стабильностью.\n- Gentoo - дистрибутив для опытных пользователей, позволяющий настраивать систему под свои нужды.\n- Snappy Ubuntu Core - для интернета вещей, с использованием snap-пакетов и простотой установки ПО.\n- CentOS - надежный серверный дистрибутив для малых и средних организаций, основанный на Red Hat Enterprise Linux.\n- Red Hat Enterprise Linux (RHEL) - корпоративный серверный дистрибутив, с 67% рынка для крупных организаций и 75% доходов компании Red Hat.\n\nВ конце автор призывает читателей выбирать дистрибутив в зависимости от своих потребностей и целей использования Linux.","summary":"В статье рассматриваются перспективные дистрибутивы Linux для различных областей использования, такие как системный администратор, разработка, интернет вещей и корпоративные серверы.\n\nСреди них:\n\n- Parrot Linux - для системного администратора, с большим количеством инструментов для тестирования систем на проникновение и обеспечения безопасности.\n- LXLE - легковесный дистрибутив для настольных компьютеров, с минимальными требованиями к ресурсам и широким функционалом.\n- Elementary OS - настольный дистрибутив с красивым внешним видом и стабильностью.\n- Gentoo - дистрибутив для опытных пользователей, позволяющий настраивать систему под свои нужды.\n- Snappy Ubuntu Core - для интернета вещей, с использованием snap-пакетов и простотой установки ПО.\n- CentOS - надежный серверный дистрибутив для малых и средних организаций, основанный на Red Hat Enterprise Linux.\n- Red Hat Enterprise Linux (RHEL) - корпоративный серверный дистрибутив, с 67% рынка для крупных организаций и 75% доходов компании Red Hat.\n\nВ конце автор призывает читателей выбирать дистрибутив в зависимости от своих потребностей и целей использования Linux."}
{"Text":"Конкуренты: как страшно жить и как выжить, если тебя копируют / Habr                            \n\n16-02-2016\nХотите, чтобы руководитель компании вам солгал? Спросите у него про отношение к конкурентам. Большинство ответит так. Мы любим наши конкурентов. Они помогают нам становиться лучше. Мы никогда бы не стали лидером, если бы нам не помогали конкуренты. Мы не боимся конкурентов, мы сильнее их. Ну вы же понимаете, что всё это неправда... Как бы ни хотелось представить отношения к конкурентам вот так: На самом деле всё намного жёстче: Тут самое время процитировать Ницше «Всё, что нас не убивает, делает нас сильнее». На это могу ответить, что всё, что нас не убивает, вполне может попробовать сделать это ещё раз. А еще есть такое мнение: Конечно всегда можно успокаивать себя верой в то, что рынок систем мониторинга персонала в России ещё не сформировался, а значит пирог большой, на всех хватит. Но я должен напомнить аксиому Траута, описанную им в «Маркетинговых войнах». Не бывает компаний номер два. Бывает номер один и все остальные. По мере того, как рынок начнет становится всё более зрелым, кусочек пирога номеров два будет всё больше сокращаться. Вы хотите, чтобы ваша компания была номер два? Я — нет. Какая главная опасность исходит от конкурентов на рынке программного обеспечения? Конечно же, копирование. Передний привод автомобиля впервые был применён в 1929 году в американском автомобиле Cord L29. Потрясающая техническая новинка, изменившая как само автомобилестроение, так и методы вождения. А вот марка Cord не существует с 1937 года. Если продукт или сервис пользуется спросом и может быть скопирован, то он будет скопирован. Чем бы не занимался ваш стартап, знайте, что множество конкурентов уже присматривается к вашему продукту. Вы настолько инновационны, что вас вообще нет конкурентов? Не волнуйтесь, уже завтра они появятся. Любая команда друзей-программистов или компания, где завалялись несколько лишних сотен тысяч рублей могут достаточно быстро программу или сервис не хуже вашего. А возможно и лучше. Потому что они-то видят сильные и слабые стороны существующего продукта, они не будут пробовать, ошибаться и собирать обратную связь от клиентов. Я уже писал как простота разработки систем учета рабочего времени сделали «входной билет» на этот рынок настолько дешевле, что расплодилось достаточно много мало отличающихся друг от друга систем. А завтра их будет вдвое больше :) «Что делать будем, бояре?» Можно патентовать и регистрировать. Все что по российским законам поддаётся патентованию. Проблема только в том, что в России мало что может быть запатентовано. По большому счету можно защитить защитить логотип и код программы. Можно не дать конкурентам создать программу, интерфейс которой один-в один будет повторять вашу. То есть отбиться от мошенников, которые воруют код, маскируются под вас или пытаются вести бизнес от вашего имени вы сможете. Но статья 52 Европейской патентной конвенции прямо запрещает патентование алгоритмов, методов и программ для ЭВМ. А поскольку мы — часть Европы, то и у нас американские методы защиты в виде патентования алгоритмов и программ не действуют. Итак, если конкурент захочет создать своё приложение со схожим (но не идентичным интерфейсом), использующее тебе самы алгоритмы, то он это сделает. И помешать вы ему никак не сможете. Можно использовать тот факт, что ваша компания первой создала продукт (а остальные скопировали). Можно рассказывать, что вы на рынке уже 10/20/30 лет, а про ваших предшественников еще год назад никто не слышал. Вы когда-нибудь слышали про VisiCalc? Думаю нет. А ведь это первая в мире электронная таблица. И даже больше. Часто говорится, что именно Visicalc превратил микрокомпьютеры из инструмента развлечения и хобби в серьёзный инструмент ведения делового оборота. Всего было продано более 700 000 копий программы (при том что количество компьютеров тогда было гораздо меньше, чем сейчас). Выпуск в 1979 году первой версии программы для компьютера Apple II привёл к тому, что эта машина из инструмента для хобби превратилась в желанный и полезный финансовый инструмент ведения делопроизводства. По-видимому, именно это привело к тому, что IBM приняло решение выйти на рынок персональных компьютеров. И где сейчас эта программа? Ещё не верите, что первенство и количество лет на рынке не помогает в борьбе с конкурентами? Тогда давайте я расскажу про SuperCalc (в далеких 80-х и 90-х сам на нём работал), про Lotus 1-2-3, про Quattro Pro… Где все эти программы? И самое главное — где все эти компании? Их всех смела с лица земли Microsoft со своим Excel. Потому что Excel отлично работала с графическим интерфейсом Windows, а все вышеперечисленные программы — нет. Итак, рынок программного обеспечения — это не рынок вин, коньяков, шоколада и всего прочего, где ценятся традиции. Потребителю важнее, что он может получить от продукта прямо сейчас, чем то, какие места занимал этот продукт год-два-три назад. Бесполезно пугать даже тем, что конкурент может неожиданной уйти с рынка. «Вот уйдёт», — думает покупатель- «тогда будем выбирать что-то ещё. Вы мне все внедрение за час предлагаете, так что стоимость перехода не будет большой». Можно снизить цены, чтобы стать привлекательней конкурентов. Ооооо, вот это точно путь в никуда. Во-первых, всегда может найтись конкурент с более богатым инвестором, который с радостью включится в эту увлекательную гонку скидок (и дождётся-таки пока вы разоритесь). Во-вторых, в результате демпинга вы можете отучить потребителей платить за ваш товар. И останетесь нищим победителем. Ну и наконец, помог ли полностью бесплатный Internet Explorer выиграть Microsoft войну браузеров Тупик? Нет, happy end ещё впереди. На самом деле бороться с конкурентами можно и вот как. Будьте быстрее остальных Архитектура вашего продукта и бизнес-процессы вашей компании должны быть устроены так, чтобы максимально быстро реагировать как на потребности клиентов, так и на изменения рынка и окружающей среды. Помните, что электронные таблицы погубил даже не недостаток функционала (им и ранние версии Excel не блистали), а то, что Excel гораздо лучше работала под Windows. А все остальные выход этой графической среды (а позже операционной системы) проморгали. Будьте более открыты для клиента, чем остальные Больше рассказывайте. О том как делается продукт, как принимаются решения о его дальнейшем развитии. Почему был выбран тот или иной функционал. Как продукт устроен (конечно не нужно выдавать секреты, но покупатели не любят чёрные ящики, особенно если им приписывают чудодейственные свойства). Больше слушайте. Организуйте обратную связь с клиентами, дайте им общаться между собой, обмениваться опытом, формировать комьюнити. Люди должны понимать, что они главная часть вашего бизнеса. Создавайте свой продукт вместе с клиентами Стив Джобс всегда знал, что нужно покупателям ещё до того момента как люди об этом начинали задумываться. Если бы за день до объявления iPhone он предложил людям использовать палец вместо стилуса, то получил бы бурю возмущения. Но он объявил и получил бурю восхищения. Значит ли это, что не нужно слушать клиентов? Безусловно клиентов нужно слушать. И реализовывать их предложения. Но не менее обязательно пытаться влезть в шкуру клиента и представить — что удобно, что не удобно, что добавить, что убрать. Слишком многие продукты сделаны разработчиками для себя. Так как им удобно, как им нравится, как им проще было программировать. А потом разрабочики удивляются, что у клиентов другое мнение относительно их творения Делитесь планами Рассказывать о том, какие новшества будут в вашем продукте полезно по трем причинам. Во-первых, клиенты видят, что продукт развивается и платя вам деньги они вкладывают их в развитие. Во-вторых, вы демонстрируете ту самую открытость и вовлечённость клиентов. И, самое главное, когда потенциальные покупатели будет сравнивать ваш продукт с продуктами конкурента они будут сравнивать объем функционала, который у конкурентов уже есть с тем, который у вас есть и который еще только будет создан! И пусть у вас в бизнесе всё будет хорошо (если вы конечно не мои конкуренты ;) ) Андрей Игнатов\n","metric":0.51513671880000000414,"metric_input":"Текст статьи:\nКонкуренты: как страшно жить и как выжить, если тебя копируют / Habr                            \n\n16-02-2016\nХотите, чтобы руководитель компании вам солгал? Спросите у него про отношение к конкурентам. Большинство ответит так. Мы любим наши конкурентов. Они помогают нам становиться лучше. Мы никогда бы не стали лидером, если бы нам не помогали конкуренты. Мы не боимся конкурентов, мы сильнее их. Ну вы же понимаете, что всё это неправда... Как бы ни хотелось представить отношения к конкурентам вот так: На самом деле всё намного жёстче: Тут самое время процитировать Ницше «Всё, что нас не убивает, делает нас сильнее». На это могу ответить, что всё, что нас не убивает, вполне может попробовать сделать это ещё раз. А еще есть такое мнение: Конечно всегда можно успокаивать себя верой в то, что рынок систем мониторинга персонала в России ещё не сформировался, а значит пирог большой, на всех хватит. Но я должен напомнить аксиому Траута, описанную им в «Маркетинговых войнах». Не бывает компаний номер два. Бывает номер один и все остальные. По мере того, как рынок начнет становится всё более зрелым, кусочек пирога номеров два будет всё больше сокращаться. Вы хотите, чтобы ваша компания была номер два? Я — нет. Какая главная опасность исходит от конкурентов на рынке программного обеспечения? Конечно же, копирование. Передний привод автомобиля впервые был применён в 1929 году в американском автомобиле Cord L29. Потрясающая техническая новинка, изменившая как само автомобилестроение, так и методы вождения. А вот марка Cord не существует с 1937 года. Если продукт или сервис пользуется спросом и может быть скопирован, то он будет скопирован. Чем бы не занимался ваш стартап, знайте, что множество конкурентов уже присматривается к вашему продукту. Вы настолько инновационны, что вас вообще нет конкурентов? Не волнуйтесь, уже завтра они появятся. Любая команда друзей-программистов или компания, где завалялись несколько лишних сотен тысяч рублей могут достаточно быстро программу или сервис не хуже вашего. А возможно и лучше. Потому что они-то видят сильные и слабые стороны существующего продукта, они не будут пробовать, ошибаться и собирать обратную связь от клиентов. Я уже писал как простота разработки систем учета рабочего времени сделали «входной билет» на этот рынок настолько дешевле, что расплодилось достаточно много мало отличающихся друг от друга систем. А завтра их будет вдвое больше :) «Что делать будем, бояре?» Можно патентовать и регистрировать. Все что по российским законам поддаётся патентованию. Проблема только в том, что в России мало что может быть запатентовано. По большому счету можно защитить защитить логотип и код программы. Можно не дать конкурентам создать программу, интерфейс которой один-в один будет повторять вашу. То есть отбиться от мошенников, которые воруют код, маскируются под вас или пытаются вести бизнес от вашего имени вы сможете. Но статья 52 Европейской патентной конвенции прямо запрещает патентование алгоритмов, методов и программ для ЭВМ. А поскольку мы — часть Европы, то и у нас американские методы защиты в виде патентования алгоритмов и программ не действуют. Итак, если конкурент захочет создать своё приложение со схожим (но не идентичным интерфейсом), использующее тебе самы алгоритмы, то он это сделает. И помешать вы ему никак не сможете. Можно использовать тот факт, что ваша компания первой создала продукт (а остальные скопировали). Можно рассказывать, что вы на рынке уже 10/20/30 лет, а про ваших предшественников еще год назад никто не слышал. Вы когда-нибудь слышали про VisiCalc? Думаю нет. А ведь это первая в мире электронная таблица. И даже больше. Часто говорится, что именно Visicalc превратил микрокомпьютеры из инструмента развлечения и хобби в серьёзный инструмент ведения делового оборота. Всего было продано более 700 000 копий программы (при том что количество компьютеров тогда было гораздо меньше, чем сейчас). Выпуск в 1979 году первой версии программы для компьютера Apple II привёл к тому, что эта машина из инструмента для хобби превратилась в желанный и полезный финансовый инструмент ведения делопроизводства. По-видимому, именно это привело к тому, что IBM приняло решение выйти на рынок персональных компьютеров. И где сейчас эта программа? Ещё не верите, что первенство и количество лет на рынке не помогает в борьбе с конкурентами? Тогда давайте я расскажу про SuperCalc (в далеких 80-х и 90-х сам на нём работал), про Lotus 1-2-3, про Quattro Pro… Где все эти программы? И самое главное — где все эти компании? Их всех смела с лица земли Microsoft со своим Excel. Потому что Excel отлично работала с графическим интерфейсом Windows, а все вышеперечисленные программы — нет. Итак, рынок программного обеспечения — это не рынок вин, коньяков, шоколада и всего прочего, где ценятся традиции. Потребителю важнее, что он может получить от продукта прямо сейчас, чем то, какие места занимал этот продукт год-два-три назад. Бесполезно пугать даже тем, что конкурент может неожиданной уйти с рынка. «Вот уйдёт», — думает покупатель- «тогда будем выбирать что-то ещё. Вы мне все внедрение за час предлагаете, так что стоимость перехода не будет большой». Можно снизить цены, чтобы стать привлекательней конкурентов. Ооооо, вот это точно путь в никуда. Во-первых, всегда может найтись конкурент с более богатым инвестором, который с радостью включится в эту увлекательную гонку скидок (и дождётся-таки пока вы разоритесь). Во-вторых, в результате демпинга вы можете отучить потребителей платить за ваш товар. И останетесь нищим победителем. Ну и наконец, помог ли полностью бесплатный Internet Explorer выиграть Microsoft войну браузеров Тупик? Нет, happy end ещё впереди. На самом деле бороться с конкурентами можно и вот как. Будьте быстрее остальных Архитектура вашего продукта и бизнес-процессы вашей компании должны быть устроены так, чтобы максимально быстро реагировать как на потребности клиентов, так и на изменения рынка и окружающей среды. Помните, что электронные таблицы погубил даже не недостаток функционала (им и ранние версии Excel не блистали), а то, что Excel гораздо лучше работала под Windows. А все остальные выход этой графической среды (а позже операционной системы) проморгали. Будьте более открыты для клиента, чем остальные Больше рассказывайте. О том как делается продукт, как принимаются решения о его дальнейшем развитии. Почему был выбран тот или иной функционал. Как продукт устроен (конечно не нужно выдавать секреты, но покупатели не любят чёрные ящики, особенно если им приписывают чудодейственные свойства). Больше слушайте. Организуйте обратную связь с клиентами, дайте им общаться между собой, обмениваться опытом, формировать комьюнити. Люди должны понимать, что они главная часть вашего бизнеса. Создавайте свой продукт вместе с клиентами Стив Джобс всегда знал, что нужно покупателям ещё до того момента как люди об этом начинали задумываться. Если бы за день до объявления iPhone он предложил людям использовать палец вместо стилуса, то получил бы бурю возмущения. Но он объявил и получил бурю восхищения. Значит ли это, что не нужно слушать клиентов? Безусловно клиентов нужно слушать. И реализовывать их предложения. Но не менее обязательно пытаться влезть в шкуру клиента и представить — что удобно, что не удобно, что добавить, что убрать. Слишком многие продукты сделаны разработчиками для себя. Так как им удобно, как им нравится, как им проще было программировать. А потом разрабочики удивляются, что у клиентов другое мнение относительно их творения Делитесь планами Рассказывать о том, какие новшества будут в вашем продукте полезно по трем причинам. Во-первых, клиенты видят, что продукт развивается и платя вам деньги они вкладывают их в развитие. Во-вторых, вы демонстрируете ту самую открытость и вовлечённость клиентов. И, самое главное, когда потенциальные покупатели будет сравнивать ваш продукт с продуктами конкурента они будут сравнивать объем функционала, который у конкурентов уже есть с тем, который у вас есть и который еще только будет создан! И пусть у вас в бизнесе всё будет хорошо (если вы конечно не мои конкуренты ;) ) Андрей Игнатов\n\n\nКраткое содержание:\nВ статье рассматривается проблема копирования продуктов и сервисов конкурентами на рынке программного обеспечения. Автор утверждает, что любая инновация рано или поздно будет скопирована, и компании должны быть готовы к этому.\n\nАвтор приводит примеры того, как конкуренты копировали успешные продукты, такие как автомобиль Cord и программы для компьютеров. Он также отмечает, что даже патенты не могут защитить компанию от копирования.\n\nВ статье предлагаются несколько способов борьбы с конкурентами: быть быстрее остальных, быть более открытым для клиентов и делиться планами развития продукта.\n\nВ целом, статья призывает компании быть инновационными, но при этом не забывать о потребностях клиентов и их ожиданиях.","summary":"В статье рассматривается проблема копирования продуктов и сервисов конкурентами на рынке программного обеспечения. Автор утверждает, что любая инновация рано или поздно будет скопирована, и компании должны быть готовы к этому.\n\nАвтор приводит примеры того, как конкуренты копировали успешные продукты, такие как автомобиль Cord и программы для компьютеров. Он также отмечает, что даже патенты не могут защитить компанию от копирования.\n\nВ статье предлагаются несколько способов борьбы с конкурентами: быть быстрее остальных, быть более открытым для клиентов и делиться планами развития продукта.\n\nВ целом, статья призывает компании быть инновационными, но при этом не забывать о потребностях клиентов и их ожиданиях."}
{"Text":"Паттерны разработки: MVC vs MVP vs MVVM vs MVI / Хабр                \n\n07-12-2017\nОт переводчика: данная статья является переработкой английской статьи по паттернам разработки. В процессе адаптации на русский немало пришлось изменить. Оригинал Выбор между различными паттернами разработки, всегда сопровождается рядом споров и дискуссий, а разные взгляды разработчиков на это еще больше усложняют задачу. Существует ли решение этой идеологической проблемы? Давайте поговорим о MVC, MVP, MVVM и MVI прагматично. Давайте ответим на вопросы: “Почему?”, “Как найти консенсус?” Вступление Вопрос выбора между MVC, MVP, MVVM и MVI коснулся меня, когда я делал приложение для Warta Mobile вместе с моей командой. Нам было необходимо продвинуться от минимально жизнеспособного продукта к проверенному и полностью укомплектованному приложению, и мы знали, что необходимо будет ввести какую-либо архитектуру. У многих есть непоколебимое мнение насчет различных паттернов разработки. Но когда вы рассматриваете архитектуру, такую как MVC, то, казалось бы, полностью стандартные и определенные элементы: модель (Model), представление (View) и контроллер (Controller), разные люди описывают по разному. Трюгве Реенскауг (Trygve Reenskaug) — изобрел и определил MVC. Через 24 года после этого, он описал это не как архитектуру, а как набор реальных моделей, которые были основаны на идее MVC. Я пришел к выводу, что поскольку каждый проект — уникален, то нет идеальной архитектуры. Необходимо детально рассмотреть различные способы реализации, и подумать над преимуществами и недостатками каждой. Чего мы хотим добиться? Масштабируемость, сопровождаемость, надежность Очевидно, что масштабируемость (scalability) — возможность расширять проект, реализовывать новые функции. Сопровождаемость (maintainability) — можно определить как необходимость небольших, атомарных изменений после того, как все функции реализованы. Например, это может быть изменение цвета в пользовательском интерфейсе. Чем лучше сопровождаемость проекта, тем легче новым разработчикам поддерживать проект. Надежность (reliability) — понятно, что никто не станет тратить нервы на нестабильные приложения! Разделение отвественности, повторное использование кода, тестируемость Важнейшим элементом здесь является разделение ответсвенности (Separation of Concerns): различные идеи должны быть разделены. Если мы хотим изменить что-то, мы не должны ходить по разным участкам кода. Без разделения ответственности ни повторное использование кода (Code Reusability), ни тестируемость (Testability) практически невозможно реализовать. Ключ в независимости, как заметил Uncle Bob в Clean Architecture. К примеру, если вы используете библиотеку для загрузки изображений, вы не захотите использовать другую библиотеку, для решения проблем, созданных первой! Независимость в архитектуре приложения — частично реализует масштабируемость и сопровождаемость. Model View Controller У архитектуры MVC есть два варианта: контроллер-супервизор (supervising controller) и пассивное представление (passive view). В мобильной экосистеме — практически никогда не встречается реализация контроллера-супервизора. Архитектуру MVC можно охарактеризовать двумя пунктами: Представление — это визуальная проекция модели Контроллер — это соединение между пользователем и системой Диаграмма иллюстрирует идеологию паттерна. Здecь, представление определяет как слушателей, так и обратные вызовы; представление передает вход в контроллер. Контроллер принимает входные данные, а представление — выходные, однако большое число операций происходит и между ними. Данная архитектура хорошо подходит только для небольших проектов. Пассивное представление MVC Главная идея пассивного представления MVC — это то, что представление полностью управляется контроллером. Помимо этого, код четко разделен на два уровня: бизнес логику и логику отображения: Бизнес логика — то, как работает приложение Логика отображения — то, как выглядит приложение Massive View Controller Нельзя трактовать Активити как представление (view). Необходимо рассматривать его как слой отображения, а сам контроллер выносить в отдельный класс. А чтобы уменьшить код контроллеров представлений, можно разделить представления или определить субпредставления (subviews) с их собственными контроллерами. Реализация MVC паттерна таким образом, позволяет легко разбивать код на модули. Однако, при таком подходе появляются некоторые проблемы: Объединение логики отображения и бизнес логики Трудности при тестировании Решение этих проблем кроется за созданием абстрактного интерфейса для представления. Таким образом, презентер будет работать только с этой абстракцией, а не самим представлением. Тесты станут простыми, а проблемы решенными. Все это — и есть главная идея MVP. Model View Presenter Данная архитектура облегчает unit-тестирование, презентер (presenter) прост для написание тестов, а также может многократно использоваться, потому что представление может реализовать несколько интерфейсов. С точки зрения того, как лучше и корректней создавать интерфейсы, необходимо рассматривать MVP и MVC только как основные идеи, а не паттерны разработки. Use cases Создание use cases — это процесс выноса бизнес логики в отдельные классы, делая их частью модели. Они независимы от контроллера и каждый содержит в себе одно бизнес-правило. Это повышает возможность многократного использования, и упрощает написание тестов. В примере на GitHub, в login controller, вынесен use case валидации и use case логина. Логин производит соединение с сетью. Если есть общие бизнес правила в других контроллерах или презентерах, можно будет переиспользовать эти use case’ы. Привязывание представления (View Bindings) В реализации MVP есть четыре линейный функции, которые ничего не делают, кроме небольших изменений в пользовательском интерфейсе. Можно избежать этого лишнего кода, использовав view binding. Все способы биндинга можно найти здесь. Здесь простой подход: легко тестировать, и еще легче представить элементы представления как параметры через интерфейс, а не функции. Стоит отметить, что с точки зрения презентера — ничего не изменилось. Model View View-Model Существует другой способ биндинга: вместо привязывания представления к интерфейсу, мы привязываем элементы представления к параметрам view-модели — такая архитектура называется MVVM. В нашем примере, поля email, password, и разметка определены с помощью связываний. Когда мы меняем параметры в нашей модели, в разметку тоже вносятся изменения. ViewModel’и просты для написания тестов, потому что они не требуют написания mock-объектов — потому что вы меняете свой собственный элемент, а потом проверяете как он изменился. Model View Intent Еще один элемент, который можно ввести в архитектуре, обычно называется MVI. Если взять какой-либо элемент разметки, например кнопку, то можно сказать, что кнопка ничего не делает, кроме того, что производит какие-либо данные, в частности посылает сведения о том что она нажата или нет. В библиотеке RxJava, то, что создает события — называется observable, то есть кнопка будет являться observable в парадигме реактивного программирования. А вот TextView только отображает какой-либо текст и никаких данных не создает. В RxJava такие элементы, которые только принимают данные, называются consumer. Также существуют элементы, которые делают и то и то, т. е. и принимают и отправляют информацию, например TextEdit. Такой элемент одновременно является и создателем (producer) и приемником (receiver), а в RxJava он называется subject. При таком подходе — все есть поток, и каждый поток начинается с того момента, как какой-либо producer, начинает испускать информацию, а заканчивается на каком-либо receiver, который, в свою очередь, информацию принимает. Как результат, приложение можно рассматривать как потоки данных. Потоки данных — главная идея RxJava. Заключение Несмотря на то, что внедрение разделения ответсвенности требует усилий, это хороший способ повысить качество кода в целом, сделать его масштабируемым, легким в понимании и надежным. Другие паттерны, такие как MVVM, удаление шаблонного кода, MVI могут еще сильнее улучшить масштабируемость, но сделают проект зависимым от RxJava. Также, следует помнить, что можно выбрать всего лишь часть из этих элементов, и сконфигурировать для конечного приложения в зависимости от задач. Все исходники можно найти здесь.\n","metric":0.1728515625,"metric_input":"Текст статьи:\nПаттерны разработки: MVC vs MVP vs MVVM vs MVI / Хабр                \n\n07-12-2017\nОт переводчика: данная статья является переработкой английской статьи по паттернам разработки. В процессе адаптации на русский немало пришлось изменить. Оригинал Выбор между различными паттернами разработки, всегда сопровождается рядом споров и дискуссий, а разные взгляды разработчиков на это еще больше усложняют задачу. Существует ли решение этой идеологической проблемы? Давайте поговорим о MVC, MVP, MVVM и MVI прагматично. Давайте ответим на вопросы: “Почему?”, “Как найти консенсус?” Вступление Вопрос выбора между MVC, MVP, MVVM и MVI коснулся меня, когда я делал приложение для Warta Mobile вместе с моей командой. Нам было необходимо продвинуться от минимально жизнеспособного продукта к проверенному и полностью укомплектованному приложению, и мы знали, что необходимо будет ввести какую-либо архитектуру. У многих есть непоколебимое мнение насчет различных паттернов разработки. Но когда вы рассматриваете архитектуру, такую как MVC, то, казалось бы, полностью стандартные и определенные элементы: модель (Model), представление (View) и контроллер (Controller), разные люди описывают по разному. Трюгве Реенскауг (Trygve Reenskaug) — изобрел и определил MVC. Через 24 года после этого, он описал это не как архитектуру, а как набор реальных моделей, которые были основаны на идее MVC. Я пришел к выводу, что поскольку каждый проект — уникален, то нет идеальной архитектуры. Необходимо детально рассмотреть различные способы реализации, и подумать над преимуществами и недостатками каждой. Чего мы хотим добиться? Масштабируемость, сопровождаемость, надежность Очевидно, что масштабируемость (scalability) — возможность расширять проект, реализовывать новые функции. Сопровождаемость (maintainability) — можно определить как необходимость небольших, атомарных изменений после того, как все функции реализованы. Например, это может быть изменение цвета в пользовательском интерфейсе. Чем лучше сопровождаемость проекта, тем легче новым разработчикам поддерживать проект. Надежность (reliability) — понятно, что никто не станет тратить нервы на нестабильные приложения! Разделение отвественности, повторное использование кода, тестируемость Важнейшим элементом здесь является разделение ответсвенности (Separation of Concerns): различные идеи должны быть разделены. Если мы хотим изменить что-то, мы не должны ходить по разным участкам кода. Без разделения ответственности ни повторное использование кода (Code Reusability), ни тестируемость (Testability) практически невозможно реализовать. Ключ в независимости, как заметил Uncle Bob в Clean Architecture. К примеру, если вы используете библиотеку для загрузки изображений, вы не захотите использовать другую библиотеку, для решения проблем, созданных первой! Независимость в архитектуре приложения — частично реализует масштабируемость и сопровождаемость. Model View Controller У архитектуры MVC есть два варианта: контроллер-супервизор (supervising controller) и пассивное представление (passive view). В мобильной экосистеме — практически никогда не встречается реализация контроллера-супервизора. Архитектуру MVC можно охарактеризовать двумя пунктами: Представление — это визуальная проекция модели Контроллер — это соединение между пользователем и системой Диаграмма иллюстрирует идеологию паттерна. Здecь, представление определяет как слушателей, так и обратные вызовы; представление передает вход в контроллер. Контроллер принимает входные данные, а представление — выходные, однако большое число операций происходит и между ними. Данная архитектура хорошо подходит только для небольших проектов. Пассивное представление MVC Главная идея пассивного представления MVC — это то, что представление полностью управляется контроллером. Помимо этого, код четко разделен на два уровня: бизнес логику и логику отображения: Бизнес логика — то, как работает приложение Логика отображения — то, как выглядит приложение Massive View Controller Нельзя трактовать Активити как представление (view). Необходимо рассматривать его как слой отображения, а сам контроллер выносить в отдельный класс. А чтобы уменьшить код контроллеров представлений, можно разделить представления или определить субпредставления (subviews) с их собственными контроллерами. Реализация MVC паттерна таким образом, позволяет легко разбивать код на модули. Однако, при таком подходе появляются некоторые проблемы: Объединение логики отображения и бизнес логики Трудности при тестировании Решение этих проблем кроется за созданием абстрактного интерфейса для представления. Таким образом, презентер будет работать только с этой абстракцией, а не самим представлением. Тесты станут простыми, а проблемы решенными. Все это — и есть главная идея MVP. Model View Presenter Данная архитектура облегчает unit-тестирование, презентер (presenter) прост для написание тестов, а также может многократно использоваться, потому что представление может реализовать несколько интерфейсов. С точки зрения того, как лучше и корректней создавать интерфейсы, необходимо рассматривать MVP и MVC только как основные идеи, а не паттерны разработки. Use cases Создание use cases — это процесс выноса бизнес логики в отдельные классы, делая их частью модели. Они независимы от контроллера и каждый содержит в себе одно бизнес-правило. Это повышает возможность многократного использования, и упрощает написание тестов. В примере на GitHub, в login controller, вынесен use case валидации и use case логина. Логин производит соединение с сетью. Если есть общие бизнес правила в других контроллерах или презентерах, можно будет переиспользовать эти use case’ы. Привязывание представления (View Bindings) В реализации MVP есть четыре линейный функции, которые ничего не делают, кроме небольших изменений в пользовательском интерфейсе. Можно избежать этого лишнего кода, использовав view binding. Все способы биндинга можно найти здесь. Здесь простой подход: легко тестировать, и еще легче представить элементы представления как параметры через интерфейс, а не функции. Стоит отметить, что с точки зрения презентера — ничего не изменилось. Model View View-Model Существует другой способ биндинга: вместо привязывания представления к интерфейсу, мы привязываем элементы представления к параметрам view-модели — такая архитектура называется MVVM. В нашем примере, поля email, password, и разметка определены с помощью связываний. Когда мы меняем параметры в нашей модели, в разметку тоже вносятся изменения. ViewModel’и просты для написания тестов, потому что они не требуют написания mock-объектов — потому что вы меняете свой собственный элемент, а потом проверяете как он изменился. Model View Intent Еще один элемент, который можно ввести в архитектуре, обычно называется MVI. Если взять какой-либо элемент разметки, например кнопку, то можно сказать, что кнопка ничего не делает, кроме того, что производит какие-либо данные, в частности посылает сведения о том что она нажата или нет. В библиотеке RxJava, то, что создает события — называется observable, то есть кнопка будет являться observable в парадигме реактивного программирования. А вот TextView только отображает какой-либо текст и никаких данных не создает. В RxJava такие элементы, которые только принимают данные, называются consumer. Также существуют элементы, которые делают и то и то, т. е. и принимают и отправляют информацию, например TextEdit. Такой элемент одновременно является и создателем (producer) и приемником (receiver), а в RxJava он называется subject. При таком подходе — все есть поток, и каждый поток начинается с того момента, как какой-либо producer, начинает испускать информацию, а заканчивается на каком-либо receiver, который, в свою очередь, информацию принимает. Как результат, приложение можно рассматривать как потоки данных. Потоки данных — главная идея RxJava. Заключение Несмотря на то, что внедрение разделения ответсвенности требует усилий, это хороший способ повысить качество кода в целом, сделать его масштабируемым, легким в понимании и надежным. Другие паттерны, такие как MVVM, удаление шаблонного кода, MVI могут еще сильнее улучшить масштабируемость, но сделают проект зависимым от RxJava. Также, следует помнить, что можно выбрать всего лишь часть из этих элементов, и сконфигурировать для конечного приложения в зависимости от задач. Все исходники можно найти здесь.\n\n\nКраткое содержание:\nПаттерны разработки: MVC vs MVP vs MVVM vs MVI / Хабр                \n\nОт переводчика: данная статья является переработкой английской статьи по паттернам разработки. В процессе адаптации на русский немало пришлось изменить. Оригинал Выбор между различными паттернами разработки, всегда сопровождается рядом споров и дискуссий, а разные взгляды разработчиков на это еще больше усложняют задачу. Существует ли решение этой идеологической проблемы? Давайте поговорим о MVC, MVP, MVVM и MVI прагматично. Давайте ответим на вопросы: “Почему?”, “Как найти консенсус?” Вступление Вопрос выбора между MVC, MVP, MVVM и MVI коснулся меня, когда я делал приложение для Warta Mobile вместе с моей командой. Нам было необходимо продвинуться от минимально жизнеспособного продукта к проверенному и полностью укомплектованному приложению, и мы знали, что необходимо будет ввести какую-либо архитектуру. У многих есть непоколебимое мнение насчет различных паттернов разработки. Но когда вы рассматриваете архитектуру, такую как MVC, то, казалось бы, полностью стандартные и определенные элементы: модель (Model), представление (View) и контроллер (Controller), разные люди описывают по разному. Трюгве Реенскауг (Trygve Reenskaug) — изобрел и определил MVC. Через 24 года после этого, он описал это не как архитектуру, а как набор реальных моделей, которые были основаны на идее MVC. Я пришел к выводу, что поскольку каждый проект — уникален, то нет идеальной архитектуры. Необходимо детально рассмотреть различные способы реализации, и подумать над преимуществами и недостатками каждой. Чего мы хотим добиться? Масштабируемость, сопровождаемость, надежность Очевидно, что масштабируемость (scalability) — возможность расширять проект, реализовывать новые функции. Сопровождаемость (maintainability) — можно определить как необходимость небольших, атомарных изменений после того, как все функции реализованы. Например, это может быть изменение цвета в пользовательском интерфейсе. Чем лучше сопровожда","summary":"Паттерны разработки: MVC vs MVP vs MVVM vs MVI / Хабр                \n\nОт переводчика: данная статья является переработкой английской статьи по паттернам разработки. В процессе адаптации на русский немало пришлось изменить. Оригинал Выбор между различными паттернами разработки, всегда сопровождается рядом споров и дискуссий, а разные взгляды разработчиков на это еще больше усложняют задачу. Существует ли решение этой идеологической проблемы? Давайте поговорим о MVC, MVP, MVVM и MVI прагматично. Давайте ответим на вопросы: “Почему?”, “Как найти консенсус?” Вступление Вопрос выбора между MVC, MVP, MVVM и MVI коснулся меня, когда я делал приложение для Warta Mobile вместе с моей командой. Нам было необходимо продвинуться от минимально жизнеспособного продукта к проверенному и полностью укомплектованному приложению, и мы знали, что необходимо будет ввести какую-либо архитектуру. У многих есть непоколебимое мнение насчет различных паттернов разработки. Но когда вы рассматриваете архитектуру, такую как MVC, то, казалось бы, полностью стандартные и определенные элементы: модель (Model), представление (View) и контроллер (Controller), разные люди описывают по разному. Трюгве Реенскауг (Trygve Reenskaug) — изобрел и определил MVC. Через 24 года после этого, он описал это не как архитектуру, а как набор реальных моделей, которые были основаны на идее MVC. Я пришел к выводу, что поскольку каждый проект — уникален, то нет идеальной архитектуры. Необходимо детально рассмотреть различные способы реализации, и подумать над преимуществами и недостатками каждой. Чего мы хотим добиться? Масштабируемость, сопровождаемость, надежность Очевидно, что масштабируемость (scalability) — возможность расширять проект, реализовывать новые функции. Сопровождаемость (maintainability) — можно определить как необходимость небольших, атомарных изменений после того, как все функции реализованы. Например, это может быть изменение цвета в пользовательском интерфейсе. Чем лучше сопровожда"}
{"Text":"AOP или как написать свой велосипед для аналитики / Habr                         \n\n13-07-2015\nВ крупных проектах, при реализации логики трекинга событий, часто встают перед проблемой загрязнения кода вызовами методов трекинга, неудобством явного связывания объектов с событиями и поддержкой этих событий при изменении моделей или ui поведения. Из-за вышеописанных причин, мне пришло в голову написать свой решение, которое конечно же, не уйдет дальше моего git репозитория и этой статьи. Кто не боится рефлексии и медленного кода — прошу под кат. Может не нужно? В продуктовых приложениях от того, сколько раз тапнут на эту кнопку или как низко прокрутят список зависит развитие приложения как продукта, его внешний интерфейс и функциональность. Из за этого, код начинают разрезать вызовы методов для трекинга событий, которые разбросаны по всему представлению и еще учитывают состояние определенных объектов. Я хочу сделать менее болезненным добавление новых событий и решить несколько проблем, взятых с воздуха с которыми мне приходилось сталкиваться. Я хочу добиться следующего: 1) Минимальное количество кода для нового события; 2) Минимально количество кода в представлении; 3) Удобная система привязки обьектов к событиям. Решение строится на аннотациях, рефлексии и аспектах. Для реализации аспектной части нашего приложения я буду использовать AspectJ. Он является аспектно-ориентированным расширением для языка Java. На данный момент это, наверное, самый популярный AOP движок. Кстати этот движок был разработан теми самыми людьми, которые и предложили парадигму аспектов. Как это работает Чтобы перехватывать вызов нужных нам методов создаем класс помеченный как @Aspect. Делаее создаем точку соединения с нашими методами и создаем метод помеченный @Around который будет выполняться на точке соединения. AspectJ функционально богат и поддерживает большое количество вариантов точек срезов и советов, но сейчас не об этом. @Aspect public class ViewEventsInjector { private static final String POINTCUT_METHOD = \"execution(@com.makarov.ui.tracker.library.annotations.ViewEvent * *(..))\"; @Pointcut(POINTCUT_METHOD) public void methodAnnotatedWithViewEvent() { } @Around(\"methodAnnotatedWithViewEvent()\") public Object joinPoint(ProceedingJoinPoint joinPoint) throws Throwable { MethodSignature ms = (MethodSignature) joinPoint.getSignature(); Method method = ms.getMethod(); Object object = joinPoint.getThis(); Object[] arg = joinPoint.getArgs(); /* зная метод, входные параметры и объект класса чей метод вызывали мы можем получить всю нужную нам информацию */ Object result = joinPoint.proceed(); return result; } } Реализация Аннотация для наблюдаемых view @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.FIELD }) public @interface LoggerView { String value(); } Параметр аннотации — имя view элемента для более удобного чтения событий/логов. В итоге, после инициализации, у нас есть Map в котором лежат id view элементов, отслеживаемых нами. Перехват событий полностью ложится на плечи аспектов. Мы будем ориентироваться на аннотации, которыми у нас помечены методы view событий. Логика такая: 1) Перехватываем вызов метода; 2) Находим его обработчик, который мы добавили в map с всеми возможными обработчиками методов; 3) Находим по параметрам аннотации все объекты, которые нужно отследить; 4) Создаем обьекта Event из наших полученных данных; 4) Сохраняем событие. Аннотация для методов, на которые будут повешаны наши события: @Retention(RetentionPolicy.CLASS) @Target({ ElementType.CONSTRUCTOR, ElementType.METHOD }) public @interface ViewEvent { String[] value(); } Чтобы унифицировать модели, которые мы хотим привязывать к нашим событиям, вводим интерфейс, который должна реализовывать модель: public interface LoggingModel { Map<String, String> getModelLogState(); } Пример реализации интерфейса: public class Artist implements LoggingModel { private final String mId; private final String mName; public Artist(String id, String name){ mId = id; mName = name; } /* ... */ @Override public Map<String, String> getModelLogState() { Map<String, String> logMap = new HashMap<>(); logMap.put(\"artistId\", mId); logMap.put(\"artistName\", mName); return logMap; } } Собираем все это вместе Ну и наконец собираем все это и в несколько аннотаций у нас начинают трекаться нужные нам события. public class MainActivity extends AppCompatActivity implements View.OnClickListener, TextWatcher{ public static final String TAG = MainActivity.class.getSimpleName(); @LoggerView(\"first button\") public Button button; public Button button2; @LoggerView(\"test editText\") public EditText editText; public Artist artist = new Artist(\"123\", \"qwe\"); public Track track = new Track(\"ABS\"); @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); /* инициализация view элементов */ ViewEventsInjector.init(); ViewEventsInjector.inject(this); } @Override @AttachState({\"artist\",\"track\"}) @ViewEvent(ViewEventsTracker.CLICK) public void onClick(View v) { Log.d(TAG, \"method onClick - \" + v.getId()); } @Override public void beforeTextChanged(CharSequence s, int start, int count, int after) { } @Override public void onTextChanged(CharSequence s, int start, int before, int count) { } @Override @AttachState({\"artist\"}) @ViewEvent(ViewEventsTracker.AFTER_TEXT_CHANGED) public void afterTextChanged(Editable s) { Log.d(TAG, \"afterTextChanged\"); } } Запускаем проект, пробуем тапнуть по кнопке, затем ввести что-нибудь в текстовое поле. И видим наши долгожданные логи, без единой строчки логики в представлении. 07-13 13:52:16.406 D/SimpleRepository﹕ Event{nameView='fist button', nameEvent='onClick', mModelList=[Artist@52a30ec8, Track@52a31040], methodParameters = null, mDate = Mon Jul 13 13:52:16 EDT 2015} 07-13 13:52:24.254 D/SimpleRepository﹕ Event{nameView='textView', nameEvent='afterTextChanged', mModelList=[Artist@52a30ec8], methodParameters= {text = hello}, mDate=Mon Jul 13 13:52:24 EDT 2015} На мой взгляд мы даже этим простым проектиком решили несколько проблем и возможно съэкономили какое то количество времени для рутиных действий. Если потратить еще какое-то количество времени, то можно было неплохо оптимизировать логику аспекта, например, немного переделать хранение обьектов, чтобы не получать их каждый раз через рефлексию. Если кто-то вдруг надумает взяться и довести до ума эту штуку то милости прошу сюда.\n","metric":0.61474609380000000414,"metric_input":"Текст статьи:\nAOP или как написать свой велосипед для аналитики / Habr                         \n\n13-07-2015\nВ крупных проектах, при реализации логики трекинга событий, часто встают перед проблемой загрязнения кода вызовами методов трекинга, неудобством явного связывания объектов с событиями и поддержкой этих событий при изменении моделей или ui поведения. Из-за вышеописанных причин, мне пришло в голову написать свой решение, которое конечно же, не уйдет дальше моего git репозитория и этой статьи. Кто не боится рефлексии и медленного кода — прошу под кат. Может не нужно? В продуктовых приложениях от того, сколько раз тапнут на эту кнопку или как низко прокрутят список зависит развитие приложения как продукта, его внешний интерфейс и функциональность. Из за этого, код начинают разрезать вызовы методов для трекинга событий, которые разбросаны по всему представлению и еще учитывают состояние определенных объектов. Я хочу сделать менее болезненным добавление новых событий и решить несколько проблем, взятых с воздуха с которыми мне приходилось сталкиваться. Я хочу добиться следующего: 1) Минимальное количество кода для нового события; 2) Минимально количество кода в представлении; 3) Удобная система привязки обьектов к событиям. Решение строится на аннотациях, рефлексии и аспектах. Для реализации аспектной части нашего приложения я буду использовать AspectJ. Он является аспектно-ориентированным расширением для языка Java. На данный момент это, наверное, самый популярный AOP движок. Кстати этот движок был разработан теми самыми людьми, которые и предложили парадигму аспектов. Как это работает Чтобы перехватывать вызов нужных нам методов создаем класс помеченный как @Aspect. Делаее создаем точку соединения с нашими методами и создаем метод помеченный @Around который будет выполняться на точке соединения. AspectJ функционально богат и поддерживает большое количество вариантов точек срезов и советов, но сейчас не об этом. @Aspect public class ViewEventsInjector { private static final String POINTCUT_METHOD = \"execution(@com.makarov.ui.tracker.library.annotations.ViewEvent * *(..))\"; @Pointcut(POINTCUT_METHOD) public void methodAnnotatedWithViewEvent() { } @Around(\"methodAnnotatedWithViewEvent()\") public Object joinPoint(ProceedingJoinPoint joinPoint) throws Throwable { MethodSignature ms = (MethodSignature) joinPoint.getSignature(); Method method = ms.getMethod(); Object object = joinPoint.getThis(); Object[] arg = joinPoint.getArgs(); /* зная метод, входные параметры и объект класса чей метод вызывали мы можем получить всю нужную нам информацию */ Object result = joinPoint.proceed(); return result; } } Реализация Аннотация для наблюдаемых view @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.FIELD }) public @interface LoggerView { String value(); } Параметр аннотации — имя view элемента для более удобного чтения событий/логов. В итоге, после инициализации, у нас есть Map в котором лежат id view элементов, отслеживаемых нами. Перехват событий полностью ложится на плечи аспектов. Мы будем ориентироваться на аннотации, которыми у нас помечены методы view событий. Логика такая: 1) Перехватываем вызов метода; 2) Находим его обработчик, который мы добавили в map с всеми возможными обработчиками методов; 3) Находим по параметрам аннотации все объекты, которые нужно отследить; 4) Создаем обьекта Event из наших полученных данных; 4) Сохраняем событие. Аннотация для методов, на которые будут повешаны наши события: @Retention(RetentionPolicy.CLASS) @Target({ ElementType.CONSTRUCTOR, ElementType.METHOD }) public @interface ViewEvent { String[] value(); } Чтобы унифицировать модели, которые мы хотим привязывать к нашим событиям, вводим интерфейс, который должна реализовывать модель: public interface LoggingModel { Map<String, String> getModelLogState(); } Пример реализации интерфейса: public class Artist implements LoggingModel { private final String mId; private final String mName; public Artist(String id, String name){ mId = id; mName = name; } /* ... */ @Override public Map<String, String> getModelLogState() { Map<String, String> logMap = new HashMap<>(); logMap.put(\"artistId\", mId); logMap.put(\"artistName\", mName); return logMap; } } Собираем все это вместе Ну и наконец собираем все это и в несколько аннотаций у нас начинают трекаться нужные нам события. public class MainActivity extends AppCompatActivity implements View.OnClickListener, TextWatcher{ public static final String TAG = MainActivity.class.getSimpleName(); @LoggerView(\"first button\") public Button button; public Button button2; @LoggerView(\"test editText\") public EditText editText; public Artist artist = new Artist(\"123\", \"qwe\"); public Track track = new Track(\"ABS\"); @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); /* инициализация view элементов */ ViewEventsInjector.init(); ViewEventsInjector.inject(this); } @Override @AttachState({\"artist\",\"track\"}) @ViewEvent(ViewEventsTracker.CLICK) public void onClick(View v) { Log.d(TAG, \"method onClick - \" + v.getId()); } @Override public void beforeTextChanged(CharSequence s, int start, int count, int after) { } @Override public void onTextChanged(CharSequence s, int start, int before, int count) { } @Override @AttachState({\"artist\"}) @ViewEvent(ViewEventsTracker.AFTER_TEXT_CHANGED) public void afterTextChanged(Editable s) { Log.d(TAG, \"afterTextChanged\"); } } Запускаем проект, пробуем тапнуть по кнопке, затем ввести что-нибудь в текстовое поле. И видим наши долгожданные логи, без единой строчки логики в представлении. 07-13 13:52:16.406 D/SimpleRepository﹕ Event{nameView='fist button', nameEvent='onClick', mModelList=[Artist@52a30ec8, Track@52a31040], methodParameters = null, mDate = Mon Jul 13 13:52:16 EDT 2015} 07-13 13:52:24.254 D/SimpleRepository﹕ Event{nameView='textView', nameEvent='afterTextChanged', mModelList=[Artist@52a30ec8], methodParameters= {text = hello}, mDate=Mon Jul 13 13:52:24 EDT 2015} На мой взгляд мы даже этим простым проектиком решили несколько проблем и возможно съэкономили какое то количество времени для рутиных действий. Если потратить еще какое-то количество времени, то можно было неплохо оптимизировать логику аспекта, например, немного переделать хранение обьектов, чтобы не получать их каждый раз через рефлексию. Если кто-то вдруг надумает взяться и довести до ума эту штуку то милости прошу сюда.\n\n\nКраткое содержание:\nВ статье рассматривается проблема загрязнения кода вызовами методов трекинга событий и неудобством явного связывания объектов с событиями в крупных проектах. Предлагается решение на основе AOP (Aspect-Oriented Programming) для упрощения добавления новых событий и поддержки изменений моделей или UI поведения. Решение строится на использовании AspectJ, популярного AOP движка для Java. Описывается, как создать аннотации для отслеживания событий и как использовать аспекты для перехвата вызовов методов. Приводится пример реализации и запуска проекта с использованием предложенной методики.","summary":"В статье рассматривается проблема загрязнения кода вызовами методов трекинга событий и неудобством явного связывания объектов с событиями в крупных проектах. Предлагается решение на основе AOP (Aspect-Oriented Programming) для упрощения добавления новых событий и поддержки изменений моделей или UI поведения. Решение строится на использовании AspectJ, популярного AOP движка для Java. Описывается, как создать аннотации для отслеживания событий и как использовать аспекты для перехвата вызовов методов. Приводится пример реализации и запуска проекта с использованием предложенной методики."}
{"Text":"Рассказываем о пользе и вреде FullStack-фреймворков на примере Meteor.js / Habr            \n\n28-09-2022\nВ последнее время появляется довольно много Fullstack-фреймворков, они становятся популярны, их обсуждают. Если посмотреть на результаты The State of JS 2021 в разделе «Библиотеки — Бэкенд-фреймворки», то минимум 5 из них (возможно, больше) будут как раз FullStack. Отсортировав бэкенд-фреймворки по заинтересованности, в самом верху списка мы снова увидим именно FullStack. Это понятно — они востребованы и лежат в основе разных проектов. Однако на наш взгляд, область их применимости несколько ограничена. Почему — объясняем под катом. Привет, Хабр! Меня зовут Александр Захаров, я backend-разработчик на Node.js в компании МойОфис. Коротко расскажу о том, как я вообще пришел к Node.js. В начале пути, около 8 лет назад, я писал на C++, Ruby, немного на Python и еще нескольких языках. Конечно же, был в моей жизни и frontend. В JavaScript я заметил интересную особенность, которую до этого видел только у Qt — можно не опрашивать что-либо в цикле и не ждать выполнения системного вызова, а подписаться на событие и, когда оно произойдет, выполнить некоторые действия. Чуть позже я услышу термин «реактивное программирование» и, спустя еще некоторое время, свяжу это с миром JS — мне покажется, что за этим будущее. Потом я узнаю о Node.js, перестану плеваться от асинхронных операций (в этом изрядно помогут промисы и async/await). И вот я здесь. История повторяется Термин «FullStack-фреймворк» появился довольно давно. Как мне кажется, это прямое следствие идеи «один код для frontend и backend», которая очень часто звучала в первые годы популярности Node.js. Одним из ранних «больших» FullStack-фреймворков был Meteor.js. Он появился в начале 2012 года и довольно быстро стал востребованным. Ниже я выделю аргументы, которые приводились в его пользу, и несколько их обобщу. А параллельно с этим покажу, почему эти «плюсы» не всегда оказываются полезными в реальных проектах, а иногда и мешают им. Плюсы и минусы FullStack-фреймворков Быстрое начало разработки: плюс В FullStack-фреймворке уже приняты основные решения. Особенно решения, касающиеся взаимодействия frontend и backend (иначе это уже не похоже на FullStack). То есть, скорее всего, команде не придется договариваться о том, что она понимает под Rest и какой формат сообщений, посылаемый через WebSockets, правильный. Более того, скорее всего в шаблоне приложения у вас уже будут и линтер, и настроенный TypeScript, и заранее известная структура директорий, и еще какие-нибудь приятные для взаимопонимания вещи. В случае Meteor он еще и выберет за вас БД (это будет mongoDB). Быстрое начало разработки: минус Со временем появятся новые ответы на старые вопросы. Или вы наткнетесь на протекающую абстракцию какого-либо из ответов. Хорошим примером, мне кажется, служит развитие возможностей языка. Однажды я поставил на новый компьютер свежую Node.js и решил собрать React приложение. У меня ничего не вышло — это было вскоре после появления 17 версии Node.js, и React просто не успел её поддержать. С большой долей вероятности у FullStack-фреймворка время поддержки новых версий будет больше. Если же вы попадете на проект, где используется старый фреймворк, то он может блокировать обновление версий. С Meteor, например, не получится использовать Node.js выше 14 версии. Быстрый ввод новичков в команду: плюс Нового человека в команде, не знакомого с фреймворком, можно посадить на денек-другой смотреть YouTube и читать StackOverflow. Материалов много, они поданы и рассказаны интересно, технология популярна. После пары (ну, может, тройки) дней человек уже будет знать основные принципы и сможет приступить к выполнению тасок из Jira. На примере Meteor.js: легко найти видео «создай свой чат за 20 минут», «напиши блог за 30 минут» и тому подобное. Быстрый ввод новичков в команду: минус Быстрая адаптация новых сотрудников оборачивается тем, что когда фреймворк теряет популярность, люди не хотят с ним разбираться. Технология, утратившая актуальность, кажется не слишком перспективной для развития карьеры, и тратить на неё свое время не хочется. А тем более копаться в большом количестве легаси на «пещерных» технологиях. Локальный запуск проекта: плюс Часто бывает, что разработчик хочет запустить проект локально и поотлаживать то, что он написал. В «традиционном» подходе сначала запускается база данных, потом разворачивается backend с livereload, потом frontend (тоже с автоматическим обновлением). Наверняка не обходится без пары компонентов в докере. Для FullStack, как правило, есть одна команда, которая запускает весь проект с автоматическим обновлением при изменениях. И это правда довольно удобно. Локальный запуск проекта: минус Это случится постепенно. Время сборки проекта будет расти. Время пересборки при обновлении одной строчки будет расти. Время установки зависимостей будет расти. Время CI будет расти. Чужие изменения будут ломать код, который, казалось бы, никак к ним не относится. И вам захочется собирать всё раздельно. И это станет проблемой, так как код уже настолько переплелся, что найти, где заканчивается одно и начинается другое, будет почти невозможно. Решения FullStack-задач из коробки: плюс Некоторые задачи требуют тесного взаимодействия frontend и backend. В качестве примера можно привести переводы (i18n) и различные способы авторизации (кто пытался реализовать поддержку OAuth самостоятельно, меня поймет). Для FullStack-фреймворков найти готовое решение таких задач обычно довольно легко. В случае же «традиционного» подхода придется разбираться. В Meteor это реализовано его собственной системой пакетов. Решения FullStack-задач из коробки: минус Вскоре после того, как вы наткнетесь на сложность, обозначенную пунктом выше, вам захочется разделить код. И те самые готовые решения станут огромной частью проблемы. Как правило, их придется переписывать, но сохраняя интерфейсы. Это будет совсем не легко, и придется очень глубоко вникнуть в то, как это работает изнутри и почему именно так. Киллер-фича: плюс Это собирательное название для того, чем «продают» фреймворк. В Meteor стали использовать довольно остроумное решение для избежания CallbackHell (напоминаю, 2012 год) SvelteKit и многие другие говорят о легкости реализации SSR Remix говорит, что его код можно исполнять на нодах Cloudflare Киллер-фича: минус Любая киллер-фича — это решение какой-либо проблемы. И у любой проблемы есть более одного решения. В Node.js был выбран другой подход к решению CallbackHell, нежели тот, что предлагал Meteor. Теперь код на нем кажется странным. Я верю в то, что рано или поздно можно будет отказаться от SSR ради SEO-оптимизаций и решить проблему скорости загрузки страницы за счет более быстрого интернета и использования ServiceWorker. Про распределение вычислительных ресурсов географически за небольшие деньги и у меня пока нет идей, но это не значит, что они не появятся в будущем. Подводя итог Имеет ли смысл использовать FullStack-фреймворки? Безусловно, да. Быстрая разработка и заинтересованные специалисты — это просто подарок для любого проекта. Можно ли как-то спасти себя от минусов FullStack-фреймворков? Конечно. На том этапе, когда проект перестанет быть MVP, и вы займетесь разбором техдолга, постарайтесь уделить больше внимания разделению проекта на независимые составляющие, выделите интерфейсы и проведите архитектурные границы. Разберитесь с «магией», которую делают сторонние компоненты — скорее всего, для вашего случая её можно реализовать более эффективно. Ну и, конечно, как только появятся стандартизованные средства, для решения проблем постарайтесь использовать их.\n","metric":0.7099609375,"metric_input":"Текст статьи:\nРассказываем о пользе и вреде FullStack-фреймворков на примере Meteor.js / Habr            \n\n28-09-2022\nВ последнее время появляется довольно много Fullstack-фреймворков, они становятся популярны, их обсуждают. Если посмотреть на результаты The State of JS 2021 в разделе «Библиотеки — Бэкенд-фреймворки», то минимум 5 из них (возможно, больше) будут как раз FullStack. Отсортировав бэкенд-фреймворки по заинтересованности, в самом верху списка мы снова увидим именно FullStack. Это понятно — они востребованы и лежат в основе разных проектов. Однако на наш взгляд, область их применимости несколько ограничена. Почему — объясняем под катом. Привет, Хабр! Меня зовут Александр Захаров, я backend-разработчик на Node.js в компании МойОфис. Коротко расскажу о том, как я вообще пришел к Node.js. В начале пути, около 8 лет назад, я писал на C++, Ruby, немного на Python и еще нескольких языках. Конечно же, был в моей жизни и frontend. В JavaScript я заметил интересную особенность, которую до этого видел только у Qt — можно не опрашивать что-либо в цикле и не ждать выполнения системного вызова, а подписаться на событие и, когда оно произойдет, выполнить некоторые действия. Чуть позже я услышу термин «реактивное программирование» и, спустя еще некоторое время, свяжу это с миром JS — мне покажется, что за этим будущее. Потом я узнаю о Node.js, перестану плеваться от асинхронных операций (в этом изрядно помогут промисы и async/await). И вот я здесь. История повторяется Термин «FullStack-фреймворк» появился довольно давно. Как мне кажется, это прямое следствие идеи «один код для frontend и backend», которая очень часто звучала в первые годы популярности Node.js. Одним из ранних «больших» FullStack-фреймворков был Meteor.js. Он появился в начале 2012 года и довольно быстро стал востребованным. Ниже я выделю аргументы, которые приводились в его пользу, и несколько их обобщу. А параллельно с этим покажу, почему эти «плюсы» не всегда оказываются полезными в реальных проектах, а иногда и мешают им. Плюсы и минусы FullStack-фреймворков Быстрое начало разработки: плюс В FullStack-фреймворке уже приняты основные решения. Особенно решения, касающиеся взаимодействия frontend и backend (иначе это уже не похоже на FullStack). То есть, скорее всего, команде не придется договариваться о том, что она понимает под Rest и какой формат сообщений, посылаемый через WebSockets, правильный. Более того, скорее всего в шаблоне приложения у вас уже будут и линтер, и настроенный TypeScript, и заранее известная структура директорий, и еще какие-нибудь приятные для взаимопонимания вещи. В случае Meteor он еще и выберет за вас БД (это будет mongoDB). Быстрое начало разработки: минус Со временем появятся новые ответы на старые вопросы. Или вы наткнетесь на протекающую абстракцию какого-либо из ответов. Хорошим примером, мне кажется, служит развитие возможностей языка. Однажды я поставил на новый компьютер свежую Node.js и решил собрать React приложение. У меня ничего не вышло — это было вскоре после появления 17 версии Node.js, и React просто не успел её поддержать. С большой долей вероятности у FullStack-фреймворка время поддержки новых версий будет больше. Если же вы попадете на проект, где используется старый фреймворк, то он может блокировать обновление версий. С Meteor, например, не получится использовать Node.js выше 14 версии. Быстрый ввод новичков в команду: плюс Нового человека в команде, не знакомого с фреймворком, можно посадить на денек-другой смотреть YouTube и читать StackOverflow. Материалов много, они поданы и рассказаны интересно, технология популярна. После пары (ну, может, тройки) дней человек уже будет знать основные принципы и сможет приступить к выполнению тасок из Jira. На примере Meteor.js: легко найти видео «создай свой чат за 20 минут», «напиши блог за 30 минут» и тому подобное. Быстрый ввод новичков в команду: минус Быстрая адаптация новых сотрудников оборачивается тем, что когда фреймворк теряет популярность, люди не хотят с ним разбираться. Технология, утратившая актуальность, кажется не слишком перспективной для развития карьеры, и тратить на неё свое время не хочется. А тем более копаться в большом количестве легаси на «пещерных» технологиях. Локальный запуск проекта: плюс Часто бывает, что разработчик хочет запустить проект локально и поотлаживать то, что он написал. В «традиционном» подходе сначала запускается база данных, потом разворачивается backend с livereload, потом frontend (тоже с автоматическим обновлением). Наверняка не обходится без пары компонентов в докере. Для FullStack, как правило, есть одна команда, которая запускает весь проект с автоматическим обновлением при изменениях. И это правда довольно удобно. Локальный запуск проекта: минус Это случится постепенно. Время сборки проекта будет расти. Время пересборки при обновлении одной строчки будет расти. Время установки зависимостей будет расти. Время CI будет расти. Чужие изменения будут ломать код, который, казалось бы, никак к ним не относится. И вам захочется собирать всё раздельно. И это станет проблемой, так как код уже настолько переплелся, что найти, где заканчивается одно и начинается другое, будет почти невозможно. Решения FullStack-задач из коробки: плюс Некоторые задачи требуют тесного взаимодействия frontend и backend. В качестве примера можно привести переводы (i18n) и различные способы авторизации (кто пытался реализовать поддержку OAuth самостоятельно, меня поймет). Для FullStack-фреймворков найти готовое решение таких задач обычно довольно легко. В случае же «традиционного» подхода придется разбираться. В Meteor это реализовано его собственной системой пакетов. Решения FullStack-задач из коробки: минус Вскоре после того, как вы наткнетесь на сложность, обозначенную пунктом выше, вам захочется разделить код. И те самые готовые решения станут огромной частью проблемы. Как правило, их придется переписывать, но сохраняя интерфейсы. Это будет совсем не легко, и придется очень глубоко вникнуть в то, как это работает изнутри и почему именно так. Киллер-фича: плюс Это собирательное название для того, чем «продают» фреймворк. В Meteor стали использовать довольно остроумное решение для избежания CallbackHell (напоминаю, 2012 год) SvelteKit и многие другие говорят о легкости реализации SSR Remix говорит, что его код можно исполнять на нодах Cloudflare Киллер-фича: минус Любая киллер-фича — это решение какой-либо проблемы. И у любой проблемы есть более одного решения. В Node.js был выбран другой подход к решению CallbackHell, нежели тот, что предлагал Meteor. Теперь код на нем кажется странным. Я верю в то, что рано или поздно можно будет отказаться от SSR ради SEO-оптимизаций и решить проблему скорости загрузки страницы за счет более быстрого интернета и использования ServiceWorker. Про распределение вычислительных ресурсов географически за небольшие деньги и у меня пока нет идей, но это не значит, что они не появятся в будущем. Подводя итог Имеет ли смысл использовать FullStack-фреймворки? Безусловно, да. Быстрая разработка и заинтересованные специалисты — это просто подарок для любого проекта. Можно ли как-то спасти себя от минусов FullStack-фреймворков? Конечно. На том этапе, когда проект перестанет быть MVP, и вы займетесь разбором техдолга, постарайтесь уделить больше внимания разделению проекта на независимые составляющие, выделите интерфейсы и проведите архитектурные границы. Разберитесь с «магией», которую делают сторонние компоненты — скорее всего, для вашего случая её можно реализовать более эффективно. Ну и, конечно, как только появятся стандартизованные средства, для решения проблем постарайтесь использовать их.\n\n\nКраткое содержание:\nFullStack-фреймворки, такие как Meteor.js, становятся популярными, но их область применимости ограничена. Они имеют свои плюсы, такие как быстрая разработка и заинтересованные специалисты, но также имеют минусы, такие как сложность адаптации новых сотрудников и проблемы при разделении проекта на независимые составляющие. Для спасения от минусов FullStack-фреймворков, на этапе, когда проект перестанет быть MVP, следует уделить больше внимания разделению проекта на независимые составляющие, выделить интерфейсы и провести архитектурные границы. Также стоит использовать стандартизованные средства для решения проблем, когда они появятся.","summary":"FullStack-фреймворки, такие как Meteor.js, становятся популярными, но их область применимости ограничена. Они имеют свои плюсы, такие как быстрая разработка и заинтересованные специалисты, но также имеют минусы, такие как сложность адаптации новых сотрудников и проблемы при разделении проекта на независимые составляющие. Для спасения от минусов FullStack-фреймворков, на этапе, когда проект перестанет быть MVP, следует уделить больше внимания разделению проекта на независимые составляющие, выделить интерфейсы и провести архитектурные границы. Также стоит использовать стандартизованные средства для решения проблем, когда они появятся."}
{"Text":"Календари: эргономика и дизайн / Habr              \n\n09-06-2011\nОт переводчика Продолжаем публикацию статей, посвященных анализу эргономической специфики различных элементов интерфейсов. Сегодня мы предлагаем нашим читателями статью по проблемам эргономики календарей, которые получили широкое распространение на сайтах, осуществляющих продажу различных услуг через Интернет (продажа билетов, бронирование гостиниц и аппартаментов, доставка товаров на дом и т. п.). На многих веб-сайтах приходится иметь дело с процедурой выбора даты. Для облегчения выбора и преодотвращения возможных ошибок сайты снабжаются календарями. Нередко приходится сталкиваться с тем, что календари плохо адаптированы под потребности посетителей сайтов, и поэтому их использование сопряжено с затруднениями. В данной статье мы хотели бы дать несколько практических рекомендаций по упрощению процедур ввода даты и улучшения эргономических характеристик календарей. Упрощение ввода даты Даже в эпоху компьютерных технологий традиционный календарь остается предметом повседневного обихода. Чем больше календарь на сайте напоминает обычный бумажный календарь, тем меньше вопросов будет возникать у пользователей относительно специфики работы с ним. Лучше основываться на распространенных стандартах, чем придумывать новую эстетику, какой бы оригинальной она ни была. Календарь в виде радуги (пример взят с сайта badusability.com): все дни месяца отображены, но на календарь это совершенно не похоже, что только затрудняет выбор даты. Ввод даты является очень трудоемким процессом: нужно ввести цифры и буквы, разделенные косой чертой. Подобные комбинации символов не используются в повседневной коммуникации. Календарь предлагает быстрый и удобный выбор даты одним щелчком мыши. Такой способ ввода, более простой по сравнению с вводом с клавиатуры, позволяет избежать ошибок. На сайте авиакомпании Air France отсутсвует иконка календаря; календарь возникает на экране при переходе в раздел «Регистрация онлайн». Это может вызвать затруднения. Традиционный вид календаря помогает пользователю лучше сориентироваться во времени. По горизонтали располагаются дни недели, а по вертикали — соответствующие им дни месяца. На некоторых сайтах в календаре отображаются только дни текущего месяца; иногда отображаются также дни предыдущего и следующего месяцев. Выбор внешнего вида календаря обусловливается спецификой его использования и соображениями удобства. Отображение дней, не имеющих отношения к текущему месяцу, может помочь пользователю быстрее выстроить планы на будущее, т. е. быстрее выбрать дату в следующем месяце. На сайте SNCF (французская государственная железнодорожная компания), дни предыдущего и следующего месяца отображаются при поиске поезда (слева), но не при поиске гостиниц (справа). При поиске поезда дата предустановлена, тогда как при поиске гостиниц задан лишь формата для ее ввода. Чтобы на экране не возникало некрасивых и бесполезных пустых мест, в календаре предпочтительно отображать дни предыдущего и следующего месяцев. Во избежание ошибок их лучше выделить графически (например, менее ярким шрифтом). Еще один классический элемент дизайна календарей — метки с названиями дней недели. Здесь следует учитывать специфику страны: например, во Франции неделя начинается с понедельника, тогда как в США — с воскресенья. Особые дни (например, праздники) также могут быть каким-то образом выделены. Такое выделение очень важно на сайтах, предоставляющих определенные типы услуг (например, на сайтах, предлагающих билеты на транспорт, иногда возникает необходимость выделение дней открытия и закрытия сообщения). Графическое выделение предпочтительно сопровождать текстовым пояснением (например, в виде подсказки, появляющейся при подведении курсора). Графически выделяются и выходные дни (это особенно важно при заказе билетов на поезд и самолет, турпутевок, гостиничных номеров и т. п.). Определенным образом должны выделяться и недоступные дни (на которые проданы все билеты, заняты все номера в гостинице и т. п.). В верхней части календаря расположены обозначения дней недели. Некоторые туристические фирмы (см. рисунок справа), на своих сайтах вводят систему цветовых обозначений дат при бронировании гостиничного номера. Под календарем находится легенда с разъяснением всех используемых обозначений. Рекомендации На сайте должна присутствовать иконка календаря Календарь должен быть представлен в привычной для пользователя форме, т. е. выглядеть так же, как традиционные бумажные календари Календарь должен быть адаптирован под культурные особенности пользователей сайта Важные дни месяца необходимо выделять графически Упрощение выбора даты Поиск даты может осуществляться двумя способами: ввод даты в текстовом поле или же выбор в разворачивающемся календаре. Чтобы ничем не ограничивать пользователей и учитывать их индивидуальные особенности, лучше предложить им на выбор оба этих варианта. Если календарь вызывается кликом мыши, крайне желательно, чтобы он не закрывал поле для ручного ввода. Он может быть расположен как под полем ввода, так и сбоку от него. На сайте агентства по продаже авиабилетов Opodo при наведении курсора на поле ручного ввода появляется календарь; поле ввода он при этом не заслоняет. При вводе даты вручную она выделяется и в календаре. Дни предыдущего и следующего месяев в календаря не отображаются. При открытии календаря для пользователя должна задаваться точка отчета. Обычно за точку отчета принимается текущий день, и это позволяет пользователю сориентироваться и сделать верный выбор. Дни, доступные для выбора (например, при продаже билетов или бронировании гостиничных номеров), могут быть особым образом выделен. Когда выбор уже сделан (например, когда определены дни отправления и возвращения или дни въезда и выезда из гостиницы), предпочтительно отобразить в календаре не один, а два или три месяца. Это поможет пользователю лучше сориентироваться во времени и при необходимости скорректировать выбор. На сайте Hotels.com при выборе дат въезда и выезда на экране отображаются два месяца, чтобы пользователь мог получить более наглядное представление о периоде своего пребывания в гостинице. В некоторые случаях возникает необходимость отобразить порядковый номер недели. Номера недель обычно располагаются в начале каждой строки. Для быстрого поиска нужной даты используются различные средства графического выделения. Пример обозначения номеров недель в календаре. Рекомендации Пользователю должна быть предоставлена возможность выбора между ручным вводом даты и выбором ее в календаре Поле для ручного ввода даты не должно быть пустым; желательно, чтобы оно не включало форм типа дд/мм/гг Способ отображения календаря должен быть адаптирован под специфику решаемых пользователем задач (например, при выборе авиарейса совершенно бесполезно отображать предыдущий месяц и предыдущие дни текущего месяца) Размер календаря так же должен быть адаптирован под специфику задач (например, при бронировании гостиницы на неделю и более целесообразно отображать на экране не один, а два месяца) Заключение Календари получают все большее распространение и все чаще используются в качестве альтернативы трудоемкому вводы даты с клавиатуры. Процедура ручного ввода может представлять серьезные трудности, в особенности если ввод нужно осуществлять в каком-то определенном формате. Календари и выпадающие списки являются элементами интерфейса, облегчающими ввод и помогающими пользователю быстрее заполнить необходимые поля. Чтобы работа с этими элементами была простой и эффективной, необходимо иметь четкое представление о контексте, в котором она будет осуществляться. Для этого представляется целесообразным создавать прототипы будущих интерфейсов.\n","metric":0.640625,"metric_input":"Текст статьи:\nКалендари: эргономика и дизайн / Habr              \n\n09-06-2011\nОт переводчика Продолжаем публикацию статей, посвященных анализу эргономической специфики различных элементов интерфейсов. Сегодня мы предлагаем нашим читателями статью по проблемам эргономики календарей, которые получили широкое распространение на сайтах, осуществляющих продажу различных услуг через Интернет (продажа билетов, бронирование гостиниц и аппартаментов, доставка товаров на дом и т. п.). На многих веб-сайтах приходится иметь дело с процедурой выбора даты. Для облегчения выбора и преодотвращения возможных ошибок сайты снабжаются календарями. Нередко приходится сталкиваться с тем, что календари плохо адаптированы под потребности посетителей сайтов, и поэтому их использование сопряжено с затруднениями. В данной статье мы хотели бы дать несколько практических рекомендаций по упрощению процедур ввода даты и улучшения эргономических характеристик календарей. Упрощение ввода даты Даже в эпоху компьютерных технологий традиционный календарь остается предметом повседневного обихода. Чем больше календарь на сайте напоминает обычный бумажный календарь, тем меньше вопросов будет возникать у пользователей относительно специфики работы с ним. Лучше основываться на распространенных стандартах, чем придумывать новую эстетику, какой бы оригинальной она ни была. Календарь в виде радуги (пример взят с сайта badusability.com): все дни месяца отображены, но на календарь это совершенно не похоже, что только затрудняет выбор даты. Ввод даты является очень трудоемким процессом: нужно ввести цифры и буквы, разделенные косой чертой. Подобные комбинации символов не используются в повседневной коммуникации. Календарь предлагает быстрый и удобный выбор даты одним щелчком мыши. Такой способ ввода, более простой по сравнению с вводом с клавиатуры, позволяет избежать ошибок. На сайте авиакомпании Air France отсутсвует иконка календаря; календарь возникает на экране при переходе в раздел «Регистрация онлайн». Это может вызвать затруднения. Традиционный вид календаря помогает пользователю лучше сориентироваться во времени. По горизонтали располагаются дни недели, а по вертикали — соответствующие им дни месяца. На некоторых сайтах в календаре отображаются только дни текущего месяца; иногда отображаются также дни предыдущего и следующего месяцев. Выбор внешнего вида календаря обусловливается спецификой его использования и соображениями удобства. Отображение дней, не имеющих отношения к текущему месяцу, может помочь пользователю быстрее выстроить планы на будущее, т. е. быстрее выбрать дату в следующем месяце. На сайте SNCF (французская государственная железнодорожная компания), дни предыдущего и следующего месяца отображаются при поиске поезда (слева), но не при поиске гостиниц (справа). При поиске поезда дата предустановлена, тогда как при поиске гостиниц задан лишь формата для ее ввода. Чтобы на экране не возникало некрасивых и бесполезных пустых мест, в календаре предпочтительно отображать дни предыдущего и следующего месяцев. Во избежание ошибок их лучше выделить графически (например, менее ярким шрифтом). Еще один классический элемент дизайна календарей — метки с названиями дней недели. Здесь следует учитывать специфику страны: например, во Франции неделя начинается с понедельника, тогда как в США — с воскресенья. Особые дни (например, праздники) также могут быть каким-то образом выделены. Такое выделение очень важно на сайтах, предоставляющих определенные типы услуг (например, на сайтах, предлагающих билеты на транспорт, иногда возникает необходимость выделение дней открытия и закрытия сообщения). Графическое выделение предпочтительно сопровождать текстовым пояснением (например, в виде подсказки, появляющейся при подведении курсора). Графически выделяются и выходные дни (это особенно важно при заказе билетов на поезд и самолет, турпутевок, гостиничных номеров и т. п.). Определенным образом должны выделяться и недоступные дни (на которые проданы все билеты, заняты все номера в гостинице и т. п.). В верхней части календаря расположены обозначения дней недели. Некоторые туристические фирмы (см. рисунок справа), на своих сайтах вводят систему цветовых обозначений дат при бронировании гостиничного номера. Под календарем находится легенда с разъяснением всех используемых обозначений. Рекомендации На сайте должна присутствовать иконка календаря Календарь должен быть представлен в привычной для пользователя форме, т. е. выглядеть так же, как традиционные бумажные календари Календарь должен быть адаптирован под культурные особенности пользователей сайта Важные дни месяца необходимо выделять графически Упрощение выбора даты Поиск даты может осуществляться двумя способами: ввод даты в текстовом поле или же выбор в разворачивающемся календаре. Чтобы ничем не ограничивать пользователей и учитывать их индивидуальные особенности, лучше предложить им на выбор оба этих варианта. Если календарь вызывается кликом мыши, крайне желательно, чтобы он не закрывал поле для ручного ввода. Он может быть расположен как под полем ввода, так и сбоку от него. На сайте агентства по продаже авиабилетов Opodo при наведении курсора на поле ручного ввода появляется календарь; поле ввода он при этом не заслоняет. При вводе даты вручную она выделяется и в календаре. Дни предыдущего и следующего месяев в календаря не отображаются. При открытии календаря для пользователя должна задаваться точка отчета. Обычно за точку отчета принимается текущий день, и это позволяет пользователю сориентироваться и сделать верный выбор. Дни, доступные для выбора (например, при продаже билетов или бронировании гостиничных номеров), могут быть особым образом выделен. Когда выбор уже сделан (например, когда определены дни отправления и возвращения или дни въезда и выезда из гостиницы), предпочтительно отобразить в календаре не один, а два или три месяца. Это поможет пользователю лучше сориентироваться во времени и при необходимости скорректировать выбор. На сайте Hotels.com при выборе дат въезда и выезда на экране отображаются два месяца, чтобы пользователь мог получить более наглядное представление о периоде своего пребывания в гостинице. В некоторые случаях возникает необходимость отобразить порядковый номер недели. Номера недель обычно располагаются в начале каждой строки. Для быстрого поиска нужной даты используются различные средства графического выделения. Пример обозначения номеров недель в календаре. Рекомендации Пользователю должна быть предоставлена возможность выбора между ручным вводом даты и выбором ее в календаре Поле для ручного ввода даты не должно быть пустым; желательно, чтобы оно не включало форм типа дд/мм/гг Способ отображения календаря должен быть адаптирован под специфику решаемых пользователем задач (например, при выборе авиарейса совершенно бесполезно отображать предыдущий месяц и предыдущие дни текущего месяца) Размер календаря так же должен быть адаптирован под специфику задач (например, при бронировании гостиницы на неделю и более целесообразно отображать на экране не один, а два месяца) Заключение Календари получают все большее распространение и все чаще используются в качестве альтернативы трудоемкому вводы даты с клавиатуры. Процедура ручного ввода может представлять серьезные трудности, в особенности если ввод нужно осуществлять в каком-то определенном формате. Календари и выпадающие списки являются элементами интерфейса, облегчающими ввод и помогающими пользователю быстрее заполнить необходимые поля. Чтобы работа с этими элементами была простой и эффективной, необходимо иметь четкое представление о контексте, в котором она будет осуществляться. Для этого представляется целесообразным создавать прототипы будущих интерфейсов.\n\n\nКраткое содержание:\nСтатья посвящена проблемам эргономики календарей, которые получили широкое распространение на сайтах, осуществляющих продажу различных услуг через Интернет. Авторы предлагают несколько практических рекомендаций по упрощению процедур ввода даты и улучшения эргономических характеристик календарей.\n\nОсновные рекомендации:\n\n- Упрощение ввода даты. Лучше основываться на распространенных стандартах, чем придумывать новую эстетику. Календарь должен быть адаптирован под культурные особенности пользователей сайта.\n\n- Выделение важных дней месяца графически.\n\n- Предоставление выбора между ручным вводом даты и выбором ее в календаре.\n\n- Отображение двух или трех месяцев при выборе даты, если выбор уже сделан.\n\n- Графическое выделение порядкового номера недели.\n\n- Использование различных средств графического выделения для быстрого поиска нужной даты.\n\nВ целом, календари получают все большее распространение и все чаще используются в качестве альтернативы трудоемкому вводы даты с клавиатуры. Процедура ручного ввода может представлять серьезные трудности, в особенности если ввод нужно осуществлять в каком-то определенном формате. Календари и выпадающие списки являются элементами интерфейса, облегчающими ввод и помогающими пользователю быстрее заполнить необходимые поля. Чтобы работа с этими элементами была простой и эффективной, необходимо иметь четкое представление о контексте, в котором она будет осуществляться. Для этого представляется целесообразным создавать прототипы будущих интерфейсов.","summary":"Статья посвящена проблемам эргономики календарей, которые получили широкое распространение на сайтах, осуществляющих продажу различных услуг через Интернет. Авторы предлагают несколько практических рекомендаций по упрощению процедур ввода даты и улучшения эргономических характеристик календарей.\n\nОсновные рекомендации:\n\n- Упрощение ввода даты. Лучше основываться на распространенных стандартах, чем придумывать новую эстетику. Календарь должен быть адаптирован под культурные особенности пользователей сайта.\n\n- Выделение важных дней месяца графически.\n\n- Предоставление выбора между ручным вводом даты и выбором ее в календаре.\n\n- Отображение двух или трех месяцев при выборе даты, если выбор уже сделан.\n\n- Графическое выделение порядкового номера недели.\n\n- Использование различных средств графического выделения для быстрого поиска нужной даты.\n\nВ целом, календари получают все большее распространение и все чаще используются в качестве альтернативы трудоемкому вводы даты с клавиатуры. Процедура ручного ввода может представлять серьезные трудности, в особенности если ввод нужно осуществлять в каком-то определенном формате. Календари и выпадающие списки являются элементами интерфейса, облегчающими ввод и помогающими пользователю быстрее заполнить необходимые поля. Чтобы работа с этими элементами была простой и эффективной, необходимо иметь четкое представление о контексте, в котором она будет осуществляться. Для этого представляется целесообразным создавать прототипы будущих интерфейсов."}
{"Text":"Испанская инквизиция и робот для унижения: на что готовы «хищнические» конференции ради денег / Habr           \n\n24-12-2019\nПредисловие История, которую я вам хочу рассказать, началась с простой шутки. Некоторое время назад моя сестра рассказала мне про Джеймса Вейтча, который показал миру, как можно весело троллить надоедливых спамеров и заставлять их спасаться бегством от чрезмерно настырного адресата своих же рассылок. Примерно в то же время я начал понемногу публиковать свои скромные труды по нейроинтерфейсам в небольших журналах и отмечаться докладами в сборниках конференций. Разумеется, с первыми же публикациями на мою рабочую почту начали приходить рассылки от различных журналов, издательств и прочих научных и околонаучных источников. Из всех них по драматической силе особенно выделялись такие. Вкратце, некоему маститому доктору наук, основателю мощного центра по ИИ, председателю крупной международной конференции, безумно понравились мои последние исследования. Он рассыпается в комплиментах, приглашает меня на конференцию почетным гостем, где вся элита отрасли соберётся, чтобы послушать моё выступление. А чего добился ты, читатель? Честно говоря, я думал, что подвох тут достаточно очевиден, но полистав просторы интернета, обнаружил множество людей, пострадавших от деятельности подобных докторов наук, их конференций и собственного слегка завышенного самолюбия. Моя статья будет о том, как я попробовал пообщаться с организаторами подобных мероприятий в духе Вейтча, и что из этого внезапно получилось. Но сначала – немного теории для тех, кто пока не вполне понимает, о чём речь. Медные трубы для молодого учёного Всякий исследователь (студент/аспирант/учёный/разработчик/нужное подчеркнуть) обычно желает (или не желает, но обязан) опубликовать свои труды в солидном журнале и выступить с ними перед солидной аудиторией. Однако критерии этой «солидности» каждый понимает по-своему. В принципе, человек, минимально разбирающийся в вопросе, без труда отличит нормальный рейтинговый журнал от принимающей всё подряд «помойки», а полезную конференцию – от собрания любителей потрепать языком. Но люди понаивнее рискуют нарваться на такое или такое. В двух словах, хищнические конференции (остановимся сегодня именно на них как на менее очевидном для многих феномене) – это мероприятия, которые мимикрируют под крупные международные научные события, по факту обладающие нулевой или околонулевой научной ценностью. Цель организаторов таких конференций – привлечь на них как можно больше участников, создав видимость масштабности события, авторитета его организаторов, ценности лично вашей работы и т.д. Зачем? Разумеется, чтобы получить как можно больше оргвзносов (весьма немаленьких,) от участников. Исходно жертва, которую в письме обязательно назовут «invited speaker-ом», будет рассчитывать, как минимум, на бесплатный перелет и скидку, но ничего такого, скорее всего, не произойдёт. Распознать хищническую конференцию, в отличие от журнала, не всегда можно с первого взгляда, поскольку совокупный научный вес и титулы заявленных там хэдлайнеров (т.е. программного комитета или основных спикеров) может действительно быть весьма внушительным. Основная проблема таких мероприятий кроется в том, что экспертиза поданных туда материалов отсутствует как таковая, и по факту получить добро на свой доклад там может абсолютно любой персонаж. Ну, например, магистр ордена испанской инквизиции с работой, посвящённой истязанию неверных. Не верите? Давайте проверим. «Боль – 2020» и Тёмный Рыцарь De La Isum Вот моё приглашение на конференцию Pain-2020. Посмотрим на структуру письма. Во-первых, это якобы повторное письмо (не исключено, что так оно и есть, поскольку первое «приглашение» я мог просто рефлекторно сунуть в корзину). То есть меня и мою «enlightening story of research» прямо-таки настойчиво хочет увидеть почтенная публика. Таким же приёмом часто пользуются в массовой рассылке рекрутеры. Отметим и размах конференции: не буду приводить здесь все скрины, но поверьте, что отдельных «треков» там штук 80. На сайте всё тоже прекрасно: внушительный список почётных профессоров-спикеров, программный комитет из полусотни докторов наук, журналы-партнёры, медиа-партнёры, проживание в отличной гостинице. Да, удовольствие недешёвое, но может, оно того стоит? Нас позвали на конференцию про боль, а значит, создадим образ такого персонажа, который с удовольствием рассказал бы доклад на эту тему. Например, такого. Cкажете, что с ходу представляться тёмным рыцарем и признаваться в любви к пыточным орудиям – это перебор? Я тоже так думал. Честно говоря, когда я писал это, то думал, что рассылка просто прекратится. Однако всё закрутилось гораздо интереснее. Мне выделили подходящий трек и предложили выслать название статьи. Думаю, вы догадались, что большое количество треков – это просто длинный набор ключевых слов в области искусственного интеллекта, созданный для того, чтобы привлечь максимально широкий круг потенциальных спикеров. Интересно, что в данном случае трек для настоящего меня, Анатолия Бобе, выбран вполне корректно. А вот для созданного мной персонажа – не совсем… Впрочем, персонаж не сильно требовательный, поэтому стерпит. Разве что потребует денег для своего Ордена и попросит величать его согласно прилагаемым званиям. «Новейшие инструментальные методы быстрого извлечения признания от еретиков, ведьм и язычников»? Да, всё отлично, только трек, наверное, стоит поменять. Ну и аннотация понадобится, должен же программный комитет хоть немного оценить качество работы. Наш инквизитор, будучи человеком образованным, аннотацию, разумеется, выслал. Отметим, впрочем, что остальные вопросы были просто проигнорированы. Анна по-прежнему называет меня моим настоящим именем, хотя я каждый раз подписываюсь по-другому. Анне вообще безразличны мои причуды, она целенаправленно толкает меня к регистрации. И – о чудо – экспертная комиссия, состоящая из, несомненно, виднейших специалистов в области «мотивации, эмоций и когнитивных наук», одобрила нашу заявку! На самом деле, во всех предыдущих действиях не было ни малейшего смысла. Безо всякого предварительного «одобрения» можно было оставить свою тему и аннотацию на сайте с тем же самым результатом. Плюс отсутствие реакции на мои специфические запросы наталкивает на мысль, что Анна – всего лишь робот. Поскольку я не собирался нигде регистрироваться, ситуация начала заходить в тупик, и тут я решил прибегнуть к радикальным методам воздействия. А именно — написал уважаемой Анне, что требую официального подтверждения принятия моей работы на конференцию, чтобы мой Орден смог на основе этого принять решение о выделении финансирования. Я предполагал, что перевод разговора на деньги вызовет какое-то оживление, но, честно говоря, не ожидал, что они таки сделают вот это! Теперь этот потрясающий документ висит у меня над рабочим столом, отпугивая коллег. Разумеется, это просто криво сваянная в фотошопе бумажка с отсканированной с сайта подписью. Но, как минимум, место моей работы уже указано «правильно», да и запрос не совсем тривиальный для робота, а значит, кто-то действительно сел и составил этот абсурдный документ на полном серьёзе. К сожалению, дальнейшего содержательного разговора рыцаря испанской инквизиции с международным конгрессом по нейронаукам не получилось: на мои дальнейшие письма они уже не отвечали. Седина в бороду, мармайт в лицо Думаете, это отдельный курьёзный случай? Вовсе нет. Вернёмся к письму от GSERITA-2020, которое было в самом начале поста. Эта конференция не принадлежит к уже печально известной структуре BIT, и простым гуглом вы не найдёте никаких негативных отзывов о её качестве. И взнос там менее дикий, всего $700. Да и сайт вылядит куда менее колхозно. Моя «статья» и аннотация для них приведены ниже. Для тех, кто не знает, мармайт – это довольно специфический на вкус пищевой спред, ставший в Великобритании мемом благодаря вирусной рекламе. А для тех, кто не силён в английском, здесь предлагается робот, который будет подносить банку с этим самым мармайтом к человеку с предложением покушать, а в случае отказа выплескивать ему эту банку в лицо. Система призвана развивать здоровый образ жизни, а также покорность судьбе. Кстати, в этом случае организаторы были немного хитрее насчет официальной бумаги и потребовали предоплату в 100-200 долларов. Я отболтался тем, что мы живём в СССР и безосновательные валютные переводы от частных лиц за границу недопустимы. Мораль сей истории проста: внимательно проверяйте репутацию тех мероприятий, где собираетесь представлять свои достижения. В противном случае есть риск потратить кучу денег и пролететь полмира лишь для того, чтобы в итоге оказаться на одной площадке с тёмным рыцарем, безумным любителем мармайта и аналогичными по научной значимости персонажами. Вообще, если бы у меня были лишняя штука баксов и несколько дней свободного времени, я бы подумал над тем, чтобы довести дело до реального доклада, с видеосъёмкой и последующей публикацией :)\n","metric":0.490234375,"metric_input":"Текст статьи:\nИспанская инквизиция и робот для унижения: на что готовы «хищнические» конференции ради денег / Habr           \n\n24-12-2019\nПредисловие История, которую я вам хочу рассказать, началась с простой шутки. Некоторое время назад моя сестра рассказала мне про Джеймса Вейтча, который показал миру, как можно весело троллить надоедливых спамеров и заставлять их спасаться бегством от чрезмерно настырного адресата своих же рассылок. Примерно в то же время я начал понемногу публиковать свои скромные труды по нейроинтерфейсам в небольших журналах и отмечаться докладами в сборниках конференций. Разумеется, с первыми же публикациями на мою рабочую почту начали приходить рассылки от различных журналов, издательств и прочих научных и околонаучных источников. Из всех них по драматической силе особенно выделялись такие. Вкратце, некоему маститому доктору наук, основателю мощного центра по ИИ, председателю крупной международной конференции, безумно понравились мои последние исследования. Он рассыпается в комплиментах, приглашает меня на конференцию почетным гостем, где вся элита отрасли соберётся, чтобы послушать моё выступление. А чего добился ты, читатель? Честно говоря, я думал, что подвох тут достаточно очевиден, но полистав просторы интернета, обнаружил множество людей, пострадавших от деятельности подобных докторов наук, их конференций и собственного слегка завышенного самолюбия. Моя статья будет о том, как я попробовал пообщаться с организаторами подобных мероприятий в духе Вейтча, и что из этого внезапно получилось. Но сначала – немного теории для тех, кто пока не вполне понимает, о чём речь. Медные трубы для молодого учёного Всякий исследователь (студент/аспирант/учёный/разработчик/нужное подчеркнуть) обычно желает (или не желает, но обязан) опубликовать свои труды в солидном журнале и выступить с ними перед солидной аудиторией. Однако критерии этой «солидности» каждый понимает по-своему. В принципе, человек, минимально разбирающийся в вопросе, без труда отличит нормальный рейтинговый журнал от принимающей всё подряд «помойки», а полезную конференцию – от собрания любителей потрепать языком. Но люди понаивнее рискуют нарваться на такое или такое. В двух словах, хищнические конференции (остановимся сегодня именно на них как на менее очевидном для многих феномене) – это мероприятия, которые мимикрируют под крупные международные научные события, по факту обладающие нулевой или околонулевой научной ценностью. Цель организаторов таких конференций – привлечь на них как можно больше участников, создав видимость масштабности события, авторитета его организаторов, ценности лично вашей работы и т.д. Зачем? Разумеется, чтобы получить как можно больше оргвзносов (весьма немаленьких,) от участников. Исходно жертва, которую в письме обязательно назовут «invited speaker-ом», будет рассчитывать, как минимум, на бесплатный перелет и скидку, но ничего такого, скорее всего, не произойдёт. Распознать хищническую конференцию, в отличие от журнала, не всегда можно с первого взгляда, поскольку совокупный научный вес и титулы заявленных там хэдлайнеров (т.е. программного комитета или основных спикеров) может действительно быть весьма внушительным. Основная проблема таких мероприятий кроется в том, что экспертиза поданных туда материалов отсутствует как таковая, и по факту получить добро на свой доклад там может абсолютно любой персонаж. Ну, например, магистр ордена испанской инквизиции с работой, посвящённой истязанию неверных. Не верите? Давайте проверим. «Боль – 2020» и Тёмный Рыцарь De La Isum Вот моё приглашение на конференцию Pain-2020. Посмотрим на структуру письма. Во-первых, это якобы повторное письмо (не исключено, что так оно и есть, поскольку первое «приглашение» я мог просто рефлекторно сунуть в корзину). То есть меня и мою «enlightening story of research» прямо-таки настойчиво хочет увидеть почтенная публика. Таким же приёмом часто пользуются в массовой рассылке рекрутеры. Отметим и размах конференции: не буду приводить здесь все скрины, но поверьте, что отдельных «треков» там штук 80. На сайте всё тоже прекрасно: внушительный список почётных профессоров-спикеров, программный комитет из полусотни докторов наук, журналы-партнёры, медиа-партнёры, проживание в отличной гостинице. Да, удовольствие недешёвое, но может, оно того стоит? Нас позвали на конференцию про боль, а значит, создадим образ такого персонажа, который с удовольствием рассказал бы доклад на эту тему. Например, такого. Cкажете, что с ходу представляться тёмным рыцарем и признаваться в любви к пыточным орудиям – это перебор? Я тоже так думал. Честно говоря, когда я писал это, то думал, что рассылка просто прекратится. Однако всё закрутилось гораздо интереснее. Мне выделили подходящий трек и предложили выслать название статьи. Думаю, вы догадались, что большое количество треков – это просто длинный набор ключевых слов в области искусственного интеллекта, созданный для того, чтобы привлечь максимально широкий круг потенциальных спикеров. Интересно, что в данном случае трек для настоящего меня, Анатолия Бобе, выбран вполне корректно. А вот для созданного мной персонажа – не совсем… Впрочем, персонаж не сильно требовательный, поэтому стерпит. Разве что потребует денег для своего Ордена и попросит величать его согласно прилагаемым званиям. «Новейшие инструментальные методы быстрого извлечения признания от еретиков, ведьм и язычников»? Да, всё отлично, только трек, наверное, стоит поменять. Ну и аннотация понадобится, должен же программный комитет хоть немного оценить качество работы. Наш инквизитор, будучи человеком образованным, аннотацию, разумеется, выслал. Отметим, впрочем, что остальные вопросы были просто проигнорированы. Анна по-прежнему называет меня моим настоящим именем, хотя я каждый раз подписываюсь по-другому. Анне вообще безразличны мои причуды, она целенаправленно толкает меня к регистрации. И – о чудо – экспертная комиссия, состоящая из, несомненно, виднейших специалистов в области «мотивации, эмоций и когнитивных наук», одобрила нашу заявку! На самом деле, во всех предыдущих действиях не было ни малейшего смысла. Безо всякого предварительного «одобрения» можно было оставить свою тему и аннотацию на сайте с тем же самым результатом. Плюс отсутствие реакции на мои специфические запросы наталкивает на мысль, что Анна – всего лишь робот. Поскольку я не собирался нигде регистрироваться, ситуация начала заходить в тупик, и тут я решил прибегнуть к радикальным методам воздействия. А именно — написал уважаемой Анне, что требую официального подтверждения принятия моей работы на конференцию, чтобы мой Орден смог на основе этого принять решение о выделении финансирования. Я предполагал, что перевод разговора на деньги вызовет какое-то оживление, но, честно говоря, не ожидал, что они таки сделают вот это! Теперь этот потрясающий документ висит у меня над рабочим столом, отпугивая коллег. Разумеется, это просто криво сваянная в фотошопе бумажка с отсканированной с сайта подписью. Но, как минимум, место моей работы уже указано «правильно», да и запрос не совсем тривиальный для робота, а значит, кто-то действительно сел и составил этот абсурдный документ на полном серьёзе. К сожалению, дальнейшего содержательного разговора рыцаря испанской инквизиции с международным конгрессом по нейронаукам не получилось: на мои дальнейшие письма они уже не отвечали. Седина в бороду, мармайт в лицо Думаете, это отдельный курьёзный случай? Вовсе нет. Вернёмся к письму от GSERITA-2020, которое было в самом начале поста. Эта конференция не принадлежит к уже печально известной структуре BIT, и простым гуглом вы не найдёте никаких негативных отзывов о её качестве. И взнос там менее дикий, всего $700. Да и сайт вылядит куда менее колхозно. Моя «статья» и аннотация для них приведены ниже. Для тех, кто не знает, мармайт – это довольно специфический на вкус пищевой спред, ставший в Великобритании мемом благодаря вирусной рекламе. А для тех, кто не силён в английском, здесь предлагается робот, который будет подносить банку с этим самым мармайтом к человеку с предложением покушать, а в случае отказа выплескивать ему эту банку в лицо. Система призвана развивать здоровый образ жизни, а также покорность судьбе. Кстати, в этом случае организаторы были немного хитрее насчет официальной бумаги и потребовали предоплату в 100-200 долларов. Я отболтался тем, что мы живём в СССР и безосновательные валютные переводы от частных лиц за границу недопустимы. Мораль сей истории проста: внимательно проверяйте репутацию тех мероприятий, где собираетесь представлять свои достижения. В противном случае есть риск потратить кучу денег и пролететь полмира лишь для того, чтобы в итоге оказаться на одной площадке с тёмным рыцарем, безумным любителем мармайта и аналогичными по научной значимости персонажами. Вообще, если бы у меня были лишняя штука баксов и несколько дней свободного времени, я бы подумал над тем, чтобы довести дело до реального доклада, с видеосъёмкой и последующей публикацией :)\n\n\nКраткое содержание:\nСтатья рассказывает о том, как автор попытался разыграть организаторов конференций, которые выдают себя за крупные научные мероприятия, но на самом деле не имеют научной ценности. Автор отправил им письмо от имени персонажа, который представился тёмным рыцарем, любящим пытки. Организаторы приняли это за чистую монету и даже потребовали предоплату.","summary":"Статья рассказывает о том, как автор попытался разыграть организаторов конференций, которые выдают себя за крупные научные мероприятия, но на самом деле не имеют научной ценности. Автор отправил им письмо от имени персонажа, который представился тёмным рыцарем, любящим пытки. Организаторы приняли это за чистую монету и даже потребовали предоплату."}
{"Text":"Ноутбучный процессор i9-10980HK может потреблять до 135 Вт / Habr             \n\nНа прошлой неделе появились спецификации для процессоров Intel на базе Comet Lake. Согласно документам, корпорация установила предел энергопотребления 135 Вт для процессора Core i9-10980HK. Этот показатель на 10 ватт больше, чем у 8-ядерного i9-9980HK\n\nДля ноутбучного процессора i9-10980HK заявлен теплопакет 45 ватт. Согласно спецификациям i9-10980HK, в режиме PL2 процессор может находиться не более 56 секунд. Конечное решение, допускать такой режим или нет, стоит за интеграторами. Но кратковременная нагрузка переводит процессор на несколько десятков секунд в режим энергопотребления PL2 с мощностью 107 Вт. Вероятно, в паре с процессором будет находиться мощный видеоускоритель. В итоге с мобильным вариантом RTX 2080 выходит около 200–250 Вт.\n\n_Фото: www.guru3d.com_\n\nЗаявленный Thermal Velicity Boost\nповышает\nчастоту тактового усиления в одном потоке на 100 МГц, если температура процессора составляет от 65°С до 85°С, однако, если температура ниже 65°С, это значение равно 200 МГц. То есть рекламируемое значение 5,3 ГГц можно достичь только на одном ядре и только тогда, когда процессор не нагрелся выше 65 °C, что для ноутбука будет редкостью.\n\nIntel продолжает изготавливать процессоры на техпроцессе 14 нм. Впервые компания начала поставлять чипы на его основе в конце 2014 года, с тех пор было заявлено несколько улучшений и оптимизаций.\n\nПроизводители игровых ноутбуков\nначали продумывать\nновые решения по охлаждению. Так, в новых игровых ноутбуках ROG с процессорами Intel Core 10-го поколения от Asus вместо термопасты намерены использовать жидкометаллическое соединение. Дополнительная сложность состоит в том, что обычно жидкий металл наносят вручную, что невозможно в условиях массового производства. Как отметили в Asus, компания уже более года разрабатывает запатентованный процесс и устройство, способное выполнить эту работу.\n\nAMD анонсировала свои мобильные процессоры Ryzen серии 4000 на базе Zen 2 на CES 2020. В марте компания\nвыпустила спецификации\n, а первые устройства с ними уже скоро поступят в продажу. В мобильных процессорах новой серии планируется увеличить обмен данными между потоками одного или разных процессов (IPC) примерно на 25% по сравнению с мобильными компонентами на основе Zen 1.\n\nНоутбучные Ryzen 4000 — это мобильные процессоры x86. Как и десктопные процессоры Zen 2, их изготавливают на основе техпроцесса 7 нм фабрик TSMC. В линейке серии Ryzen 4000 компания AMD будет поставлять процессоры с 8 ядрами и встроенным видеоускорителем. Как и у i9-10980HK, теплопакет таких процессоров составляет 45 ватт.\n\nAMD заявляет о снижении энергопотребления на 20%, удвоении производительности на ватт, пятикратном ускорении переключения состояний и улучшении примерно в 3,4 раза относительной энергоэффективности по сравнению со своей мобильной платформой от 2015 года. Также AMD внесла ряд изменений в серию Ryzen 4000 для дальнейшего повышения ее эффективности. Infinity Fabric, которая связывает все IP и функциональные блоки в процессорах, получила более детальную динамическую оптимизацию мощности. AMD говорит о повышении эффективности энергопотребления на уровне до 75% благодаря этой оптимизации энергопотребления и настроенной пропускной способности. Компания даже заявила о превосходстве 45-ваттного мобильного процессора над десктопным i7-9700K с разблокированным множителем в некоторых тестах.\n\nMercury Research, ведущая аналитическая компания на рынке процессоров,\nопубликовала показатели\nкомпаний в четвертом квартале 2019 года. Уже девятый квартал подряд AMD наращивает долю в сегментах настольных компьютеров, мобильных устройств и центров обработки данных. AMD занимала 18,3% доли рынка настольных процессоров, набрав 0,3 процентных пункта по сравнению с прошлым кварталом. Примечательно, что в четвертом квартале наблюдался самый сильный рост AMD за каждый из предыдущих трех лет. Прирост компании на рынке серверов составил 0,2% за квартал и 1,4% за год.\n\nК первому кварталу этого года AMD\nпланировала контролировать\nпятую часть рынка процессоров для ноутбуков. Этому может способствовать и\nдефицит процессоров Intel\n, который, как сообщается, может сохраняться в течение всего 2020 года.\n\nВ третьем квартале 2019 года доля рынка мобильных процессоров AMD составила 14,7%, что на 3,8% больше по сравнению с аналогичным периодом прошлого года.\n\n> См. также:\n>\n>\n>\n>\n>\n> « Intel не может определиться, кто быстрее: Comet Lake или Ice Lake »\n>\n> « На CES 2020 AMD представила 64-ядерный десктопный Ryzen Threadripper 3990X, видеокарту Radeon RX 5600 и мобильные CPU »\n>\n> « 64-ядерный Ryzen Threadripper 3990X разгромил в тестах все процессоры Intel, включая Xeon стоимостью \\$20 000 »\n>\n> « Рейтинг лучших CPU для игровых ПК в 2019 году »\n>\n> « Intel Comet Lake: немного 14 нм в десятом поколении »\n>\n>","metric":0.43188476559999999793,"metric_input":"Текст статьи:\nНоутбучный процессор i9-10980HK может потреблять до 135 Вт / Habr             \n\nНа прошлой неделе появились спецификации для процессоров Intel на базе Comet Lake. Согласно документам, корпорация установила предел энергопотребления 135 Вт для процессора Core i9-10980HK. Этот показатель на 10 ватт больше, чем у 8-ядерного i9-9980HK\n\nДля ноутбучного процессора i9-10980HK заявлен теплопакет 45 ватт. Согласно спецификациям i9-10980HK, в режиме PL2 процессор может находиться не более 56 секунд. Конечное решение, допускать такой режим или нет, стоит за интеграторами. Но кратковременная нагрузка переводит процессор на несколько десятков секунд в режим энергопотребления PL2 с мощностью 107 Вт. Вероятно, в паре с процессором будет находиться мощный видеоускоритель. В итоге с мобильным вариантом RTX 2080 выходит около 200–250 Вт.\n\n_Фото: www.guru3d.com_\n\nЗаявленный Thermal Velicity Boost\nповышает\nчастоту тактового усиления в одном потоке на 100 МГц, если температура процессора составляет от 65°С до 85°С, однако, если температура ниже 65°С, это значение равно 200 МГц. То есть рекламируемое значение 5,3 ГГц можно достичь только на одном ядре и только тогда, когда процессор не нагрелся выше 65 °C, что для ноутбука будет редкостью.\n\nIntel продолжает изготавливать процессоры на техпроцессе 14 нм. Впервые компания начала поставлять чипы на его основе в конце 2014 года, с тех пор было заявлено несколько улучшений и оптимизаций.\n\nПроизводители игровых ноутбуков\nначали продумывать\nновые решения по охлаждению. Так, в новых игровых ноутбуках ROG с процессорами Intel Core 10-го поколения от Asus вместо термопасты намерены использовать жидкометаллическое соединение. Дополнительная сложность состоит в том, что обычно жидкий металл наносят вручную, что невозможно в условиях массового производства. Как отметили в Asus, компания уже более года разрабатывает запатентованный процесс и устройство, способное выполнить эту работу.\n\nAMD анонсировала свои мобильные процессоры Ryzen серии 4000 на базе Zen 2 на CES 2020. В марте компания\nвыпустила спецификации\n, а первые устройства с ними уже скоро поступят в продажу. В мобильных процессорах новой серии планируется увеличить обмен данными между потоками одного или разных процессов (IPC) примерно на 25% по сравнению с мобильными компонентами на основе Zen 1.\n\nНоутбучные Ryzen 4000 — это мобильные процессоры x86. Как и десктопные процессоры Zen 2, их изготавливают на основе техпроцесса 7 нм фабрик TSMC. В линейке серии Ryzen 4000 компания AMD будет поставлять процессоры с 8 ядрами и встроенным видеоускорителем. Как и у i9-10980HK, теплопакет таких процессоров составляет 45 ватт.\n\nAMD заявляет о снижении энергопотребления на 20%, удвоении производительности на ватт, пятикратном ускорении переключения состояний и улучшении примерно в 3,4 раза относительной энергоэффективности по сравнению со своей мобильной платформой от 2015 года. Также AMD внесла ряд изменений в серию Ryzen 4000 для дальнейшего повышения ее эффективности. Infinity Fabric, которая связывает все IP и функциональные блоки в процессорах, получила более детальную динамическую оптимизацию мощности. AMD говорит о повышении эффективности энергопотребления на уровне до 75% благодаря этой оптимизации энергопотребления и настроенной пропускной способности. Компания даже заявила о превосходстве 45-ваттного мобильного процессора над десктопным i7-9700K с разблокированным множителем в некоторых тестах.\n\nMercury Research, ведущая аналитическая компания на рынке процессоров,\nопубликовала показатели\nкомпаний в четвертом квартале 2019 года. Уже девятый квартал подряд AMD наращивает долю в сегментах настольных компьютеров, мобильных устройств и центров обработки данных. AMD занимала 18,3% доли рынка настольных процессоров, набрав 0,3 процентных пункта по сравнению с прошлым кварталом. Примечательно, что в четвертом квартале наблюдался самый сильный рост AMD за каждый из предыдущих трех лет. Прирост компании на рынке серверов составил 0,2% за квартал и 1,4% за год.\n\nК первому кварталу этого года AMD\nпланировала контролировать\nпятую часть рынка процессоров для ноутбуков. Этому может способствовать и\nдефицит процессоров Intel\n, который, как сообщается, может сохраняться в течение всего 2020 года.\n\nВ третьем квартале 2019 года доля рынка мобильных процессоров AMD составила 14,7%, что на 3,8% больше по сравнению с аналогичным периодом прошлого года.\n\n> См. также:\n>\n>\n>\n>\n>\n> « Intel не может определиться, кто быстрее: Comet Lake или Ice Lake »\n>\n> « На CES 2020 AMD представила 64-ядерный десктопный Ryzen Threadripper 3990X, видеокарту Radeon RX 5600 и мобильные CPU »\n>\n> « 64-ядерный Ryzen Threadripper 3990X разгромил в тестах все процессоры Intel, включая Xeon стоимостью \\$20 000 »\n>\n> « Рейтинг лучших CPU для игровых ПК в 2019 году »\n>\n> « Intel Comet Lake: немного 14 нм в десятом поколении »\n>\n>\n\nКраткое содержание:\nНа прошлой неделе появились спецификации для процессоров Intel на базе Comet Lake. Согласно документам, корпорация установила предел энергопотребления 135 Вт для процессора Core i9-10980HK. Для ноутбучного процессора i9-10980HK заявлен теплопакет 45 ватт.\n\nIntel продолжает изготавливать процессоры на техпроцессе 14 нм. Производители игровых ноутбуков начали продумывать новые решения по охлаждению. AMD анонсировала свои мобильные процессоры Ryzen серии 4000 на базе Zen 2 на CES 2020. Ноутбучные Ryzen 4000 — это мобильные процессоры x86. Теплопакет таких процессоров составляет 45 ватт. AMD заявляет о снижении энергопотребления на 20%, удвоении производительности на ватт, пятикратном ускорении переключения состояний и улучшении примерно в 3,4 раза относительной энергоэффективности по сравнению со своей мобильной платформой от 2015 года.\n\nAMD наращивает долю в сегментах настольных компьютеров, мобильных устройств и центров обработки данных. К первому кварталу этого года AMD планировала контролировать пятую часть рынка процессоров для ноутбуков. Этому может способствовать и дефицит процессоров Intel, который, как сообщается, может сохраняться в течение всего 2020 года.","summary":"На прошлой неделе появились спецификации для процессоров Intel на базе Comet Lake. Согласно документам, корпорация установила предел энергопотребления 135 Вт для процессора Core i9-10980HK. Для ноутбучного процессора i9-10980HK заявлен теплопакет 45 ватт.\n\nIntel продолжает изготавливать процессоры на техпроцессе 14 нм. Производители игровых ноутбуков начали продумывать новые решения по охлаждению. AMD анонсировала свои мобильные процессоры Ryzen серии 4000 на базе Zen 2 на CES 2020. Ноутбучные Ryzen 4000 — это мобильные процессоры x86. Теплопакет таких процессоров составляет 45 ватт. AMD заявляет о снижении энергопотребления на 20%, удвоении производительности на ватт, пятикратном ускорении переключения состояний и улучшении примерно в 3,4 раза относительной энергоэффективности по сравнению со своей мобильной платформой от 2015 года.\n\nAMD наращивает долю в сегментах настольных компьютеров, мобильных устройств и центров обработки данных. К первому кварталу этого года AMD планировала контролировать пятую часть рынка процессоров для ноутбуков. Этому может способствовать и дефицит процессоров Intel, который, как сообщается, может сохраняться в течение всего 2020 года."}
{"Text":"Детекция аномалий на снимках с БПЛА / Хабр                                                          \n\nОсенью мы с друзьями участвовали в хакатоне DIGITAL SUPERHERO от организаторов хакатона ИТС и СЦ, о котором я писал в статье . Хакатон проходил с 18 по 23 сентября 2020 года. Мы выбрали задачу Распознавание аномалий (объектов и инцидентов) на фотоматериалах, полученных с беспилотных летательных аппаратов (БПЛА) в треке \"Разработка алгоритмов распознавания. В рамках кейса нужно было разработать алгоритм по автоматическому выявлению аномалий на изображении и разработать веб-интерфейс для загрузки и разметки изображений. В этой статье я бы хотел рассказать о нашем решении для детекции аномалий с помощью модели YOLOv5 в виде практического туториала. Кому интересно, прошу под кат.\n### Описание задачи ###\nЗадачу предоставила нефтяная компания Татнефть. Татнефть использует БПЛА для мониторинга районов разработки нефти. В таких районах могут возникнуть различные аномалии такие, как разлив нефти, вскрытие грунта, складирование материала, работа спец. техники. Обычно детекция таких аномалий выполняется операторами вручную, что требует много времени. Такую задачу имеет смысл автоматизировать с применением методов компьютерного зрения. Наша задача была представить такой алгоритм автоматического выявления аномалий на изображении с БПЛА.\n### Исследование данных ###\nК сожалению, мы не смогли найти датасет изображений с БПЛА, на котором имеются подобные аномалии. Поэтому пришлось использовать датасет, предоставленный организаторами.\nНам был предоставлен довольно маленький набор данных, сотоящий из изображений размера 6000x4000, разделенных на 4 вышеупомянутых класса аномалий. Классы оказались сильно несбалансированными: например много семплов пришлось на класс складирование материала (15 изображений и до 5 объектов класса на изображении) и очень мало на класс работы спец. техники (6 изображений). Также было много изображений, не включающих ни одной аномалии (негативные семплы). Стоит отметить, что на одном изображении встречаются только аномалии одного класса, что сильно упрощает задачу.\nПример изображения\n\n## Подготовка данных для обучения ##\nКак я уже упомянул нам предоставили изображения размера 6000x4000 со следующим распределением по классам: складирование материала - 15 семплов, вскрытие грунта - 8, разлив нефти - 15, работа спец. техники - 6 и 121 изображение без аномалий.\nДля обучения моделей детекции объектов необходимо тысячи и даже миллионы образцов. Поэтому мы решили резать все изображения на тайлы размером 600x400 с перекрытием 25% методом скользящего окна.\nПример тайла\n\nДля разметки мы использовали онлайн сервис makesense.ai. Его интерфейс очень прост и не требуется регистрации. Просто нажимаем кнопку Get started и загружаем файлы папкой через драг-н-дроп или файд аплоадер, создаем список меток и нажимаем Start project. Также можно выбрать автоматическую разметку изображений на основе искусственного интеллекта. После окончания разметки результат можно экспортировать в формате YOLO.\nИзначально у нас было 27K изображений (после нарезки на тайлы). Обучение модели YOLOv5 на таком большом наборе данных занимает очень много времени, поэтому мы уменьшили датасет до 10K.\nЗатем мы применили аугментацию данных с помощью библиотеки imgaug  для увеличения датасета и улучшения качества детекции. О важности аугментации данных в задаче детекции можно прочитать здесь . YOLOv5 применяет некоторые техники аугментации \"из коробки\" (например mosaic data augmentation). Мы в это задаче использовали поворот, горизонтальный и вертикальное отражение (flip), сдвиг по горизонтали и вертикали. Наш скрипт для аугментации данных доступен здесь . Для аугментации данных также можно использовать сервис roboflow .\nПримеры аугментации\n\nВыбор модели\nЗдесь мы выбрали YOLOv5, недавний релиз серии YOLO. По информации из сети YOLO обходит по точности другие SOTA модели на датасете COCO и представляет хороший баланс между точностью и скоростью выполения. Здесь представлен хороший сравнительный бенчмарк различных SOTA моделей.\n### Обучение модели ###\nМы использовали PyTorch реализацию YOLOv5 с этого github репозитория. Подробную инструкцию по обучению модели на своем датасете можно найти здесь . Для обучения лучше использовать Google Colab, поскольку он предоставляет бесплатно GPU (правда есть ограничение на 8 часов в сутки).\nПросто клонируем репозиторий и устанавливаем зависимости:\n```\npip install -qr requirements.txt\n```\nСкопируем файл coco128.yaml в папке data чтобы в дальнейшем\n```\ncd data && cp coco128.yaml anomalies.yaml\n```\nи начнем его изменять под нашу задачу. Сначала зададим пути к обучающему и валидационному датасетам\n```\ntrain: ../dataaug/images/train/  val: ../dataaug/images/val/\n```\nИзменим число классов для обучения\n```\n# number of classes: 4\n```\nи укажем имена наших классов\n```\n# class names: ['excavation', 'service', 'spill', 'storage']\n```\nТакже изменим число классов в файле models/yolov5s.yaml\n```\nnc: 4 # number of classes\n```\nИ запусим обучение\n```\npython train.py --img 416 --batch 64 --epochs 20 --data ./data/anomalies.yaml --cfg ./models/yolov5s.yaml --weights ''\n```\nЗдесь мы используем модель yolov5s.\nМодель yolov5 создает лог обучения в папке runs\n```\ncd yolov5 && ls -l runs\ndrwx------ 2 root root 4096 Sep 21 09:52 exp0\ndrwx------ 2 root root 4096 Sep 21 15:07 exp1\n…\n```\nМетрики сохраняются в виде изображения results.png\n\nРаспределение меток классов в данных сохраняются в виде изображения labels.png\n\nПримеры одного из самых ранних результатов детекции на нашем датасете с дефолтными значениями параметров обучения YOLOv5\n\n### Оптимизация производительности ###\nДля оптимизации эффективности обучения можно тюнить размер батча, пробовать различные размеры входных изображений. Кроме того вы можете менять параметры обучения для анкоров в файле yolov5s.yaml.\nНа этом все. Удачи в обучении YOLOv5 для детекции объектов и до новых встреч.","metric":0.64013671880000000414,"metric_input":"Текст статьи:\nДетекция аномалий на снимках с БПЛА / Хабр                                                          \n\nОсенью мы с друзьями участвовали в хакатоне DIGITAL SUPERHERO от организаторов хакатона ИТС и СЦ, о котором я писал в статье . Хакатон проходил с 18 по 23 сентября 2020 года. Мы выбрали задачу Распознавание аномалий (объектов и инцидентов) на фотоматериалах, полученных с беспилотных летательных аппаратов (БПЛА) в треке \"Разработка алгоритмов распознавания. В рамках кейса нужно было разработать алгоритм по автоматическому выявлению аномалий на изображении и разработать веб-интерфейс для загрузки и разметки изображений. В этой статье я бы хотел рассказать о нашем решении для детекции аномалий с помощью модели YOLOv5 в виде практического туториала. Кому интересно, прошу под кат.\n### Описание задачи ###\nЗадачу предоставила нефтяная компания Татнефть. Татнефть использует БПЛА для мониторинга районов разработки нефти. В таких районах могут возникнуть различные аномалии такие, как разлив нефти, вскрытие грунта, складирование материала, работа спец. техники. Обычно детекция таких аномалий выполняется операторами вручную, что требует много времени. Такую задачу имеет смысл автоматизировать с применением методов компьютерного зрения. Наша задача была представить такой алгоритм автоматического выявления аномалий на изображении с БПЛА.\n### Исследование данных ###\nК сожалению, мы не смогли найти датасет изображений с БПЛА, на котором имеются подобные аномалии. Поэтому пришлось использовать датасет, предоставленный организаторами.\nНам был предоставлен довольно маленький набор данных, сотоящий из изображений размера 6000x4000, разделенных на 4 вышеупомянутых класса аномалий. Классы оказались сильно несбалансированными: например много семплов пришлось на класс складирование материала (15 изображений и до 5 объектов класса на изображении) и очень мало на класс работы спец. техники (6 изображений). Также было много изображений, не включающих ни одной аномалии (негативные семплы). Стоит отметить, что на одном изображении встречаются только аномалии одного класса, что сильно упрощает задачу.\nПример изображения\n\n## Подготовка данных для обучения ##\nКак я уже упомянул нам предоставили изображения размера 6000x4000 со следующим распределением по классам: складирование материала - 15 семплов, вскрытие грунта - 8, разлив нефти - 15, работа спец. техники - 6 и 121 изображение без аномалий.\nДля обучения моделей детекции объектов необходимо тысячи и даже миллионы образцов. Поэтому мы решили резать все изображения на тайлы размером 600x400 с перекрытием 25% методом скользящего окна.\nПример тайла\n\nДля разметки мы использовали онлайн сервис makesense.ai. Его интерфейс очень прост и не требуется регистрации. Просто нажимаем кнопку Get started и загружаем файлы папкой через драг-н-дроп или файд аплоадер, создаем список меток и нажимаем Start project. Также можно выбрать автоматическую разметку изображений на основе искусственного интеллекта. После окончания разметки результат можно экспортировать в формате YOLO.\nИзначально у нас было 27K изображений (после нарезки на тайлы). Обучение модели YOLOv5 на таком большом наборе данных занимает очень много времени, поэтому мы уменьшили датасет до 10K.\nЗатем мы применили аугментацию данных с помощью библиотеки imgaug  для увеличения датасета и улучшения качества детекции. О важности аугментации данных в задаче детекции можно прочитать здесь . YOLOv5 применяет некоторые техники аугментации \"из коробки\" (например mosaic data augmentation). Мы в это задаче использовали поворот, горизонтальный и вертикальное отражение (flip), сдвиг по горизонтали и вертикали. Наш скрипт для аугментации данных доступен здесь . Для аугментации данных также можно использовать сервис roboflow .\nПримеры аугментации\n\nВыбор модели\nЗдесь мы выбрали YOLOv5, недавний релиз серии YOLO. По информации из сети YOLO обходит по точности другие SOTA модели на датасете COCO и представляет хороший баланс между точностью и скоростью выполения. Здесь представлен хороший сравнительный бенчмарк различных SOTA моделей.\n### Обучение модели ###\nМы использовали PyTorch реализацию YOLOv5 с этого github репозитория. Подробную инструкцию по обучению модели на своем датасете можно найти здесь . Для обучения лучше использовать Google Colab, поскольку он предоставляет бесплатно GPU (правда есть ограничение на 8 часов в сутки).\nПросто клонируем репозиторий и устанавливаем зависимости:\n```\npip install -qr requirements.txt\n```\nСкопируем файл coco128.yaml в папке data чтобы в дальнейшем\n```\ncd data && cp coco128.yaml anomalies.yaml\n```\nи начнем его изменять под нашу задачу. Сначала зададим пути к обучающему и валидационному датасетам\n```\ntrain: ../dataaug/images/train/  val: ../dataaug/images/val/\n```\nИзменим число классов для обучения\n```\n# number of classes: 4\n```\nи укажем имена наших классов\n```\n# class names: ['excavation', 'service', 'spill', 'storage']\n```\nТакже изменим число классов в файле models/yolov5s.yaml\n```\nnc: 4 # number of classes\n```\nИ запусим обучение\n```\npython train.py --img 416 --batch 64 --epochs 20 --data ./data/anomalies.yaml --cfg ./models/yolov5s.yaml --weights ''\n```\nЗдесь мы используем модель yolov5s.\nМодель yolov5 создает лог обучения в папке runs\n```\ncd yolov5 && ls -l runs\ndrwx------ 2 root root 4096 Sep 21 09:52 exp0\ndrwx------ 2 root root 4096 Sep 21 15:07 exp1\n…\n```\nМетрики сохраняются в виде изображения results.png\n\nРаспределение меток классов в данных сохраняются в виде изображения labels.png\n\nПримеры одного из самых ранних результатов детекции на нашем датасете с дефолтными значениями параметров обучения YOLOv5\n\n### Оптимизация производительности ###\nДля оптимизации эффективности обучения можно тюнить размер батча, пробовать различные размеры входных изображений. Кроме того вы можете менять параметры обучения для анкоров в файле yolov5s.yaml.\nНа этом все. Удачи в обучении YOLOv5 для детекции объектов и до новых встреч.\n\nКраткое содержание:\nВ статье рассматривается задача детекции аномалий на изображениях с БПЛА. Описывается исследование данных и подготовка датасета для обучения модели. Для обучения выбрана модель YOLOv5. Приведены примеры аугментации данных и оптимизации производительности.","summary":"В статье рассматривается задача детекции аномалий на изображениях с БПЛА. Описывается исследование данных и подготовка датасета для обучения модели. Для обучения выбрана модель YOLOv5. Приведены примеры аугментации данных и оптимизации производительности."}
{"Text":"Инфраструктура просто и легко с «частным облаком» / Хабр                \n\n04-07-2011\nМесяц назад компании Cisco, NetApp и Microsoft объявили о разработке совместных облачных платформ в рамках инициативы Hyper-V Cloud FastTrack. Эта инициатива призвана объединить производителей аппаратного обеспечения и облачную платформу Microsoft с целью максимально быстрого и лёгкого построения подобных решений. Давайте теперь более подробно рассмотрим компоненты данного решения в исполнении NetApp и Cisco. Решение построено на гибкой унифицированной архитектуре (Unified Architecture). Как правило, основными компонентами решения Hyper-V Cloud Fast Track — с точки зрения аппаратного обеспечения — являются следующие элементы: 1) сервера x64-архитектуры (Рекомендуется подобрать несколько одинаковых серверных узлов и объединить их в кластер) 2) системы хранения данных (желательно чтобы эти системы обладали возможностями виртуализации дискового пространства) 3) коммуникационное оборудование для передачи IP и SAN трафика (это могут быть как конвергентные устройства, так и отдельные устройства под IP трафик и SAN трафик соответственно) В нашем конкретном случае схема выглядит так: 1) Сервера Cisco UCS – компания Cisco недавно на серверном рынке, однако активно представляет свои решения в данной категории. Отличительными особенностями данных серверов является оптимизация под приложения передачи голосовых и видеоданных как на виртуальной, так и на физических платформах. Также такие технологии, как Cisco VN-Link и Cisco Extended Memory Technology, позволяют упростить задачи передачи данных, а также увеличить емкость ОЗУ, тем самым повысить масштабируемость системы без лишних затрат. 2) Унифицированная система хранения данных NetApp – данный компонент отлично приспособлен к облачным решениям, он обладает технологией виртуализации дискового пространства, а также одновременно может обеспечивать доступ по разным интерфейсам (iSCSI, FC, FCoE) и поддерживает различные файловые системы (NFS, CIFS). Это по-настоящему унифицированное решение – вы можете без проблем разместить ваши Linux и Windows виртуальные машины в пределах одной СХД, каждые на своей файловой системе. Благодаря технологиям дедупликации облачная инфраструктура становится не такой «толстой», хранятся только уникальные блоки данных, все повторяющиеся блоки данных заменяются ссылками на уникальные оригинальные блоки. 3) Коммуникационное оборудование Cisco UCS Fabric и Cisco Nexus – унифицированное и конвергентное оборудование от лидера телеком-сектора. Для обеспечения надежной отказоустойчивой и динамической облачной инфраструктуры следует использовать решения для работы на скорости 10 Gbit Ethernet, используя поверх этого канала iSCSI или FCoE (NetApp поддерживает оба). Для передачи пользовательских данных предлагается использовать коммутаторы с меньшей пропускной способностью в зависимости от бизнес-задач и возможностей. Все основные аппаратные компоненты решения мы перечислили, теперь давайте посмотрим, что все-таки делает из этой груды метала облачную платформу: 1) Microsoft Hyper-V – гипервизор, основной компонент для виртуализации, входит в состав операционной системы Microsoft Windows Server 2008 R2 SP1. Позволяет создавать кластерные платформы виртуализации, что является важным компонентом облачной платформы. 2) Семейство Microsoft System Center – набор продуктов для управления, поддержки и развития гетерогенной облачной инфраструктуры. Состоит из множества продуктов, ключевыми являются следующие: a. System Center Virtual Machine Manager – Продукт для создания и управления виртуальными машинами, виртуальными сетями, облачными средами. Позволяет управлять системами хранения данных средствами продукта по протоколу SMI-S. Поддерживает различные платформы виртуализации: VMware, Citrix, Microsoft. Также позволяет управлять процессом развертывания Hyper-V на «голые» сервера посредством поддержки протоколов BMC, iDRAC, SMASH. Также отвечает за процессы обновления серверов виртуализации. b. System Center Operations Manager – продукт для оптимизации производительности и поддержки ИТ-инфраструктуры. При интеграции с SCVMM позволяет задействовать PRO-функционал облачной платформы, а именно — отслеживать загрузку виртуальных машин, оптимизировать их размещение в зависимости от загрузки, управлять энергопотреблением за счет отключения избыточных серверных узлов. c. System Center Data Protection Manager – продукт, который отвечает за резервное копирование и восстановление компонентов ИТ-инфраструктуры. Работает как с физическими, так и с виртуальными компонентами. Поддерживает целостное online-копирование всех продуктов Microsoft (SQL, Exchange, SharePoint, Lync). Тесно интегрируется с СХД NetApp, позволяя получать моментальные и надежные резервные копии. d. System Center Service Manager – система сервис-деск, которая тесно интегрируется с остальными компонентами System Center и позволяет автоматизировать рутинные задачи посредством заявок в сервис-деск. Позволяет реализовать работу ИТ-службы в соответствии с ITIL и MOF. e. System Center Orchestrator (Opalis) – продукт построения и автоматизации ИТ-процессов в рамках инфраструктуры. Является ключевым компонентом для интеграции решений и решений задач автоматизации рутинных процессов, которые присутствую в любой ИТ-службе (создание новых виртуальных машин, запуск резервного копирования или восстановления при наступлении определённого события и т.д.) Все эти компоненты вместе образуют облако. Каким оно будет – частным или публичным – зависит уже от вас и ваших задач: вы можете обеспечить свое предприятие качественной, надежной, гибкой и быстрой ИТ-инфраструктурой или обеспечить качественный сервис для сторонних пользователей, реализую различные облачные решения – IaaS, SaaS и другие. В России пока что подобных решений мало из-за новизны облачных платформ. Но, несмотря на новизну, некоторые компании уже готовят и готовы демонстрировать подобные решения сегодня. Посмотреть, как работает облачная платформа Microsoft, на базе оборудование Cisco и NetApp, вы можете в демонстрационном центре «Микротеста». В компании развернуто частное облако на базе продуктов семейства System Center 2012. Данное семейство продуктов еще находится в состоянии Beta, однако в конце 2011 года будет представлена конечная версия продуктов. Однако уже сегодня есть возможность пробовать и проверять на деле то, что будут использовать в буквальном смысле завтра.\n","metric":0.54541015630000000414,"metric_input":"Текст статьи:\nИнфраструктура просто и легко с «частным облаком» / Хабр                \n\n04-07-2011\nМесяц назад компании Cisco, NetApp и Microsoft объявили о разработке совместных облачных платформ в рамках инициативы Hyper-V Cloud FastTrack. Эта инициатива призвана объединить производителей аппаратного обеспечения и облачную платформу Microsoft с целью максимально быстрого и лёгкого построения подобных решений. Давайте теперь более подробно рассмотрим компоненты данного решения в исполнении NetApp и Cisco. Решение построено на гибкой унифицированной архитектуре (Unified Architecture). Как правило, основными компонентами решения Hyper-V Cloud Fast Track — с точки зрения аппаратного обеспечения — являются следующие элементы: 1) сервера x64-архитектуры (Рекомендуется подобрать несколько одинаковых серверных узлов и объединить их в кластер) 2) системы хранения данных (желательно чтобы эти системы обладали возможностями виртуализации дискового пространства) 3) коммуникационное оборудование для передачи IP и SAN трафика (это могут быть как конвергентные устройства, так и отдельные устройства под IP трафик и SAN трафик соответственно) В нашем конкретном случае схема выглядит так: 1) Сервера Cisco UCS – компания Cisco недавно на серверном рынке, однако активно представляет свои решения в данной категории. Отличительными особенностями данных серверов является оптимизация под приложения передачи голосовых и видеоданных как на виртуальной, так и на физических платформах. Также такие технологии, как Cisco VN-Link и Cisco Extended Memory Technology, позволяют упростить задачи передачи данных, а также увеличить емкость ОЗУ, тем самым повысить масштабируемость системы без лишних затрат. 2) Унифицированная система хранения данных NetApp – данный компонент отлично приспособлен к облачным решениям, он обладает технологией виртуализации дискового пространства, а также одновременно может обеспечивать доступ по разным интерфейсам (iSCSI, FC, FCoE) и поддерживает различные файловые системы (NFS, CIFS). Это по-настоящему унифицированное решение – вы можете без проблем разместить ваши Linux и Windows виртуальные машины в пределах одной СХД, каждые на своей файловой системе. Благодаря технологиям дедупликации облачная инфраструктура становится не такой «толстой», хранятся только уникальные блоки данных, все повторяющиеся блоки данных заменяются ссылками на уникальные оригинальные блоки. 3) Коммуникационное оборудование Cisco UCS Fabric и Cisco Nexus – унифицированное и конвергентное оборудование от лидера телеком-сектора. Для обеспечения надежной отказоустойчивой и динамической облачной инфраструктуры следует использовать решения для работы на скорости 10 Gbit Ethernet, используя поверх этого канала iSCSI или FCoE (NetApp поддерживает оба). Для передачи пользовательских данных предлагается использовать коммутаторы с меньшей пропускной способностью в зависимости от бизнес-задач и возможностей. Все основные аппаратные компоненты решения мы перечислили, теперь давайте посмотрим, что все-таки делает из этой груды метала облачную платформу: 1) Microsoft Hyper-V – гипервизор, основной компонент для виртуализации, входит в состав операционной системы Microsoft Windows Server 2008 R2 SP1. Позволяет создавать кластерные платформы виртуализации, что является важным компонентом облачной платформы. 2) Семейство Microsoft System Center – набор продуктов для управления, поддержки и развития гетерогенной облачной инфраструктуры. Состоит из множества продуктов, ключевыми являются следующие: a. System Center Virtual Machine Manager – Продукт для создания и управления виртуальными машинами, виртуальными сетями, облачными средами. Позволяет управлять системами хранения данных средствами продукта по протоколу SMI-S. Поддерживает различные платформы виртуализации: VMware, Citrix, Microsoft. Также позволяет управлять процессом развертывания Hyper-V на «голые» сервера посредством поддержки протоколов BMC, iDRAC, SMASH. Также отвечает за процессы обновления серверов виртуализации. b. System Center Operations Manager – продукт для оптимизации производительности и поддержки ИТ-инфраструктуры. При интеграции с SCVMM позволяет задействовать PRO-функционал облачной платформы, а именно — отслеживать загрузку виртуальных машин, оптимизировать их размещение в зависимости от загрузки, управлять энергопотреблением за счет отключения избыточных серверных узлов. c. System Center Data Protection Manager – продукт, который отвечает за резервное копирование и восстановление компонентов ИТ-инфраструктуры. Работает как с физическими, так и с виртуальными компонентами. Поддерживает целостное online-копирование всех продуктов Microsoft (SQL, Exchange, SharePoint, Lync). Тесно интегрируется с СХД NetApp, позволяя получать моментальные и надежные резервные копии. d. System Center Service Manager – система сервис-деск, которая тесно интегрируется с остальными компонентами System Center и позволяет автоматизировать рутинные задачи посредством заявок в сервис-деск. Позволяет реализовать работу ИТ-службы в соответствии с ITIL и MOF. e. System Center Orchestrator (Opalis) – продукт построения и автоматизации ИТ-процессов в рамках инфраструктуры. Является ключевым компонентом для интеграции решений и решений задач автоматизации рутинных процессов, которые присутствую в любой ИТ-службе (создание новых виртуальных машин, запуск резервного копирования или восстановления при наступлении определённого события и т.д.) Все эти компоненты вместе образуют облако. Каким оно будет – частным или публичным – зависит уже от вас и ваших задач: вы можете обеспечить свое предприятие качественной, надежной, гибкой и быстрой ИТ-инфраструктурой или обеспечить качественный сервис для сторонних пользователей, реализую различные облачные решения – IaaS, SaaS и другие. В России пока что подобных решений мало из-за новизны облачных платформ. Но, несмотря на новизну, некоторые компании уже готовят и готовы демонстрировать подобные решения сегодня. Посмотреть, как работает облачная платформа Microsoft, на базе оборудование Cisco и NetApp, вы можете в демонстрационном центре «Микротеста». В компании развернуто частное облако на базе продуктов семейства System Center 2012. Данное семейство продуктов еще находится в состоянии Beta, однако в конце 2011 года будет представлена конечная версия продуктов. Однако уже сегодня есть возможность пробовать и проверять на деле то, что будут использовать в буквальном смысле завтра.\n\n\nКраткое содержание:\nСтатья описывает совместное решение Cisco, NetApp и Microsoft для облачных платформ на основе Hyper-V Cloud FastTrack. Решение использует унифицированную архитектуру, сервера Cisco UCS, систему хранения данных NetApp, коммуникационное оборудование Cisco UCS Fabric и Cisco Nexus. Microsoft Hyper-V и семейство продуктов System Center от Microsoft управляют и поддерживают эту инфраструктуру.","summary":"Статья описывает совместное решение Cisco, NetApp и Microsoft для облачных платформ на основе Hyper-V Cloud FastTrack. Решение использует унифицированную архитектуру, сервера Cisco UCS, систему хранения данных NetApp, коммуникационное оборудование Cisco UCS Fabric и Cisco Nexus. Microsoft Hyper-V и семейство продуктов System Center от Microsoft управляют и поддерживают эту инфраструктуру."}
{"Text":"Как вручную заблокировать блокировку блокировщиков рекламы / Habr                        \n\nВсё больше сайтов (особенно медийных) пытаются воспрепятствовать доступу пользователей, у которых установлен блокировщик рекламы. Они обычно показывают всплывающее окно с просьбой оформить подписку или сделать пожертвование, а в некоторых случаях вообще блокируют доступ к сайту, пока вы не внесёте их в белый список блокировщика.\n\nДля обхода этой защиты разработаны отдельные расширения. Например,\nNano Defender\n(\nдля Chrome\nи Firefox) интегрируется с uBlock Origin или Nano Adblocker, см.\nинструкции по настройке\n.\n\nПользователи сообщают, что в последнее время Nano Defender\nвременно не работал\n. Хотя проблема уже исправлена, но в таких ситуациях можно легко заблокировать блокировку блокировщиков рекламы вручную.\n\nИнструкция\nопубликована\nна сайте\n_gHacks.net_\n:\n\nДобавить в блокировщик рекламы следующий скрипт. В случае uBlock Origin это делается через «Настройки» → «Мои фильтры». Возможно, скрипт будет работать и с другими расширениями/дополнениями.\n\nЗаменить адрес _example.com_  на URL сайта, который детектирует ваш блокировщик рекламы и пытается воспрепятствовать доступу.\n\nНажать кнопку «Применить изменения».\n\nПерезагрузите страницу, к которой вы пытались получить доступ. Теперь всё должно нормально работать. Это придётся сделать для каждого сайта, где работают анти-блокировщики.\n\n# Зачем блокировать вручную? #\n\nНа это есть несколько причин. Например, фильтры вашего блокировщика могут не работать должным образом, как мы видели на примере временного сбоя Nano Defender. Пока разработчики не исправят ситуацию, у нас включается запасной вариант.\n\nВредоносные скрипты, которые заставляют вас отключать блокировщик рекламы, всё время совершенствуются, учатся обходить блокировщики разными способами — и требуется некоторое время, прежде чем мейнтейнеры списка фильтров обновят их, чтобы фильтры снова заработали.\n\nРасширения типа uBlock Origin, фактически, являются частью системы безопасности компьютера вместе с вашим антивирусом и файрволом. В некотором смысле блокировщики обеспечивают даже лучшую защиту, чем антивирусы. К тому же они не увеличивают нагрузку на CPU, а наоборот, снижают её.\n\nКонечно, отдельные проверенные сайты можно включить в белый список, чтобы люди могли заработать хоть немножко на рекламе, но таких сайтов крайне мало.\n\n# Польза блокировщиков рекламы #\n\nЧерез рекламные баннеры и сторонние скрипты происходят инъекции вредоносного кода. О проблемах конфиденциальности из-за следящих куков и скриптов можно даже не упоминать.\n\nВ прошлом году Mozilla опубликовала результаты исследования\n_The Effect of Ad Blocking on User Engagement with the Web_\n.\n\nНекоторые из выводов предыдущих исследований на эту тему:\n\nБлокировщики рекламы значительно уменьшают объём передаваемых данных, что увеличивает скорость загрузки страниц и экономит трафик на мобильных устройствах с тарифицируемым трафиком. На крупнейших сайтах объём передаваемых данных сокращается примерно вдвое.\n\nБлокировка рекламы уменьшает угрозу утечки конфиденциальных данных через поведенческий трекинг и вредоносные баннеры.\n\nПольза блокировщиков понятна большинству пользователей.\n\nMozilla провела анализ телеметрических данных Firefox, сравнив показатели пользователей с установленными блокировщиками — и контрольной группы без них.\n\nВыяснилось, что первая группа в среднем на 28% больше времени проводит в интернете и загружает на 15% больше страниц.\n\nОчевидно, что повышение активности коррелирует с использованием блокировщика, хотя и не обязательно является\n_следствием_\nего установки. Анализ показал, что у пользователя с\n_любым_\nрасширением к браузеру регистрируется повышенная активность в интернете примерно на 10% (в таблице вверху приведены показатели контрольных групп с расширениями Video DownloadHelper и McAfee WebAdvisor. Другими словами, это показатель более продвинутого и активного пользователя. Но среди всех расширений к браузеру именно у этой группы эффект проявляется наиболее сильно.\n\nИнтересно также, что ни одно расширение из исследования не коррелирует с изменением количества поисковых запросов, хотя в среднем установка любого расширения снижает количество поисковых запросов на 10% (это связано с тем, что некоторые расширения изменяют функциональность поисковой строки в браузере).\n\n«Противники утверждают, что блокировка рекламы наносит существенный урон современным сайтам и приложениям, ухудшая пользовательский опыт и взаимодействие в вебе. Этот результат, по их мнению, может угрожать\n\nбудущему росту Сети. С другой стороны, сторонники блокировки утверждают, что реклама мешает просмотру сайтов, а пользователи хотят полностью удалить баннеры, — сказано в выводах Mozilla по результатам исследования. — Мы\n\nобнаружили, что установка расширений для блокировки рекламы существенно увеличивает как активное время, проведённое в браузере, так и количество просмотренных страниц. Это эмпирическое доказательство поддерживает позицию сторонников блокировки рекламы и опровергает утверждение о том, что блокировка рекламы ухудшает взаимодействие пользователей с Сетью».\n\nИсследование Mozilla стало первым в своём роде. Это на самом деле важные результаты. Они показывают, что блокировка рекламы вовсе не «ломает веб», как утверждают издатели. На самом деле всё наоборот — блокировщики\n**помогают использовать интернет более эффективно**\n. Бюро интернет-рекламы недавно назвало блокировщики «главной эксзистенциальной угрозой». Но исследование Mozilla демонстрирует, что это скорее назойливая реклама мешает нормальной работе и представляет угрозу для развития Сети, а не блокировщики.","metric":0.626953125,"metric_input":"Текст статьи:\nКак вручную заблокировать блокировку блокировщиков рекламы / Habr                        \n\nВсё больше сайтов (особенно медийных) пытаются воспрепятствовать доступу пользователей, у которых установлен блокировщик рекламы. Они обычно показывают всплывающее окно с просьбой оформить подписку или сделать пожертвование, а в некоторых случаях вообще блокируют доступ к сайту, пока вы не внесёте их в белый список блокировщика.\n\nДля обхода этой защиты разработаны отдельные расширения. Например,\nNano Defender\n(\nдля Chrome\nи Firefox) интегрируется с uBlock Origin или Nano Adblocker, см.\nинструкции по настройке\n.\n\nПользователи сообщают, что в последнее время Nano Defender\nвременно не работал\n. Хотя проблема уже исправлена, но в таких ситуациях можно легко заблокировать блокировку блокировщиков рекламы вручную.\n\nИнструкция\nопубликована\nна сайте\n_gHacks.net_\n:\n\nДобавить в блокировщик рекламы следующий скрипт. В случае uBlock Origin это делается через «Настройки» → «Мои фильтры». Возможно, скрипт будет работать и с другими расширениями/дополнениями.\n\nЗаменить адрес _example.com_  на URL сайта, который детектирует ваш блокировщик рекламы и пытается воспрепятствовать доступу.\n\nНажать кнопку «Применить изменения».\n\nПерезагрузите страницу, к которой вы пытались получить доступ. Теперь всё должно нормально работать. Это придётся сделать для каждого сайта, где работают анти-блокировщики.\n\n# Зачем блокировать вручную? #\n\nНа это есть несколько причин. Например, фильтры вашего блокировщика могут не работать должным образом, как мы видели на примере временного сбоя Nano Defender. Пока разработчики не исправят ситуацию, у нас включается запасной вариант.\n\nВредоносные скрипты, которые заставляют вас отключать блокировщик рекламы, всё время совершенствуются, учатся обходить блокировщики разными способами — и требуется некоторое время, прежде чем мейнтейнеры списка фильтров обновят их, чтобы фильтры снова заработали.\n\nРасширения типа uBlock Origin, фактически, являются частью системы безопасности компьютера вместе с вашим антивирусом и файрволом. В некотором смысле блокировщики обеспечивают даже лучшую защиту, чем антивирусы. К тому же они не увеличивают нагрузку на CPU, а наоборот, снижают её.\n\nКонечно, отдельные проверенные сайты можно включить в белый список, чтобы люди могли заработать хоть немножко на рекламе, но таких сайтов крайне мало.\n\n# Польза блокировщиков рекламы #\n\nЧерез рекламные баннеры и сторонние скрипты происходят инъекции вредоносного кода. О проблемах конфиденциальности из-за следящих куков и скриптов можно даже не упоминать.\n\nВ прошлом году Mozilla опубликовала результаты исследования\n_The Effect of Ad Blocking on User Engagement with the Web_\n.\n\nНекоторые из выводов предыдущих исследований на эту тему:\n\nБлокировщики рекламы значительно уменьшают объём передаваемых данных, что увеличивает скорость загрузки страниц и экономит трафик на мобильных устройствах с тарифицируемым трафиком. На крупнейших сайтах объём передаваемых данных сокращается примерно вдвое.\n\nБлокировка рекламы уменьшает угрозу утечки конфиденциальных данных через поведенческий трекинг и вредоносные баннеры.\n\nПольза блокировщиков понятна большинству пользователей.\n\nMozilla провела анализ телеметрических данных Firefox, сравнив показатели пользователей с установленными блокировщиками — и контрольной группы без них.\n\nВыяснилось, что первая группа в среднем на 28% больше времени проводит в интернете и загружает на 15% больше страниц.\n\nОчевидно, что повышение активности коррелирует с использованием блокировщика, хотя и не обязательно является\n_следствием_\nего установки. Анализ показал, что у пользователя с\n_любым_\nрасширением к браузеру регистрируется повышенная активность в интернете примерно на 10% (в таблице вверху приведены показатели контрольных групп с расширениями Video DownloadHelper и McAfee WebAdvisor. Другими словами, это показатель более продвинутого и активного пользователя. Но среди всех расширений к браузеру именно у этой группы эффект проявляется наиболее сильно.\n\nИнтересно также, что ни одно расширение из исследования не коррелирует с изменением количества поисковых запросов, хотя в среднем установка любого расширения снижает количество поисковых запросов на 10% (это связано с тем, что некоторые расширения изменяют функциональность поисковой строки в браузере).\n\n«Противники утверждают, что блокировка рекламы наносит существенный урон современным сайтам и приложениям, ухудшая пользовательский опыт и взаимодействие в вебе. Этот результат, по их мнению, может угрожать\n\nбудущему росту Сети. С другой стороны, сторонники блокировки утверждают, что реклама мешает просмотру сайтов, а пользователи хотят полностью удалить баннеры, — сказано в выводах Mozilla по результатам исследования. — Мы\n\nобнаружили, что установка расширений для блокировки рекламы существенно увеличивает как активное время, проведённое в браузере, так и количество просмотренных страниц. Это эмпирическое доказательство поддерживает позицию сторонников блокировки рекламы и опровергает утверждение о том, что блокировка рекламы ухудшает взаимодействие пользователей с Сетью».\n\nИсследование Mozilla стало первым в своём роде. Это на самом деле важные результаты. Они показывают, что блокировка рекламы вовсе не «ломает веб», как утверждают издатели. На самом деле всё наоборот — блокировщики\n**помогают использовать интернет более эффективно**\n. Бюро интернет-рекламы недавно назвало блокировщики «главной эксзистенциальной угрозой». Но исследование Mozilla демонстрирует, что это скорее назойливая реклама мешает нормальной работе и представляет угрозу для развития Сети, а не блокировщики.\n\nКраткое содержание:\nВ статье рассматривается проблема блокировки доступа к сайтам, которые используют рекламу, для пользователей с блокировщиками рекламы. Описывается, как можно обойти эту блокировку вручную с помощью скрипта.\n\nТакже приводятся аргументы в пользу использования блокировщиков рекламы, такие как уменьшение объема передаваемых данных, снижение угрозы утечки конфиденциальных данных и повышение активности пользователей в интернете.\n\nКроме того, приводятся результаты исследования Mozilla, которое показало, что установка блокировщиков рекламы увеличивает активность пользователей в интернете и количество просмотренных страниц.","summary":"В статье рассматривается проблема блокировки доступа к сайтам, которые используют рекламу, для пользователей с блокировщиками рекламы. Описывается, как можно обойти эту блокировку вручную с помощью скрипта.\n\nТакже приводятся аргументы в пользу использования блокировщиков рекламы, такие как уменьшение объема передаваемых данных, снижение угрозы утечки конфиденциальных данных и повышение активности пользователей в интернете.\n\nКроме того, приводятся результаты исследования Mozilla, которое показало, что установка блокировщиков рекламы увеличивает активность пользователей в интернете и количество просмотренных страниц."}
{"Text":"Детализированный мониторинг запросов к Apache при помощи Python и Munin / Хабр                                                           \n\n17-12-2013\nОдним из заданий во время моей недавней стажировки в ТомТоме было улучшение средств мониторинга веб-серверов компании. Конкретнее, необходимо было написать несколько плагинов к Munin, которые выводили бы подробную статистику запросов к нашим API из логов доступа Apache. Задача эта изначально может показаться примитивной — всего-то и нужно, что распарсить файл лога и вывести результаты в понимаемом Munin формате. Однако, для создания действительно надежной и удобной утилиты с гибкими возможностями настройки мне пришлось немало повозиться. Чтобы эта работа не пропала зря, я разместил код на Гитхабе под названием elfstats. В этой статье я расскажу о том, какие задачи способна решать данная утилита, чтобы заинтересовавшиеся сетевые инженеры смогли применить её в своих окружениях. Вкратце, перед началом работы с elfstats необходимо описать обрабатываемые веб-сервером запросы при помощи регулярных выражений. После этого программа будет регулярно парсить логи доступа и генерировать файл отчета, содержащий количество запросов и распределение времени отклика для каждого из описанных шаблонов. Далее, этот отчет будет обрабатываться при помощи Munin и выводиться в виде соответствующих графиков. Таким образом, установив и настроив elfstats, вы сможете легко отслеживать изменения в работе ваших приложений и поведении пользователей на уровне веб-сервера. Сейчас существует огромное количество различных средств мониторинга веб-серверов. Однако, проанализировав наиболее популярные из них, я не нашел утилит, которые позволяли бы отслеживать подробные данные по запросам в (около) реальном времени, отображали всю информацию в одном месте и легко встроились в уже существовавшую в компании инфраструктуру мониторинга. До того, как я начал работать над своим решением, для анализа запросов к API у нас использовались устаревшие и по сути неподдерживаемые скрипты из двух частей — аггрегатора данных и плагинов к Munin. С ними существовало несколько серьёзных проблем: невозможность более-менее гибкой конфигурации без изменения исходных кодов, недостаточная функциональность и банальные баги. Думаю, мы были не единственным местом, где задача разбора и отображения информации из логов решалась написанными на коленке скриптами. Вывод количества запросов к группе API, описанной одним выражением За основу нового решения я также взял двухкомпонентный подход. Для сбора статистики из логов был написан демон, который запускается через определенные интервалы времени и анализирует данные, сгенерированные со времени предыдущего запуска. Собранная информация записывается в файл отчета, который потом может быть использован плагинами к Munin или к другим средствам визуального мониторинга реального времени. Поскольку все задачи обработки данных возлагаются на демон, плагины получаются очень простыми и отрабатывают очень быстро, что весьма критично для нашего центрального сервера Munin, который медленно и печально собирает статистику по очереди с нескольких десятков нод. Кроме этого, поскольку плагины практически не содержат никакой логики, кроме разбора короткого файла отчета и возврата значений в munin-node, они легко могут быть переписаны для других утилит мониторинга. Демон при этом трогать не придется. Текущая версия программы поддерживает следующую функциональность. Разбор формата лога Apache. Вне зависимости от того, как вы настроили сервер, демон правильно разберет формат генерируемых логов. Для этого достаточно скопировать строку конфигурации Apache в файл настроек демона. Работа с несколькими различными файлами логов на одной машине из одного процесса. Дочитывание и разбор предыдущего файла с поддержкой различных вариантов ротации (ротация на месте или создание нового файла с временным шаблоном в имени). Гибкая настройка разбираемых и игнорируемых запросов при помощи регулярных выражений. Поддержка лог-файлов с записями сдвинутыми относительно времени на целевой машине. Группировка схожих запросов в группы для упрощения настройки и контроля за системой. Распределение времени отклика для одного из вызовов API При помощи elfstats можно мониторить следующие данные. Общее количество запросов в логе. Количество запросов по каждой описанной группе (и отдельно количество медленных запросов). Распределение времени отклика по каждой группе — минимум, максимум, среднее, произвольные перцентили. Распределение кодов возврата — в целом для сервера и отдельно для каждой группы запросов. Количество (различных) совпадений для заданных шаблонов. Количество разобранных, пропущенных и ошибочных записей для мониторинга демона и отладки конфигурации. Для отображения этих данных используются шесть различных плагинов для Munin, написанных на основе фреймворка PyMunin. Плагины распространяются отдельно от демона, они независимы друг от друга и могут включаться и выключаться по необходимости. Мониторинг кодов возврата Код демона и плагинов написан на Python 2.6, в будущем возможна поддержа версии 3.х. Бинарная сборка доступна в виде rpm-пакетов, собранных на Linux RedHat 6. Для работы с другими операционными системами необходимо собрать демон и плагины из исходных кодов. Для упрощения установки elfstats на машины с RHEL6 поддерживается RPM с предустановленным виртуальным окружением для питона, которое включает все необходимые зависимости. Исходные коды демона открыты под лицензией MIT, плагины распространяются под GPLv3, как того требует взятый за основу фреймворк PyMunin. Для первичного развертывания и обновления elfstats на серверах используются манифесты Puppet. Они же модифицируют файлы настроек под необходимые задачи, разные для всех серверов. Манифестов в репозиториях пока нет, они будут добавлены несколько позже. Как мне кажется, написанная мной программа получилась довольно функциональной и гибкой. Уже несколько месяцев она используется для мониторинга довольно большого количества серверов и работает без нареканий. Я буду рад, если вы захотите применить elfstats для контроля своих серверов и с удовольствием отвечу на вопросы по работе этой утилиты в комментариях и трекере.\n","metric":0.59130859380000000414,"metric_input":"Текст статьи:\nДетализированный мониторинг запросов к Apache при помощи Python и Munin / Хабр                                                           \n\n17-12-2013\nОдним из заданий во время моей недавней стажировки в ТомТоме было улучшение средств мониторинга веб-серверов компании. Конкретнее, необходимо было написать несколько плагинов к Munin, которые выводили бы подробную статистику запросов к нашим API из логов доступа Apache. Задача эта изначально может показаться примитивной — всего-то и нужно, что распарсить файл лога и вывести результаты в понимаемом Munin формате. Однако, для создания действительно надежной и удобной утилиты с гибкими возможностями настройки мне пришлось немало повозиться. Чтобы эта работа не пропала зря, я разместил код на Гитхабе под названием elfstats. В этой статье я расскажу о том, какие задачи способна решать данная утилита, чтобы заинтересовавшиеся сетевые инженеры смогли применить её в своих окружениях. Вкратце, перед началом работы с elfstats необходимо описать обрабатываемые веб-сервером запросы при помощи регулярных выражений. После этого программа будет регулярно парсить логи доступа и генерировать файл отчета, содержащий количество запросов и распределение времени отклика для каждого из описанных шаблонов. Далее, этот отчет будет обрабатываться при помощи Munin и выводиться в виде соответствующих графиков. Таким образом, установив и настроив elfstats, вы сможете легко отслеживать изменения в работе ваших приложений и поведении пользователей на уровне веб-сервера. Сейчас существует огромное количество различных средств мониторинга веб-серверов. Однако, проанализировав наиболее популярные из них, я не нашел утилит, которые позволяли бы отслеживать подробные данные по запросам в (около) реальном времени, отображали всю информацию в одном месте и легко встроились в уже существовавшую в компании инфраструктуру мониторинга. До того, как я начал работать над своим решением, для анализа запросов к API у нас использовались устаревшие и по сути неподдерживаемые скрипты из двух частей — аггрегатора данных и плагинов к Munin. С ними существовало несколько серьёзных проблем: невозможность более-менее гибкой конфигурации без изменения исходных кодов, недостаточная функциональность и банальные баги. Думаю, мы были не единственным местом, где задача разбора и отображения информации из логов решалась написанными на коленке скриптами. Вывод количества запросов к группе API, описанной одним выражением За основу нового решения я также взял двухкомпонентный подход. Для сбора статистики из логов был написан демон, который запускается через определенные интервалы времени и анализирует данные, сгенерированные со времени предыдущего запуска. Собранная информация записывается в файл отчета, который потом может быть использован плагинами к Munin или к другим средствам визуального мониторинга реального времени. Поскольку все задачи обработки данных возлагаются на демон, плагины получаются очень простыми и отрабатывают очень быстро, что весьма критично для нашего центрального сервера Munin, который медленно и печально собирает статистику по очереди с нескольких десятков нод. Кроме этого, поскольку плагины практически не содержат никакой логики, кроме разбора короткого файла отчета и возврата значений в munin-node, они легко могут быть переписаны для других утилит мониторинга. Демон при этом трогать не придется. Текущая версия программы поддерживает следующую функциональность. Разбор формата лога Apache. Вне зависимости от того, как вы настроили сервер, демон правильно разберет формат генерируемых логов. Для этого достаточно скопировать строку конфигурации Apache в файл настроек демона. Работа с несколькими различными файлами логов на одной машине из одного процесса. Дочитывание и разбор предыдущего файла с поддержкой различных вариантов ротации (ротация на месте или создание нового файла с временным шаблоном в имени). Гибкая настройка разбираемых и игнорируемых запросов при помощи регулярных выражений. Поддержка лог-файлов с записями сдвинутыми относительно времени на целевой машине. Группировка схожих запросов в группы для упрощения настройки и контроля за системой. Распределение времени отклика для одного из вызовов API При помощи elfstats можно мониторить следующие данные. Общее количество запросов в логе. Количество запросов по каждой описанной группе (и отдельно количество медленных запросов). Распределение времени отклика по каждой группе — минимум, максимум, среднее, произвольные перцентили. Распределение кодов возврата — в целом для сервера и отдельно для каждой группы запросов. Количество (различных) совпадений для заданных шаблонов. Количество разобранных, пропущенных и ошибочных записей для мониторинга демона и отладки конфигурации. Для отображения этих данных используются шесть различных плагинов для Munin, написанных на основе фреймворка PyMunin. Плагины распространяются отдельно от демона, они независимы друг от друга и могут включаться и выключаться по необходимости. Мониторинг кодов возврата Код демона и плагинов написан на Python 2.6, в будущем возможна поддержа версии 3.х. Бинарная сборка доступна в виде rpm-пакетов, собранных на Linux RedHat 6. Для работы с другими операционными системами необходимо собрать демон и плагины из исходных кодов. Для упрощения установки elfstats на машины с RHEL6 поддерживается RPM с предустановленным виртуальным окружением для питона, которое включает все необходимые зависимости. Исходные коды демона открыты под лицензией MIT, плагины распространяются под GPLv3, как того требует взятый за основу фреймворк PyMunin. Для первичного развертывания и обновления elfstats на серверах используются манифесты Puppet. Они же модифицируют файлы настроек под необходимые задачи, разные для всех серверов. Манифестов в репозиториях пока нет, они будут добавлены несколько позже. Как мне кажется, написанная мной программа получилась довольно функциональной и гибкой. Уже несколько месяцев она используется для мониторинга довольно большого количества серверов и работает без нареканий. Я буду рад, если вы захотите применить elfstats для контроля своих серверов и с удовольствием отвечу на вопросы по работе этой утилиты в комментариях и трекере.\n\n\nКраткое содержание:\nСтатья описывает процесс создания и использования утилиты для мониторинга запросов к Apache при помощи Python и Munin. Утилита, названная elfstats, использует демон для анализа логов и генерации отчетов, которые затем обрабатываются плагинами Munin для отображения в виде графиков. elfstats предоставляет подробную информацию о количестве запросов, времени отклика и распределении кодов возврата для каждого шаблона, что позволяет сетевым инженерам отслеживать изменения в работе приложений и поведении пользователей на уровне веб-сервера.","summary":"Статья описывает процесс создания и использования утилиты для мониторинга запросов к Apache при помощи Python и Munin. Утилита, названная elfstats, использует демон для анализа логов и генерации отчетов, которые затем обрабатываются плагинами Munin для отображения в виде графиков. elfstats предоставляет подробную информацию о количестве запросов, времени отклика и распределении кодов возврата для каждого шаблона, что позволяет сетевым инженерам отслеживать изменения в работе приложений и поведении пользователей на уровне веб-сервера."}
{"Text":"Экономика токенов: дисконты — основные проблемы / Habr                        \n\n23-10-2017\nНачнём с главного: зачем это знать тем, кто живёт на it-ресурсе? Причин, минимум, две: первая — львиная доля современных проектов так или иначе строит собственную программу лояльности или подключает одну из действующих, но в любом случае речь идёт в первую очередь о скидках (иногда — бонусах); вторая — ICO — пузырь, а значит — после того, как он схлопнется, сам процесс привлечения средств нормализуется и всё только увеличится, то есть нужно быть готовым к востребованности услуги. Второе, о чём стоит говорить, — популярность: дисконты сейчас пытаются «запихать» во все товарные, app- и другие подвиды токенов. Только когда это оправдано, а когда нет — остаётся большим вопросом. И главное — открытым. Третье — дисконтируют многие: правильно — единицы. Даже не знаю, стоит ли приводить примеры: скажем, купонаторы в РФ это делали явно не правильно. Многие e-commerce проекты и т.д., поэтому сказать, что сфера — сложная, значит не просто заставить немого произнести вдохновляющую речь громко и чётко, но ещё и отрубить ему при этом все выходы на остальные сенсоры. Пожалуй, начнём. Можно выделить несколько групп дисконтов и видов в них. Группа №1. Дисконты на товары и на услуги: казалось бы, в чём разница? На самом деле она есть и существенная: товар, если мы не говорим о цифровом только, всегда присутствует в качестве физической сущности: банки, коробки, упаковки и т.д., то есть выражается в какой-то количественной единице. Услуга же в первую очередь измеряется по временной шкале. Конечно, есть товары, для которых время — также критично, но в любом случае оно будет производно от полезных свойств самого товара. Молоко в ряде случаев полезно, пока не скисло, но именно потому, что оно — молоко, т.е. продукт с определённым набором качеств. Именно поэтому объединение в одну группу товарных и сервисных (на услуги) токенов — на мой взгляд в корне не верно. Часто от такого объединения страдают крупные системы лояльности, где под одним флагом унифицированы разные поставщики и производители: представим, что я даю постоянный дисконт — 10% — на юридические услуги по программе лояльность №х. И производитель, например, тракторов, тоже даёт дисконт, но в 5%: по той же программе лояльности и постоянно. Нужно ли объяснять на сколько мы будем не равны в случае, когда цена на мои услуги и их товары возрастёт пропорционально, например, курсу доллара? Скажем, трактор, стоил 1 000 000. Затем стал по ряду причин стоить 2 000 000. Но скидка как была 5%, так и осталась, т.е. в деньгах потребитель изначально получал 50 000, а затем — уже 100 000. Допустим, у компании таких потребителей 100, т.е. разница составит 50 000 * 100 = 5 000 000. На сколько это будет обосновано при падении покупательской способности, как это было, скажем, в РФ с 2014 по 2016 гг.? Вижу, как надвигается огненная лавина критики экономистов, начинающаяся с вопроса: «это же очевидно?». Да, но проанализируйте любое количество ICO-проектов, построенных на дисконтах, и увидите, что очевидно — не значит реализовано: более того, не значит это, что даже будет когда-либо реализовано. Второй важный аспект — расчёт скидки. Очень часто вижу (да и вы наверняка видели такую ситуацию): сделай это и получи скидку, нажми сюда — и получи ещё. В большинстве случаев (по моим оценка текущих ICO: с 2016 по 2017 г. — 72%) обоснование подобных действий минимально или вовсе отсутствует, но главное — не вписывается в экономику проекта, т.е. не стоимость действий не привязана к итоговой стоимости лида. Можно опять же долго спорить и обсуждать, но факт заключается в том, что экономика дисконтного токена требует тщательной проработки по трём аспектам: Стоимость лида относительно стоимости скидки Временные рамки существования возможностей накопительных скидок Сценарии движения стоимости скидок в зависимости от показателя прибыли-убытков Впрочем, ограничиваться этими аспектами не всегда резонно, но как ликбез по сути подхода — вполне. Третье: дисконт и бонус — это разные вещи. Абсолютно. Понимаю, что деление можно заменить умножением, а вычитание — сложением, но всё же недаром они введены в матаппарат как отдельные инструменты. Не даром. Во-первых, бонус идёт сверх цены и поэтому он может попасть в разные сетки маржинальности: например, дали бонус за покупку телевизора (маржа — 5%), а потратили его на покупку чехла для телефона (маржа — 200%). Дисконт в этом смысле более лоялен к «продавцу». Во-вторых, дисконты и бонусы могут быть накопительными. И это всех путает. В ряде случаев накопительный дисконт лучше: обычно тогда, когда покупки итак периодические, а важно увеличить средний чек и общую лояльность (из живых примеров — магазины продажи косметики). Бонус же позволяет покупать с опредёлнного момента не часть какой-то вещи или услуги, а всю услугу или вещь. При этом бывают системы бонусов такие, что трата определённой суммы — снижает коэффициент накопления (скажем, если 1000 бонусов, то копится по 1 баллу с каждых 100 руб., а если 10 000, то 2 балла с каждых 100 руб.). Такой подход помогает «продавцу» (в кавычках, т.к. это может быть любой сервис, реализующий токены), поскольку ставит психологическую преграду к продаже накопленного. Но в то же время есть риск при больших накоплениях потерять слишком дорогую вещь. Наконец, ещё одна проблема вытекает напрямую из понятия: «дисконт — разница между ценами, по которым продается материальная ценность или товар в настоящий момент, и ценой ее номинальной стоимости при продаже или при погашении». В первую очередь это проблема проектов с обратным выкупом, о котором мы ещё будем говорить отдельно (ZrCoin, скажем): купили токены с дисконтом 25%. Погашение, допустим, через год. Купили их по 75, но стоимость 100. А гашение — по рыночной цене. И спекулянты сделали памп. И продаём их уже по 200. Так какой дисконт получает инвестор? У меня нет задачи критиковать кого-либо конкретно или давать наставления рынку: он сам справиться. Но нормализовать отношения хочется. Возможно, многим всё одно, что есть ICO, что это точка кипения блокчейна, что блокчейн — не просто база знаний или технология, но новая парадигма мышления. Возможно: только не мне. Поэтому всем начинающим предпринимателям, чьи проекты «заточены» на программы лояльности, бонусы или скидки, хотелось бы посоветовать следующее: Никогда не считайте дисконт от конкурентов или тем более — «похожих» проектов или от среднего по рынку: бизнес индивидуален и история это доказало — велосипед не изобрести; Пожалуйста, запомните, что ставка дисконтирования и дисконт — две большие разницы; Главное: не заходите в этот бизнес с универсальной ставкой «на всё про всё и всех про всех», т.к. разрыв по товарным категориям (категориям услуг), разные временные шкалы циклов использования и другие показатели просто нивелируют все усилия. Напомню, что я — блокчейн-энтузиаст и именно поэтому готов тратить своё время и силы на развитие индустрии шаг-за-шагом. Поэтому — благодарю за прочтение и вопросы — ниже.\n","metric":0.67041015630000000414,"metric_input":"Текст статьи:\nЭкономика токенов: дисконты — основные проблемы / Habr                        \n\n23-10-2017\nНачнём с главного: зачем это знать тем, кто живёт на it-ресурсе? Причин, минимум, две: первая — львиная доля современных проектов так или иначе строит собственную программу лояльности или подключает одну из действующих, но в любом случае речь идёт в первую очередь о скидках (иногда — бонусах); вторая — ICO — пузырь, а значит — после того, как он схлопнется, сам процесс привлечения средств нормализуется и всё только увеличится, то есть нужно быть готовым к востребованности услуги. Второе, о чём стоит говорить, — популярность: дисконты сейчас пытаются «запихать» во все товарные, app- и другие подвиды токенов. Только когда это оправдано, а когда нет — остаётся большим вопросом. И главное — открытым. Третье — дисконтируют многие: правильно — единицы. Даже не знаю, стоит ли приводить примеры: скажем, купонаторы в РФ это делали явно не правильно. Многие e-commerce проекты и т.д., поэтому сказать, что сфера — сложная, значит не просто заставить немого произнести вдохновляющую речь громко и чётко, но ещё и отрубить ему при этом все выходы на остальные сенсоры. Пожалуй, начнём. Можно выделить несколько групп дисконтов и видов в них. Группа №1. Дисконты на товары и на услуги: казалось бы, в чём разница? На самом деле она есть и существенная: товар, если мы не говорим о цифровом только, всегда присутствует в качестве физической сущности: банки, коробки, упаковки и т.д., то есть выражается в какой-то количественной единице. Услуга же в первую очередь измеряется по временной шкале. Конечно, есть товары, для которых время — также критично, но в любом случае оно будет производно от полезных свойств самого товара. Молоко в ряде случаев полезно, пока не скисло, но именно потому, что оно — молоко, т.е. продукт с определённым набором качеств. Именно поэтому объединение в одну группу товарных и сервисных (на услуги) токенов — на мой взгляд в корне не верно. Часто от такого объединения страдают крупные системы лояльности, где под одним флагом унифицированы разные поставщики и производители: представим, что я даю постоянный дисконт — 10% — на юридические услуги по программе лояльность №х. И производитель, например, тракторов, тоже даёт дисконт, но в 5%: по той же программе лояльности и постоянно. Нужно ли объяснять на сколько мы будем не равны в случае, когда цена на мои услуги и их товары возрастёт пропорционально, например, курсу доллара? Скажем, трактор, стоил 1 000 000. Затем стал по ряду причин стоить 2 000 000. Но скидка как была 5%, так и осталась, т.е. в деньгах потребитель изначально получал 50 000, а затем — уже 100 000. Допустим, у компании таких потребителей 100, т.е. разница составит 50 000 * 100 = 5 000 000. На сколько это будет обосновано при падении покупательской способности, как это было, скажем, в РФ с 2014 по 2016 гг.? Вижу, как надвигается огненная лавина критики экономистов, начинающаяся с вопроса: «это же очевидно?». Да, но проанализируйте любое количество ICO-проектов, построенных на дисконтах, и увидите, что очевидно — не значит реализовано: более того, не значит это, что даже будет когда-либо реализовано. Второй важный аспект — расчёт скидки. Очень часто вижу (да и вы наверняка видели такую ситуацию): сделай это и получи скидку, нажми сюда — и получи ещё. В большинстве случаев (по моим оценка текущих ICO: с 2016 по 2017 г. — 72%) обоснование подобных действий минимально или вовсе отсутствует, но главное — не вписывается в экономику проекта, т.е. не стоимость действий не привязана к итоговой стоимости лида. Можно опять же долго спорить и обсуждать, но факт заключается в том, что экономика дисконтного токена требует тщательной проработки по трём аспектам: Стоимость лида относительно стоимости скидки Временные рамки существования возможностей накопительных скидок Сценарии движения стоимости скидок в зависимости от показателя прибыли-убытков Впрочем, ограничиваться этими аспектами не всегда резонно, но как ликбез по сути подхода — вполне. Третье: дисконт и бонус — это разные вещи. Абсолютно. Понимаю, что деление можно заменить умножением, а вычитание — сложением, но всё же недаром они введены в матаппарат как отдельные инструменты. Не даром. Во-первых, бонус идёт сверх цены и поэтому он может попасть в разные сетки маржинальности: например, дали бонус за покупку телевизора (маржа — 5%), а потратили его на покупку чехла для телефона (маржа — 200%). Дисконт в этом смысле более лоялен к «продавцу». Во-вторых, дисконты и бонусы могут быть накопительными. И это всех путает. В ряде случаев накопительный дисконт лучше: обычно тогда, когда покупки итак периодические, а важно увеличить средний чек и общую лояльность (из живых примеров — магазины продажи косметики). Бонус же позволяет покупать с опредёлнного момента не часть какой-то вещи или услуги, а всю услугу или вещь. При этом бывают системы бонусов такие, что трата определённой суммы — снижает коэффициент накопления (скажем, если 1000 бонусов, то копится по 1 баллу с каждых 100 руб., а если 10 000, то 2 балла с каждых 100 руб.). Такой подход помогает «продавцу» (в кавычках, т.к. это может быть любой сервис, реализующий токены), поскольку ставит психологическую преграду к продаже накопленного. Но в то же время есть риск при больших накоплениях потерять слишком дорогую вещь. Наконец, ещё одна проблема вытекает напрямую из понятия: «дисконт — разница между ценами, по которым продается материальная ценность или товар в настоящий момент, и ценой ее номинальной стоимости при продаже или при погашении». В первую очередь это проблема проектов с обратным выкупом, о котором мы ещё будем говорить отдельно (ZrCoin, скажем): купили токены с дисконтом 25%. Погашение, допустим, через год. Купили их по 75, но стоимость 100. А гашение — по рыночной цене. И спекулянты сделали памп. И продаём их уже по 200. Так какой дисконт получает инвестор? У меня нет задачи критиковать кого-либо конкретно или давать наставления рынку: он сам справиться. Но нормализовать отношения хочется. Возможно, многим всё одно, что есть ICO, что это точка кипения блокчейна, что блокчейн — не просто база знаний или технология, но новая парадигма мышления. Возможно: только не мне. Поэтому всем начинающим предпринимателям, чьи проекты «заточены» на программы лояльности, бонусы или скидки, хотелось бы посоветовать следующее: Никогда не считайте дисконт от конкурентов или тем более — «похожих» проектов или от среднего по рынку: бизнес индивидуален и история это доказало — велосипед не изобрести; Пожалуйста, запомните, что ставка дисконтирования и дисконт — две большие разницы; Главное: не заходите в этот бизнес с универсальной ставкой «на всё про всё и всех про всех», т.к. разрыв по товарным категориям (категориям услуг), разные временные шкалы циклов использования и другие показатели просто нивелируют все усилия. Напомню, что я — блокчейн-энтузиаст и именно поэтому готов тратить своё время и силы на развитие индустрии шаг-за-шагом. Поэтому — благодарю за прочтение и вопросы — ниже.\n\n\nКраткое содержание:\nСтатья обсуждает проблемы, связанные с использованием дисконтов в индустрии криптовалют и токенов. Автор выделяет несколько групп дисконтов и видов в них, а также подчеркивает важность тщательной проработки экономики дисконтного токена по трем аспектам: стоимость лида относительно стоимости скидки, временные рамки существования возможностей накопительных скидок и сценарии движения стоимости скидок в зависимости от показателя прибыли-убытков. Автор также разделяет понятия дисконта и бонуса, подчеркивая их различия и важность правильного использования. В заключение, автор призывает предпринимателей не считать дисконт от конкурентов или похожих проектов, помнить о разнице между ставкой дисконтирования и дисконтом, и не заходить в этот бизнес с универсальной ставкой \"на все случаи жизни\".","summary":"Статья обсуждает проблемы, связанные с использованием дисконтов в индустрии криптовалют и токенов. Автор выделяет несколько групп дисконтов и видов в них, а также подчеркивает важность тщательной проработки экономики дисконтного токена по трем аспектам: стоимость лида относительно стоимости скидки, временные рамки существования возможностей накопительных скидок и сценарии движения стоимости скидок в зависимости от показателя прибыли-убытков. Автор также разделяет понятия дисконта и бонуса, подчеркивая их различия и важность правильного использования. В заключение, автор призывает предпринимателей не считать дисконт от конкурентов или похожих проектов, помнить о разнице между ставкой дисконтирования и дисконтом, и не заходить в этот бизнес с универсальной ставкой \"на все случаи жизни\"."}
{"Text":"Гендир Postgres Professional Олег Бартунов рассказывает Фариде Рословец о PostgreSQL и бизнесе в России / Habr                        \n\n10-12-2019\nКак построить бизнес в России на основе открытого ПО? Рассказывает Олег Бартунов — сооснователь и CEO Postgres Professional, профессиональный астроном. Поговорили немного об астрономии в России, кто такой астроном и чем он занимается, про интеграцию IT-технологий и науки, о том, как выиграть в конкурсе деньги и не получить их от государства, что такое СУБДстроение, и о том, как они с Крюковым и Лысаковым Rambler делали на базе Астронета. PostgreSQL – популярная СУБД, которую используют многие частные компании и госструктуры не только в России, но и по всему миру. Большой вклад в её развитие и продвижение вносит российская компания Postgres Professional, которая умудряется хорошо на этом зарабатывать. Давайте узнаем, как. Полную версию интервью (час с небольшим) смотрите на моем YouTube-канале, там все очень живо и интересно, а в первом комментарии есть тайм-коды. Здесь же в сухом сжатом виде приведу некоторые моменты, творчески переработанные под печатный формат. Фаря: — Давайте, начнем сначала – как вы познакомились с Postgres? Коротко и последовательно Олег: — Я занимался изучением сверхновых звезд и в процессе изучения я заинтересовался базами данных. В перестройку я попал в Калифорнию в университет Беркли, где мне порекомендовали свободно распространяемую базу Ingres. В 95 году я перешел с неё на Postgres и начал использовать ее для решения своих астрономических задач. Потом я обнаружил, что чего-то в ней не хватает, ну и про традиции опенсорса, начал что-то добавлять. Таким образом я стал потихонечку разработчиком Postgres. — То есть получается, что будучи научным сотрудником, вы заинтересовались IT-технологиями, в частности базами данных, и внедрили их в свою работу — Формат работа астронома изменился, теперь мы работаем с огромными массивами данных и решаем более глобальные задачи, чем раньше, поэтому без технологий работать очень тяжело. Мы ведь должны оцифровать каждый объект на небе, координаты и параметры миллиардов звезд. Сейчас, если ты хочешь стать успешным астрономом, нужно владеть технологиями – знать SQL, Python и т.д. — Из чего состоит ваша рабочая рутина как астронома? — Раньше я писал программы, считал взрывы звезд, ездил на наблюдения, пытался открыть сверхновые звезды. Потом создавал IT-сервисы для астрономов, проводил сети, делал сервера, Астронет, виртуальную обсерваторию. А после того, как все научные базы института (ГАИШ) перешли на Postgres, у меня появилась новая ответственность – помочь, если что-то случится. И вообще наша компания Postgres Professional очень тесно связана с ГАИШ. — Давайте теперь к Postgres. Как вы решили создать компанию? — Идея появилась еще в 2010 году. А в 2011 я выступал на CNews конференции по импортозамещению, где говорил, что не нужно изобретать велосипед, а надо взять опенсорс, добавить свою экспертизу и развивать свою базу данных на этой основе. Много лет после этого мне говорили «давай организовывай компанию», но я не хотел, потому что я не бизнесмен, не хотелось связываться с этим миром нечистым. Но когда у меня появились дети, надо было думать об их будущем, поэтому компания обязана своим появлениям моим девочкам. Я пошел по компаниям, начал искать партнеров, но, к сожалению, все большие игроки хотели сделать просто карманный бизнес для решения их задач, а мне хотелось сделать независимую компанию. К счастью, сарафанное радио привело меня к Сушкевичу. Мы с ним буквально 5 минут поговорили, и я его убедил вложиться. — Вы 3-4 года искали инвесторов и не могли найти — Более того, я даже думал про то, что государство мне поможет. Я разговаривал с правительством Москвы, писал письмо в Минэкономразвития о том, что нужно лишь немного денег, чтобы создать центр компетенции баз данных. То есть я был готов работать на государство. Но никто реально не хотел этим заниматься. — А как вы обосновывали необходимость? — Я говорил, что России нужна своя база данных. Если мы называем себя серьезной страной, мы должны иметь базовые корневые технологии. Не то, чтобы изоляцией заниматься, но владеть своими технологиями мы обязаны. — Какие у вас есть крупные клиенты? — Сбербанк, Яндекс, который, кстати, был нашим первым заказчиком. ФНС, Минфин. Представляете, как трудно было убедить такие службы? Это стало возможным только с появлением Postgres Professional. Хоть все и понимали, что Postgres – хорошая СУБД, если нет компании, нет поддержки и ответственности, большой энтерпрайз не будет это использовать. — Каково вообще продавать в госструктуры? Они ведь не очень гибкие — Очень негибкие. Это занимает годы. Они будут годами тестировать, растить свою экспертизу. Это вполне нормально, у них серьезные большие системы, они должны быть уверены, проверить все сто раз. — У вас больше госкомпаний или коммерческих? — Больше госкомпаний. Сейчас сложилась ситуация, когда верхи понимают, что близится 2022 год, когда надо будет отчитываться об импортозамещении, низы понимают, что Postgres хороший, проблема в среднем слое управляющем. Потому что он сформировался во те годы, когда в вузах учили по курсам Oracle и MS SQL. А потом им говорят, вот «православный постгрес», и они естественно не уверены. Но это проходит. — А коммерческие компании за чем обращаются? — Они обычно просят консалтинг, расследовать какие-нибудь инциденты. Мы же поддерживаем и обычный Postgres, который бесплатный. — Тяжело вести IT-бизнес в России? Какие сложности? — Тяжело, но можно. Мы пришли из дикого капитализма, но постепенно ситуация становится цивилизованней. Люди начинают понимать, что вести бизнес хорошо, открыто – это выгодно, это нормально. Из сложностей – непрозрачность, закупок, например. Мы не знаем, как они идут. У нас еще законодательство странное бывает. Например, у нас сейчас идет импортозамещение, а Oracle все больше и больше закупают. Каким это образом? Потому что в законодательстве есть дыры, которые позволяют это обходить. На словах все поддерживают российское, а на деле нам ни копейки не дали, ни гранта, ничего. Мы выиграли в конкурсе министерства связи в 2015 году, но выигрыш нам так и не дали. Это были бы приличные деньги, которые дали бы нам старт хороший, но в результате мы боролись за то, чтобы как-то вырасти. И не распались мы, я считаю, только благодаря упорству и желанию сделать проект. — Сколько сейчас платят в Postgres Professional? — Джуниор зарабатывает не более 80-100 тысяч. Опытный разработчик может получать до 200 тысяч. У сеньора совсем много. Но набирать сотрудников непросто, это штучная работа. Когда люди понимаю, что разработка Postgres – это кровь, кишки и мясо, не все готовы закопаться в это. — Какие планы на будущее? — Мы развиваем наш проект, у нас очень много планов. Некоторые из которые выходят на передний край науки. Ищем научные коллективы, с кем сотрудничать. Если вам понравилось, приглашаю к просмотру полной версии . Вы также узнаете, как происходит разработка PostgreSQL, кого Олег готов взять на работу, как происходит внедрение СУБД, и почему интеграция с нейросетями – это самая горячая тема.\n","metric":0.5546875,"metric_input":"Текст статьи:\nГендир Postgres Professional Олег Бартунов рассказывает Фариде Рословец о PostgreSQL и бизнесе в России / Habr                        \n\n10-12-2019\nКак построить бизнес в России на основе открытого ПО? Рассказывает Олег Бартунов — сооснователь и CEO Postgres Professional, профессиональный астроном. Поговорили немного об астрономии в России, кто такой астроном и чем он занимается, про интеграцию IT-технологий и науки, о том, как выиграть в конкурсе деньги и не получить их от государства, что такое СУБДстроение, и о том, как они с Крюковым и Лысаковым Rambler делали на базе Астронета. PostgreSQL – популярная СУБД, которую используют многие частные компании и госструктуры не только в России, но и по всему миру. Большой вклад в её развитие и продвижение вносит российская компания Postgres Professional, которая умудряется хорошо на этом зарабатывать. Давайте узнаем, как. Полную версию интервью (час с небольшим) смотрите на моем YouTube-канале, там все очень живо и интересно, а в первом комментарии есть тайм-коды. Здесь же в сухом сжатом виде приведу некоторые моменты, творчески переработанные под печатный формат. Фаря: — Давайте, начнем сначала – как вы познакомились с Postgres? Коротко и последовательно Олег: — Я занимался изучением сверхновых звезд и в процессе изучения я заинтересовался базами данных. В перестройку я попал в Калифорнию в университет Беркли, где мне порекомендовали свободно распространяемую базу Ingres. В 95 году я перешел с неё на Postgres и начал использовать ее для решения своих астрономических задач. Потом я обнаружил, что чего-то в ней не хватает, ну и про традиции опенсорса, начал что-то добавлять. Таким образом я стал потихонечку разработчиком Postgres. — То есть получается, что будучи научным сотрудником, вы заинтересовались IT-технологиями, в частности базами данных, и внедрили их в свою работу — Формат работа астронома изменился, теперь мы работаем с огромными массивами данных и решаем более глобальные задачи, чем раньше, поэтому без технологий работать очень тяжело. Мы ведь должны оцифровать каждый объект на небе, координаты и параметры миллиардов звезд. Сейчас, если ты хочешь стать успешным астрономом, нужно владеть технологиями – знать SQL, Python и т.д. — Из чего состоит ваша рабочая рутина как астронома? — Раньше я писал программы, считал взрывы звезд, ездил на наблюдения, пытался открыть сверхновые звезды. Потом создавал IT-сервисы для астрономов, проводил сети, делал сервера, Астронет, виртуальную обсерваторию. А после того, как все научные базы института (ГАИШ) перешли на Postgres, у меня появилась новая ответственность – помочь, если что-то случится. И вообще наша компания Postgres Professional очень тесно связана с ГАИШ. — Давайте теперь к Postgres. Как вы решили создать компанию? — Идея появилась еще в 2010 году. А в 2011 я выступал на CNews конференции по импортозамещению, где говорил, что не нужно изобретать велосипед, а надо взять опенсорс, добавить свою экспертизу и развивать свою базу данных на этой основе. Много лет после этого мне говорили «давай организовывай компанию», но я не хотел, потому что я не бизнесмен, не хотелось связываться с этим миром нечистым. Но когда у меня появились дети, надо было думать об их будущем, поэтому компания обязана своим появлениям моим девочкам. Я пошел по компаниям, начал искать партнеров, но, к сожалению, все большие игроки хотели сделать просто карманный бизнес для решения их задач, а мне хотелось сделать независимую компанию. К счастью, сарафанное радио привело меня к Сушкевичу. Мы с ним буквально 5 минут поговорили, и я его убедил вложиться. — Вы 3-4 года искали инвесторов и не могли найти — Более того, я даже думал про то, что государство мне поможет. Я разговаривал с правительством Москвы, писал письмо в Минэкономразвития о том, что нужно лишь немного денег, чтобы создать центр компетенции баз данных. То есть я был готов работать на государство. Но никто реально не хотел этим заниматься. — А как вы обосновывали необходимость? — Я говорил, что России нужна своя база данных. Если мы называем себя серьезной страной, мы должны иметь базовые корневые технологии. Не то, чтобы изоляцией заниматься, но владеть своими технологиями мы обязаны. — Какие у вас есть крупные клиенты? — Сбербанк, Яндекс, который, кстати, был нашим первым заказчиком. ФНС, Минфин. Представляете, как трудно было убедить такие службы? Это стало возможным только с появлением Postgres Professional. Хоть все и понимали, что Postgres – хорошая СУБД, если нет компании, нет поддержки и ответственности, большой энтерпрайз не будет это использовать. — Каково вообще продавать в госструктуры? Они ведь не очень гибкие — Очень негибкие. Это занимает годы. Они будут годами тестировать, растить свою экспертизу. Это вполне нормально, у них серьезные большие системы, они должны быть уверены, проверить все сто раз. — У вас больше госкомпаний или коммерческих? — Больше госкомпаний. Сейчас сложилась ситуация, когда верхи понимают, что близится 2022 год, когда надо будет отчитываться об импортозамещении, низы понимают, что Postgres хороший, проблема в среднем слое управляющем. Потому что он сформировался во те годы, когда в вузах учили по курсам Oracle и MS SQL. А потом им говорят, вот «православный постгрес», и они естественно не уверены. Но это проходит. — А коммерческие компании за чем обращаются? — Они обычно просят консалтинг, расследовать какие-нибудь инциденты. Мы же поддерживаем и обычный Postgres, который бесплатный. — Тяжело вести IT-бизнес в России? Какие сложности? — Тяжело, но можно. Мы пришли из дикого капитализма, но постепенно ситуация становится цивилизованней. Люди начинают понимать, что вести бизнес хорошо, открыто – это выгодно, это нормально. Из сложностей – непрозрачность, закупок, например. Мы не знаем, как они идут. У нас еще законодательство странное бывает. Например, у нас сейчас идет импортозамещение, а Oracle все больше и больше закупают. Каким это образом? Потому что в законодательстве есть дыры, которые позволяют это обходить. На словах все поддерживают российское, а на деле нам ни копейки не дали, ни гранта, ничего. Мы выиграли в конкурсе министерства связи в 2015 году, но выигрыш нам так и не дали. Это были бы приличные деньги, которые дали бы нам старт хороший, но в результате мы боролись за то, чтобы как-то вырасти. И не распались мы, я считаю, только благодаря упорству и желанию сделать проект. — Сколько сейчас платят в Postgres Professional? — Джуниор зарабатывает не более 80-100 тысяч. Опытный разработчик может получать до 200 тысяч. У сеньора совсем много. Но набирать сотрудников непросто, это штучная работа. Когда люди понимаю, что разработка Postgres – это кровь, кишки и мясо, не все готовы закопаться в это. — Какие планы на будущее? — Мы развиваем наш проект, у нас очень много планов. Некоторые из которые выходят на передний край науки. Ищем научные коллективы, с кем сотрудничать. Если вам понравилось, приглашаю к просмотру полной версии . Вы также узнаете, как происходит разработка PostgreSQL, кого Олег готов взять на работу, как происходит внедрение СУБД, и почему интеграция с нейросетями – это самая горячая тема.\n\n\nКраткое содержание:\nСтатья рассказывает о бизнес-модели компании Postgres Professional, основанной Олегом Бартуновым, сооснователем и CEO компании. Компания занимается разработкой и продвижением СУБД PostgreSQL, популярной базы данных, используемой многими компаниями и государственными структурами.\n\nИнтервью начинается с рассказа Бартунова о его опыте использования PostgreSQL и развития базы данных. Он также обсуждает свою роль в создании компании Postgres Professional.\n\nДалее интервьюер переходит к обсуждению бизнес-модели компании, включая поиск инвесторов и привлечение клиентов. Бартунов также делится своим опытом продажи PostgreSQL в государственных структурах, где процесс может занять годы.\n\nОн также обсуждает сложности ведения IT-бизнеса в России, включая непрозрачность закупок и отсутствие поддержки со стороны государства.\n\nВ заключение, Бартунов делится своими планами на будущее и приглашает к просмотру полной версии интервью для получения дополнительной информации о разработке PostgreSQL и бизнес-стратегии компании.","summary":"Статья рассказывает о бизнес-модели компании Postgres Professional, основанной Олегом Бартуновым, сооснователем и CEO компании. Компания занимается разработкой и продвижением СУБД PostgreSQL, популярной базы данных, используемой многими компаниями и государственными структурами.\n\nИнтервью начинается с рассказа Бартунова о его опыте использования PostgreSQL и развития базы данных. Он также обсуждает свою роль в создании компании Postgres Professional.\n\nДалее интервьюер переходит к обсуждению бизнес-модели компании, включая поиск инвесторов и привлечение клиентов. Бартунов также делится своим опытом продажи PostgreSQL в государственных структурах, где процесс может занять годы.\n\nОн также обсуждает сложности ведения IT-бизнеса в России, включая непрозрачность закупок и отсутствие поддержки со стороны государства.\n\nВ заключение, Бартунов делится своими планами на будущее и приглашает к просмотру полной версии интервью для получения дополнительной информации о разработке PostgreSQL и бизнес-стратегии компании."}
{"Text":"Цифры в культуре Китая / Хабр                \n\n04-05-2011\nКаждый, кто ближе знакомится с культурой Китая или начинает общаться с китайцами, отмечает, насколько все сферы китайской жизни пронизаны цифровой символикой. Будь то принципы философии, медицины, музыки или архитектуры, мечты простого китайца завести себе телефонный номер с восьмеркой, табу в отношении цифры 4 или нездоровое желание подобрать удачное сочетание цифр в контракте. «Числовые суеверия» Если у нас подобные суеверия скорее повод для шуток, то в Китае к нумерологии относятся серьезно и вдумчиво. «Удачливость» числа в народном сознании китайцев определяется его созвучием с каким-либо словом. Например, число 4 звучит аналогично слову «умереть» (без учета тона), что породило в странах Дальнего Востока суеверие, аналогичное нашей боязни числа 13. Так, во многих гостиницах и больницах вы не найдете номеров с числом 4, а иногда даже и 4-го этажа. Пальму первенства среди «удачных» чисел в Китае держит цифра 8 — символ богатства и процветания. Дело в том, что произношение числа 8 похоже на произношение слова , которое вместе со словом «богатство» означает «разбогатеть». Это суеверие настолько популярно, что им не пренебрегают даже на официальном уровне. Достаточно вспомнить, что церемония открытия Олимпиады в Пекине была назначена на 08.08.08, 8 часов вечера 8 минут и 8 секунд. Другое «благоприятное» число — 6. В кантонском диалекте 6 звучит как , что значит «жалование, благопожелание карьерного роста». 9 же является омофоном слова «долгий, вечный», а посему считается хорошим символом, который можно удачно обыграть на свадьбе и в деловых переговорах. Правда, с цифрами 7 и 9 нужно быть особенно аккуратным, иначе может выйти неприятная история. Дело в том, что в Гонконге они звучат похоже на грубое ругательство :) Кстати, в разных китайских диалектах цифры произносятся по-разному. Поэтому цифровая символика, как и суеверия, может отличаться от одного региона Китая к другому. Как китайцы «шифруются» Логотип бара в городе Ухань, построенный на игре слов и чисел. Слово «бар» и слово 98 ( ) произносятся одинаково — как jiǔbā Поистине, фантазия китайцев в обращении с цифрами не знает границ! Игра в омофоны стала насколько популярной в молодежной среде, что породила немыслимое количество цифровых сокращений, построенных по аналогии с английскими B2B и 4U. Но китайцы пошли дальше: они умудряются зашифровывать цифрами целые предложения! Каким образом? А вот каким. Звучание некоторых китайских слов действительно совпадает с произношением цифр. Но для языковой игры достаточно и частичной омонимии — совпадения лишь нескольких начальных звуков шифруемого слова со звучанием заменяющей его цифры. На первый взгляд кажется, что правила эти слишком размыты и в результате никто и не догадается, о чем идет речь. Но в действительности смысл всего выражения можно понять по контексту. Некоторые особо популярные сокращения уже закрепились в языке или, по крайней мере, в молодежном и интернет-сленге. Вот наиболее популярные соответствия между числом и словом: 0 ( líng) — распространенное сокращение для nín (уважительное «Вы»). Хотя так оно может звучать только при гнусавости или сильном насморке :) 1 (— yī) — используется и в прямом значении как «один» или «все, целиком» Но кроме того, при чтении номеров единицу читают как yāo, что звучит как слово «должен, нужно, хотеть» (yào). 2 ( èr) — в кантонском диалекте похоже по звучанию на слово — «легко». 3 ( sān) — в кантонском диалекте похоже по звучанию на слово — «рождаться, появляться», также используется как сокращение для любого слова на s. 4 ( sì) — омофон слова «умереть». 5 ( wǔ) — омофон слова «я» (wú), «не иметь» (wú) и слова «нет» в запретах (wù). 6 ( liù) — звучит похоже на слово — «течь», а также в определенных диалектах это омофон слова «падать», «дорога» или «жалование». Сокращение любого слова, начинающегося на звук l. 7 ( qī) — заменяет собой слова, начинающиеся на q. Например, «подниматься» (qǐ) или «злить(ся)» (qì). 8 ( bā) — омофон слова «разбогатеть», а также сокращения любого слова, начинающегося со звука b, например, отрицания . 9 ( jiǔ) — схожа по звучанию со словом «долгий» . Может использоваться как сокращение любого слова на j. Теперь, вооруженные этим знанием, вы можете попробовать сами расшифровать следующие популярные числовые акронимы: а) благопожелание 168 б) проклятие 5104 в) перепалка двух влюбленных по смс: 07456 и ответ 8137 Екатерина Маничева Департамент разработки технологий По материалам блога команды ABBYY Lingvo\n","metric":0.5947265625,"metric_input":"Текст статьи:\nЦифры в культуре Китая / Хабр                \n\n04-05-2011\nКаждый, кто ближе знакомится с культурой Китая или начинает общаться с китайцами, отмечает, насколько все сферы китайской жизни пронизаны цифровой символикой. Будь то принципы философии, медицины, музыки или архитектуры, мечты простого китайца завести себе телефонный номер с восьмеркой, табу в отношении цифры 4 или нездоровое желание подобрать удачное сочетание цифр в контракте. «Числовые суеверия» Если у нас подобные суеверия скорее повод для шуток, то в Китае к нумерологии относятся серьезно и вдумчиво. «Удачливость» числа в народном сознании китайцев определяется его созвучием с каким-либо словом. Например, число 4 звучит аналогично слову «умереть» (без учета тона), что породило в странах Дальнего Востока суеверие, аналогичное нашей боязни числа 13. Так, во многих гостиницах и больницах вы не найдете номеров с числом 4, а иногда даже и 4-го этажа. Пальму первенства среди «удачных» чисел в Китае держит цифра 8 — символ богатства и процветания. Дело в том, что произношение числа 8 похоже на произношение слова , которое вместе со словом «богатство» означает «разбогатеть». Это суеверие настолько популярно, что им не пренебрегают даже на официальном уровне. Достаточно вспомнить, что церемония открытия Олимпиады в Пекине была назначена на 08.08.08, 8 часов вечера 8 минут и 8 секунд. Другое «благоприятное» число — 6. В кантонском диалекте 6 звучит как , что значит «жалование, благопожелание карьерного роста». 9 же является омофоном слова «долгий, вечный», а посему считается хорошим символом, который можно удачно обыграть на свадьбе и в деловых переговорах. Правда, с цифрами 7 и 9 нужно быть особенно аккуратным, иначе может выйти неприятная история. Дело в том, что в Гонконге они звучат похоже на грубое ругательство :) Кстати, в разных китайских диалектах цифры произносятся по-разному. Поэтому цифровая символика, как и суеверия, может отличаться от одного региона Китая к другому. Как китайцы «шифруются» Логотип бара в городе Ухань, построенный на игре слов и чисел. Слово «бар» и слово 98 ( ) произносятся одинаково — как jiǔbā Поистине, фантазия китайцев в обращении с цифрами не знает границ! Игра в омофоны стала насколько популярной в молодежной среде, что породила немыслимое количество цифровых сокращений, построенных по аналогии с английскими B2B и 4U. Но китайцы пошли дальше: они умудряются зашифровывать цифрами целые предложения! Каким образом? А вот каким. Звучание некоторых китайских слов действительно совпадает с произношением цифр. Но для языковой игры достаточно и частичной омонимии — совпадения лишь нескольких начальных звуков шифруемого слова со звучанием заменяющей его цифры. На первый взгляд кажется, что правила эти слишком размыты и в результате никто и не догадается, о чем идет речь. Но в действительности смысл всего выражения можно понять по контексту. Некоторые особо популярные сокращения уже закрепились в языке или, по крайней мере, в молодежном и интернет-сленге. Вот наиболее популярные соответствия между числом и словом: 0 ( líng) — распространенное сокращение для nín (уважительное «Вы»). Хотя так оно может звучать только при гнусавости или сильном насморке :) 1 (— yī) — используется и в прямом значении как «один» или «все, целиком» Но кроме того, при чтении номеров единицу читают как yāo, что звучит как слово «должен, нужно, хотеть» (yào). 2 ( èr) — в кантонском диалекте похоже по звучанию на слово — «легко». 3 ( sān) — в кантонском диалекте похоже по звучанию на слово — «рождаться, появляться», также используется как сокращение для любого слова на s. 4 ( sì) — омофон слова «умереть». 5 ( wǔ) — омофон слова «я» (wú), «не иметь» (wú) и слова «нет» в запретах (wù). 6 ( liù) — звучит похоже на слово — «течь», а также в определенных диалектах это омофон слова «падать», «дорога» или «жалование». Сокращение любого слова, начинающегося на звук l. 7 ( qī) — заменяет собой слова, начинающиеся на q. Например, «подниматься» (qǐ) или «злить(ся)» (qì). 8 ( bā) — омофон слова «разбогатеть», а также сокращения любого слова, начинающегося со звука b, например, отрицания . 9 ( jiǔ) — схожа по звучанию со словом «долгий» . Может использоваться как сокращение любого слова на j. Теперь, вооруженные этим знанием, вы можете попробовать сами расшифровать следующие популярные числовые акронимы: а) благопожелание 168 б) проклятие 5104 в) перепалка двух влюбленных по смс: 07456 и ответ 8137 Екатерина Маничева Департамент разработки технологий По материалам блога команды ABBYY Lingvo\n\n\nКраткое содержание:\nСтатья обсуждает цифровую символику в китайской культуре и то, как она пронизывает различные аспекты жизни в Китае. Цифры имеют глубокое значение и связаны с суевериями, которые могут быть очень странными для западной культуры. Например, число 4 считается несчастливым из-за его созвучия со словом \"умереть\".\n\nТакже обсуждаются числовые сокращения, которые стали популярными в молодежной среде. Они основаны на омонимах, когда цифры звучат похоже на слова. Например, 0 может означать \"Вы\", а 1 может означать \"один\" или \"все\".\n\nСтатья также описывает, как китайцы используют числа для шифрования целых предложений, что делает их смысл понятным только по контексту.","summary":"Статья обсуждает цифровую символику в китайской культуре и то, как она пронизывает различные аспекты жизни в Китае. Цифры имеют глубокое значение и связаны с суевериями, которые могут быть очень странными для западной культуры. Например, число 4 считается несчастливым из-за его созвучия со словом \"умереть\".\n\nТакже обсуждаются числовые сокращения, которые стали популярными в молодежной среде. Они основаны на омонимах, когда цифры звучат похоже на слова. Например, 0 может означать \"Вы\", а 1 может означать \"один\" или \"все\".\n\nСтатья также описывает, как китайцы используют числа для шифрования целых предложений, что делает их смысл понятным только по контексту."}
{"Text":"BPM или ERP? / Habr                       \n\n05-03-2014\nПреамбула: В каментах к прошлому посту задали вопрос: Почему BPM, а не ERP? Ниже я постарался ответить. Пару слов о процессном подходе и отличиях от функциональной схемы управления. Зачем вообще переходить от административно-функциональной схемы к процессной? 1. Ориентированность на результат. 2. Одна цель, один результат, один начальник — ответственный. 3. Разгрузка руководителей высшего звена. Руководство занимается стратегией развития, а не ручным управлением. 4. Повышение операционной эффективности. 5. Регламенты бизнес-процессов – новый персонал получает доступ в базу знаний, все действия и реакции описаны. Есть статьи, более подробно поясняющие что зачем и почему, с картинками, поэтому не буду останавливаться подробно(или стоит?) Почему именно BPM, а не, скажем, ERP? Я расставлю точки над ё, а вы меня поправьте. ERP система автоматизирует бизнес-процессы, но по сути своей «пассивна» — она может ответить на любой вопрос, но только если знаешь как спросить. С другой стороны BPM система — это «активная» система, которая раздает задания исполнителям, передает следующим исполнителям в цепочке, контролирует сроки. Каждый исполнитель знает, какой приоритет, в какие сроки нужно выполнить. BPM управляет бизнес-процессом. Любая современная ERP система содержит внутри себя некий BPM движок — workflow маршрутизатор. Но необходимо учитывать, что постоянно «запиливать» ERP под свои изменяющиеся бизнес-процессы не так-то просто, а вот BPM как раз для этого и предназначен. Резюмируя: ERP – здание, BPM – система лифтов и эскалаторов. Мне понравилось сравнение из телекома: ERP – это бэкбон оператора, а BPM – последняя миля до клиента. Эффект BPM Основной эффект от внедрения процессного управления люди видят в том, что бизнес-процессы становятся управляемыми. А раз процессами можно управлять, значит есть возможности для оптимизации. Сокращение издержек и времени выполнения. А, скажем, при выводе нового продукта на рынок это очень важно. Еще отмечают эффект, связанный с улучшением качества взаимодействия подразделений в сквозных бизнес-процессах. Экономический эффект можно рассмотреть на примере такого кейса: Уменьшение времени обработки заявки, уменьшает количество потерянных клиентов(которые не дожидаясь ответа уходят к конкурентам), что в свою очередь приводит к увеличению объема продаж, что при больших объемах может составлять миллионы долларов. Я взял из презентации IBM реальные результаты внедрения. – снижение внутренних затрат — на 40% – сокращение вдвое время исполнения процесса – повышение пропускной способности инфраструктуры на 100% – 3 года ROI — 32% и 5-лет ROI — 170% – Автоматизированно 400 процессов. От идеи до промышленной эксплуатации меньше недели! – сокращение ручной работы на 80%, релизы каждые 90 дней – уменьшение сложности — на 30%, повышение эффективности на 60% – Обеспечить 300% рост в течение 18 месяцев, Увеличение коэффициента удовлетворения клиента с 85% до 92%, релиз меньше, чем за 90 дней! – ROI за 3 года – 150% Там, где про ROI – я бы не стал слепо доверять, ведь многое зависит от того, как считать. В остальном, это картина от одного из лидеров BPM решений. Успешные внедрения. Banco espirito santo – испанский инновационный банк. Результат: в промышленной эксплуатации более 100 процессов, 1 день от диз айна процес с а до его имплементации, реализованы критические процессы банка: ипотека, кредиты для физ и юр-лиц, кредитные карты, обращения-рекламации (сервис-деск) клиентов, открытие счета. Банк «Тинькофф Кредитные Системы» Мы сократили среднее время обработки заявки с часов и дней до минут. При этом новая система позволяет легко настраивать вариации кредитной процедуры под специфические условия канала привлечения и другие факторы. Так, если раньше на то, чтобы внести малейшее изменение в бизнес-процесс, уходило несколько недель и требовались усилия ИТ-специалистов, то теперь сроки модификации бизнес-процесса измеряются днями (от идеи до запуска) и, в основном, осуществляются силами бизнес- пользователей МТС банк Результаты проекта Создана методология для описания бизнес-процессов Обучены сотрудники МТС- Банка Разработаны скрипты отчетности по моделям ARIS Вывод Первый вопрос: Стоит ли тратить время и деньги на реализацию BPM? Если вы банк или телеком – внедрять незамедлительно, скорость вывода новых продуктов – это важнейшее конкурентное преимущество. Остальным – считать эффект от внедрения в каждом конкретном случае. Второй вопрос: Внедрять ERP+BPM или что-то одно и если оба, то в какой последовательности? Для максимального эффекта внедрять оба и ROI считать в совокупности. Причем по мнению некоторых экспертов, имеет смысл внедрять сначала ERP и тогда BPM «упадет на взрыхленную плодородную почву». Но тем не менее, оба продукта могут существовать отдельно. PS: Я умышленно обошел стоимость внедрения, потому что эта тема требует отдельной статьи и проработки с указанием конкретных решений, плюсов и минусов. Если есть информация или соображения на этот счет – добро пожаловать в комменты.\n","metric":0.5263671875,"metric_input":"Текст статьи:\nBPM или ERP? / Habr                       \n\n05-03-2014\nПреамбула: В каментах к прошлому посту задали вопрос: Почему BPM, а не ERP? Ниже я постарался ответить. Пару слов о процессном подходе и отличиях от функциональной схемы управления. Зачем вообще переходить от административно-функциональной схемы к процессной? 1. Ориентированность на результат. 2. Одна цель, один результат, один начальник — ответственный. 3. Разгрузка руководителей высшего звена. Руководство занимается стратегией развития, а не ручным управлением. 4. Повышение операционной эффективности. 5. Регламенты бизнес-процессов – новый персонал получает доступ в базу знаний, все действия и реакции описаны. Есть статьи, более подробно поясняющие что зачем и почему, с картинками, поэтому не буду останавливаться подробно(или стоит?) Почему именно BPM, а не, скажем, ERP? Я расставлю точки над ё, а вы меня поправьте. ERP система автоматизирует бизнес-процессы, но по сути своей «пассивна» — она может ответить на любой вопрос, но только если знаешь как спросить. С другой стороны BPM система — это «активная» система, которая раздает задания исполнителям, передает следующим исполнителям в цепочке, контролирует сроки. Каждый исполнитель знает, какой приоритет, в какие сроки нужно выполнить. BPM управляет бизнес-процессом. Любая современная ERP система содержит внутри себя некий BPM движок — workflow маршрутизатор. Но необходимо учитывать, что постоянно «запиливать» ERP под свои изменяющиеся бизнес-процессы не так-то просто, а вот BPM как раз для этого и предназначен. Резюмируя: ERP – здание, BPM – система лифтов и эскалаторов. Мне понравилось сравнение из телекома: ERP – это бэкбон оператора, а BPM – последняя миля до клиента. Эффект BPM Основной эффект от внедрения процессного управления люди видят в том, что бизнес-процессы становятся управляемыми. А раз процессами можно управлять, значит есть возможности для оптимизации. Сокращение издержек и времени выполнения. А, скажем, при выводе нового продукта на рынок это очень важно. Еще отмечают эффект, связанный с улучшением качества взаимодействия подразделений в сквозных бизнес-процессах. Экономический эффект можно рассмотреть на примере такого кейса: Уменьшение времени обработки заявки, уменьшает количество потерянных клиентов(которые не дожидаясь ответа уходят к конкурентам), что в свою очередь приводит к увеличению объема продаж, что при больших объемах может составлять миллионы долларов. Я взял из презентации IBM реальные результаты внедрения. – снижение внутренних затрат — на 40% – сокращение вдвое время исполнения процесса – повышение пропускной способности инфраструктуры на 100% – 3 года ROI — 32% и 5-лет ROI — 170% – Автоматизированно 400 процессов. От идеи до промышленной эксплуатации меньше недели! – сокращение ручной работы на 80%, релизы каждые 90 дней – уменьшение сложности — на 30%, повышение эффективности на 60% – Обеспечить 300% рост в течение 18 месяцев, Увеличение коэффициента удовлетворения клиента с 85% до 92%, релиз меньше, чем за 90 дней! – ROI за 3 года – 150% Там, где про ROI – я бы не стал слепо доверять, ведь многое зависит от того, как считать. В остальном, это картина от одного из лидеров BPM решений. Успешные внедрения. Banco espirito santo – испанский инновационный банк. Результат: в промышленной эксплуатации более 100 процессов, 1 день от диз айна процес с а до его имплементации, реализованы критические процессы банка: ипотека, кредиты для физ и юр-лиц, кредитные карты, обращения-рекламации (сервис-деск) клиентов, открытие счета. Банк «Тинькофф Кредитные Системы» Мы сократили среднее время обработки заявки с часов и дней до минут. При этом новая система позволяет легко настраивать вариации кредитной процедуры под специфические условия канала привлечения и другие факторы. Так, если раньше на то, чтобы внести малейшее изменение в бизнес-процесс, уходило несколько недель и требовались усилия ИТ-специалистов, то теперь сроки модификации бизнес-процесса измеряются днями (от идеи до запуска) и, в основном, осуществляются силами бизнес- пользователей МТС банк Результаты проекта Создана методология для описания бизнес-процессов Обучены сотрудники МТС- Банка Разработаны скрипты отчетности по моделям ARIS Вывод Первый вопрос: Стоит ли тратить время и деньги на реализацию BPM? Если вы банк или телеком – внедрять незамедлительно, скорость вывода новых продуктов – это важнейшее конкурентное преимущество. Остальным – считать эффект от внедрения в каждом конкретном случае. Второй вопрос: Внедрять ERP+BPM или что-то одно и если оба, то в какой последовательности? Для максимального эффекта внедрять оба и ROI считать в совокупности. Причем по мнению некоторых экспертов, имеет смысл внедрять сначала ERP и тогда BPM «упадет на взрыхленную плодородную почву». Но тем не менее, оба продукта могут существовать отдельно. PS: Я умышленно обошел стоимость внедрения, потому что эта тема требует отдельной статьи и проработки с указанием конкретных решений, плюсов и минусов. Если есть информация или соображения на этот счет – добро пожаловать в комменты.\n\n\nКраткое содержание:\nBPM (Business Process Management) и ERP (Enterprise Resource Planning) - это два основных подхода к управлению бизнесом. BPM фокусируется на процессном подходе, где каждый шаг процесса управляется и контролируется, в то время как ERP больше ориентирована на функциональную схему управления.\n\nПреимущества BPM включают:\n\n- Ориентированность на результат\n- Одна цель, один результат, один начальник - ответственный\n- Разгрузка руководителей высшего звена\n- Повышение операционной эффективности\n- Регламенты бизнес-процессов - новый персонал получает доступ в базу знаний, все действия и реакции описаны\n\nОднако, BPM система \"активна\" и раздает задания исполнителям, передает следующим исполнителям в цепочке, контролирует сроки. Каждый исполнитель знает, какой приоритет, в какие сроки нужно выполнить. BPM управляет бизнес-процессом.\n\nERP система автоматизирует бизнес-процессы, но по сути своей \"пассивна\" - она может ответить на любой вопрос, но только если знаешь как спросить. ERP система содержит внутри себя некий BPM движок - workflow маршрутизатор.\n\nЭффект BPM заключается в том, что бизнес-процессы становятся управляемыми, что позволяет оптимизировать процессы и сократить издержки и время выполнения.\n\nВ целом, BPM и ERP имеют свои преимущества и недостатки, и выбор между ними зависит от конкретных потребностей и целей компании.","summary":"BPM (Business Process Management) и ERP (Enterprise Resource Planning) - это два основных подхода к управлению бизнесом. BPM фокусируется на процессном подходе, где каждый шаг процесса управляется и контролируется, в то время как ERP больше ориентирована на функциональную схему управления.\n\nПреимущества BPM включают:\n\n- Ориентированность на результат\n- Одна цель, один результат, один начальник - ответственный\n- Разгрузка руководителей высшего звена\n- Повышение операционной эффективности\n- Регламенты бизнес-процессов - новый персонал получает доступ в базу знаний, все действия и реакции описаны\n\nОднако, BPM система \"активна\" и раздает задания исполнителям, передает следующим исполнителям в цепочке, контролирует сроки. Каждый исполнитель знает, какой приоритет, в какие сроки нужно выполнить. BPM управляет бизнес-процессом.\n\nERP система автоматизирует бизнес-процессы, но по сути своей \"пассивна\" - она может ответить на любой вопрос, но только если знаешь как спросить. ERP система содержит внутри себя некий BPM движок - workflow маршрутизатор.\n\nЭффект BPM заключается в том, что бизнес-процессы становятся управляемыми, что позволяет оптимизировать процессы и сократить издержки и время выполнения.\n\nВ целом, BPM и ERP имеют свои преимущества и недостатки, и выбор между ними зависит от конкретных потребностей и целей компании."}
{"Text":"Жизнь и смерть / Хабр                                                              \n\n24-10-2014\nТридцать спиц соединяются в одной ступице, [образуя колесо], но употребление колеса зависит от пустоты между [спицами]. Из глины делают сосуды, но употребление сосудов зависит от пустоты. Пробивают двери и окна, чтобы сделать дом, но пользование домом зависит от пустоты в нем. Вот почему полезность чего-либо имеющегося зависит от пустоты. Лао Цзы \"Дао дэ Цзин\" Камни в Го одинаковы и после установки на доску их нельзя двигать. Кажется, что это простая игра, но это не так. Простые правила ведут к невообразимому разнообразию тактических комбинаций. Проблематика «жизни и смерти» — хорошая тому иллюстрация. Не понимаешь ситё — не играй в Го Камни на доске не двигаются, но их можно снимать с доски целыми группами (и только таким образом). Для того чтобы «убить» группу, требуется её полностью окружить. Группа в Го состоит из произвольного количества соединенных между собой камней (возможно из одного камня). Соединенными считаются камни, расположенные на соседних пересечениях линий доски. Поскольку диагональных соединений в Го нет, на рисунке слева можно видеть группу из трёх черных камней, почти окруженную тремя белыми группами. Снизу эта группа ограничена краем доски, представляющим собой, своего рода, «неразрушимую» стену. Свободные пересечения, с которыми соседствует группа, называются «точками дыхания» (дамэ). Ситуация, в которой группа имеет всего одно дамэ, называется атари и является непосредственной угрозой группе. Если игрок, оказавшись в такой ситуации, не устранит угрозу, его противник, следующим ходом, может закрыть последнюю «точку дыхания» и убрать всю группу с доски. Если убитая группа достаточно велика, это может решить исход партии. Как можно устранить угрозу? Атакованный игрок может «удлиниться», сходив в последнее оставшееся дамэ. Если такой ход увеличит количество «точек дыхания» группы, непосредственная угроза будет устранена, но это еще не означает, что группа будет спасена. Этот тактический приём называется «лестница» (ситё). Попав в такое положение, неопытный игрок может попытаться спасти группу из двух камней, но будет снова и снова попадать в положение атари. С каждым ходом обречённая группа будет увеличиваться и угроза её потери будет становиться всё более и более весомой. В конце концов, «лестница» доберётся до края доски и вся группа будет потеряна. Но ситё — обоюдоострое оружие! В этой позиции, на пути «лестницы» оказался черный камень, который позволил группе выйти из окружения. Более того, после неудавшегося ситё, чёрные используют уязвимость позиции белых и ставят двойное атари (что-то вроде «вилки» в Шахматах или Рендзю). Белые могут защититься лишь от одной из угроз. Один из белых камней будет снят и чёрные вырвутся на свободу. Эстетика ситё часто используется в задачах Го. Вот прекрасный пример такой задачи, упомянутой в книге Миура Юсуюки \"Го и восточная бизнес-стратегия\": Отдай малое, возьми большое Часто, чтобы уничтожить большую группу камней противника, приходится жертвовать своими камнями. Примером может служить очень красивая позиция с поэтичным названием «Гнездо журавля». Действуя прямолинейно, чёрные позволяют группе белых вырваться из окружения. Правильной тактикой, в этой позиции, является жертва одного камня: Белые берут один камень, но теряют гораздо больше. Эта ситуация сходна с той, что возникла в ситё. Попав в безнадёжную ситуацию, чёрным следовало пожертвовать двумя камнями, не пытаясь «вытянуть» безнадёжную группу. Ситё является хорошей иллюстрацией одной из \"10 заповедей Го\", связываемых с именем Хонъимбо Сюсаку — \"Если грозит опасность, не задумываясь отдавай\" (ещё одна из «10 заповедей» вынесена в заголовок раздела). Группа с двумя глазами живёт, с одним — умирает Когда партия подходит к завершению и доска бывает почти заполнена, даже очень большие группы могут быть полностью окружены. Группа потерявшая все дамэ немедленно умрёт. Но что будет, если организовать «отдушину» внутри группы? В отличии от возможных (и как мы видели выше, зачастую оправданных) жертв, «самоубийства» в Го запрещены. Игрок, своим ходом, не может создать группу, лишённую дамэ (строго говоря, существуют варианты правил, разрешающие самоубийства групп, состоящих из двух камней и более, но самоубийственные ходы одиночными камнями запрещены безусловно). Такое дамэ, защищённое собственной группой, называется «глазом». Является ли наличие «глаза» гарантией «бессмертия» группы? Разумеется нет (это было бы слишком просто): Дело в том, что ход, приводящий к снятию камней противника, самоубийственным не считается (это правило называется «преимуществом атакующего»). Противник имеет право заполнить «глаз», лишив группу последнего дамэ (и тем самым убив её). Но как быть, если таких «глаз» два или больше? Такая группа живёт, даже будучи полностью окруженной! Противник не может заполнить «глаза» по очереди, поскольку каждый из таких ходов будет считаться самоубийственным. Разумеется, «глаз» имеет право закрыть сам игрок, но он этого делать не будет, если не хочет потерять группу. Попав в безнадёжное окружение, игроку стоит позаботиться о создании «глаз» внутри окружённой группы. Именно об этом говорит последняя из «10 заповедей» (\"Если безнадежно изолирован, избери мирный путь\"), но воплотить её на практике бывает непросто. Эти две группы мертвы, поскольку одно из дамэ «глазом» не является. Противник может сходить в него, взяв часть камней, после чего группа с одним «глазом» будет взята следующим ходом. Новички часто теряют группы своих камней, принимая ложный «глаз» за настоящий. К слову сказать, два «глаза», также, вполне могут принадлежать двум группам (такая позиция показана на рисунке в начале статьи). С ложными и истинными «глазами» разобраться непросто. Ещё сложнее бывают ситуации с определением жизнеспособности групп, содержащих территории, состоящие из нескольких дамэ (такие территории, внутри групп, также называются «глазами»). Часто, жизнь или смерть таких групп зависит от очерёдности хода и может быть обеспечена лишь правильной последовательностью ходов. Задачи (цумэ-го) по тематике «жизни и смерти» очень полезны для развития, поскольку подобные ситуации встречаются в реальных партиях очень часто. Одним из лучших, на мой взгляд, задачников, по этой тематике, является \"Энциклопедия жизни и смерти\" за авторством одного из сильнейших игроков современности Тё Тикуна. Её не требуется переводить на разные языки, поскольку в ней очень мало текста. Используя лишь различные позиции Го, автор последовательно ведёт читателя от простого к сложному, вводя всё новые и новые концепции. Во всех задачах из этой книги, первыми ходят чёрные. В зависимости от задачи (как правило это легко определить), их цель либо построить живую форму (с двумя глазами) либо не дать сделать этого белым. Мне, как новичку, было тяжело решать некоторые из этих задач в уме (сейчас уже стало легче) и я начал переводить их в SGF-формат, чтобы иметь возможность проиграть все варианты на доске. Желающие, могут найти файлы здесь (разумеется, без решений), впрочем, я не советую ими злоупотреблять. Более полезно руководствоваться в решении задач следующим подходом.\n","metric":0.45629882809999999793,"metric_input":"Текст статьи:\nЖизнь и смерть / Хабр                                                              \n\n24-10-2014\nТридцать спиц соединяются в одной ступице, [образуя колесо], но употребление колеса зависит от пустоты между [спицами]. Из глины делают сосуды, но употребление сосудов зависит от пустоты. Пробивают двери и окна, чтобы сделать дом, но пользование домом зависит от пустоты в нем. Вот почему полезность чего-либо имеющегося зависит от пустоты. Лао Цзы \"Дао дэ Цзин\" Камни в Го одинаковы и после установки на доску их нельзя двигать. Кажется, что это простая игра, но это не так. Простые правила ведут к невообразимому разнообразию тактических комбинаций. Проблематика «жизни и смерти» — хорошая тому иллюстрация. Не понимаешь ситё — не играй в Го Камни на доске не двигаются, но их можно снимать с доски целыми группами (и только таким образом). Для того чтобы «убить» группу, требуется её полностью окружить. Группа в Го состоит из произвольного количества соединенных между собой камней (возможно из одного камня). Соединенными считаются камни, расположенные на соседних пересечениях линий доски. Поскольку диагональных соединений в Го нет, на рисунке слева можно видеть группу из трёх черных камней, почти окруженную тремя белыми группами. Снизу эта группа ограничена краем доски, представляющим собой, своего рода, «неразрушимую» стену. Свободные пересечения, с которыми соседствует группа, называются «точками дыхания» (дамэ). Ситуация, в которой группа имеет всего одно дамэ, называется атари и является непосредственной угрозой группе. Если игрок, оказавшись в такой ситуации, не устранит угрозу, его противник, следующим ходом, может закрыть последнюю «точку дыхания» и убрать всю группу с доски. Если убитая группа достаточно велика, это может решить исход партии. Как можно устранить угрозу? Атакованный игрок может «удлиниться», сходив в последнее оставшееся дамэ. Если такой ход увеличит количество «точек дыхания» группы, непосредственная угроза будет устранена, но это еще не означает, что группа будет спасена. Этот тактический приём называется «лестница» (ситё). Попав в такое положение, неопытный игрок может попытаться спасти группу из двух камней, но будет снова и снова попадать в положение атари. С каждым ходом обречённая группа будет увеличиваться и угроза её потери будет становиться всё более и более весомой. В конце концов, «лестница» доберётся до края доски и вся группа будет потеряна. Но ситё — обоюдоострое оружие! В этой позиции, на пути «лестницы» оказался черный камень, который позволил группе выйти из окружения. Более того, после неудавшегося ситё, чёрные используют уязвимость позиции белых и ставят двойное атари (что-то вроде «вилки» в Шахматах или Рендзю). Белые могут защититься лишь от одной из угроз. Один из белых камней будет снят и чёрные вырвутся на свободу. Эстетика ситё часто используется в задачах Го. Вот прекрасный пример такой задачи, упомянутой в книге Миура Юсуюки \"Го и восточная бизнес-стратегия\": Отдай малое, возьми большое Часто, чтобы уничтожить большую группу камней противника, приходится жертвовать своими камнями. Примером может служить очень красивая позиция с поэтичным названием «Гнездо журавля». Действуя прямолинейно, чёрные позволяют группе белых вырваться из окружения. Правильной тактикой, в этой позиции, является жертва одного камня: Белые берут один камень, но теряют гораздо больше. Эта ситуация сходна с той, что возникла в ситё. Попав в безнадёжную ситуацию, чёрным следовало пожертвовать двумя камнями, не пытаясь «вытянуть» безнадёжную группу. Ситё является хорошей иллюстрацией одной из \"10 заповедей Го\", связываемых с именем Хонъимбо Сюсаку — \"Если грозит опасность, не задумываясь отдавай\" (ещё одна из «10 заповедей» вынесена в заголовок раздела). Группа с двумя глазами живёт, с одним — умирает Когда партия подходит к завершению и доска бывает почти заполнена, даже очень большие группы могут быть полностью окружены. Группа потерявшая все дамэ немедленно умрёт. Но что будет, если организовать «отдушину» внутри группы? В отличии от возможных (и как мы видели выше, зачастую оправданных) жертв, «самоубийства» в Го запрещены. Игрок, своим ходом, не может создать группу, лишённую дамэ (строго говоря, существуют варианты правил, разрешающие самоубийства групп, состоящих из двух камней и более, но самоубийственные ходы одиночными камнями запрещены безусловно). Такое дамэ, защищённое собственной группой, называется «глазом». Является ли наличие «глаза» гарантией «бессмертия» группы? Разумеется нет (это было бы слишком просто): Дело в том, что ход, приводящий к снятию камней противника, самоубийственным не считается (это правило называется «преимуществом атакующего»). Противник имеет право заполнить «глаз», лишив группу последнего дамэ (и тем самым убив её). Но как быть, если таких «глаз» два или больше? Такая группа живёт, даже будучи полностью окруженной! Противник не может заполнить «глаза» по очереди, поскольку каждый из таких ходов будет считаться самоубийственным. Разумеется, «глаз» имеет право закрыть сам игрок, но он этого делать не будет, если не хочет потерять группу. Попав в безнадёжное окружение, игроку стоит позаботиться о создании «глаз» внутри окружённой группы. Именно об этом говорит последняя из «10 заповедей» (\"Если безнадежно изолирован, избери мирный путь\"), но воплотить её на практике бывает непросто. Эти две группы мертвы, поскольку одно из дамэ «глазом» не является. Противник может сходить в него, взяв часть камней, после чего группа с одним «глазом» будет взята следующим ходом. Новички часто теряют группы своих камней, принимая ложный «глаз» за настоящий. К слову сказать, два «глаза», также, вполне могут принадлежать двум группам (такая позиция показана на рисунке в начале статьи). С ложными и истинными «глазами» разобраться непросто. Ещё сложнее бывают ситуации с определением жизнеспособности групп, содержащих территории, состоящие из нескольких дамэ (такие территории, внутри групп, также называются «глазами»). Часто, жизнь или смерть таких групп зависит от очерёдности хода и может быть обеспечена лишь правильной последовательностью ходов. Задачи (цумэ-го) по тематике «жизни и смерти» очень полезны для развития, поскольку подобные ситуации встречаются в реальных партиях очень часто. Одним из лучших, на мой взгляд, задачников, по этой тематике, является \"Энциклопедия жизни и смерти\" за авторством одного из сильнейших игроков современности Тё Тикуна. Её не требуется переводить на разные языки, поскольку в ней очень мало текста. Используя лишь различные позиции Го, автор последовательно ведёт читателя от простого к сложному, вводя всё новые и новые концепции. Во всех задачах из этой книги, первыми ходят чёрные. В зависимости от задачи (как правило это легко определить), их цель либо построить живую форму (с двумя глазами) либо не дать сделать этого белым. Мне, как новичку, было тяжело решать некоторые из этих задач в уме (сейчас уже стало легче) и я начал переводить их в SGF-формат, чтобы иметь возможность проиграть все варианты на доске. Желающие, могут найти файлы здесь (разумеется, без решений), впрочем, я не советую ими злоупотреблять. Более полезно руководствоваться в решении задач следующим подходом.\n\n\nКраткое содержание:\nСтатья обсуждает концепцию \"жизни и смерти\" в игре Го, которая является стратегической настольной игрой, в которой игроки пытаются окружить и уничтожить группы камней противника. Статья объясняет различные тактики и стратегии, используемые в Го для устранения угрозы и спасения групп камней. Она также обсуждает концепцию \"глаз\" - защищенных дамэ внутри группы, которые могут обеспечить \"бессмертие\" группы. Статья также рекомендует задачи Го как эффективный способ обучения и развития навыков игры.","summary":"Статья обсуждает концепцию \"жизни и смерти\" в игре Го, которая является стратегической настольной игрой, в которой игроки пытаются окружить и уничтожить группы камней противника. Статья объясняет различные тактики и стратегии, используемые в Го для устранения угрозы и спасения групп камней. Она также обсуждает концепцию \"глаз\" - защищенных дамэ внутри группы, которые могут обеспечить \"бессмертие\" группы. Статья также рекомендует задачи Го как эффективный способ обучения и развития навыков игры."}
{"Text":"Решения для систем видеонаблюдения нуждаются в стратегии корпоративного уровня / Хабр                 \n\n02-02-2016\nКоличество и разнообразие устройств видеонаблюдения растёт, а вместе с ним растёт количество собираемых данных. Хранение этих данных становится важным компонентом любой стратегии видеонаблюдения, и от этого для многих предприятий зависит выбор стратегии безопасности. Рынок средств видеонаблюдения переживает бурный рост. Вихреобразно увеличивается разнообразие и количество устройств, возрастают периоды хранения данных, спрос на аналитику угроз – при том, что бюджеты остаются ограниченными. По прогнозу исследовательской фирмы IHS при сохранении скорости роста (вдвое превышающей скорость роста рынка традиционного аппаратных средств рынка ИТ) к 2018 году рынок видеонаблюдения в сочетании с рынком корпоративных систем хранения данных будет стоить более $ 25 млрд. Устройства наблюдения можно найти везде: от аэропортов до банков, от розничных магазинов до школ и хайвэев; на углах улиц, во дворах резиденций и даже на самих людях. Трудно не угодить где-нибудь на запись в течение дня. В одних только Соединенных Штатах насчитывают на каждые 10 человек одну камеру с замкнутой телесистемой. Но камеры являются лишь верхушкой айсберга. Чтобы успевать за ростом угроз, необходимо наращивать комплексную инфраструктуру безопасности, что ведёт к расширению и номенклатуры, и количества устройств. Например, по мере того как камеры появляются во всё большем числе мест, количество собранной видео-информации, которую необходимо хранить, растёт в геометрической прогрессии. Две недели запоминания видео требует примерно 18,4 ГБ памяти. Аналитики прогнозируют, что к 2020 году накопится примерно 3,3 млрд. часов запомненного видеонаблюдения. Кроме того, что растёт объём запомненных данных, номенклатура устройств также растет за счёт таких типов, как нательные камеры, камеры беспилотных летательных аппаратов, автомобильные видеорегистраторы, видеонаблюдение, спутниковое наблюдение, съёмка номерного знака, аудио-регистрация и т.п. Организации также соревнуются в модернизации: вводят IP-камеры и камеры с высоким разрешением, а также добиваются хранения данных в течение более длительных периодов времени. В совокупности все эти факторы подпитывают «видео-вихрь». В конечном счете, в результате этого роста достигается не только повышение безопасности. По мере развития устройств наблюдения и технологий аналитики собираемые данные могут также быть использованы рентабельно, например, для предотвращения убытков и в задачах бизнес-аналитики. Однако, при достижении этого уровня развития неизбежно появление новых задач, среди которых управления ростом – это первая в ряду. Чтобы обеспечить это управление, работники ИТ должны изменить своё восприятие данных наблюдения и начать использовать их таким же образом, как другие корпоративные приложения, а не продолжать следовать устаревшему подходу к контролю и управлению данными наблюдения на чисто техническом уровне. Традиционной модели работы, трактующей любое покупное устройство как чёрный ящик или нечто, что можно только целиком «вырвать с корнем и заменить» (rip-and-replace) должна прийти на смену открытая, гибкая корпоративная модель (сети, вычислительных процессов, хранения, виртуализации и безопасности). Эта инфраструктура должна обеспечивать устойчивую архитектуру открытого, гибкого и надежного типа, снабжённую встроенными функциями. Этот переход от модели прибора к модели предприятия гармонирует с большее широкой тенденцией – сближения физических систем безопасности ИТ и систем безопасности посредством ИТ. ИТ должна быть задействована в работе физической системы безопасности, потому что она несёт сведения об инфраструктуре. В то же время, персонал безопасности должен хорошо разбираться в ИТ, чтобы лучше понять технологические возможности своих систем. Это означает, что система видеорегистрации должна быть включена в инфраструктуру хранения данных, которая может хранить видеозаписи таким способом, который соответствует техническим требованиям инфраструктуры. Например, она должна поддерживать видео высокого разрешения от мегапиксельных камер и иметь возможность записать более высокую частоту кадров без потери данных. Она также должна быть масштабируемой, чтобы дать место потенциальным мощностям; располагать вариантами обеспечения безопасности, чтобы предотвратить удаление или изменение файлов; быть в состоянии управлять всем жизненным циклом данных. Эта задача решается путем комбинирования архитектур хранения данных. Большинство современных архитектур наблюдения следуют распределенной модели с несколькими сотнями устройств видеонаблюдения, записывающих видео и другие данные локально, как правило, на начальном этапе рядом с камерой. После энного периода времени видео, которое должно быть сохранено, перемещается в централизованную СХД. Другие фрагменты видео удаляются в распределённой структуре, когда срок хранения заканчивается. Управление хранением как распределенных, так и централизованных данных создаёт многочисленные проблемы и настоятельно требует целостного подхода к системе хранения. С переходом на распределенные архитектуры, многие решения видеонаблюдения сместили функциональность и обработку данных на периферию сети, ближе к камерам и видеозаписи. Часто эти системы используют получаемые данные в других приложениях или системах, например, контроля доступа или обнаружения взлома без оглядки на централизованное хранение. Это снижает риски, возникающие в связи с наличием единой критической точки, и распределяет нагрузку обработки. Для многих предприятий гибридная модель, которая использует свойства распределенных и централизованных архитектурных подходов, является идеальной. В конце концов, не все данные должны находиться на периферии, и, конечно, не вечно. В свое время видеоданные, предназначенные для долгосрочного хранения, перемещается в центральное хранилище. Для централизованного хранения вся проблема заключается в масштабе. Централизованные архитектуры наблюдения предназначены для управления средами с большим числом камер или других устройств, где часто задействованы тысячи наблюдательных устройств, которые выдают крупные объёмы данных наблюдения. Централизованное хранение данных наблюдения должно быть эффективным и поддерживать высокий уровень загрузки, чтобы оправдать вложенные средства. Кроме того, время перехода должно быть чрезвычайно низким, чтобы изменения разрешения или пикселизации можно было вносить плавно. Как EMC решает эту проблему с помощью VNX-VSS Предлагаемый корпорацией EMC сбалансированный комплекс решений в сфере видеонаблюдения как раз нацелен на эти проблемы. Модель EMC сочетает распределенный и централизованный архитектурный подход. Система хранения EMC VNX-VSS, будучи специально построена и настроена для широко распределенных сред видеонаблюдения и преимущественно используема в распределенных структурах, допускает корпоративный подход к инфраструктуре наблюдения. Следовательно, хотя VNX-VSS выполнена по простому принципу «plug-and-play», т.е. поставляется заранее настроенной и готовой к немедленному развертыванию, её возможности выходят далеко за рамки, предполагаемые такой моделью. Данное решение корпоративного класса обеспечивает беспрецедентную производительность и надежность: проверено, что его уровень доступности составляет 99,999% («пять девяток»); имеются возможности удаленного управления; предусмотрены 50 проверок решений наблюдения. Возможности передачи видео VNX -VSS обеспечивают совместимость, оптимизирующую передачу на расстояние, позволяя организациям передавать видеозаписи с периферии хранения в централизованное хранилище для поддержания многоуровневых данных, а также осуществлять интеллектуальное исследование или анализ данных наблюдения (напр., распознавание лица и движения, цвета, стереотипы поведения покупателей, распознание лицензионной записи). VNX-VSS полностью совместима со всеми компонентами экосистемы видеонаблюдения, что снижает риски развертывания и эксплуатации и делает его легким для предприятий. Благодаря этому пользователь может легко переключаться между различными партнерами. Комплекс VNX-VSS поддерживает камеры всех ведущих производителей и имеет лицензированное ПО для управления видео таких компаний, как Genetec, Milestone, Verint, Honeywell и Bosch. VNX-VSS – чрезвычайно масштабируемая система. Она поставляется в стартовых конфигурация 24 ТБ и 120 ТБ и может расширяться до 480 ТБ для удовлетворения потребностей в наращивании потенциала самых требовательных распределенных структур. Кроме того, VNX-VSS оптимизирована для виртуализации. Управление видеозаписями повсеместно становится все более виртуализированным, а любое решение для хранения данных должно быть в состоянии соответствовать этому тренду. Системы VNX-VSS предлагают многочисленные точки интеграции для VMware VSphere и Microsoft Hyper-V. Успешно развёртываются VAAI и VASA с VMware и ODX и SCO с Microsoft, обеспечивающие приложения виртуализированного управления видео. Для тех предприятий с централизованным хранением наблюдения, чьи потребности VNX- VSS не может удовлетворить, EMC Isilon Video Surveillance Solutions предлагает массивную внемасштабную систему хранения на уровне ядра сети. Вместе эти два решения могут поддерживать больше сетевых камер на периферии, увеличить количество часов наблюдения, сохранять видео высокого разрешения и сохранить контент на периферии в течение более длительных периодов. По вопросам обращаться: emc@muk.ua. Стоит отметить, что решения EMC через группу компаний доступны теперь в Молдове, Грузии, Азербайджане и Казахстане – недавно на территории этих стран был подписан дистрибуторский контракт. МУК-Сервис — все виды ИТ ремонта: гарантийный, не гарантийный ремонт, продажа запасных частей, контрактное обслуживание\n","metric":0.4208984375,"metric_input":"Текст статьи:\nРешения для систем видеонаблюдения нуждаются в стратегии корпоративного уровня / Хабр                 \n\n02-02-2016\nКоличество и разнообразие устройств видеонаблюдения растёт, а вместе с ним растёт количество собираемых данных. Хранение этих данных становится важным компонентом любой стратегии видеонаблюдения, и от этого для многих предприятий зависит выбор стратегии безопасности. Рынок средств видеонаблюдения переживает бурный рост. Вихреобразно увеличивается разнообразие и количество устройств, возрастают периоды хранения данных, спрос на аналитику угроз – при том, что бюджеты остаются ограниченными. По прогнозу исследовательской фирмы IHS при сохранении скорости роста (вдвое превышающей скорость роста рынка традиционного аппаратных средств рынка ИТ) к 2018 году рынок видеонаблюдения в сочетании с рынком корпоративных систем хранения данных будет стоить более $ 25 млрд. Устройства наблюдения можно найти везде: от аэропортов до банков, от розничных магазинов до школ и хайвэев; на углах улиц, во дворах резиденций и даже на самих людях. Трудно не угодить где-нибудь на запись в течение дня. В одних только Соединенных Штатах насчитывают на каждые 10 человек одну камеру с замкнутой телесистемой. Но камеры являются лишь верхушкой айсберга. Чтобы успевать за ростом угроз, необходимо наращивать комплексную инфраструктуру безопасности, что ведёт к расширению и номенклатуры, и количества устройств. Например, по мере того как камеры появляются во всё большем числе мест, количество собранной видео-информации, которую необходимо хранить, растёт в геометрической прогрессии. Две недели запоминания видео требует примерно 18,4 ГБ памяти. Аналитики прогнозируют, что к 2020 году накопится примерно 3,3 млрд. часов запомненного видеонаблюдения. Кроме того, что растёт объём запомненных данных, номенклатура устройств также растет за счёт таких типов, как нательные камеры, камеры беспилотных летательных аппаратов, автомобильные видеорегистраторы, видеонаблюдение, спутниковое наблюдение, съёмка номерного знака, аудио-регистрация и т.п. Организации также соревнуются в модернизации: вводят IP-камеры и камеры с высоким разрешением, а также добиваются хранения данных в течение более длительных периодов времени. В совокупности все эти факторы подпитывают «видео-вихрь». В конечном счете, в результате этого роста достигается не только повышение безопасности. По мере развития устройств наблюдения и технологий аналитики собираемые данные могут также быть использованы рентабельно, например, для предотвращения убытков и в задачах бизнес-аналитики. Однако, при достижении этого уровня развития неизбежно появление новых задач, среди которых управления ростом – это первая в ряду. Чтобы обеспечить это управление, работники ИТ должны изменить своё восприятие данных наблюдения и начать использовать их таким же образом, как другие корпоративные приложения, а не продолжать следовать устаревшему подходу к контролю и управлению данными наблюдения на чисто техническом уровне. Традиционной модели работы, трактующей любое покупное устройство как чёрный ящик или нечто, что можно только целиком «вырвать с корнем и заменить» (rip-and-replace) должна прийти на смену открытая, гибкая корпоративная модель (сети, вычислительных процессов, хранения, виртуализации и безопасности). Эта инфраструктура должна обеспечивать устойчивую архитектуру открытого, гибкого и надежного типа, снабжённую встроенными функциями. Этот переход от модели прибора к модели предприятия гармонирует с большее широкой тенденцией – сближения физических систем безопасности ИТ и систем безопасности посредством ИТ. ИТ должна быть задействована в работе физической системы безопасности, потому что она несёт сведения об инфраструктуре. В то же время, персонал безопасности должен хорошо разбираться в ИТ, чтобы лучше понять технологические возможности своих систем. Это означает, что система видеорегистрации должна быть включена в инфраструктуру хранения данных, которая может хранить видеозаписи таким способом, который соответствует техническим требованиям инфраструктуры. Например, она должна поддерживать видео высокого разрешения от мегапиксельных камер и иметь возможность записать более высокую частоту кадров без потери данных. Она также должна быть масштабируемой, чтобы дать место потенциальным мощностям; располагать вариантами обеспечения безопасности, чтобы предотвратить удаление или изменение файлов; быть в состоянии управлять всем жизненным циклом данных. Эта задача решается путем комбинирования архитектур хранения данных. Большинство современных архитектур наблюдения следуют распределенной модели с несколькими сотнями устройств видеонаблюдения, записывающих видео и другие данные локально, как правило, на начальном этапе рядом с камерой. После энного периода времени видео, которое должно быть сохранено, перемещается в централизованную СХД. Другие фрагменты видео удаляются в распределённой структуре, когда срок хранения заканчивается. Управление хранением как распределенных, так и централизованных данных создаёт многочисленные проблемы и настоятельно требует целостного подхода к системе хранения. С переходом на распределенные архитектуры, многие решения видеонаблюдения сместили функциональность и обработку данных на периферию сети, ближе к камерам и видеозаписи. Часто эти системы используют получаемые данные в других приложениях или системах, например, контроля доступа или обнаружения взлома без оглядки на централизованное хранение. Это снижает риски, возникающие в связи с наличием единой критической точки, и распределяет нагрузку обработки. Для многих предприятий гибридная модель, которая использует свойства распределенных и централизованных архитектурных подходов, является идеальной. В конце концов, не все данные должны находиться на периферии, и, конечно, не вечно. В свое время видеоданные, предназначенные для долгосрочного хранения, перемещается в центральное хранилище. Для централизованного хранения вся проблема заключается в масштабе. Централизованные архитектуры наблюдения предназначены для управления средами с большим числом камер или других устройств, где часто задействованы тысячи наблюдательных устройств, которые выдают крупные объёмы данных наблюдения. Централизованное хранение данных наблюдения должно быть эффективным и поддерживать высокий уровень загрузки, чтобы оправдать вложенные средства. Кроме того, время перехода должно быть чрезвычайно низким, чтобы изменения разрешения или пикселизации можно было вносить плавно. Как EMC решает эту проблему с помощью VNX-VSS Предлагаемый корпорацией EMC сбалансированный комплекс решений в сфере видеонаблюдения как раз нацелен на эти проблемы. Модель EMC сочетает распределенный и централизованный архитектурный подход. Система хранения EMC VNX-VSS, будучи специально построена и настроена для широко распределенных сред видеонаблюдения и преимущественно используема в распределенных структурах, допускает корпоративный подход к инфраструктуре наблюдения. Следовательно, хотя VNX-VSS выполнена по простому принципу «plug-and-play», т.е. поставляется заранее настроенной и готовой к немедленному развертыванию, её возможности выходят далеко за рамки, предполагаемые такой моделью. Данное решение корпоративного класса обеспечивает беспрецедентную производительность и надежность: проверено, что его уровень доступности составляет 99,999% («пять девяток»); имеются возможности удаленного управления; предусмотрены 50 проверок решений наблюдения. Возможности передачи видео VNX -VSS обеспечивают совместимость, оптимизирующую передачу на расстояние, позволяя организациям передавать видеозаписи с периферии хранения в централизованное хранилище для поддержания многоуровневых данных, а также осуществлять интеллектуальное исследование или анализ данных наблюдения (напр., распознавание лица и движения, цвета, стереотипы поведения покупателей, распознание лицензионной записи). VNX-VSS полностью совместима со всеми компонентами экосистемы видеонаблюдения, что снижает риски развертывания и эксплуатации и делает его легким для предприятий. Благодаря этому пользователь может легко переключаться между различными партнерами. Комплекс VNX-VSS поддерживает камеры всех ведущих производителей и имеет лицензированное ПО для управления видео таких компаний, как Genetec, Milestone, Verint, Honeywell и Bosch. VNX-VSS – чрезвычайно масштабируемая система. Она поставляется в стартовых конфигурация 24 ТБ и 120 ТБ и может расширяться до 480 ТБ для удовлетворения потребностей в наращивании потенциала самых требовательных распределенных структур. Кроме того, VNX-VSS оптимизирована для виртуализации. Управление видеозаписями повсеместно становится все более виртуализированным, а любое решение для хранения данных должно быть в состоянии соответствовать этому тренду. Системы VNX-VSS предлагают многочисленные точки интеграции для VMware VSphere и Microsoft Hyper-V. Успешно развёртываются VAAI и VASA с VMware и ODX и SCO с Microsoft, обеспечивающие приложения виртуализированного управления видео. Для тех предприятий с централизованным хранением наблюдения, чьи потребности VNX- VSS не может удовлетворить, EMC Isilon Video Surveillance Solutions предлагает массивную внемасштабную систему хранения на уровне ядра сети. Вместе эти два решения могут поддерживать больше сетевых камер на периферии, увеличить количество часов наблюдения, сохранять видео высокого разрешения и сохранить контент на периферии в течение более длительных периодов. По вопросам обращаться: emc@muk.ua. Стоит отметить, что решения EMC через группу компаний доступны теперь в Молдове, Грузии, Азербайджане и Казахстане – недавно на территории этих стран был подписан дистрибуторский контракт. МУК-Сервис — все виды ИТ ремонта: гарантийный, не гарантийный ремонт, продажа запасных частей, контрактное обслуживание\n\n\nКраткое содержание:\nРешения для систем видеонаблюдения нуждаются в стратегии корпоративного уровня.\n\nРынок средств видеонаблюдения переживает бурный рост.\n\nУстройства наблюдения можно найти везде: от аэропортов до банков, от розничных магазинов до школ и хайвэев.\n\nОрганизации также соревнуются в модернизации: вводят IP-камеры и камеры с высоким разрешением.\n\nИТ должна быть задействована в работе физической системы безопасности, потому что она несёт сведения об инфраструктуре.\n\nПерсонал безопасности должен хорошо разбираться в ИТ, чтобы лучше понять технологические возможности своих систем.\n\nСистема видеонаблюдения должна быть включена в инфраструктуру хранения данных.\n\nОна должна поддерживать видео высокого разрешения от мегапиксельных камер и иметь возможность записать более высокую частоту кадров без потери данных.\n\nОна также должна быть масштабируемой, чтобы дать место потенциальным мощностям; располагать вариантами обеспечения безопасности, чтобы предотвратить удаление или изменение файлов; быть в состоянии управлять всем жизненным циклом данных.\n\nДля централизованного хранения данных наблюдения предназначена система EMC VNX-VSS.\n\nVNX-VSS полностью совместима со всеми компонентами экосистемы видеонаблюдения, что снижает риски развертывания и эксплуатации и делает его легким для предприятий.\n\nКомплекс VNX-VSS поддерживает камеры всех ведущих производителей и имеет лицензированное ПО для управления видео таких компаний, как Genetec, Milestone, Verint, Honeywell и Bosch.\n\nVNX-VSS – чрезвычайно масштабируемая система. Она поставляется в стартовых конфигурация 24 ТБ и 120 ТБ и может расширяться до 480 ТБ для удовлетворения потребностей в наращивании потенциала самых требовательных распределенных структур.\n\nКроме того, VNX-VSS оптимизирована для виртуализации.\n\nВместе эти два решения могут поддерживать больше сетевых камер на периферии, увеличить количество часов наблюдения, сохранять видео высокого разрешения и сохранить контент на периферии в течение более длительных периодов.","summary":"Решения для систем видеонаблюдения нуждаются в стратегии корпоративного уровня.\n\nРынок средств видеонаблюдения переживает бурный рост.\n\nУстройства наблюдения можно найти везде: от аэропортов до банков, от розничных магазинов до школ и хайвэев.\n\nОрганизации также соревнуются в модернизации: вводят IP-камеры и камеры с высоким разрешением.\n\nИТ должна быть задействована в работе физической системы безопасности, потому что она несёт сведения об инфраструктуре.\n\nПерсонал безопасности должен хорошо разбираться в ИТ, чтобы лучше понять технологические возможности своих систем.\n\nСистема видеонаблюдения должна быть включена в инфраструктуру хранения данных.\n\nОна должна поддерживать видео высокого разрешения от мегапиксельных камер и иметь возможность записать более высокую частоту кадров без потери данных.\n\nОна также должна быть масштабируемой, чтобы дать место потенциальным мощностям; располагать вариантами обеспечения безопасности, чтобы предотвратить удаление или изменение файлов; быть в состоянии управлять всем жизненным циклом данных.\n\nДля централизованного хранения данных наблюдения предназначена система EMC VNX-VSS.\n\nVNX-VSS полностью совместима со всеми компонентами экосистемы видеонаблюдения, что снижает риски развертывания и эксплуатации и делает его легким для предприятий.\n\nКомплекс VNX-VSS поддерживает камеры всех ведущих производителей и имеет лицензированное ПО для управления видео таких компаний, как Genetec, Milestone, Verint, Honeywell и Bosch.\n\nVNX-VSS – чрезвычайно масштабируемая система. Она поставляется в стартовых конфигурация 24 ТБ и 120 ТБ и может расширяться до 480 ТБ для удовлетворения потребностей в наращивании потенциала самых требовательных распределенных структур.\n\nКроме того, VNX-VSS оптимизирована для виртуализации.\n\nВместе эти два решения могут поддерживать больше сетевых камер на периферии, увеличить количество часов наблюдения, сохранять видео высокого разрешения и сохранить контент на периферии в течение более длительных периодов."}
{"Text":"Tiny11: «похудевшая» Windows 11. Действительно ли она в состоянии работать на старых ПК? Часть 1 / Хабр                                                         \n\nНесколько дней назад на Хабре\nпоявилась новость\nо том, что сторонние разработчики выпустили «быструю» версию Windows 11 Pro 22H2. В отличие от оригинала, ей требуется всего лишь 2 ГБ ОЗУ и 8 ГБ места на рабочем диске. Вообще не похоже на требования современной ОС, правда?\n\nТе, кто уже опробовал эту версию ОС, говорят, что она действительно справляется с ограниченным объемом ресурсов, и неплохо. И это при том, что оригинальной версии Windows 11 требуется 4 ГБ ОЗУ и 64 ГБ свободного места на диске. Как и почему все это работает? Об этом под катом. И да, сразу скажем, что собираемся протестировать сборку на Sony Vaio VGN-P11ZR из 2009 года. Там как раз соответствующие характеристики.\n\n### Что это за система такая? ###\n\nЭто модификация Windows 11, которую собрала команда разработчиков NTDEV. По словам разработчиков, из основной ОС вырезаны многие элементы, без которых можно обойтись. В итоге получился дистрибутив, который способен работать на действительно старых ПК и ноутбуках.\n\nВ том его кардинальное отличие от родительской системы, которую часто называют самой требовательной ОС от Microsoft за всю историю этой операционной системы.\n\n### Как создали Tiny 11? ###\n\nЗа ее основу взяли дистрибутив Windows 11 Pro версии 22Н2 образца сентября 2022 г., который разработчики легально загрузили с сайта корпорации Microsoft. Для того, чтобы создать свою версию Windows 11, команда энтузиастов убрала множество элементов, без которых можно было обойтись. Среди них — браузер Edge, про который говорят, что его используют лишь для того, чтобы загрузить другой браузер после установки Windows. На текущий момент его доля рынка составляет всего 4,46%.\n\nВсе бы хорошо, но его отсутствие означает, что при установке нового дистрибутива придется использовать установочный пакет нужного пользователю браузера. Конечно, в том есть определенное неудобство, но минимализм требует жертв.\n\nЧто касается стандартных приложений Windows 11, то из них осталось немногое: блокнот (без него в любом случае никуда), калькулятор, плюс специальные возможности. Естественно, разработчики оставили еще и Microsoft Store, плюс Paint (ну как без него обойдешься, правда?).\n\nСообщается, что разработчики новой версии Windows 11 решили отказаться от хранилища компонентов (WinSxS). Соответственно, установить нечто вроде подсистемы Windows для Linux (WSL) не получится. Но зато в tiny11 можно установить NodeJS и Docker (на основе WSL2 вместо Hyper-V).\n\nПлюс этого дистрибутива в том, что он не требует создания нового профиля, при установке можно обойтись локальной учеткой без регистрации и доступа к интернету.\n\n### Так это пиратка, что ли? ###\n\nНет, по словам разработчиков, эта версия вполне легальна. Во всяком случае, после установки ее будет нужно активировать. Для ее полноценной работы придется использовать лицензионный ключ.\n\nПонятно, что разработчики могут заявлять что угодно, но поставщик оригинальной операционной системы, корпорация Microsoft, своего мнения пока по поводу новой версии Windows 11 не высказала. Возможно, корпорация просто пропустит это событие мимо внимания — для нее оно не очень значительно.\n\nНу а команда разработчиков выложила дистрибутив на разных торрент ресурсах, плюс его можно загрузить и с других источников. Кстати, есть и другой подобный вариант ОС от Microsoft той же команды — речь идет про облегченную версию Windows 10. Многие пользователи об этом и не слышали, но в любом случае, попробовать стоит.\n\n### Ну хорошо, так эта ОС действительно может работать на старых ПК? ###\n\nМы думаем что да, но решили испытать дистрибутив самостоятельно. В прошлом году\nмы публиковали статью\nпро ультрабук Sony Vaio VGN-P11ZR из 2009 года. Почему? Ну как раз потому, что у него всего 2 ГБ ОЗУ и 60 ГБ — жесткий диск. Вот полные характеристики устройства:\n\n• Операционная система: Windows Vista Home Premium\n\n• Процессор: Intel Atom Z520 1.33 ГГц. Объем кэша L2 512 КБ. Частота системной шины 533 МГц.\n\n• Память: 2 ГБ DDR2 533 МГц\n\n• Экран: 8 дюймов, 1600x768, широкоформатный\n\n• Видеопроцессор: Intel GMA 500\n\n• Накопитель: HDD объёмом в 60 ГБ, ATA100, 4200 об/мин.\n\n• LAN/Modem: сетевая карта\n\n• Беспроводная связь: Wi-Fi IEEE 802.11g, Bluetooth, 3G (UMTS)\n\n• Интерфейсы: USB 2.0 Type A x 2, интерфейс док-станции\n\n• Слоты: Устройство для чтения флэш-карт Memory Stick, SD\n\n• Питание: Li-Ion аккумулятор\n\n• Устройства позиционирования: PointStick\n\n• Звук: Встроенные колонки, Встроенный микрофон\n\n• Веб-камера: есть\n\n• Размеры (ДхШхТ): 245x120x19.8 мм\n\n• Вес: 0.6 кг\n\nУ нашей команды после публикации поста про этот девайс была идея провести с устройством в офисе и дома какое-то время, чтобы понять, насколько он практичен в наше время. И сейчас, похоже, пришло время это реализовать.\n\nКонечно, даже с оптимизированной ОС сложно ожидать чего-то выдающегося от устройства, которое появилось в продаже 14 лет назад. Но попробовать стоит — хотя бы в качестве офисной «рабочей лошадки». В первую очередь это набор текста и коммуникации по сети. Собираемся задействовать девайс в еженедельных совещаниях, плюс 1-2 статьи набрать и опубликовать с его помощью.\n\nПлюс девайса — портативность. К слову, его родное ПО, включая офисные пакеты, просто «летает». Владелец девайса как-то ставил на похожую модель Windows 10, и все равно система работала неплохо, особых тормозов не было.\n\nАвтономность устройства неплохая, если работать в режиме «открыл — выполнил срочную работу — закрыл», то на световой день хватит.\n\n>  **Возможно, эти тексты тоже вас заинтересуют:**\n>\n>\n>\n>\n\n→\n\n Нужны ли изменения в работе команды? Рассчитываем ответ по формуле Глейчера\n>\n>\n\n→\n\n Как эффективно делиться результатами своей работы? О «хвастовстве» здорового человека\n>\n>\n\n→\n\n Гиперскейлинг в 2023 году: что мы делали, чтобы поддержать бизнес клиентов\n\nНа данный момент система уже загружена и устанавливается. В ближайшее время мы протестируем ее и вынесем вердикт — действительно ли она годится для маломощных систем. Так что не переключайтесь, вскоре появится вторая часть этой статьи, уже не теоретическая, а практическая.","metric":0.61474609380000000414,"metric_input":"Текст статьи:\nTiny11: «похудевшая» Windows 11. Действительно ли она в состоянии работать на старых ПК? Часть 1 / Хабр                                                         \n\nНесколько дней назад на Хабре\nпоявилась новость\nо том, что сторонние разработчики выпустили «быструю» версию Windows 11 Pro 22H2. В отличие от оригинала, ей требуется всего лишь 2 ГБ ОЗУ и 8 ГБ места на рабочем диске. Вообще не похоже на требования современной ОС, правда?\n\nТе, кто уже опробовал эту версию ОС, говорят, что она действительно справляется с ограниченным объемом ресурсов, и неплохо. И это при том, что оригинальной версии Windows 11 требуется 4 ГБ ОЗУ и 64 ГБ свободного места на диске. Как и почему все это работает? Об этом под катом. И да, сразу скажем, что собираемся протестировать сборку на Sony Vaio VGN-P11ZR из 2009 года. Там как раз соответствующие характеристики.\n\n### Что это за система такая? ###\n\nЭто модификация Windows 11, которую собрала команда разработчиков NTDEV. По словам разработчиков, из основной ОС вырезаны многие элементы, без которых можно обойтись. В итоге получился дистрибутив, который способен работать на действительно старых ПК и ноутбуках.\n\nВ том его кардинальное отличие от родительской системы, которую часто называют самой требовательной ОС от Microsoft за всю историю этой операционной системы.\n\n### Как создали Tiny 11? ###\n\nЗа ее основу взяли дистрибутив Windows 11 Pro версии 22Н2 образца сентября 2022 г., который разработчики легально загрузили с сайта корпорации Microsoft. Для того, чтобы создать свою версию Windows 11, команда энтузиастов убрала множество элементов, без которых можно было обойтись. Среди них — браузер Edge, про который говорят, что его используют лишь для того, чтобы загрузить другой браузер после установки Windows. На текущий момент его доля рынка составляет всего 4,46%.\n\nВсе бы хорошо, но его отсутствие означает, что при установке нового дистрибутива придется использовать установочный пакет нужного пользователю браузера. Конечно, в том есть определенное неудобство, но минимализм требует жертв.\n\nЧто касается стандартных приложений Windows 11, то из них осталось немногое: блокнот (без него в любом случае никуда), калькулятор, плюс специальные возможности. Естественно, разработчики оставили еще и Microsoft Store, плюс Paint (ну как без него обойдешься, правда?).\n\nСообщается, что разработчики новой версии Windows 11 решили отказаться от хранилища компонентов (WinSxS). Соответственно, установить нечто вроде подсистемы Windows для Linux (WSL) не получится. Но зато в tiny11 можно установить NodeJS и Docker (на основе WSL2 вместо Hyper-V).\n\nПлюс этого дистрибутива в том, что он не требует создания нового профиля, при установке можно обойтись локальной учеткой без регистрации и доступа к интернету.\n\n### Так это пиратка, что ли? ###\n\nНет, по словам разработчиков, эта версия вполне легальна. Во всяком случае, после установки ее будет нужно активировать. Для ее полноценной работы придется использовать лицензионный ключ.\n\nПонятно, что разработчики могут заявлять что угодно, но поставщик оригинальной операционной системы, корпорация Microsoft, своего мнения пока по поводу новой версии Windows 11 не высказала. Возможно, корпорация просто пропустит это событие мимо внимания — для нее оно не очень значительно.\n\nНу а команда разработчиков выложила дистрибутив на разных торрент ресурсах, плюс его можно загрузить и с других источников. Кстати, есть и другой подобный вариант ОС от Microsoft той же команды — речь идет про облегченную версию Windows 10. Многие пользователи об этом и не слышали, но в любом случае, попробовать стоит.\n\n### Ну хорошо, так эта ОС действительно может работать на старых ПК? ###\n\nМы думаем что да, но решили испытать дистрибутив самостоятельно. В прошлом году\nмы публиковали статью\nпро ультрабук Sony Vaio VGN-P11ZR из 2009 года. Почему? Ну как раз потому, что у него всего 2 ГБ ОЗУ и 60 ГБ — жесткий диск. Вот полные характеристики устройства:\n\n• Операционная система: Windows Vista Home Premium\n\n• Процессор: Intel Atom Z520 1.33 ГГц. Объем кэша L2 512 КБ. Частота системной шины 533 МГц.\n\n• Память: 2 ГБ DDR2 533 МГц\n\n• Экран: 8 дюймов, 1600x768, широкоформатный\n\n• Видеопроцессор: Intel GMA 500\n\n• Накопитель: HDD объёмом в 60 ГБ, ATA100, 4200 об/мин.\n\n• LAN/Modem: сетевая карта\n\n• Беспроводная связь: Wi-Fi IEEE 802.11g, Bluetooth, 3G (UMTS)\n\n• Интерфейсы: USB 2.0 Type A x 2, интерфейс док-станции\n\n• Слоты: Устройство для чтения флэш-карт Memory Stick, SD\n\n• Питание: Li-Ion аккумулятор\n\n• Устройства позиционирования: PointStick\n\n• Звук: Встроенные колонки, Встроенный микрофон\n\n• Веб-камера: есть\n\n• Размеры (ДхШхТ): 245x120x19.8 мм\n\n• Вес: 0.6 кг\n\nУ нашей команды после публикации поста про этот девайс была идея провести с устройством в офисе и дома какое-то время, чтобы понять, насколько он практичен в наше время. И сейчас, похоже, пришло время это реализовать.\n\nКонечно, даже с оптимизированной ОС сложно ожидать чего-то выдающегося от устройства, которое появилось в продаже 14 лет назад. Но попробовать стоит — хотя бы в качестве офисной «рабочей лошадки». В первую очередь это набор текста и коммуникации по сети. Собираемся задействовать девайс в еженедельных совещаниях, плюс 1-2 статьи набрать и опубликовать с его помощью.\n\nПлюс девайса — портативность. К слову, его родное ПО, включая офисные пакеты, просто «летает». Владелец девайса как-то ставил на похожую модель Windows 10, и все равно система работала неплохо, особых тормозов не было.\n\nАвтономность устройства неплохая, если работать в режиме «открыл — выполнил срочную работу — закрыл», то на световой день хватит.\n\n>  **Возможно, эти тексты тоже вас заинтересуют:**\n>\n>\n>\n>\n\n→\n\n Нужны ли изменения в работе команды? Рассчитываем ответ по формуле Глейчера\n>\n>\n\n→\n\n Как эффективно делиться результатами своей работы? О «хвастовстве» здорового человека\n>\n>\n\n→\n\n Гиперскейлинг в 2023 году: что мы делали, чтобы поддержать бизнес клиентов\n\nНа данный момент система уже загружена и устанавливается. В ближайшее время мы протестируем ее и вынесем вердикт — действительно ли она годится для маломощных систем. Так что не переключайтесь, вскоре появится вторая часть этой статьи, уже не теоретическая, а практическая.\n\nКраткое содержание:\nСтатья рассказывает о новой версии Windows 11, которая называется Tiny11. Она была создана для работы на старых компьютерах и ноутбуках с ограниченными ресурсами. В статье описываются изменения, которые были внесены в оригинальную версию Windows 11 для создания Tiny11, а также тестирование этой системы на старом компьютере Sony Vaio VGN-P11ZR. Авторы планируют продолжить тестирование и опубликовать вторую часть статьи, которая будет содержать практические результаты.","summary":"Статья рассказывает о новой версии Windows 11, которая называется Tiny11. Она была создана для работы на старых компьютерах и ноутбуках с ограниченными ресурсами. В статье описываются изменения, которые были внесены в оригинальную версию Windows 11 для создания Tiny11, а также тестирование этой системы на старом компьютере Sony Vaio VGN-P11ZR. Авторы планируют продолжить тестирование и опубликовать вторую часть статьи, которая будет содержать практические результаты."}
{"Text":"Pip-boy в реальности: компьютеры на предплечье / Хабр                 \n\n24-04-2016\nPip-Boy из Fallout помнят все. А некоторые — хотят такое устройство себе, хотя носить его, согласитесь, совсем не удобно. И те же люди могут говорить о бесполезности современных умных часов — дело в атмосфере киберпанка. Устройства такого же формата были и есть в реальности, но они чаще представляют собой узкоспециализированные гаджеты для представителей определённых профессий. Под катом — несколько примеров. Одними из первых компьютеров в форм-факторе в стиле Pip-Boy стали узкоспециализированные гаджеты для дайверов. Можно, конечно, сравнить это устройство 1959 года с обычными компасами и другими гаджетами подобного форм-фактора, уже доступными на тот момент с 1920-х годов. И да, это не цифровое устройство на фотографии, но оно стало прародителем огромного количества существующих сегодня подводных компьютеров. EON Steel, выпущенный в 2014 году, показывает глубину, время погружения, уровень кислорода в баллонах и другие полезные данные. Это лишь один из примеров современных приборов данного типа. Стоит сказать, что на одном заряде это устройство будет работать до 40 часов на глубине до 150 метров. Устройство имеет TFT дисплей с разрешением 240х320 пикселей и весит 347 граммов. В 1985 году японская компания Seiko выпустила свои вторые «умные часы» — наручный терминал Seiko UC-2000, способный синхронизироваться с популярными на тот момент персональными компьютерами, включая Apple II и II+, Commodore 64, IBM PC, NEC 8201, Tandy Color Computer и другими, часть из которых вы найдёте в этой статье. Но такой гаджет не очень похож на Pip-Boy, поэтому поговорим о других примерах. Носимый компьютер Zypad WR1100 был создан в 2008 году специально для работы в сложных условиях, когда пользователю необходимо иметь свободные руки. Он имеет защищённый от пыли, влаги и ударов корпус, работает на Linux, имеет 3,5-дюймовый сенсорный экран с разрешением 640х480 пикселей. Над экраном специально поработали, чтобы изображение было видно при прямом солнечном свете. Гаджет также оснастили GPS, электронным компасом, биометрической защитой — датчиком отпечатка пальца, и системой экономии батареи — экран не работал, когда рука висела вдоль тела. Устройство построено на базе процессора Marvell PXA 270 416 МГц, имеет 256 мегабайт оперативной и 128 мегабайт встроенной памяти, которую можно расширить с помощью microSD. Zypad WR1100 весит 650 граммов, так что для его ношения нужны сильные руки. В 2010 году дюжина американских солдат получила на тест разные версии одного и того же устройства, сделанные компаниями LG Display, Universal Display и L-3 Display Systems UDC. Компьютер с изогнутым 4,3-дюймовым сенсорным дисплеем крепился на предплечье — и он очень похож на Pip-Boy с несколько меньшим уровнем киберпанка. Новая версия Zypad — модель WL1500 — была выпущена в 2010-2011 годах. Производители снова сконцентрировались не на массовом рынке. На этот раз целевой аудиторией стали корпоративные клиенты в лице медицинских учреждений и представителей других профессий, которым нужны портативные мобильные гаджеты, позволяющие оставить свободными руки. Устройство построено на базе процессора с частотой 400 МГц, оснащено 3,5-дюймовым экраном с разрешением 320х240, поддерживает Wi-Fi, Bluetooth и GPS. Устройство работало на Windows CE 6.0. В 2012 году в Афганистан отправились гаджеты General Dynamics GD300, работающие на операционной системе на базе Android. Устройство построено на базе процессора ARM Cortex A8 600 МГц, имеет 256 мегабайт оперативной памяти и 8 гигабайт встроенной, и всего лишь 3,5-дюймовый дисплей с разрешением 800х480, который реагирует на прикосновения в перчатках и виден при солнечном свете. В 2012 году вышел огромный наручный телефон на Android 2.2 — Watch SmartPhone c дисплеем диагональю 2 дюйма. Гаджет кроме того был оснащён 2-мегапиксельной камерой. По нему можно было звонить — были встроенные микрофон, спикер и беспроводная гарнитура. Процессор устройства — с частотой 426 МГц, разрешение экрана — 320х240 пикселей, 262 мегабайта встроенной памяти, которую можно расширить с помощью MicroSD до 8 гигабайт. Терминал сбора данных Motorola WT4090 — устройство, созданное для работников складов. Он может падать с высоты до 1,2 метров и поддерживает текстовую и голосовую комплектацию товара и имеет 2,8-дюймовый дисплей. Этот гаджет может работать при температуре до -20 °C, то есть даже в морозильных камерах. Это всего лишь несколько примеров использования подобного форм-фактора для создания портативных компьютеров, способных освободит руки пользователя и помогающих при выполнении определённых задач, будь то работа с товарами на складе или наведение истребителей на цели. На данный момент компьютеры на запястье не стали достоянием массового рынка, в отличие от умных часов, не настолько узкоспециализированных гаджетов. Вся причина — в том, что наручные компьютеры не очень удобны обычному потребителю, ведь к часам мы уже привыкли, а носить 350-650 граммов на предплечье, особенно зимой, занятие не из приятных.\n","metric":0.45336914059999999793,"metric_input":"Текст статьи:\nPip-boy в реальности: компьютеры на предплечье / Хабр                 \n\n24-04-2016\nPip-Boy из Fallout помнят все. А некоторые — хотят такое устройство себе, хотя носить его, согласитесь, совсем не удобно. И те же люди могут говорить о бесполезности современных умных часов — дело в атмосфере киберпанка. Устройства такого же формата были и есть в реальности, но они чаще представляют собой узкоспециализированные гаджеты для представителей определённых профессий. Под катом — несколько примеров. Одними из первых компьютеров в форм-факторе в стиле Pip-Boy стали узкоспециализированные гаджеты для дайверов. Можно, конечно, сравнить это устройство 1959 года с обычными компасами и другими гаджетами подобного форм-фактора, уже доступными на тот момент с 1920-х годов. И да, это не цифровое устройство на фотографии, но оно стало прародителем огромного количества существующих сегодня подводных компьютеров. EON Steel, выпущенный в 2014 году, показывает глубину, время погружения, уровень кислорода в баллонах и другие полезные данные. Это лишь один из примеров современных приборов данного типа. Стоит сказать, что на одном заряде это устройство будет работать до 40 часов на глубине до 150 метров. Устройство имеет TFT дисплей с разрешением 240х320 пикселей и весит 347 граммов. В 1985 году японская компания Seiko выпустила свои вторые «умные часы» — наручный терминал Seiko UC-2000, способный синхронизироваться с популярными на тот момент персональными компьютерами, включая Apple II и II+, Commodore 64, IBM PC, NEC 8201, Tandy Color Computer и другими, часть из которых вы найдёте в этой статье. Но такой гаджет не очень похож на Pip-Boy, поэтому поговорим о других примерах. Носимый компьютер Zypad WR1100 был создан в 2008 году специально для работы в сложных условиях, когда пользователю необходимо иметь свободные руки. Он имеет защищённый от пыли, влаги и ударов корпус, работает на Linux, имеет 3,5-дюймовый сенсорный экран с разрешением 640х480 пикселей. Над экраном специально поработали, чтобы изображение было видно при прямом солнечном свете. Гаджет также оснастили GPS, электронным компасом, биометрической защитой — датчиком отпечатка пальца, и системой экономии батареи — экран не работал, когда рука висела вдоль тела. Устройство построено на базе процессора Marvell PXA 270 416 МГц, имеет 256 мегабайт оперативной и 128 мегабайт встроенной памяти, которую можно расширить с помощью microSD. Zypad WR1100 весит 650 граммов, так что для его ношения нужны сильные руки. В 2010 году дюжина американских солдат получила на тест разные версии одного и того же устройства, сделанные компаниями LG Display, Universal Display и L-3 Display Systems UDC. Компьютер с изогнутым 4,3-дюймовым сенсорным дисплеем крепился на предплечье — и он очень похож на Pip-Boy с несколько меньшим уровнем киберпанка. Новая версия Zypad — модель WL1500 — была выпущена в 2010-2011 годах. Производители снова сконцентрировались не на массовом рынке. На этот раз целевой аудиторией стали корпоративные клиенты в лице медицинских учреждений и представителей других профессий, которым нужны портативные мобильные гаджеты, позволяющие оставить свободными руки. Устройство построено на базе процессора с частотой 400 МГц, оснащено 3,5-дюймовым экраном с разрешением 320х240, поддерживает Wi-Fi, Bluetooth и GPS. Устройство работало на Windows CE 6.0. В 2012 году в Афганистан отправились гаджеты General Dynamics GD300, работающие на операционной системе на базе Android. Устройство построено на базе процессора ARM Cortex A8 600 МГц, имеет 256 мегабайт оперативной памяти и 8 гигабайт встроенной, и всего лишь 3,5-дюймовый дисплей с разрешением 800х480, который реагирует на прикосновения в перчатках и виден при солнечном свете. В 2012 году вышел огромный наручный телефон на Android 2.2 — Watch SmartPhone c дисплеем диагональю 2 дюйма. Гаджет кроме того был оснащён 2-мегапиксельной камерой. По нему можно было звонить — были встроенные микрофон, спикер и беспроводная гарнитура. Процессор устройства — с частотой 426 МГц, разрешение экрана — 320х240 пикселей, 262 мегабайта встроенной памяти, которую можно расширить с помощью MicroSD до 8 гигабайт. Терминал сбора данных Motorola WT4090 — устройство, созданное для работников складов. Он может падать с высоты до 1,2 метров и поддерживает текстовую и голосовую комплектацию товара и имеет 2,8-дюймовый дисплей. Этот гаджет может работать при температуре до -20 °C, то есть даже в морозильных камерах. Это всего лишь несколько примеров использования подобного форм-фактора для создания портативных компьютеров, способных освободит руки пользователя и помогающих при выполнении определённых задач, будь то работа с товарами на складе или наведение истребителей на цели. На данный момент компьютеры на запястье не стали достоянием массового рынка, в отличие от умных часов, не настолько узкоспециализированных гаджетов. Вся причина — в том, что наручные компьютеры не очень удобны обычному потребителю, ведь к часам мы уже привыкли, а носить 350-650 граммов на предплечье, особенно зимой, занятие не из приятных.\n\n\nКраткое содержание:\nВ статье рассказывается о носимых компьютерах в форм-факторе Pip-Boy, которые существуют в реальности. Это узкоспециализированные устройства для дайверов, военных, медицинских работников и других профессионалов. Они имеют защищенный корпус, GPS, электронные компасы и биометрическую защиту.\n\nПримеры таких гаджетов:\n\n- EON Steel - подводный компьютер для дайвинга, 1959 год.\n- Наручный терминал Seiko UC-2000, 1985 год.\n- Zypad WR1100, 2008 год.\n- General Dynamics GD300, 2012 год.\n- Watch SmartPhone, 2012 год.\n\nВсе эти гаджеты имеют разные размеры и характеристики, но их объединяет форм-фактор наручных часов. Они предназначены для работы в сложных условиях, когда нужно освободить руки.","summary":"В статье рассказывается о носимых компьютерах в форм-факторе Pip-Boy, которые существуют в реальности. Это узкоспециализированные устройства для дайверов, военных, медицинских работников и других профессионалов. Они имеют защищенный корпус, GPS, электронные компасы и биометрическую защиту.\n\nПримеры таких гаджетов:\n\n- EON Steel - подводный компьютер для дайвинга, 1959 год.\n- Наручный терминал Seiko UC-2000, 1985 год.\n- Zypad WR1100, 2008 год.\n- General Dynamics GD300, 2012 год.\n- Watch SmartPhone, 2012 год.\n\nВсе эти гаджеты имеют разные размеры и характеристики, но их объединяет форм-фактор наручных часов. Они предназначены для работы в сложных условиях, когда нужно освободить руки."}
{"Text":"Как мы создали собственную систему распределения жидкостей / Habr             \n\nВы узнаете:\nзачем вообще нам это понадобилось\n\nможно ли работать с отечественными производителями без страха и упрека\n\nпочему не стоит экономить на деталях для изделий (спойлер: если у вас железные нервы, то можно)\n\nкак не скатиться в отчаянье, а научиться управлять рисками.\n#### Разработчик — это звучит гордо ####\nКак думают многие: придумываешь идею, разрабатываешь продукт, зарабатываешь миллионы. Как на самом деле: чтобы собрать робота, нужно думать как человек. Мыслить как человек с хорошо отлаженными процессами в голове, осознающий риски, считающий время, деньги и нервы как свои, так и команды, ну и конечно, заказчика.\n#### Для чего нужен гистологический процессор ####\nВ доковидные времена, а именно — в начале 2018-го, наша команда запустила инжиниринговый стартап Nextelligence и стартовала в проекте по созданию вакуумного процессора для гистологических лабораторий.\nЕсли коротко, то это прибор, в который помещают специальные перфорированные пластиковые кассеты с материалами человеческого происхождения (живого или уже не очень), чтобы впоследствии морфолог (гистолог/патологоанатом) мог посмотреть под микроскопом микропрепарат и сделать гистологическое заключение.\nПервый и самый затратный по времени из 5 этапов гистологической пробоподготовки – осуществить замещение присутствующей в материале воды на парафиновую смесь. Процесс замещения воды парафином называется гистологической проводкой или просто проводкой. Этот процесс и нужно было автоматизировать, то есть создать прибор, который в автоматическом режиме последовательно заливает исследуемый материал разными реагентами. При этом важно было свести риск утраты исследуемого материала к минимуму — по статистике в лабораториях теряется практически каждая пятая проба.\n## Проработка концепта ##\nКонцепт прибора казался весьма простым, типа «реагент – насос – ткань», пока дело не дошло до проектирования системы распределения реагентов внутри прибора. Дело в том, что помимо своей агрессивности (например, ксилол запросто растворяет или необратимо портит большинство известных пластиков и эластомеров) реактивы имеют свойство образовывать солевые отложения, преципитаты, на внутренних частях прибора, что, в свою очередь, быстро выводит его из строя.\nЕстественно, заказчик хотел сделать изделие надёжным, так что мы сразу отмели вариант использования острова с соленоидными клапанами, выполняющего роль распределителя реактивов, просвет которых быстро забивается. Использование шаровых клапанов было также нежелательно из-за их громоздкости и неповоротливости, поэтому было решено пойти по «своему особому пути» и спроектировать компактный многопозиционный ротационный клапан.\nПримерно концепт ротационного клапана можно описать следующим образом: в центре клапана лежат 2 специальных диска, один из которых, стационарный, имеет отверстия по периметру и в центре (рис. 1), а второй, ротационный – канавку от центра к краю (рис. 2).\nРисунок 1. Стационарный диск\nРисунок 2. Ротационный диск\n#### Сложности, отчаянье и надежда из Дюссельдорфа ####\nПри проработке концепта мы рассматривали несколько вариантов материала для этих дисков. Самые простые в исполнении, пластик и металл, имели свои недостатки. Пластик, даже фторопласт, царапался при попадании между дисками кристаллов соли из забуференного формалина и со временем сборка теряла герметичность. Металл имел ту же проблему, что и пластик, а также значительнее последнего расширялся при прогреве, да еще 2 прецизионно отполированных металлических диска, прижимаемых друг к другу пружиной, быстро прикипали друг к другу, даже при наличии между ними смазки.\nКогда руки уже почти опустились, а проблема начала казаться нерешаемой, мне довелось съездить на выставку Medica/Compamed в Дюссельдорфе и подсмотреть у одного всемирно известного производителя инженерных решений для медицины и промышленности идею керамического ротационного клапана. К сожалению, у самого производителя не было в ассортименте таких клапанов с нужными нам характеристиками, поэтому от идеи разработать свой клапан я не отказался.\n#### Горе-керамисты, убитое время и почти хэппи энд ####\nНа удивление легко оказалось найти в России производителя технической керамики, который взялся изготовить диски клапана по нашему заданию из оксида алюминия (искусственный корунд) методом шликерного литья. На выходе должно было получиться изделие твердостью 9 из 10 возможных по шкале Мооса, что значительно тверже практически любого солевого кристалла, имеющее низкий коэффициент температурного расширения, высокую химическую стойкость и минимальный риск прикипания двух компонентов друг к другу.\nЗаявленная стоимость и сроки производства необходимых нам изделий внушали осторожный оптимизм. Опыт взаимодействия с российскими производителями призывал не радоваться раньше времени и не зря.\nСпустя заявленные в договоре на изготовление 3 месяца, началась борьба бобра с ослом. Производитель начал кормить нас завтраками, ссылаться на форс-мажоры, запой главного технолога, поломку фрезы для выточки пресс-формы и т.д. В таком режиме мы прожили еще около месяца и, наконец, мне на почту пришло долгожданное письмо: «Изделия готовы, можете забирать». Моя радость была яркой, но длилась лишь до тех пор, пока транспортная компания не привезла мне груз. Диски оказались круглыми с выдержанными внешними размерами, но на этом их достоинства заканчивались. Практически все обнизки под посадку О-колец были в разной степени кривыми, в некоторых расхождение с чертежом было далеко за гранью приличия, про выдерживание допусков речи не шло вообще. Создалось впечатление, что на предоставленные нами чертежи подрядчик смотрел только в начале работы, да и то, издалека.\nПосле недолгого, но очень содержательного диалога с «керамистами» они удалились читать чертежи и переделывать. Еще на месяц…\nНе вдаваясь в подробности, надо признать, что второй подход к снаряду нашему подрядчику удался гораздо лучше. За некоторым исключением изделия стали больше походить на то, что мы задумывали. Наступила долгожданная пора утомительных испытаний.\nРисунок 3. Диски ротационного клапана вживую\n#### Заключительный этап челленджа — испытания ####\nМы собрали стенд с этим клапаном, разлили по ведрам химию, подключили нагревательные элементы и чиллер для аквариумов с креветками (!), чтобы более-менее воссоздать условия, при которых клапану пришлось бы трудиться в реальности и настроились на месяцы бесперебойной работы стенда, но… Думаю, вы уже догадались, что оптимизм был недолгим.\nРисунок 4. Сборка ротационного клапана в стальном ложе со внешним окружением\nБеда пришла откуда не ждали. В изобилии доступные на российском рынке О-кольца из NBR и FKM/FPM/Viton, уплотняющие стационарный диск ротационного клапана, приказали долго жить. Первые — после недели работы, вторые после трёх. Оказалось, что ксилол, перепады температур и механическая нагрузка делают даже из хваленого Viton труху за каких-то несколько недель.\nВыход нашли достаточно быстро. Оказалось, что во всем цивилизованном мире уже давно для таких сложных задач используют уплотнительные кольца из перфторкаучука (другие названия FFKM, Kalrez), которые незначительно дороже упомянутых выше, но отменно работают в самых жестких условиях — как-раз то, что нам нужно.\nОставлю для хоррор-публицистики проблемы с китайскими мотор-редукторами и уральскими патронными ТЭНами, которые у нас всплыли по дороге. Скажу лишь, что работая над данным проектом пообещал сам себе более не экономить на критически важных компонентах даже, если очень хочется.\n## Резюме ##\nВ качестве резюме этой хардкор-разработки выделяю несколько тезисов-рекомендаций:\nНе экономьте на компонентах. Чем в более сложных условиях должно работать ваше изделие, тем меньше должно быть ваше желание порезать косты. Дёшево = плохо, чудес не бывает.\n\nРаботать с отечественными производителями сложно, но можно. Надо только придирчиво подходить к выбору подрядчика и стараться контролировать его на разных стадиях выполнения вашего заказа, а также заставлять почаще читать ваши чертежи.\n\nЕсли вы решились на п. 2, пропишите в контракте побольше штрафов, так у подрядчика будет больше стимулов сдать вам то, что вы хотите когда вы хотите.\n\nВ общем, управляйте рисками. ISO 14971 вам в помощь.","metric":0.62744140630000000414,"metric_input":"Текст статьи:\nКак мы создали собственную систему распределения жидкостей / Habr             \n\nВы узнаете:\nзачем вообще нам это понадобилось\n\nможно ли работать с отечественными производителями без страха и упрека\n\nпочему не стоит экономить на деталях для изделий (спойлер: если у вас железные нервы, то можно)\n\nкак не скатиться в отчаянье, а научиться управлять рисками.\n#### Разработчик — это звучит гордо ####\nКак думают многие: придумываешь идею, разрабатываешь продукт, зарабатываешь миллионы. Как на самом деле: чтобы собрать робота, нужно думать как человек. Мыслить как человек с хорошо отлаженными процессами в голове, осознающий риски, считающий время, деньги и нервы как свои, так и команды, ну и конечно, заказчика.\n#### Для чего нужен гистологический процессор ####\nВ доковидные времена, а именно — в начале 2018-го, наша команда запустила инжиниринговый стартап Nextelligence и стартовала в проекте по созданию вакуумного процессора для гистологических лабораторий.\nЕсли коротко, то это прибор, в который помещают специальные перфорированные пластиковые кассеты с материалами человеческого происхождения (живого или уже не очень), чтобы впоследствии морфолог (гистолог/патологоанатом) мог посмотреть под микроскопом микропрепарат и сделать гистологическое заключение.\nПервый и самый затратный по времени из 5 этапов гистологической пробоподготовки – осуществить замещение присутствующей в материале воды на парафиновую смесь. Процесс замещения воды парафином называется гистологической проводкой или просто проводкой. Этот процесс и нужно было автоматизировать, то есть создать прибор, который в автоматическом режиме последовательно заливает исследуемый материал разными реагентами. При этом важно было свести риск утраты исследуемого материала к минимуму — по статистике в лабораториях теряется практически каждая пятая проба.\n## Проработка концепта ##\nКонцепт прибора казался весьма простым, типа «реагент – насос – ткань», пока дело не дошло до проектирования системы распределения реагентов внутри прибора. Дело в том, что помимо своей агрессивности (например, ксилол запросто растворяет или необратимо портит большинство известных пластиков и эластомеров) реактивы имеют свойство образовывать солевые отложения, преципитаты, на внутренних частях прибора, что, в свою очередь, быстро выводит его из строя.\nЕстественно, заказчик хотел сделать изделие надёжным, так что мы сразу отмели вариант использования острова с соленоидными клапанами, выполняющего роль распределителя реактивов, просвет которых быстро забивается. Использование шаровых клапанов было также нежелательно из-за их громоздкости и неповоротливости, поэтому было решено пойти по «своему особому пути» и спроектировать компактный многопозиционный ротационный клапан.\nПримерно концепт ротационного клапана можно описать следующим образом: в центре клапана лежат 2 специальных диска, один из которых, стационарный, имеет отверстия по периметру и в центре (рис. 1), а второй, ротационный – канавку от центра к краю (рис. 2).\nРисунок 1. Стационарный диск\nРисунок 2. Ротационный диск\n#### Сложности, отчаянье и надежда из Дюссельдорфа ####\nПри проработке концепта мы рассматривали несколько вариантов материала для этих дисков. Самые простые в исполнении, пластик и металл, имели свои недостатки. Пластик, даже фторопласт, царапался при попадании между дисками кристаллов соли из забуференного формалина и со временем сборка теряла герметичность. Металл имел ту же проблему, что и пластик, а также значительнее последнего расширялся при прогреве, да еще 2 прецизионно отполированных металлических диска, прижимаемых друг к другу пружиной, быстро прикипали друг к другу, даже при наличии между ними смазки.\nКогда руки уже почти опустились, а проблема начала казаться нерешаемой, мне довелось съездить на выставку Medica/Compamed в Дюссельдорфе и подсмотреть у одного всемирно известного производителя инженерных решений для медицины и промышленности идею керамического ротационного клапана. К сожалению, у самого производителя не было в ассортименте таких клапанов с нужными нам характеристиками, поэтому от идеи разработать свой клапан я не отказался.\n#### Горе-керамисты, убитое время и почти хэппи энд ####\nНа удивление легко оказалось найти в России производителя технической керамики, который взялся изготовить диски клапана по нашему заданию из оксида алюминия (искусственный корунд) методом шликерного литья. На выходе должно было получиться изделие твердостью 9 из 10 возможных по шкале Мооса, что значительно тверже практически любого солевого кристалла, имеющее низкий коэффициент температурного расширения, высокую химическую стойкость и минимальный риск прикипания двух компонентов друг к другу.\nЗаявленная стоимость и сроки производства необходимых нам изделий внушали осторожный оптимизм. Опыт взаимодействия с российскими производителями призывал не радоваться раньше времени и не зря.\nСпустя заявленные в договоре на изготовление 3 месяца, началась борьба бобра с ослом. Производитель начал кормить нас завтраками, ссылаться на форс-мажоры, запой главного технолога, поломку фрезы для выточки пресс-формы и т.д. В таком режиме мы прожили еще около месяца и, наконец, мне на почту пришло долгожданное письмо: «Изделия готовы, можете забирать». Моя радость была яркой, но длилась лишь до тех пор, пока транспортная компания не привезла мне груз. Диски оказались круглыми с выдержанными внешними размерами, но на этом их достоинства заканчивались. Практически все обнизки под посадку О-колец были в разной степени кривыми, в некоторых расхождение с чертежом было далеко за гранью приличия, про выдерживание допусков речи не шло вообще. Создалось впечатление, что на предоставленные нами чертежи подрядчик смотрел только в начале работы, да и то, издалека.\nПосле недолгого, но очень содержательного диалога с «керамистами» они удалились читать чертежи и переделывать. Еще на месяц…\nНе вдаваясь в подробности, надо признать, что второй подход к снаряду нашему подрядчику удался гораздо лучше. За некоторым исключением изделия стали больше походить на то, что мы задумывали. Наступила долгожданная пора утомительных испытаний.\nРисунок 3. Диски ротационного клапана вживую\n#### Заключительный этап челленджа — испытания ####\nМы собрали стенд с этим клапаном, разлили по ведрам химию, подключили нагревательные элементы и чиллер для аквариумов с креветками (!), чтобы более-менее воссоздать условия, при которых клапану пришлось бы трудиться в реальности и настроились на месяцы бесперебойной работы стенда, но… Думаю, вы уже догадались, что оптимизм был недолгим.\nРисунок 4. Сборка ротационного клапана в стальном ложе со внешним окружением\nБеда пришла откуда не ждали. В изобилии доступные на российском рынке О-кольца из NBR и FKM/FPM/Viton, уплотняющие стационарный диск ротационного клапана, приказали долго жить. Первые — после недели работы, вторые после трёх. Оказалось, что ксилол, перепады температур и механическая нагрузка делают даже из хваленого Viton труху за каких-то несколько недель.\nВыход нашли достаточно быстро. Оказалось, что во всем цивилизованном мире уже давно для таких сложных задач используют уплотнительные кольца из перфторкаучука (другие названия FFKM, Kalrez), которые незначительно дороже упомянутых выше, но отменно работают в самых жестких условиях — как-раз то, что нам нужно.\nОставлю для хоррор-публицистики проблемы с китайскими мотор-редукторами и уральскими патронными ТЭНами, которые у нас всплыли по дороге. Скажу лишь, что работая над данным проектом пообещал сам себе более не экономить на критически важных компонентах даже, если очень хочется.\n## Резюме ##\nВ качестве резюме этой хардкор-разработки выделяю несколько тезисов-рекомендаций:\nНе экономьте на компонентах. Чем в более сложных условиях должно работать ваше изделие, тем меньше должно быть ваше желание порезать косты. Дёшево = плохо, чудес не бывает.\n\nРаботать с отечественными производителями сложно, но можно. Надо только придирчиво подходить к выбору подрядчика и стараться контролировать его на разных стадиях выполнения вашего заказа, а также заставлять почаще читать ваши чертежи.\n\nЕсли вы решились на п. 2, пропишите в контракте побольше штрафов, так у подрядчика будет больше стимулов сдать вам то, что вы хотите когда вы хотите.\n\nВ общем, управляйте рисками. ISO 14971 вам в помощь.\n\nКраткое содержание:\nВ статье рассказывается о разработке вакуумного процессора для гистологических лабораторий. Авторы статьи сталкиваются с проблемами при проектировании системы распределения реагентов внутри прибора. Они рассматривают несколько вариантов материалов для дисков, но в итоге выбирают керамику. Однако, российский производитель не выполняет свои обязательства по срокам и качеству изготовления изделий. В результате, авторам статьи приходится проводить дополнительные испытания и заменять уплотнительные кольца на более подходящие. В итоге, они успешно решают все проблемы и заканчивают проект. Авторы статьи дают несколько рекомендаций по работе с отечественными производителями и управлению рисками.","summary":"В статье рассказывается о разработке вакуумного процессора для гистологических лабораторий. Авторы статьи сталкиваются с проблемами при проектировании системы распределения реагентов внутри прибора. Они рассматривают несколько вариантов материалов для дисков, но в итоге выбирают керамику. Однако, российский производитель не выполняет свои обязательства по срокам и качеству изготовления изделий. В результате, авторам статьи приходится проводить дополнительные испытания и заменять уплотнительные кольца на более подходящие. В итоге, они успешно решают все проблемы и заканчивают проект. Авторы статьи дают несколько рекомендаций по работе с отечественными производителями и управлению рисками."}
{"Text":"Если ваш бэклог не детализируется, значит вы делаете что-то не так / Habr             \n\n17-02-2021\nВ рамках курса «Agile Project Manager» делимся с вами переводом полезного материала. Также приглашаем на бесплатный демо-урок «Фасилитация онлайн». На занятии участники вместе с экспертом рассмотрят следующие вопросы: - Обучение в онлайне – как сделать это действительно эффективно? - Принятие решений Agile-командой – как меняется подход? - Гигиенические правила онлайна – проверь себя сам - Tips & Tricks при работе в онлайне – что помогает делать полезные встречи (и почему так) Большинство Scrum команд, с которыми я встречался, не занимаются уточнением своего продуктового бэклога и пытаются делать задачи, которые они понимают не до конца. Если вы добрались до планирования нового спринта, а ваш бэклог не готов, то вы работаете с ним неправильно. Когда тот продукт, который вы делаете, не получается качественным, вам следует почитать о такой вещи, как Defenition of Done. TL;DR Если вы добрались до планирования спринта, а элементы вашего бэклога по размеру не вписываются в ваш следующий спринт, и ваши разработчики их не понимают досконально, то вы работаете с ним неправильно. Вы с самого начала плывете на скалы, и у вас нет карты отмелей, чтобы предотвратить столкновение. Несмотря на то, что Руководство по Scrum не определяет детализацию как отдельное событие, вам необходимо ее проводить. Вы можете придумать свои события детализации или уточнять бэклог стихийно. Что бы вы ни выбрали, есть простая метрика успеха. Если ваши разработчики смотрят на какую-то задачу из двух предстоящих спринтов и не понимают ее, то это ваша недоработка. Если вы понимаете, что не можете доделать какую-то задачу и вам приходится делать для этого несколько итераций, или вы просто не в состоянии сделать что-то, то скорее всего, не хватает каких-то деталей, чтобы сложить полную картину. Что такое готовность продуктового бэклога? Если разработчики не понимают, чего от них хотят, то как они могут согласиться с тем, что задачи могут вписаться в спринт? Часто обнаруживается, что команды, которые не уточняют бэклог, путаются в том, почему они не могут закончить все задачи спринта. Несмотря на то, что в таком эмпирическом процессе, как Scrum, мы знаем изначально меньше, чем будет понятно по ходу дела, просто предполагать и надеяться на лучшее – решительно непрофессионально. «Выбор объема работ, которые должны быть завершены в течение спринта, может быть непростым. Однако, чем больше разработчики знают о своей производительности на основе прошлого опыта, и своем Definition of Done, тем увереннее они смогут спрогнозировать свою нагрузку на спринт.» - ScrumGuides.org Если нам не нужно определение готовности, то нужно рабочее соглашение между Product Owner-ом и разработчиками. В Scrum разработчики – это те, кто выбирает какие задачи взять на спринт, и они единственные, кто может решить, что они в состоянии сделать. Разработчики должны быть уполномочены отказываться брать задачи из бэклога, которые они либо не понимают, либо их размер слишком велик, чтобы завершить задачу за один спринт. В целом, я считаю, что команда набирает на спринт достаточно большое количество задач, поэтому эти задачи должны быть подходящего размера. Готовый бэклог – это тот бэклог, из которого разработчики могут с уверенностью выбирать задачи. Как детализировать бэклог? Уточнение не является отдельным событием в руководстве по Scrum, поскольку от продукта к продукту процесс этот может различаться. Если бы вы спросили, до какого момента нужно детализировать задачи, я бы ответил вам «столько, сколько вам нужно и больше». Слишком точная постановка задачи – это такая же трата времени, как и работа с недетализированными задачами. «Детализация бэклога продукта – это процесс декомпозиции и дальнейшего уточнения элементов бэклога. Это постоянная работа по добавлению деталей, таких как описание, порядок работы и размер задач. Атрибуты могут варьироваться в зависимости от области деятельности» - Руководство по Scrum 2020 года Количество времени, которое разработчики тратят на детализацию, зависит от потребности в этом. Однако эта потребность будет меняться в течение всего времени жизни продукта, и вы должны тратить ровно столько времени, сколько вам нужно, чтобы максимально сосредоточиться на получении ценности. Я понял, что многим командам, которые раньше не занимались уточнением деталей, может потребоваться значительно больше времени, чтобы навести порядок в бэклоге. Как только все будет готово, вы, как правило, будете придерживаться плана спринта, основанного на вашем эффективном горизонте планирования того, что вы можете достичь. Обычно я провожу первую детализацию, как воркшоп с дискуссиями. Если вы проведете его перед планированием одного спринта, то увидите его ценность к концу следующего. Для проведения воркшопа я приглашаю необходимых экспертов и Scrum-команду (Product Owner-а, разработчиков и Scrum-мастера) в аудиторию, и мы просто открываем текущий бэклог и проходимся по нему. Начните с самого верха и спросите Product Owner-а, действительно ли эта задача является самой важной. Если нет, то найдите самую важную. Затем попросите Product Owner-а прочитать и пояснить ее, а затем обсуждайте задачу все вместе, чтобы детализировать ее. Как только Product Owner отклоняется от имеющегося описания задачи в бэклоге, остановитесь и попросите кого-нибудь занести дополнительное описание в бэклог. Попросите разработчиков оценить задачу: «Поместится ли эта задача в спринт в таком виде?». Если ответ отрицательный, то задачу нужно разбить, переупорядочить бэклог и продолжить детализировать. Вы будете делать это снова и снова, пока разработчики не согласятся с тем, что задач достаточно для следующих двух спринтов. Так вы поможете Product Owner-у планировать будущие релизы, а разработчикам создать план работы над текущим. Как контролировать эффективность детализации? Во время планирования спринта ваши разработчики должны иметь возможность быстро выбрать элементы бэклога, которые подойдут под выбранную цель спринта и согласиться с тем, что эти задачи можно реализовать за отведенное время. Если у вас так и получается и в большинстве случаев вам удается доставить все запланированные функции, то вы, вероятно, уже достаточно детализировали свой бэклог. Если сейчас ситуация не такая, то вам нужно сосредоточиться на доработке вашего бэклога. Если во время обзора спринта ваш Product Owner постоянно норовит сказать, что деталей недостаточно, то скорее всего действительно такого уровня уточнений мало, чтобы команда разработчиков четко поняла, что она должна делать. Узнать подробнее о курсе «Agile Project Manager». Смотреть открытый урок «Фасилитация онлайн».\n","metric":0.626953125,"metric_input":"Текст статьи:\nЕсли ваш бэклог не детализируется, значит вы делаете что-то не так / Habr             \n\n17-02-2021\nВ рамках курса «Agile Project Manager» делимся с вами переводом полезного материала. Также приглашаем на бесплатный демо-урок «Фасилитация онлайн». На занятии участники вместе с экспертом рассмотрят следующие вопросы: - Обучение в онлайне – как сделать это действительно эффективно? - Принятие решений Agile-командой – как меняется подход? - Гигиенические правила онлайна – проверь себя сам - Tips & Tricks при работе в онлайне – что помогает делать полезные встречи (и почему так) Большинство Scrum команд, с которыми я встречался, не занимаются уточнением своего продуктового бэклога и пытаются делать задачи, которые они понимают не до конца. Если вы добрались до планирования нового спринта, а ваш бэклог не готов, то вы работаете с ним неправильно. Когда тот продукт, который вы делаете, не получается качественным, вам следует почитать о такой вещи, как Defenition of Done. TL;DR Если вы добрались до планирования спринта, а элементы вашего бэклога по размеру не вписываются в ваш следующий спринт, и ваши разработчики их не понимают досконально, то вы работаете с ним неправильно. Вы с самого начала плывете на скалы, и у вас нет карты отмелей, чтобы предотвратить столкновение. Несмотря на то, что Руководство по Scrum не определяет детализацию как отдельное событие, вам необходимо ее проводить. Вы можете придумать свои события детализации или уточнять бэклог стихийно. Что бы вы ни выбрали, есть простая метрика успеха. Если ваши разработчики смотрят на какую-то задачу из двух предстоящих спринтов и не понимают ее, то это ваша недоработка. Если вы понимаете, что не можете доделать какую-то задачу и вам приходится делать для этого несколько итераций, или вы просто не в состоянии сделать что-то, то скорее всего, не хватает каких-то деталей, чтобы сложить полную картину. Что такое готовность продуктового бэклога? Если разработчики не понимают, чего от них хотят, то как они могут согласиться с тем, что задачи могут вписаться в спринт? Часто обнаруживается, что команды, которые не уточняют бэклог, путаются в том, почему они не могут закончить все задачи спринта. Несмотря на то, что в таком эмпирическом процессе, как Scrum, мы знаем изначально меньше, чем будет понятно по ходу дела, просто предполагать и надеяться на лучшее – решительно непрофессионально. «Выбор объема работ, которые должны быть завершены в течение спринта, может быть непростым. Однако, чем больше разработчики знают о своей производительности на основе прошлого опыта, и своем Definition of Done, тем увереннее они смогут спрогнозировать свою нагрузку на спринт.» - ScrumGuides.org Если нам не нужно определение готовности, то нужно рабочее соглашение между Product Owner-ом и разработчиками. В Scrum разработчики – это те, кто выбирает какие задачи взять на спринт, и они единственные, кто может решить, что они в состоянии сделать. Разработчики должны быть уполномочены отказываться брать задачи из бэклога, которые они либо не понимают, либо их размер слишком велик, чтобы завершить задачу за один спринт. В целом, я считаю, что команда набирает на спринт достаточно большое количество задач, поэтому эти задачи должны быть подходящего размера. Готовый бэклог – это тот бэклог, из которого разработчики могут с уверенностью выбирать задачи. Как детализировать бэклог? Уточнение не является отдельным событием в руководстве по Scrum, поскольку от продукта к продукту процесс этот может различаться. Если бы вы спросили, до какого момента нужно детализировать задачи, я бы ответил вам «столько, сколько вам нужно и больше». Слишком точная постановка задачи – это такая же трата времени, как и работа с недетализированными задачами. «Детализация бэклога продукта – это процесс декомпозиции и дальнейшего уточнения элементов бэклога. Это постоянная работа по добавлению деталей, таких как описание, порядок работы и размер задач. Атрибуты могут варьироваться в зависимости от области деятельности» - Руководство по Scrum 2020 года Количество времени, которое разработчики тратят на детализацию, зависит от потребности в этом. Однако эта потребность будет меняться в течение всего времени жизни продукта, и вы должны тратить ровно столько времени, сколько вам нужно, чтобы максимально сосредоточиться на получении ценности. Я понял, что многим командам, которые раньше не занимались уточнением деталей, может потребоваться значительно больше времени, чтобы навести порядок в бэклоге. Как только все будет готово, вы, как правило, будете придерживаться плана спринта, основанного на вашем эффективном горизонте планирования того, что вы можете достичь. Обычно я провожу первую детализацию, как воркшоп с дискуссиями. Если вы проведете его перед планированием одного спринта, то увидите его ценность к концу следующего. Для проведения воркшопа я приглашаю необходимых экспертов и Scrum-команду (Product Owner-а, разработчиков и Scrum-мастера) в аудиторию, и мы просто открываем текущий бэклог и проходимся по нему. Начните с самого верха и спросите Product Owner-а, действительно ли эта задача является самой важной. Если нет, то найдите самую важную. Затем попросите Product Owner-а прочитать и пояснить ее, а затем обсуждайте задачу все вместе, чтобы детализировать ее. Как только Product Owner отклоняется от имеющегося описания задачи в бэклоге, остановитесь и попросите кого-нибудь занести дополнительное описание в бэклог. Попросите разработчиков оценить задачу: «Поместится ли эта задача в спринт в таком виде?». Если ответ отрицательный, то задачу нужно разбить, переупорядочить бэклог и продолжить детализировать. Вы будете делать это снова и снова, пока разработчики не согласятся с тем, что задач достаточно для следующих двух спринтов. Так вы поможете Product Owner-у планировать будущие релизы, а разработчикам создать план работы над текущим. Как контролировать эффективность детализации? Во время планирования спринта ваши разработчики должны иметь возможность быстро выбрать элементы бэклога, которые подойдут под выбранную цель спринта и согласиться с тем, что эти задачи можно реализовать за отведенное время. Если у вас так и получается и в большинстве случаев вам удается доставить все запланированные функции, то вы, вероятно, уже достаточно детализировали свой бэклог. Если сейчас ситуация не такая, то вам нужно сосредоточиться на доработке вашего бэклога. Если во время обзора спринта ваш Product Owner постоянно норовит сказать, что деталей недостаточно, то скорее всего действительно такого уровня уточнений мало, чтобы команда разработчиков четко поняла, что она должна делать. Узнать подробнее о курсе «Agile Project Manager». Смотреть открытый урок «Фасилитация онлайн».\n\n\nКраткое содержание:\nСтатья обсуждает важность детализации бэклога в Scrum-проектах. Если разработчики не понимают задачи и не могут завершить их за один спринт, это может быть связано с недостаточной детализацией. Команды должны проводить детализацию бэклога, чтобы разработчики могли выбирать задачи с уверенностью. Детализация не является отдельным событием в Scrum, но она важна для эффективного планирования спринтов.","summary":"Статья обсуждает важность детализации бэклога в Scrum-проектах. Если разработчики не понимают задачи и не могут завершить их за один спринт, это может быть связано с недостаточной детализацией. Команды должны проводить детализацию бэклога, чтобы разработчики могли выбирать задачи с уверенностью. Детализация не является отдельным событием в Scrum, но она важна для эффективного планирования спринтов."}
{"Text":"Система мониторинга мнений методом поточечной взаимной информации / Хабр                                                  \n\n26-03-2012\nЗдравствуйте. Если вы занимаетесь DataMining, анализом текстов на выявление мнений или вам просто интересны статистические модели для оценки эмоциональной окраски предложений — эта статья может оказаться интересной. Далее, чтобы не тратить время потенциального читателя впустую на груду теории и рассуждений, сразу краткие результаты. Реализованный подход работает приблизительно с 55% точностью в трех классах: негативный, нейтральный, позитивный. Как говорит Википедия, 70% точность приблизительно равна точности человеческих суждений в среднем (в силу субъективности трактований каждого). Следует отметить, что существует немало утилит с точностью выше полученной мной, но описанный подход, можно достаточно просто усовершенствовать (будет описано ниже) и получить в итоге 65-70%. Если после всего вышеизложенного у вас осталось желание читать — добро пожаловать под кат. Краткое описание принципа Чтобы определить эмоциональную окраску предложения (SO — sentiment orientation), необходимо понять о чем в нем идет речь. Это логично. Но как объяснить машине, что такое хорошо, а что такое плохо? Первым вариантом, сразу же появляющемся на уме, есть сумма количества плохих/хороших слов, умноженных на вес каждого. Так называемый подход, основанный на «мешке слов». Удивительно простой и быстрый алгоритм, в сочетании с предобработкой на основе правил дающий неплохие результаты (до 60 — 80% точности в зависимости от корпуса). По сути, такой подход является примером униграммной модели, а это значит, что в самом наивном случае предложения «This product rather good than bad» и «This product rather bad than good» будут иметь одинаковую SO. Решить эту проблему можно перейдя от униграммной к мультиномиальной модели. Также следует отметить, что необходимо иметь солидный постоянно обновляемый словарь, содержащий плохие и хорошие термы + их вес, который может быть специфичным в зависимости от данных. Примером простейшей мультиномиальной модели является наивный метод Байеса. На хабре есть несколько статей, посвященных нему, в частности эта. Преимущество мультиномиальной модели перед униграммной заключается в том, что мы можем учитывать контекст, в котором то или иное высказывание было произнесено. Это решает проблему с предложениями, описанными выше, но вводит новое ограничение: если в обучающей выборке отсутствует выбранный n-gram, то SO на тестовых данных будет равно 0. Эта проблема всегда была и будет. Ее можно решить 2 способами: увеличив объем обучающей выборки (не забывая, что можно попутно схватить эффект переобучения), либо используя сглаживание (например Лапласа или Good-Turing). Наконец-то мы плавно подошли к идее PMI. Наряду с формулой Байеса , введем понятие PMI — pointwise mutual information, поточечная взаимная информация. в приведенной выше формуле A и B — слова/биграммы/n-нграммы, P(A), P(B) — априорные вероятности появления термина А и B соответственно в обучающей выборке (отношение количества вхождений к общему количеству слов в корпусе), P(A near B) — вероятность термина A встретиться вместе/рядом с термином B; «рядом» можно конфигурировать вручную, по умолчанию расстояние равно 10 терминам влево и вправо; основание логарифма не играет роли, для простоты примем его равным 2. Позитивный знак логарифма будет означать положительны окрас A по сравнению с B, негативный — отрицательный. Для нахождения нейтральных отзывов, можно принять какое-то скользящее окно (в данной работе за это отвечает отрезок [-0.154, 0.154]). Окно может быть как константным, так и плавающим в зависимости от данных (показано ниже). Из сказанного выше можно прийти к следующим утверждениям: Действительно, чтобы определить к какому классу относятся высказывания «хорошая погода», «быстро ехать», достаточно проверить в обучающей выборке как часто «хорошая погода» и «быстро ехать» встречается рядом с заведомо (установленными человеком в зависимости от модели данных и тестовой выборки) хорошими и плохими словами и установить разницу. Пойдем немного дальше и вместо сравнений с 1 опорным словом с негативной и позитивной стороны, будем использовать набор заведомо хороших и плохих слов (здесь, например, использовались такие слова: Положительные: good, nice, excellent, perfect, correct, super Отрицательные: bad, nasty, poor, terrible, wrong, awful Соответственно, окончательная формула Так, с обсчетом SO разобрались, но как подобрать подходящих кандидатов? К примеру, есть у нас предложение «Сегодня чудное утро, хорошо бы поехать к озеру». Логично предположить, что эмоциональный окрас предложению добавляют в большей части прилагательные и наречия. Поэтому чтобы воспользоваться этим, построим конечный автомат, который будет по заданным шаблонам частей речи, выделять из предложения кандидатов на оценку SO. Не трудно догадаться, что предложения будет считаться позитивным отзывом, если сумма SO всех кандидатов > 0.154 В данной работе использовались следующие шаблоны: В таком случае кандидатами будут: 1. чудное утро 2. хорошо поехать Осталось только собрать все вместе и протестировать. Реализация Здесь вы найдете исходники на Java. Красоты там мало — писалось просто чтобы попробовать и решить, будет ли использоваться дальше Корпус: Amazon Product Review Data (> 5.8 миллионов отзывов) liu2.cs.uic.edu/data На этом корпусе с помощью Lucene строился инвертированный индекс, по которому и производился поиск. В случае отсутствия данных в индексе, использовались поисковые системы Google(api) и Yahoo! (с их оператором around и near соответственно). Но, к сожалению, из-за скорости работы и неточности результатов (по высокочастотным запросам поисковики выдают приближенное значение количества результатов), решение не совершенно. Для определения частей речи и токенизации использовалась библиотека OpenNLP Как лучше? Исходя из вышеизложенного, наиболее предпочтительными векторами усовершенствований являются: 1. Построение более полного дерева разбора частей речи для фильтрации кандидатов 2. Использование бОльшего корпуска в качестве обучающей выборки 3. По возможности использование обучающего корпуса из той же socialmedia, что и тестовая выборка 4. Формирование опорных слов (good|bad) в зависимости от источника данных и тематики 5. Внедрение отрицания в дерево разбора шаблонов 6. Определение сарказма Выводы В целом, система, основанная на PMI может составить конкуренцию системам, основанным на принципе «bag of words», но в идеальной реализации эти две системы должны дополнять друг друга: при отсутствии данных в обучающей выборке, в дело должна вступать система подсчета конкретных слов. Использованная литература: 1. Введение в информационный поиск. К. Маннинг, П. Рагхаван, Х. Шютце 2. Foundations of statistical Natural Language Processing. C. Manning, H. Schutze 3.Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. Peter D. Turney\n","metric":0.544921875,"metric_input":"Текст статьи:\nСистема мониторинга мнений методом поточечной взаимной информации / Хабр                                                  \n\n26-03-2012\nЗдравствуйте. Если вы занимаетесь DataMining, анализом текстов на выявление мнений или вам просто интересны статистические модели для оценки эмоциональной окраски предложений — эта статья может оказаться интересной. Далее, чтобы не тратить время потенциального читателя впустую на груду теории и рассуждений, сразу краткие результаты. Реализованный подход работает приблизительно с 55% точностью в трех классах: негативный, нейтральный, позитивный. Как говорит Википедия, 70% точность приблизительно равна точности человеческих суждений в среднем (в силу субъективности трактований каждого). Следует отметить, что существует немало утилит с точностью выше полученной мной, но описанный подход, можно достаточно просто усовершенствовать (будет описано ниже) и получить в итоге 65-70%. Если после всего вышеизложенного у вас осталось желание читать — добро пожаловать под кат. Краткое описание принципа Чтобы определить эмоциональную окраску предложения (SO — sentiment orientation), необходимо понять о чем в нем идет речь. Это логично. Но как объяснить машине, что такое хорошо, а что такое плохо? Первым вариантом, сразу же появляющемся на уме, есть сумма количества плохих/хороших слов, умноженных на вес каждого. Так называемый подход, основанный на «мешке слов». Удивительно простой и быстрый алгоритм, в сочетании с предобработкой на основе правил дающий неплохие результаты (до 60 — 80% точности в зависимости от корпуса). По сути, такой подход является примером униграммной модели, а это значит, что в самом наивном случае предложения «This product rather good than bad» и «This product rather bad than good» будут иметь одинаковую SO. Решить эту проблему можно перейдя от униграммной к мультиномиальной модели. Также следует отметить, что необходимо иметь солидный постоянно обновляемый словарь, содержащий плохие и хорошие термы + их вес, который может быть специфичным в зависимости от данных. Примером простейшей мультиномиальной модели является наивный метод Байеса. На хабре есть несколько статей, посвященных нему, в частности эта. Преимущество мультиномиальной модели перед униграммной заключается в том, что мы можем учитывать контекст, в котором то или иное высказывание было произнесено. Это решает проблему с предложениями, описанными выше, но вводит новое ограничение: если в обучающей выборке отсутствует выбранный n-gram, то SO на тестовых данных будет равно 0. Эта проблема всегда была и будет. Ее можно решить 2 способами: увеличив объем обучающей выборки (не забывая, что можно попутно схватить эффект переобучения), либо используя сглаживание (например Лапласа или Good-Turing). Наконец-то мы плавно подошли к идее PMI. Наряду с формулой Байеса , введем понятие PMI — pointwise mutual information, поточечная взаимная информация. в приведенной выше формуле A и B — слова/биграммы/n-нграммы, P(A), P(B) — априорные вероятности появления термина А и B соответственно в обучающей выборке (отношение количества вхождений к общему количеству слов в корпусе), P(A near B) — вероятность термина A встретиться вместе/рядом с термином B; «рядом» можно конфигурировать вручную, по умолчанию расстояние равно 10 терминам влево и вправо; основание логарифма не играет роли, для простоты примем его равным 2. Позитивный знак логарифма будет означать положительны окрас A по сравнению с B, негативный — отрицательный. Для нахождения нейтральных отзывов, можно принять какое-то скользящее окно (в данной работе за это отвечает отрезок [-0.154, 0.154]). Окно может быть как константным, так и плавающим в зависимости от данных (показано ниже). Из сказанного выше можно прийти к следующим утверждениям: Действительно, чтобы определить к какому классу относятся высказывания «хорошая погода», «быстро ехать», достаточно проверить в обучающей выборке как часто «хорошая погода» и «быстро ехать» встречается рядом с заведомо (установленными человеком в зависимости от модели данных и тестовой выборки) хорошими и плохими словами и установить разницу. Пойдем немного дальше и вместо сравнений с 1 опорным словом с негативной и позитивной стороны, будем использовать набор заведомо хороших и плохих слов (здесь, например, использовались такие слова: Положительные: good, nice, excellent, perfect, correct, super Отрицательные: bad, nasty, poor, terrible, wrong, awful Соответственно, окончательная формула Так, с обсчетом SO разобрались, но как подобрать подходящих кандидатов? К примеру, есть у нас предложение «Сегодня чудное утро, хорошо бы поехать к озеру». Логично предположить, что эмоциональный окрас предложению добавляют в большей части прилагательные и наречия. Поэтому чтобы воспользоваться этим, построим конечный автомат, который будет по заданным шаблонам частей речи, выделять из предложения кандидатов на оценку SO. Не трудно догадаться, что предложения будет считаться позитивным отзывом, если сумма SO всех кандидатов > 0.154 В данной работе использовались следующие шаблоны: В таком случае кандидатами будут: 1. чудное утро 2. хорошо поехать Осталось только собрать все вместе и протестировать. Реализация Здесь вы найдете исходники на Java. Красоты там мало — писалось просто чтобы попробовать и решить, будет ли использоваться дальше Корпус: Amazon Product Review Data (> 5.8 миллионов отзывов) liu2.cs.uic.edu/data На этом корпусе с помощью Lucene строился инвертированный индекс, по которому и производился поиск. В случае отсутствия данных в индексе, использовались поисковые системы Google(api) и Yahoo! (с их оператором around и near соответственно). Но, к сожалению, из-за скорости работы и неточности результатов (по высокочастотным запросам поисковики выдают приближенное значение количества результатов), решение не совершенно. Для определения частей речи и токенизации использовалась библиотека OpenNLP Как лучше? Исходя из вышеизложенного, наиболее предпочтительными векторами усовершенствований являются: 1. Построение более полного дерева разбора частей речи для фильтрации кандидатов 2. Использование бОльшего корпуска в качестве обучающей выборки 3. По возможности использование обучающего корпуса из той же socialmedia, что и тестовая выборка 4. Формирование опорных слов (good|bad) в зависимости от источника данных и тематики 5. Внедрение отрицания в дерево разбора шаблонов 6. Определение сарказма Выводы В целом, система, основанная на PMI может составить конкуренцию системам, основанным на принципе «bag of words», но в идеальной реализации эти две системы должны дополнять друг друга: при отсутствии данных в обучающей выборке, в дело должна вступать система подсчета конкретных слов. Использованная литература: 1. Введение в информационный поиск. К. Маннинг, П. Рагхаван, Х. Шютце 2. Foundations of statistical Natural Language Processing. C. Manning, H. Schutze 3.Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. Peter D. Turney\n\n\nКраткое содержание:\nВ статье рассматривается метод мониторинга мнений с использованием поточечной взаимной информации (PMI). PMI - это статистический метод, который позволяет определить эмоциональную окраску предложений на основе их контекста. Этот метод основан на подсчете вероятности появления определенных терминов в контексте друг друга.\n\nДля определения эмоциональной окраски предложения, необходимо учитывать не только отдельные слова, но и их контекст. Это можно сделать с помощью мультиномиальной модели, которая учитывает вероятность появления терминов в определенном контексте.\n\nДля обучения модели необходимо иметь большой корпус данных, содержащий различные высказывания. Затем можно использовать эту модель для определения эмоциональной окраски новых предложений.\n\nВ статье также представлены результаты эксперимента, проведенного на корпусе данных Amazon Product Review Data. Эксперимент показал, что метод PMI может определить эмоциональную окраску предложений с точностью около 55%.\n\nВ целом, метод PMI является эффективным инструментом для определения эмоциональной окраски предложений, но для улучшения точности необходимо увеличить объем обучающей выборки и учесть дополнительные факторы, такие как отрицание и сарказм.","summary":"В статье рассматривается метод мониторинга мнений с использованием поточечной взаимной информации (PMI). PMI - это статистический метод, который позволяет определить эмоциональную окраску предложений на основе их контекста. Этот метод основан на подсчете вероятности появления определенных терминов в контексте друг друга.\n\nДля определения эмоциональной окраски предложения, необходимо учитывать не только отдельные слова, но и их контекст. Это можно сделать с помощью мультиномиальной модели, которая учитывает вероятность появления терминов в определенном контексте.\n\nДля обучения модели необходимо иметь большой корпус данных, содержащий различные высказывания. Затем можно использовать эту модель для определения эмоциональной окраски новых предложений.\n\nВ статье также представлены результаты эксперимента, проведенного на корпусе данных Amazon Product Review Data. Эксперимент показал, что метод PMI может определить эмоциональную окраску предложений с точностью около 55%.\n\nВ целом, метод PMI является эффективным инструментом для определения эмоциональной окраски предложений, но для улучшения точности необходимо увеличить объем обучающей выборки и учесть дополнительные факторы, такие как отрицание и сарказм."}
{"Text":"Как я нашёл аппаратный сбой в планшете Lenovo MiiX 2 10 / Хабр                                                          \n\n04-03-2020\nВ статье речь идёт о проблемах с питанием/батареей для этого планшета такого вида: во время обычной работы планшета начинают происходить события Во время работы показатель заряда батареи перестаёт меняться При блокировке экрана на шторке в правом нижнем углу исчезает показатель заряда батареи В диспетчере задач устройство «Батарея» более не отображается По прошествии времени планшет «внезапно» отключается. После подключения зарядного устройства на кнопке включения мигает красный светодиод, сообщая о полном опустошении батареи После зарядки и загрузки планшет продолжает сбоить Как оказалось, причиной этому является аппаратная проблема с данной моделью планшетов Lenovo. Итак, я купил планшет Lenovo MiiX 2 10. Какое-то время всё было хорошо, он работал, ОС обновлялась и т. д. Затем начались сбои, описанные в вышеприведённом списке. Они происходили через каждые 5-20 дней. Обновление/замена драйверов не помогли. Поскольку планшет использует Windows (8.1), то напрашивался простой ответ: виновата винда и M$, конечно же. И да, после переустановки (восстановления) Windows планшет работал хорошо. Какое-то время я думал, что вопрос решён. Но переустанавливать винду раз в неделю как-то надоедало, и я отдал его в гарантийный ремонт в официальный сервис Lenovo. Спустя пару недель получил его обратно. В мастерской (сервисе) мне сказали, что они заменили материнскую плату + экран + батарею. Насчёт экрана вроде была правда — тач сильно тупил в последнее время до ремонта, а после — стало лучше. Однако сбои продолжались, а гарантия — кончилась. Я продолжал пользоваться планшетом — вещь хорошая, заменять на что-то другое было слишком дорого. И тут после очередной переустановки винды планшет «исправился» — перестал чудить с питанием. Обрадовавшись, я собрался написать статью о том, какой я умный как я решил эту трудную задачу, непосильную официальному сервису. Но по своему опыту я знал, что пролететь в таком вопросе очень легко, и поэтому восстановил все предыдущие условия, при которых наблюдались проблемы с питанием. И сбои вернулись… Нда, иногда они возвращаются Возникли вопросы: Почему сбои исчезали? Почему они вернулись? Что тут вообще /censored/ происходит? Обычно мне в таких случаях вспоминается песня Четырёх Неблондинок «Что за дела?! Что происходит?». Ок, начинаем разбираться. Используя научные методы действуя методом исключения (вычитания («дедукции», как переводят умственно ограниченные переводчики)) начал сравнивать два состояния: «когда не работало» против «когда работало». Простой перебор привёл к удивительному открытию: работало с новой точкой доступа, не работало — со старой. Исследования были длительными, и успел обновить оборудование во время проведения опытов. Вообще как это бывает?! Ну вот бывает. Точки отличались: старая на {802.11n, 2,4 ГГц}, новая на {802.11ac, 2,4 + 5 ГГц}. Планшет поддерживает {802.11n, 2,4 + 5 ГГц}. Дальнейшие исследования дали: планшет хорошо работал на 5 ГГц, сбоил на 2,4 ГГц, независимо от точек доступа. Для проверки купил USB сетевую карту Wi-Fi на {802.11ac, 2,4 + 5 ГГц}. С ней планшет работал в любом случае хорошо. Опять же: вообще как это бывает?! Ну вот бывает. Поняв примерно в чём тут дело, я стал копать дальше. Тут я заметил, что при сбоях в «Диспетчере Устройств Windows» у некоего «Устройства HID» возникает предупреждающий значок. Из названия устройства ничего не понятно. Но есть «хакерский» приём: в «Свойствах устройства» на вкладке «Сведения» можно посмотреть какие-то строки, описывающие его. Меня заинтересовала строка «Путь к экземпляру устройства», содержащая «ITE8568». Поискав, я нашёл, что «ITE8568» сильно похоже на (возможный) контроллер питания планшетов и маломощных ноутбуков «ITE IT8568» (Power IC Chip Chipset). Причина происходивших сбоев При работе планшета с сетью Wi-Fi через встроенный адаптер на частоте 2,4 ГГц происходят сбои и зависания микросхемы питания «ITE IT8568». Что делать дальше Можно обойти сбои, не используя сеть Wi-Fi вообще, или используя внешний Wi-Fi адаптер, подключая его по USB. Но работать с планшетом становится слишком неудобно. С USB Type A можно использовать мелкий Wi-Fi адаптер, но у планшета только microUSB, и ток отдаёт только до 0,5 А, чего не хватает для 802.11ac (или сотового модема 4G). Можно использовать внешний Wi-Fi адаптер только на частоте 5 ГГц, но тогда Уменьшается дальность работы Wi-Fi Невозможно использовать точки доступа с частотой 2,4 ГГц Невозможно соединяться со многими смартфонами и сотовыми модемами, работающими на частоте 2,4 ГГц без 5 ГГц, коих много, в т.ч. у меня. Поэтому я предпочитаю возвернуть сделку. Обращение к компании Lenovo Я купил планшет Lenovo MiiX 2 10, который никогда не работал должным образом, но сбоил. Я нашёл причину сбоев, и это есть аппаратная ошибка. Я понимаю, что ошибки бывают у всех, поэтому (пока что) особо не ругаюсь. Но надо исправить её. Предлагаю обсудить возможные способы решения данного вопроса. Один из них — возврат в исходное состояние насколько это возможно: я отдаю вам сбоящий планшет, вы мне возвращаете уплаченные мной деньги. К этому можно присоединить требования о возмещении морального вреда, издержек на проведение исследований, потерь от сбоев и прочего — надеюсь, читатели Хабра мне подскажут что можно получить. Т.к. дело происходило и происходит в России, то на большую компенсацию рассчитывать, наверное, не стоит. Можно было бы решить вопрос заменой на более новую модель, но от Windows 10 мне пришлось отказаться по причине смены политики компании Микрософт в отношении пользовательских данных (назовём это так), а до окончания поддержки Windows 8.1 уже недалеко. Т.ч. мне пришлось перейти на Linux (и немного на FreeBSD). Дополнение от 10 марта 2020 года — прояснение своих целей: Я написал эту статью чтобы любой пользователь этого планшета мог узнать, что можно сделать при сбоях в работе. Чтобы какая-то организация начала что-то делать, ей обычно надо придать стимул. Для справки: слово «стимул» происходит от латинского слова stimulus — «палка погонщика слона или острый металлический наконечник на шесте, которым погоняют буйвола (быка), запряженного в повозку». Почему на Хабре можно обсуждать падающие «Боинги» и качество лампочек, но нельзя — технические подробности ошибок в планшете, который точно есть у некоторых пользователей? Почему нельзя заставлять компанию производить качественные вещи? Какие ошибки должна совершить компания, чтобы можно было это обсуждать здесь? Какое оборудование должно сбоить — ноутбуки, сервера, самолёты? Ещё причина: я провожу подготовку к судебному разбирательству. Как происходит подача иска: приходишь в суд, попадаешь к секретарю суда, и он/она тебя первым делом спрашивает: «А говорили ли Вы с противной стороной?». Обычно не имеет смысла начинать судебное разбирательство сразу, поскольку стороны могут договориться. На Хабре ест блог компании Lenovo, я послал им извещение через сервис Хабра lenovo — теперь жду ответа. Я рассчитываю, что по итогам моей работы любой владелец этого планшета сможет решить свой вопрос по сбоям и зависаниям из-за описанной в статье ошибки — бесплатной починкой даже по истечении гарантийного срока, заменой на работающую модель или возвратом денег. Это и есть действие знающего человека и ответственного гражданина. Если кто-то не отстаивает свои права, то по сути никаких прав у этого кого-то нет — он сам (сама) от них отказался (-ась).\n","metric":0.54833984380000000414,"metric_input":"Текст статьи:\nКак я нашёл аппаратный сбой в планшете Lenovo MiiX 2 10 / Хабр                                                          \n\n04-03-2020\nВ статье речь идёт о проблемах с питанием/батареей для этого планшета такого вида: во время обычной работы планшета начинают происходить события Во время работы показатель заряда батареи перестаёт меняться При блокировке экрана на шторке в правом нижнем углу исчезает показатель заряда батареи В диспетчере задач устройство «Батарея» более не отображается По прошествии времени планшет «внезапно» отключается. После подключения зарядного устройства на кнопке включения мигает красный светодиод, сообщая о полном опустошении батареи После зарядки и загрузки планшет продолжает сбоить Как оказалось, причиной этому является аппаратная проблема с данной моделью планшетов Lenovo. Итак, я купил планшет Lenovo MiiX 2 10. Какое-то время всё было хорошо, он работал, ОС обновлялась и т. д. Затем начались сбои, описанные в вышеприведённом списке. Они происходили через каждые 5-20 дней. Обновление/замена драйверов не помогли. Поскольку планшет использует Windows (8.1), то напрашивался простой ответ: виновата винда и M$, конечно же. И да, после переустановки (восстановления) Windows планшет работал хорошо. Какое-то время я думал, что вопрос решён. Но переустанавливать винду раз в неделю как-то надоедало, и я отдал его в гарантийный ремонт в официальный сервис Lenovo. Спустя пару недель получил его обратно. В мастерской (сервисе) мне сказали, что они заменили материнскую плату + экран + батарею. Насчёт экрана вроде была правда — тач сильно тупил в последнее время до ремонта, а после — стало лучше. Однако сбои продолжались, а гарантия — кончилась. Я продолжал пользоваться планшетом — вещь хорошая, заменять на что-то другое было слишком дорого. И тут после очередной переустановки винды планшет «исправился» — перестал чудить с питанием. Обрадовавшись, я собрался написать статью о том, какой я умный как я решил эту трудную задачу, непосильную официальному сервису. Но по своему опыту я знал, что пролететь в таком вопросе очень легко, и поэтому восстановил все предыдущие условия, при которых наблюдались проблемы с питанием. И сбои вернулись… Нда, иногда они возвращаются Возникли вопросы: Почему сбои исчезали? Почему они вернулись? Что тут вообще /censored/ происходит? Обычно мне в таких случаях вспоминается песня Четырёх Неблондинок «Что за дела?! Что происходит?». Ок, начинаем разбираться. Используя научные методы действуя методом исключения (вычитания («дедукции», как переводят умственно ограниченные переводчики)) начал сравнивать два состояния: «когда не работало» против «когда работало». Простой перебор привёл к удивительному открытию: работало с новой точкой доступа, не работало — со старой. Исследования были длительными, и успел обновить оборудование во время проведения опытов. Вообще как это бывает?! Ну вот бывает. Точки отличались: старая на {802.11n, 2,4 ГГц}, новая на {802.11ac, 2,4 + 5 ГГц}. Планшет поддерживает {802.11n, 2,4 + 5 ГГц}. Дальнейшие исследования дали: планшет хорошо работал на 5 ГГц, сбоил на 2,4 ГГц, независимо от точек доступа. Для проверки купил USB сетевую карту Wi-Fi на {802.11ac, 2,4 + 5 ГГц}. С ней планшет работал в любом случае хорошо. Опять же: вообще как это бывает?! Ну вот бывает. Поняв примерно в чём тут дело, я стал копать дальше. Тут я заметил, что при сбоях в «Диспетчере Устройств Windows» у некоего «Устройства HID» возникает предупреждающий значок. Из названия устройства ничего не понятно. Но есть «хакерский» приём: в «Свойствах устройства» на вкладке «Сведения» можно посмотреть какие-то строки, описывающие его. Меня заинтересовала строка «Путь к экземпляру устройства», содержащая «ITE8568». Поискав, я нашёл, что «ITE8568» сильно похоже на (возможный) контроллер питания планшетов и маломощных ноутбуков «ITE IT8568» (Power IC Chip Chipset). Причина происходивших сбоев При работе планшета с сетью Wi-Fi через встроенный адаптер на частоте 2,4 ГГц происходят сбои и зависания микросхемы питания «ITE IT8568». Что делать дальше Можно обойти сбои, не используя сеть Wi-Fi вообще, или используя внешний Wi-Fi адаптер, подключая его по USB. Но работать с планшетом становится слишком неудобно. С USB Type A можно использовать мелкий Wi-Fi адаптер, но у планшета только microUSB, и ток отдаёт только до 0,5 А, чего не хватает для 802.11ac (или сотового модема 4G). Можно использовать внешний Wi-Fi адаптер только на частоте 5 ГГц, но тогда Уменьшается дальность работы Wi-Fi Невозможно использовать точки доступа с частотой 2,4 ГГц Невозможно соединяться со многими смартфонами и сотовыми модемами, работающими на частоте 2,4 ГГц без 5 ГГц, коих много, в т.ч. у меня. Поэтому я предпочитаю возвернуть сделку. Обращение к компании Lenovo Я купил планшет Lenovo MiiX 2 10, который никогда не работал должным образом, но сбоил. Я нашёл причину сбоев, и это есть аппаратная ошибка. Я понимаю, что ошибки бывают у всех, поэтому (пока что) особо не ругаюсь. Но надо исправить её. Предлагаю обсудить возможные способы решения данного вопроса. Один из них — возврат в исходное состояние насколько это возможно: я отдаю вам сбоящий планшет, вы мне возвращаете уплаченные мной деньги. К этому можно присоединить требования о возмещении морального вреда, издержек на проведение исследований, потерь от сбоев и прочего — надеюсь, читатели Хабра мне подскажут что можно получить. Т.к. дело происходило и происходит в России, то на большую компенсацию рассчитывать, наверное, не стоит. Можно было бы решить вопрос заменой на более новую модель, но от Windows 10 мне пришлось отказаться по причине смены политики компании Микрософт в отношении пользовательских данных (назовём это так), а до окончания поддержки Windows 8.1 уже недалеко. Т.ч. мне пришлось перейти на Linux (и немного на FreeBSD). Дополнение от 10 марта 2020 года — прояснение своих целей: Я написал эту статью чтобы любой пользователь этого планшета мог узнать, что можно сделать при сбоях в работе. Чтобы какая-то организация начала что-то делать, ей обычно надо придать стимул. Для справки: слово «стимул» происходит от латинского слова stimulus — «палка погонщика слона или острый металлический наконечник на шесте, которым погоняют буйвола (быка), запряженного в повозку». Почему на Хабре можно обсуждать падающие «Боинги» и качество лампочек, но нельзя — технические подробности ошибок в планшете, который точно есть у некоторых пользователей? Почему нельзя заставлять компанию производить качественные вещи? Какие ошибки должна совершить компания, чтобы можно было это обсуждать здесь? Какое оборудование должно сбоить — ноутбуки, сервера, самолёты? Ещё причина: я провожу подготовку к судебному разбирательству. Как происходит подача иска: приходишь в суд, попадаешь к секретарю суда, и он/она тебя первым делом спрашивает: «А говорили ли Вы с противной стороной?». Обычно не имеет смысла начинать судебное разбирательство сразу, поскольку стороны могут договориться. На Хабре ест блог компании Lenovo, я послал им извещение через сервис Хабра lenovo — теперь жду ответа. Я рассчитываю, что по итогам моей работы любой владелец этого планшета сможет решить свой вопрос по сбоям и зависаниям из-за описанной в статье ошибки — бесплатной починкой даже по истечении гарантийного срока, заменой на работающую модель или возвратом денег. Это и есть действие знающего человека и ответственного гражданина. Если кто-то не отстаивает свои права, то по сути никаких прав у этого кого-то нет — он сам (сама) от них отказался (-ась).\n\n\nКраткое содержание:\nВ статье рассматривается проблема с питанием/батареей планшета Lenovo MiiX 2 10. Автор описывает сбои в работе устройства, такие как изменение показателя заряда батареи, внезапное отключение и другие. Он проводит исследования и обнаруживает, что сбои возникают из-за аппаратной проблемы с контроллером питания \"ITE IT8568\". Автор предлагает возможные способы решения проблемы, включая возврат денег или возмещение морального ущерба.","summary":"В статье рассматривается проблема с питанием/батареей планшета Lenovo MiiX 2 10. Автор описывает сбои в работе устройства, такие как изменение показателя заряда батареи, внезапное отключение и другие. Он проводит исследования и обнаруживает, что сбои возникают из-за аппаратной проблемы с контроллером питания \"ITE IT8568\". Автор предлагает возможные способы решения проблемы, включая возврат денег или возмещение морального ущерба."}
{"Text":"Разработан скрипт для кражи ПИН-кода через браузер смартфона / Habr                         \n\n11-04-2017\nРазличные способы ввода влияют на углы наклона, но нейросеть распознаёт ПИН-код в любом случае после сбора данных скриптом Как известно, в любом смартфоне установлено множество датчиков — GPS, свет, датчики ориентации, движения и ещё около 20 сенсоров — все они непрерывно выдают данные, чтобы смартфон мог взаимодействовать с окружающим физическим миром. У разработчиков есть несколько способов доступа к информации с этих сенсоров, в том числе через мобильный браузер, согласно официальным Javascript API. Именно этот способ открывает широкое поле возможностей для удалённого взлома. Например, специалисты из Школы информатики при Университете Ньюкасла разработали скрипт PINlogger.js, с помощью которого через браузер можно определять, какие кнопки нажимает пользователь на экранной клавиатуре. Скрипт считывает информацию с сенсоров наклона мобильного устройства при каждом нажатии. Удивительно, но в некоторых браузерах (Safari) запущенный в браузере скрипт продолжает собирать данные даже с залоченного экрана. Аналогично он работает, если на соседней вкладке вы набираете пароль в онлайн-банкинге, не закрыв предыдущую вкладку. Этот способ подходит для кражи ПИН-кода как через вредоносный сайт со скриптом, так и из любого мобильного приложения, которое имеет доступ к информации с сенсоров. Точность распознавания цифр ПИН-кода довольно высока. Четырёхзначный ПИН-код распознаётся с первой попытки с вероятностью 70%, со второй — 86%, с третьей — 94%, а с пятой попытки — с вероятностью 98% для разных пользователей или 99% для одного пользователя. Векторы атаки с соседних вкладок (b) работают только в Chrome и Dolphin под iOS, а атака из фрейма (a) — во всех популярных браузерах, в том числе Safari, Chrome, Firefox, Opera и Dolphin Для практической демонстрации атаки был разработан специальный интерфейс для ввода четырёхзначных ПИН-кодов. Клиентская часть отправляла на сервер показания сенсоров ориентации и движения. Серверную часть реализовали на Node.js, а показания сенсоров сохраняли в базе данных MobgoLab. В данной демонстрации исследователям было легко определять события нажатий по клавиатуре с помощью события onkeydown. Авторы научной работы обращают внимание, что аналогичный подход используется в других программах для сборка ПИН-кодов по информации с сенсоров, таких как TouchLogger и TapLogger. Если нужно обнаружить нажатия на залоченном экране, то может понадобиться более сложный сценарий сегментации данных сенсора, чтобы вычленить нужные фрагменты данных. Например, это можно сделать по измерению пиковых амплитуд сигнала и с использованием дополнительных сенсоров, как показано в предыдущих научных работах. Код для ввода данных опубликован на GitHub. В базе данных накапливается информация о нажатиях и показаниях сенсоров в момент нажатия. Каждый из 10 добровольцев по пять раз вводил 50 четырёхзначных ПИН-кодов (всего 2488 правильно введённых ПИН-кода). Этот набор данных затем использовался для извлечения признаков и последующего обучения нейросети (70% данных использовались для обучения, 15% для проверки и 15% для тестирования). Судя по результатам сравнительных тестов, PINlogger.js гораздо лучше распознаёт ПИН-коды по информации с сенсоров движения, чем предыдущие аналогичные разработки. К тому же это первая система для кражи ПИН-кодов через браузер, а не через приложение, установленное в смартфоне. Исследователи обращают внимание, что большинство пользователей не осознают опасность утечки персональных данных путём съёма данных с сенсоров. Опрос пользователей показал, что большинство из них видят угрозу в GPS или в камере, а не в том, что представляет более реальную угрозу — во внутренних датчиках движения, наклона и других. Число 25 приведено не случайно: именно столько датчиков насчитали исследователи в стандартном смартфоне. На диаграмме внизу показаны результаты опроса — в каких из 25 сенсоров пользователи видят наибольшую угрозу. Пользователи не только не осознают угрозу. Многие даже не понимают, какую именно информацию собирает большая часть из 25 сенсоров. О некоторых сенсорах люди даже не слышали. Проблема в том, что эта информация не защищается на уровне операционной системы. Она легко доступна и для приложений, и для любых сайтов через браузеров — без разрешения. Только несколько сенсоров требуют явного разрешения на предоставление информации, в том числе камера и GPS. Остальные предоставляют информацию совершенно открыто. С помощью этих открытых данных можно не только собирать ПИН-коды и пароли, но и следить за пользователем. Любой сайт может определить, когда человек принимает звонок и кладёт трубку, … когда человек сидит, идёт или бежит. Повторим, вся эта информация доступна любому сайту и любому приложению без какого-либо разрешения. Сайты получают её стандартными средствами JavaScript. Специалистам по безопасности и крупнейшим игрокам мобильной индустрии эта проблема безопасности хорошо известна, но пока они не пришли к единому мнению, как её решить. Научная работа опубликована 7 апреля 2017 года в журнале International Journal of Information Security (doi:10.1007/s10207-017-0369-x), в открытом доступе.\n","metric":0.48754882809999999793,"metric_input":"Текст статьи:\nРазработан скрипт для кражи ПИН-кода через браузер смартфона / Habr                         \n\n11-04-2017\nРазличные способы ввода влияют на углы наклона, но нейросеть распознаёт ПИН-код в любом случае после сбора данных скриптом Как известно, в любом смартфоне установлено множество датчиков — GPS, свет, датчики ориентации, движения и ещё около 20 сенсоров — все они непрерывно выдают данные, чтобы смартфон мог взаимодействовать с окружающим физическим миром. У разработчиков есть несколько способов доступа к информации с этих сенсоров, в том числе через мобильный браузер, согласно официальным Javascript API. Именно этот способ открывает широкое поле возможностей для удалённого взлома. Например, специалисты из Школы информатики при Университете Ньюкасла разработали скрипт PINlogger.js, с помощью которого через браузер можно определять, какие кнопки нажимает пользователь на экранной клавиатуре. Скрипт считывает информацию с сенсоров наклона мобильного устройства при каждом нажатии. Удивительно, но в некоторых браузерах (Safari) запущенный в браузере скрипт продолжает собирать данные даже с залоченного экрана. Аналогично он работает, если на соседней вкладке вы набираете пароль в онлайн-банкинге, не закрыв предыдущую вкладку. Этот способ подходит для кражи ПИН-кода как через вредоносный сайт со скриптом, так и из любого мобильного приложения, которое имеет доступ к информации с сенсоров. Точность распознавания цифр ПИН-кода довольно высока. Четырёхзначный ПИН-код распознаётся с первой попытки с вероятностью 70%, со второй — 86%, с третьей — 94%, а с пятой попытки — с вероятностью 98% для разных пользователей или 99% для одного пользователя. Векторы атаки с соседних вкладок (b) работают только в Chrome и Dolphin под iOS, а атака из фрейма (a) — во всех популярных браузерах, в том числе Safari, Chrome, Firefox, Opera и Dolphin Для практической демонстрации атаки был разработан специальный интерфейс для ввода четырёхзначных ПИН-кодов. Клиентская часть отправляла на сервер показания сенсоров ориентации и движения. Серверную часть реализовали на Node.js, а показания сенсоров сохраняли в базе данных MobgoLab. В данной демонстрации исследователям было легко определять события нажатий по клавиатуре с помощью события onkeydown. Авторы научной работы обращают внимание, что аналогичный подход используется в других программах для сборка ПИН-кодов по информации с сенсоров, таких как TouchLogger и TapLogger. Если нужно обнаружить нажатия на залоченном экране, то может понадобиться более сложный сценарий сегментации данных сенсора, чтобы вычленить нужные фрагменты данных. Например, это можно сделать по измерению пиковых амплитуд сигнала и с использованием дополнительных сенсоров, как показано в предыдущих научных работах. Код для ввода данных опубликован на GitHub. В базе данных накапливается информация о нажатиях и показаниях сенсоров в момент нажатия. Каждый из 10 добровольцев по пять раз вводил 50 четырёхзначных ПИН-кодов (всего 2488 правильно введённых ПИН-кода). Этот набор данных затем использовался для извлечения признаков и последующего обучения нейросети (70% данных использовались для обучения, 15% для проверки и 15% для тестирования). Судя по результатам сравнительных тестов, PINlogger.js гораздо лучше распознаёт ПИН-коды по информации с сенсоров движения, чем предыдущие аналогичные разработки. К тому же это первая система для кражи ПИН-кодов через браузер, а не через приложение, установленное в смартфоне. Исследователи обращают внимание, что большинство пользователей не осознают опасность утечки персональных данных путём съёма данных с сенсоров. Опрос пользователей показал, что большинство из них видят угрозу в GPS или в камере, а не в том, что представляет более реальную угрозу — во внутренних датчиках движения, наклона и других. Число 25 приведено не случайно: именно столько датчиков насчитали исследователи в стандартном смартфоне. На диаграмме внизу показаны результаты опроса — в каких из 25 сенсоров пользователи видят наибольшую угрозу. Пользователи не только не осознают угрозу. Многие даже не понимают, какую именно информацию собирает большая часть из 25 сенсоров. О некоторых сенсорах люди даже не слышали. Проблема в том, что эта информация не защищается на уровне операционной системы. Она легко доступна и для приложений, и для любых сайтов через браузеров — без разрешения. Только несколько сенсоров требуют явного разрешения на предоставление информации, в том числе камера и GPS. Остальные предоставляют информацию совершенно открыто. С помощью этих открытых данных можно не только собирать ПИН-коды и пароли, но и следить за пользователем. Любой сайт может определить, когда человек принимает звонок и кладёт трубку, … когда человек сидит, идёт или бежит. Повторим, вся эта информация доступна любому сайту и любому приложению без какого-либо разрешения. Сайты получают её стандартными средствами JavaScript. Специалистам по безопасности и крупнейшим игрокам мобильной индустрии эта проблема безопасности хорошо известна, но пока они не пришли к единому мнению, как её решить. Научная работа опубликована 7 апреля 2017 года в журнале International Journal of Information Security (doi:10.1007/s10207-017-0369-x), в открытом доступе.\n\n\nКраткое содержание:\nСтатья исследует возможность кражи PIN-кодов через мобильные браузеры, используя информацию с сенсоров на смартфонах. Разработанный скрипт PINlogger.js анализирует данные с сенсоров наклона устройства при каждом нажатии на экранную клавиатуру. Точность распознавания PIN-кода высока: с первой попытки вероятность составляет 70%, со второй - 86%, с третьей - 94%, а с пятой - 98-99%. Исследователи продемонстрировали эту возможность, создав специальный интерфейс для ввода PIN-кодов. Они также обратили внимание на то, что аналогичное программное обеспечение уже используется для сбора PIN-кодов с помощью сенсоров движения.","summary":"Статья исследует возможность кражи PIN-кодов через мобильные браузеры, используя информацию с сенсоров на смартфонах. Разработанный скрипт PINlogger.js анализирует данные с сенсоров наклона устройства при каждом нажатии на экранную клавиатуру. Точность распознавания PIN-кода высока: с первой попытки вероятность составляет 70%, со второй - 86%, с третьей - 94%, а с пятой - 98-99%. Исследователи продемонстрировали эту возможность, создав специальный интерфейс для ввода PIN-кодов. Они также обратили внимание на то, что аналогичное программное обеспечение уже используется для сбора PIN-кодов с помощью сенсоров движения."}
{"Text":"Tinc — настройка VPN в Ubuntu / Habr             \n\n29-01-2022\nTinc — это открытый сетевой протокол и программная реализация, используемая для сжатых и зашифрованных виртуальных частных сетей. Это проект был начат в 1998 году Гусом Слипеном, Иво Тиммермансом и Весселем Данкерсом под лицензией GPL. К его основным достоинствам относится: 1) Распределенная топология (нет необходимости в мощном сервере VPN). 2) Работет поверх сетей любой топологии, в том числе за NAT и поверх других VPN. 3) Поддерживает активное соединение даже после переключения сети (например с wi-fi на 4g) или при входе и выходе из других VPN. 4) Работет в большинстве операционных систем, в том числе Windows XP. К этому можно добавить, что tinc входит во все дистирбутивы Linux, то есть его можно установить и сразу пользоваться. При этом tinc у широкой публики не пользуется популярностью. Одна из причин — очень лаконичная документация, которая создавалась в университетской среде, и поэтому является скорее академической, чем доступной для быстрого прочтения. В этом сообщении я опишу конфигурацию tinc VPN в операционной истеме Ubuntu. Устанавливаем пакет sudo apt-get install tinc. Поскольку сеть имеет ячеистую топологию — нет сереверного или клиентского варианта программного обеспечения. В нашем примере будет две машины. Первую условно назовем «сервер» — будет открыта для доступа из интернет по имени alpha.example.com (или по ip адресу). Вторая машина будет в некоторой сети не имея открытого в интернет ip адреса (4g, wi-fi, условно назовем ее «домашний компьютер») Теперь нам нужно выбрать имя для сети. В нашем примере это будет alpha. Сначла работем на «сервере» с адресом alpha.example.com. Создаем структуру каталогов для конфигов sudo mkdir -p /etc/tinc/alpha/hosts. Создаем конфиг сети alpha /etc/tinc/alpha/tinc.conf: Name = server_01 AddressFamily = ipv4 Interface = tun0 В каталоге /etc/tinc/alpha/hosts будут храниться открытые ключи текущей машины и других доступных в сети alpha машин. Создадим файл /etc/tinc/alpha/hosts/server_01: Address = alpha.example.com Subnet = 10.0.0.1/32 Subnet = 0.0.0.0/0 Address задается в случае если у машины есть доступный адрес (например публичный адрес в сети интернет) или доменное имя. Subnet = 10.0.0.1/32 задает адрес машины в сети alpha. Subnet = 0.0.0.0/0 включается только в том случае, когда неоходимо весь трафик от других машин в интернет направить через текущую машину (это не обязательно). Сгенерируем открытый и закрытый ключ командой sudo tincd -n alpha -K 4096. Создадим файлы, которые будут выполняться при старте и остановке сети /etc/tinc/alpha/tinc-up: #!/bin/sh ip link set $INTERFACE up ip addr add 10.0.0.1/32 dev $INTERFACE ip route add 10.0.0.0/24 dev $INTERFACE /etc/tinc/alpha/tinc-down: #!/bin/sh ip route del 10.0.0.0/24 dev $INTERFACE ip addr del 10.0.0.1/32 dev $INTERFACE ip link set $INTERFACE down Сделаем эти файлы выполняемыми sudo chmod 755 /etc/tinc/alpha/tinc-* Запустим сеть alpha на текущей машине sudo tincd -n alpha -D -d5 Теперь перейдем на «домашний компьютер» и повторим все действия (с небольшими изменениями): sudo apt-get install tinc sudo mkdir -p /etc/tinc/alpha/hosts /etc/tinc/alpha/tinc.conf: Name = client_01 AddressFamily = ipv4 Interface = tun0 ConnectTo = server_01 /etc/tinc/alpha/hosts/client_01: Subnet = 10.0.0.2/32 sudo tincd -n alpha -K 4096. /etc/tinc/alpha/tinc-up: #!/bin/sh ip link set $INTERFACE up ip addr add 10.0.0.2/32 dev $INTERFACE ip route add 10.0.0.0/24 dev $INTERFACE /etc/tinc/alpha/tinc-down: #!/bin/sh ip route del 10.0.0.0/24 dev $INTERFACE ip addr del 10.0.0.2/32 dev $INTERFACE ip link set $INTERFACE down sudo chmod 755 /etc/tinc/alpha/tinc-* sudo tincd -n alpha -D -d5 Теперь неободимо обменяться публичными ключами в каталогах /etc/tinc/alpha/hosts, чтобы их содержимое на обоеих машинах было одинаковое. Итак, сеть настроена. С компьютера 10.0.0.1 доступен компьютер 10.0.0.2 и наоборот. Иногда необходимо, чтобы вест трафик с «домашнео компьютера» шел через «сервер» (то для чего сейчас VPN используется довольно часто). Напомню, что для этого в конфиге сервера мы предусмотрели параметр Subnet = 0.0.0.0/0 Кроме этого, необходимо изменить скрипты конфигурирования сети на «домашнем компьютере: /etc/tinc/alpha/tinc-up: #!/bin/sh REMOTEADDRESS=X.X.X.X VPN_GATEWAY=10.0.0.1 ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-5` ip link set $INTERFACE up ip addr add 10.0.0.2/32 dev $INTERFACE ip route add 10.0.0.0/24 dev $INTERFACE ip route add $REMOTEADDRESS $ORIGINAL_GATEWAY ip route add $VPN_GATEWAY dev $INTERFACE ip route add 0.0.0.0/1 via $VPN_GATEWAY dev $INTERFACE ip route add 128.0.0.0/1 via $VPN_GATEWAY dev $INTERFACE /etc/tinc/alpha/tinc-down #!/bin/sh REMOTEADDRESS=X.X.X.X VPN_GATEWAY=10.0.0.1 ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-5` ip route del $REMOTEADDRESS $ORIGINAL_GATEWAY ip route del $VPN_GATEWAY dev $INTERFACE ip route del 0.0.0.0/1 dev $INTERFACE ip route del 128.0.0.0/1 dev $INTERFACE ip route del 10.0.0.0/24 dev $INTERFACE ip addr del 10.0.0.1/32 dev $INTERFACE ip link set $INTERFACE down После этого весь Ваш трафик будет направлен на „сервер“. Но это еще не означает что сервер будет его пересылать на другие адреса. Для этого нужно еще дополнительно настроить сеть на „сервере“. Дальнейшие действия будут сильно зависеть от конфигурации сети. В наиболее простом случае помогут команды, которые можно выполнять только если Вы знаете что они делают: ### Этот код может нанести вред Вашему компьютеру ### sudo iptables -P FORWARD DROP sudo iptables -I FORWARD -i tun0 -o enp35s0 -j ACCEPT sudo iptables -I FORWARD -o tun0 -i enp35s0 -j ACCEPT sudo iptables -t nat -I POSTROUTING -o enp35s0 -j MASQUERADE Полезные ссылки 1. www.digitalocean.com/community/tutorials/how-to-install-tinc-and-set-up-a-basic-vpn-on-ubuntu-18-04-ru 2. www.tinc-vpn.org/examples/windows-install 3. www.tinc-vpn.org/examples/redirect-gateway 4. zingmars.info/2018/07/14/Tinc-1.1-setup-instructions apapacy@gmail.com 30 января 2022 года\n","metric":0.45141601559999999793,"metric_input":"Текст статьи:\nTinc — настройка VPN в Ubuntu / Habr             \n\n29-01-2022\nTinc — это открытый сетевой протокол и программная реализация, используемая для сжатых и зашифрованных виртуальных частных сетей. Это проект был начат в 1998 году Гусом Слипеном, Иво Тиммермансом и Весселем Данкерсом под лицензией GPL. К его основным достоинствам относится: 1) Распределенная топология (нет необходимости в мощном сервере VPN). 2) Работет поверх сетей любой топологии, в том числе за NAT и поверх других VPN. 3) Поддерживает активное соединение даже после переключения сети (например с wi-fi на 4g) или при входе и выходе из других VPN. 4) Работет в большинстве операционных систем, в том числе Windows XP. К этому можно добавить, что tinc входит во все дистирбутивы Linux, то есть его можно установить и сразу пользоваться. При этом tinc у широкой публики не пользуется популярностью. Одна из причин — очень лаконичная документация, которая создавалась в университетской среде, и поэтому является скорее академической, чем доступной для быстрого прочтения. В этом сообщении я опишу конфигурацию tinc VPN в операционной истеме Ubuntu. Устанавливаем пакет sudo apt-get install tinc. Поскольку сеть имеет ячеистую топологию — нет сереверного или клиентского варианта программного обеспечения. В нашем примере будет две машины. Первую условно назовем «сервер» — будет открыта для доступа из интернет по имени alpha.example.com (или по ip адресу). Вторая машина будет в некоторой сети не имея открытого в интернет ip адреса (4g, wi-fi, условно назовем ее «домашний компьютер») Теперь нам нужно выбрать имя для сети. В нашем примере это будет alpha. Сначла работем на «сервере» с адресом alpha.example.com. Создаем структуру каталогов для конфигов sudo mkdir -p /etc/tinc/alpha/hosts. Создаем конфиг сети alpha /etc/tinc/alpha/tinc.conf: Name = server_01 AddressFamily = ipv4 Interface = tun0 В каталоге /etc/tinc/alpha/hosts будут храниться открытые ключи текущей машины и других доступных в сети alpha машин. Создадим файл /etc/tinc/alpha/hosts/server_01: Address = alpha.example.com Subnet = 10.0.0.1/32 Subnet = 0.0.0.0/0 Address задается в случае если у машины есть доступный адрес (например публичный адрес в сети интернет) или доменное имя. Subnet = 10.0.0.1/32 задает адрес машины в сети alpha. Subnet = 0.0.0.0/0 включается только в том случае, когда неоходимо весь трафик от других машин в интернет направить через текущую машину (это не обязательно). Сгенерируем открытый и закрытый ключ командой sudo tincd -n alpha -K 4096. Создадим файлы, которые будут выполняться при старте и остановке сети /etc/tinc/alpha/tinc-up: #!/bin/sh ip link set $INTERFACE up ip addr add 10.0.0.1/32 dev $INTERFACE ip route add 10.0.0.0/24 dev $INTERFACE /etc/tinc/alpha/tinc-down: #!/bin/sh ip route del 10.0.0.0/24 dev $INTERFACE ip addr del 10.0.0.1/32 dev $INTERFACE ip link set $INTERFACE down Сделаем эти файлы выполняемыми sudo chmod 755 /etc/tinc/alpha/tinc-* Запустим сеть alpha на текущей машине sudo tincd -n alpha -D -d5 Теперь перейдем на «домашний компьютер» и повторим все действия (с небольшими изменениями): sudo apt-get install tinc sudo mkdir -p /etc/tinc/alpha/hosts /etc/tinc/alpha/tinc.conf: Name = client_01 AddressFamily = ipv4 Interface = tun0 ConnectTo = server_01 /etc/tinc/alpha/hosts/client_01: Subnet = 10.0.0.2/32 sudo tincd -n alpha -K 4096. /etc/tinc/alpha/tinc-up: #!/bin/sh ip link set $INTERFACE up ip addr add 10.0.0.2/32 dev $INTERFACE ip route add 10.0.0.0/24 dev $INTERFACE /etc/tinc/alpha/tinc-down: #!/bin/sh ip route del 10.0.0.0/24 dev $INTERFACE ip addr del 10.0.0.2/32 dev $INTERFACE ip link set $INTERFACE down sudo chmod 755 /etc/tinc/alpha/tinc-* sudo tincd -n alpha -D -d5 Теперь неободимо обменяться публичными ключами в каталогах /etc/tinc/alpha/hosts, чтобы их содержимое на обоеих машинах было одинаковое. Итак, сеть настроена. С компьютера 10.0.0.1 доступен компьютер 10.0.0.2 и наоборот. Иногда необходимо, чтобы вест трафик с «домашнео компьютера» шел через «сервер» (то для чего сейчас VPN используется довольно часто). Напомню, что для этого в конфиге сервера мы предусмотрели параметр Subnet = 0.0.0.0/0 Кроме этого, необходимо изменить скрипты конфигурирования сети на «домашнем компьютере: /etc/tinc/alpha/tinc-up: #!/bin/sh REMOTEADDRESS=X.X.X.X VPN_GATEWAY=10.0.0.1 ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-5` ip link set $INTERFACE up ip addr add 10.0.0.2/32 dev $INTERFACE ip route add 10.0.0.0/24 dev $INTERFACE ip route add $REMOTEADDRESS $ORIGINAL_GATEWAY ip route add $VPN_GATEWAY dev $INTERFACE ip route add 0.0.0.0/1 via $VPN_GATEWAY dev $INTERFACE ip route add 128.0.0.0/1 via $VPN_GATEWAY dev $INTERFACE /etc/tinc/alpha/tinc-down #!/bin/sh REMOTEADDRESS=X.X.X.X VPN_GATEWAY=10.0.0.1 ORIGINAL_GATEWAY=`ip route show | grep ^default | cut -d ' ' -f 2-5` ip route del $REMOTEADDRESS $ORIGINAL_GATEWAY ip route del $VPN_GATEWAY dev $INTERFACE ip route del 0.0.0.0/1 dev $INTERFACE ip route del 128.0.0.0/1 dev $INTERFACE ip route del 10.0.0.0/24 dev $INTERFACE ip addr del 10.0.0.1/32 dev $INTERFACE ip link set $INTERFACE down После этого весь Ваш трафик будет направлен на „сервер“. Но это еще не означает что сервер будет его пересылать на другие адреса. Для этого нужно еще дополнительно настроить сеть на „сервере“. Дальнейшие действия будут сильно зависеть от конфигурации сети. В наиболее простом случае помогут команды, которые можно выполнять только если Вы знаете что они делают: ### Этот код может нанести вред Вашему компьютеру ### sudo iptables -P FORWARD DROP sudo iptables -I FORWARD -i tun0 -o enp35s0 -j ACCEPT sudo iptables -I FORWARD -o tun0 -i enp35s0 -j ACCEPT sudo iptables -t nat -I POSTROUTING -o enp35s0 -j MASQUERADE Полезные ссылки 1. www.digitalocean.com/community/tutorials/how-to-install-tinc-and-set-up-a-basic-vpn-on-ubuntu-18-04-ru 2. www.tinc-vpn.org/examples/windows-install 3. www.tinc-vpn.org/examples/redirect-gateway 4. zingmars.info/2018/07/14/Tinc-1.1-setup-instructions apapacy@gmail.com 30 января 2022 года\n\n\nКраткое содержание:\nTinc - это сетевой протокол и программная реализация для создания зашифрованных и сжатых виртуальных частных сетей. Он был разработан в 1998 году и распространяется под лицензией GPL. Tinc имеет распределенную топологию, работает поверх сетей любой топологии и поддерживает активное соединение даже при переключении сетей или входе и выходе из VPN. Он работает в большинстве операционных систем, включая Windows XP, и входит во все дистрибутивы Linux. В этой статье описывается конфигурация Tinc VPN в операционной системе Ubuntu.","summary":"Tinc - это сетевой протокол и программная реализация для создания зашифрованных и сжатых виртуальных частных сетей. Он был разработан в 1998 году и распространяется под лицензией GPL. Tinc имеет распределенную топологию, работает поверх сетей любой топологии и поддерживает активное соединение даже при переключении сетей или входе и выходе из VPN. Он работает в большинстве операционных систем, включая Windows XP, и входит во все дистрибутивы Linux. В этой статье описывается конфигурация Tinc VPN в операционной системе Ubuntu."}
{"Text":"Нужны ли на самом деле старт-апы крупным корпорациям / Habr            \n\n15-06-2015\nАвтор — Дмитриева Людмила На проходившем 2-3 июня в Сколково Форуме Startup Village-2015 одно из центральных мест заняла дискуссия на тему «Нужны ли старт-апы крупным корпорациям». Не удивительно, что именно эта дискуссия вызвала большой интерес. Почему? Сейчас расскажу, как все было… В дискуссии приняли участие представители SAP (Майк Кемельмахер, Вице-президент, Глава Инновационного центра SAP в Израиле), российского банка, автомобильного завода, разработчик сетевого оборудования и другие представители российского и зарубежного бизнеса. Удивительно то, что мнения на счет того, нужны ли старт-апы крупным корпорациям, или они могут благополучно прожить и без них, разделились. Более того, представители крупных компаний высказали прямо противоположные точки зрения. Начну с позитивных для старт-аперов точек зрения и подходов. Высокотехнологичные компании, такие, как, например, SAP, развивают свои экосистемы, создают специальные инвестиционные фонды, работающие со старт-апами, а также с университетами во многих странах мира, в том числе и в России. Звучит очень вдохновляюще. Ну что ж, двигаемся дальше. Следующее мнение, высказанное представителем автозавода, можно отнести, скорее, к нейтральному. Автозавод – это производитель массового продукта, здесь все сильно зарегулировано, поэтому есть ограничения на привлечение новых идей. От появления идеи до ее внедрения может пройти от года до десяти лет. Хотя есть области, где все может происходить быстрей, например, software. Тем не менее, многое зависит от hardware, которое элементарно не поспевает за инновационными софтверными разработками. Поэтому практически невозможно использовать старта-пы достаточно эффективно. Тем не менее, компания с интересом смотрит на старт-апы, которые могут быть как-то использованы в будущем. А вот что думают наши банки? Как они относятся к прогрессу и инновациям? Посмотрим… Начнем с того, что старт-аперам нужно понимать несколько ключевых вопросов, 1 – как начать работать с большой корпорацией, 2 – как продолжить работать, 3 – как закончить работать, 4 – самый важный вопрос – как не разориться. Ведь крупная корпорация развивается неторопливыми стабильными шагами, стараясь опираться на надежные проверенные решения. В результате дело до инноваций доходит тогда, когда компания утыкается в стену. Как правило, в больших корпорациях это происходит на полной скорости – вы просто влетаете в бетонную стенку головой. После этого, наконец-то, возникает потребность в инновациях. Т.е., другими словами, в организации должен произойти достаточно большой кризис. Во всех без исключения крупных компаниях существует инерция. Чем компания больше, тем сложней ее перевести на новые рельсы. И это совершенно безотносительно индустрии. Большие технологические компании также сложно переводятся на новые рельсы, как и большие банки. Потому что есть бюрократическая структура, протекция собственных решений. Если великая компания чего-то уже достигла, то она всегда считает, что все сделает сама. Совершенно ошибочное мнение, что большие компании хорошо работают с маленькими старт-апами. Как с точки зрения покупки сервисов, так и с точки зрения покупки самих компаний. Есть много предпринимателей, которые сделали компании и продали их уважаемым корпорациям. На вопрос: «А что случилось с вашей технологией?», звучит ответ: «Кто ж его знает, растворилась». Большие компании кладут инновации на полку, если покупают старт-ап — компанию. Во многом для этого и покупают, чтобы уничтожить потенциальных конкурентов. А если уж началась реальная работа, то нужно быть готовым к очень длинным пилотам. Большие компании привыкли покупать у крупных поставщиков. Понятно, что у этих компаний огромные бюджеты пре-сейла, им потратить миллион долларов ничего не стоит, особенно на какой-нибудь крупный российский банк. Поэтому, когда смотришь на гибель B2B-шных компаний небольшого размера, видно, в какой момент эта гибель происходит. Как говорят сами старт-аперы — мы начали 25 пилотов, они продолжаются уже полтора года, мы потратили весь бюджет, мы снова заняли и снова потратили, теперь мы умерли, потому что нам не хватило ресурсов, чтобы довести до конца хотя бы один пилот. Ну что ж, крупные банки всегда были и остаются консервативными организациями. Возможно, в этом и состоит их основная сила. Как же выглядит старт-апер со стороны большой корпорации. Вы, как старт-апер, приходите и говорите: «Смотрите, какая классная штука». А теперь представьте ситуацию: вот вы приходите в супермаркет и видите, что на кассе висит много классных штук. Возможно, вы совершите импульсную покупку, типа – классная вещь – пригодится. Положите в карман и пойдете дальше. В большинстве случаев человек, который выходит на шоппинг, знает, чего хочет. У него есть проблема, которую надо решить. И вот он – принципиальный момент, который прозвучал во время дискуссии: сформулируйте актуальную проблему, которую может решить старт-ап для своего потенциального заказчика. Если вы сделали отличную технологию, отличный сервис, но совершенно не знаете, как он может помочь конкретным людям в компаниях для решения их конкретных бизнес-задач, шансов нет никаких. Тем не менее, крупные компании уже, как было сказано, практически уперлись головами в бетонные стены некоторых проблем, где они ждут старт-аперов со своим идеями и наработками. Перечислю те области, которые были озвучены: 1 – работа с данными. Любой банк – большое хранилище данных (не будем называть это «кладбищем»). Все, что касается вопросов обработки данных, получение пользы от данных как внутренних, так и внешних, а также их интеграция, являются ключевыми задачами; 2 – технологии, связанные с улучшением качества риск-менеджмента, для банков, например, это основной хлеб; 3- различные платежные технологии, например, переводы денег от одного физического лица другому физическому лицу имеют крайне высокую себестоимость, которую необходимо снизить; 4 – создание приложений на платформах крупных софтверных компаний. Не знаю, насколько эта дискуссия реально поможет старт-аперам, участвовавшим в Форуме, но то, что заставит задуматься – это точно.\n","metric":0.60107421880000000414,"metric_input":"Текст статьи:\nНужны ли на самом деле старт-апы крупным корпорациям / Habr            \n\n15-06-2015\nАвтор — Дмитриева Людмила На проходившем 2-3 июня в Сколково Форуме Startup Village-2015 одно из центральных мест заняла дискуссия на тему «Нужны ли старт-апы крупным корпорациям». Не удивительно, что именно эта дискуссия вызвала большой интерес. Почему? Сейчас расскажу, как все было… В дискуссии приняли участие представители SAP (Майк Кемельмахер, Вице-президент, Глава Инновационного центра SAP в Израиле), российского банка, автомобильного завода, разработчик сетевого оборудования и другие представители российского и зарубежного бизнеса. Удивительно то, что мнения на счет того, нужны ли старт-апы крупным корпорациям, или они могут благополучно прожить и без них, разделились. Более того, представители крупных компаний высказали прямо противоположные точки зрения. Начну с позитивных для старт-аперов точек зрения и подходов. Высокотехнологичные компании, такие, как, например, SAP, развивают свои экосистемы, создают специальные инвестиционные фонды, работающие со старт-апами, а также с университетами во многих странах мира, в том числе и в России. Звучит очень вдохновляюще. Ну что ж, двигаемся дальше. Следующее мнение, высказанное представителем автозавода, можно отнести, скорее, к нейтральному. Автозавод – это производитель массового продукта, здесь все сильно зарегулировано, поэтому есть ограничения на привлечение новых идей. От появления идеи до ее внедрения может пройти от года до десяти лет. Хотя есть области, где все может происходить быстрей, например, software. Тем не менее, многое зависит от hardware, которое элементарно не поспевает за инновационными софтверными разработками. Поэтому практически невозможно использовать старта-пы достаточно эффективно. Тем не менее, компания с интересом смотрит на старт-апы, которые могут быть как-то использованы в будущем. А вот что думают наши банки? Как они относятся к прогрессу и инновациям? Посмотрим… Начнем с того, что старт-аперам нужно понимать несколько ключевых вопросов, 1 – как начать работать с большой корпорацией, 2 – как продолжить работать, 3 – как закончить работать, 4 – самый важный вопрос – как не разориться. Ведь крупная корпорация развивается неторопливыми стабильными шагами, стараясь опираться на надежные проверенные решения. В результате дело до инноваций доходит тогда, когда компания утыкается в стену. Как правило, в больших корпорациях это происходит на полной скорости – вы просто влетаете в бетонную стенку головой. После этого, наконец-то, возникает потребность в инновациях. Т.е., другими словами, в организации должен произойти достаточно большой кризис. Во всех без исключения крупных компаниях существует инерция. Чем компания больше, тем сложней ее перевести на новые рельсы. И это совершенно безотносительно индустрии. Большие технологические компании также сложно переводятся на новые рельсы, как и большие банки. Потому что есть бюрократическая структура, протекция собственных решений. Если великая компания чего-то уже достигла, то она всегда считает, что все сделает сама. Совершенно ошибочное мнение, что большие компании хорошо работают с маленькими старт-апами. Как с точки зрения покупки сервисов, так и с точки зрения покупки самих компаний. Есть много предпринимателей, которые сделали компании и продали их уважаемым корпорациям. На вопрос: «А что случилось с вашей технологией?», звучит ответ: «Кто ж его знает, растворилась». Большие компании кладут инновации на полку, если покупают старт-ап — компанию. Во многом для этого и покупают, чтобы уничтожить потенциальных конкурентов. А если уж началась реальная работа, то нужно быть готовым к очень длинным пилотам. Большие компании привыкли покупать у крупных поставщиков. Понятно, что у этих компаний огромные бюджеты пре-сейла, им потратить миллион долларов ничего не стоит, особенно на какой-нибудь крупный российский банк. Поэтому, когда смотришь на гибель B2B-шных компаний небольшого размера, видно, в какой момент эта гибель происходит. Как говорят сами старт-аперы — мы начали 25 пилотов, они продолжаются уже полтора года, мы потратили весь бюджет, мы снова заняли и снова потратили, теперь мы умерли, потому что нам не хватило ресурсов, чтобы довести до конца хотя бы один пилот. Ну что ж, крупные банки всегда были и остаются консервативными организациями. Возможно, в этом и состоит их основная сила. Как же выглядит старт-апер со стороны большой корпорации. Вы, как старт-апер, приходите и говорите: «Смотрите, какая классная штука». А теперь представьте ситуацию: вот вы приходите в супермаркет и видите, что на кассе висит много классных штук. Возможно, вы совершите импульсную покупку, типа – классная вещь – пригодится. Положите в карман и пойдете дальше. В большинстве случаев человек, который выходит на шоппинг, знает, чего хочет. У него есть проблема, которую надо решить. И вот он – принципиальный момент, который прозвучал во время дискуссии: сформулируйте актуальную проблему, которую может решить старт-ап для своего потенциального заказчика. Если вы сделали отличную технологию, отличный сервис, но совершенно не знаете, как он может помочь конкретным людям в компаниях для решения их конкретных бизнес-задач, шансов нет никаких. Тем не менее, крупные компании уже, как было сказано, практически уперлись головами в бетонные стены некоторых проблем, где они ждут старт-аперов со своим идеями и наработками. Перечислю те области, которые были озвучены: 1 – работа с данными. Любой банк – большое хранилище данных (не будем называть это «кладбищем»). Все, что касается вопросов обработки данных, получение пользы от данных как внутренних, так и внешних, а также их интеграция, являются ключевыми задачами; 2 – технологии, связанные с улучшением качества риск-менеджмента, для банков, например, это основной хлеб; 3- различные платежные технологии, например, переводы денег от одного физического лица другому физическому лицу имеют крайне высокую себестоимость, которую необходимо снизить; 4 – создание приложений на платформах крупных софтверных компаний. Не знаю, насколько эта дискуссия реально поможет старт-аперам, участвовавшим в Форуме, но то, что заставит задуматься – это точно.\n\n\nКраткое содержание:\nСтатья обсуждает дискуссию на тему \"Нужны ли старт-апы крупным корпорациям\" на Форуме Startup Village-2015. Участники дискуссии, включая представителей крупных компаний и стартапов, поделились своими мнениями о роли стартапов в развитии корпораций.\n\nУчастники дискуссии разделились во мнениях о необходимости стартапов для крупных корпораций. Представители крупных компаний высказали противоположные точки зрения. Некоторые считают, что стартапы могут быть полезными для крупных компаний, в то время как другие считают, что они могут быть сложными для эффективного использования.\n\nПредставители крупных компаний отметили, что стартапы должны понимать, как начать работу с корпорациями, продолжить работу и закончить работу. Они также отметили, что крупные компании могут не знать, как использовать новые технологии из-за своей инерционности и протекции собственных решений.\n\nВ то же время, участники дискуссии отметили, что крупные компании уже столкнулись с некоторыми проблемами, где они ждут стартапы со своими идеями и наработками. Эти области включают работу с данными, улучшение качества риск-менеджмента, различные платежные технологии и создание приложений на платформах крупных софтверных компаний.\n\nТаким образом, статья подчеркивает важность понимания потребностей и возможностей крупных компаний для успешного сотрудничества со стартапами.","summary":"Статья обсуждает дискуссию на тему \"Нужны ли старт-апы крупным корпорациям\" на Форуме Startup Village-2015. Участники дискуссии, включая представителей крупных компаний и стартапов, поделились своими мнениями о роли стартапов в развитии корпораций.\n\nУчастники дискуссии разделились во мнениях о необходимости стартапов для крупных корпораций. Представители крупных компаний высказали противоположные точки зрения. Некоторые считают, что стартапы могут быть полезными для крупных компаний, в то время как другие считают, что они могут быть сложными для эффективного использования.\n\nПредставители крупных компаний отметили, что стартапы должны понимать, как начать работу с корпорациями, продолжить работу и закончить работу. Они также отметили, что крупные компании могут не знать, как использовать новые технологии из-за своей инерционности и протекции собственных решений.\n\nВ то же время, участники дискуссии отметили, что крупные компании уже столкнулись с некоторыми проблемами, где они ждут стартапы со своими идеями и наработками. Эти области включают работу с данными, улучшение качества риск-менеджмента, различные платежные технологии и создание приложений на платформах крупных софтверных компаний.\n\nТаким образом, статья подчеркивает важность понимания потребностей и возможностей крупных компаний для успешного сотрудничества со стартапами."}
{"Text":"Объекты страницы: описание одной техники / Habr             \n\n17-05-2013\nЗдравствуйте, хабровчане. Хочу поделиться с вами одной техникой организации кода при массивной работе с DOM-элементами. Несколько лет назад, когда еще вовсе не было бэкбона и MVVC, мы писали старый добрый джаваскрипт без фреймворков: создавали объекты и заставляли их плясать на странице в общем танце. Такая практика, без сомнения, до сих пор оправдывает себя, и техника, о которой пойдет речь, применима именно к ней. Мой рассказ — о маленькой библиотечке PageObject.js (текущая версия v0.14, 2.6K) и о том, как с ее помощью можно упростить себе жизнь. Это не плагин jQuery, хотя, уверен, многие захотят ее назвать именно так. Это всего лишь функция, которая использует селекторы и кое-какие утилиты из jQuery, чтобы создать удобство в чтении и написании кода. По сути, это простая jQuery утилитка. Суть техники В создании объектов, которые манипулируют DOM-элементами, и их подключении на страницу ничего нового не придумаешь: создаем конструктор, сперва рендерим в нем определенный шаблон, затем разбираем его результат на части, «вешаем» на эти части обработчики событий, программируем остальную логику и встраиваем все это добро в наше приложение. function Calculator() { if (this.constructor.name !== 'Calculator') { throw “No way, buddy!”; } // 1. отрендерить шаблон // 2. разобрать на части // 3. повесить обработчики // 4. остальная логика } PageObject.js помогает с первыми двумя шагами. function Calculator() { if (this.constructor.name !== 'Calculator') { throw “No way, buddy!”; } var calc = this; $.turnToPageObject(calc, { template: $('#tmplCalculator').html(), containerClass: 'calc', context: { caption: \"Calculator\" }, selectors: { buttons: [ ':button', Calculator.getButtonName ], led: 'p' } }); // 3. повесить обработчики // 4. другая логика } var calc = new Calculator; $('body').append(calc.DOM.container); После того, как отработает функция $.turnToPageObject (“превратить в объект страницы”), у объекта calc появится свойство calc.DOM — неймспейс, заполненый DOM-элементами, которые будут соответствовать указанным селекторам, и еще появится calc.DOM.container — именно та легко встраиваемая в приложение часть объекта — контейнер всего-всего. Вот, пожалуйста, полный работающий пример с калькулятором. Мне же осталось рассказать подробнее о всех возможностях утилитки. $.turnToPageObject Первый аргумент должен быть объектом, у которого в последствии появится заполненый составными HTML-елементами неймспейс DOM. Второй аргумент — опции. Если НЕ указать опцию container, то контейнер будет создан (без него никак), и это будет такой же элемент, как и containerElement (по умолчанию — DIV). Если указать containerClass, то контейнеру будет присвоен класс. Если указать template, то будет отрендерен шаблон и его результат будет помещен внутрь контейнера. Если НЕ указать context, то шаблон будет отрендерен с пустым контекстом {}. template может быть либо строкой, либо функцией (интеграция с Jammit JST). Когда template — функция, он принимает только контекст и должен возвратить строку. Когда template — строка, шаблон рендерится при помощи templateEngine, которая автоматически настроится по умолчанию на использование шаблонизатора _.template, если underscore присутствует. Если в вашем проекте нет underscore, вам нужно сконфигурировать templateEngine. // вот так меняются умолчания: $.turnToPageObject.configure({ templateEngine: window.tmpl, // http://ejohn.org/blog/javascript-micro-templating containerElement: 'strong' }); templateEngine принимает два аргумента — строку шаблона и контекст — и также должен возвратить строку. Если указана опция hide, то контейнер будет спрятан, что часто бывало удобно. И главное — селекторы. Если указать опцию selectors (объект), то соответствующие селекторам найденые в содержимом контейнера HTML-элементы будут по аналогичным ключам помещены в неймспейс DOM. Если опция template была указана, то селекторы будут искать в уже отрендеренном шаблоне. Если по селектору не будет найдено ни одного элемента — будет крик. Если по селектору будет найдено более одного элемента — также будет исключение. Если все же нужно, чтобы по селектору были найдены и помещены в массив более одного элемента, нужно к значению селектора дописать [] (пустые квадратные скобки) — такое выделение себя оправдывает. Если нужно, чтобы множественные найденные елементы были помещены в объект (как в примере с калькулятором), значение селектора нужно записать в виде массива из двух элементов: первый элемент — собственно селектор, а второй — функция, которая из каждого найденого по указанному селектору элемента должна извлечь ключ (напр. айдишник) для помещения этого элемента в соответствующем неймспейсе. Стоит отметить, что дополнительные неймспейсы в селекторах образуют неймспейсы с теми же именами внутри свойства DOM. Пожалуй, на этом все. Лучшее понимание того, что и как работает, вы сможете получить, почитав тесты и заглянув в исходник на гитхабе. Наверняка также будет полезной и общая документация. Буду рад, если вы найдете данную технику и описанную выше конвенцию применимой и в ваших проектах. Особенно буду рад вашим пожеланиям и идеям, с радостью отвечу на вопросы.\n","metric":0.53173828130000000414,"metric_input":"Текст статьи:\nОбъекты страницы: описание одной техники / Habr             \n\n17-05-2013\nЗдравствуйте, хабровчане. Хочу поделиться с вами одной техникой организации кода при массивной работе с DOM-элементами. Несколько лет назад, когда еще вовсе не было бэкбона и MVVC, мы писали старый добрый джаваскрипт без фреймворков: создавали объекты и заставляли их плясать на странице в общем танце. Такая практика, без сомнения, до сих пор оправдывает себя, и техника, о которой пойдет речь, применима именно к ней. Мой рассказ — о маленькой библиотечке PageObject.js (текущая версия v0.14, 2.6K) и о том, как с ее помощью можно упростить себе жизнь. Это не плагин jQuery, хотя, уверен, многие захотят ее назвать именно так. Это всего лишь функция, которая использует селекторы и кое-какие утилиты из jQuery, чтобы создать удобство в чтении и написании кода. По сути, это простая jQuery утилитка. Суть техники В создании объектов, которые манипулируют DOM-элементами, и их подключении на страницу ничего нового не придумаешь: создаем конструктор, сперва рендерим в нем определенный шаблон, затем разбираем его результат на части, «вешаем» на эти части обработчики событий, программируем остальную логику и встраиваем все это добро в наше приложение. function Calculator() { if (this.constructor.name !== 'Calculator') { throw “No way, buddy!”; } // 1. отрендерить шаблон // 2. разобрать на части // 3. повесить обработчики // 4. остальная логика } PageObject.js помогает с первыми двумя шагами. function Calculator() { if (this.constructor.name !== 'Calculator') { throw “No way, buddy!”; } var calc = this; $.turnToPageObject(calc, { template: $('#tmplCalculator').html(), containerClass: 'calc', context: { caption: \"Calculator\" }, selectors: { buttons: [ ':button', Calculator.getButtonName ], led: 'p' } }); // 3. повесить обработчики // 4. другая логика } var calc = new Calculator; $('body').append(calc.DOM.container); После того, как отработает функция $.turnToPageObject (“превратить в объект страницы”), у объекта calc появится свойство calc.DOM — неймспейс, заполненый DOM-элементами, которые будут соответствовать указанным селекторам, и еще появится calc.DOM.container — именно та легко встраиваемая в приложение часть объекта — контейнер всего-всего. Вот, пожалуйста, полный работающий пример с калькулятором. Мне же осталось рассказать подробнее о всех возможностях утилитки. $.turnToPageObject Первый аргумент должен быть объектом, у которого в последствии появится заполненый составными HTML-елементами неймспейс DOM. Второй аргумент — опции. Если НЕ указать опцию container, то контейнер будет создан (без него никак), и это будет такой же элемент, как и containerElement (по умолчанию — DIV). Если указать containerClass, то контейнеру будет присвоен класс. Если указать template, то будет отрендерен шаблон и его результат будет помещен внутрь контейнера. Если НЕ указать context, то шаблон будет отрендерен с пустым контекстом {}. template может быть либо строкой, либо функцией (интеграция с Jammit JST). Когда template — функция, он принимает только контекст и должен возвратить строку. Когда template — строка, шаблон рендерится при помощи templateEngine, которая автоматически настроится по умолчанию на использование шаблонизатора _.template, если underscore присутствует. Если в вашем проекте нет underscore, вам нужно сконфигурировать templateEngine. // вот так меняются умолчания: $.turnToPageObject.configure({ templateEngine: window.tmpl, // http://ejohn.org/blog/javascript-micro-templating containerElement: 'strong' }); templateEngine принимает два аргумента — строку шаблона и контекст — и также должен возвратить строку. Если указана опция hide, то контейнер будет спрятан, что часто бывало удобно. И главное — селекторы. Если указать опцию selectors (объект), то соответствующие селекторам найденые в содержимом контейнера HTML-элементы будут по аналогичным ключам помещены в неймспейс DOM. Если опция template была указана, то селекторы будут искать в уже отрендеренном шаблоне. Если по селектору не будет найдено ни одного элемента — будет крик. Если по селектору будет найдено более одного элемента — также будет исключение. Если все же нужно, чтобы по селектору были найдены и помещены в массив более одного элемента, нужно к значению селектора дописать [] (пустые квадратные скобки) — такое выделение себя оправдывает. Если нужно, чтобы множественные найденные елементы были помещены в объект (как в примере с калькулятором), значение селектора нужно записать в виде массива из двух элементов: первый элемент — собственно селектор, а второй — функция, которая из каждого найденого по указанному селектору элемента должна извлечь ключ (напр. айдишник) для помещения этого элемента в соответствующем неймспейсе. Стоит отметить, что дополнительные неймспейсы в селекторах образуют неймспейсы с теми же именами внутри свойства DOM. Пожалуй, на этом все. Лучшее понимание того, что и как работает, вы сможете получить, почитав тесты и заглянув в исходник на гитхабе. Наверняка также будет полезной и общая документация. Буду рад, если вы найдете данную технику и описанную выше конвенцию применимой и в ваших проектах. Особенно буду рад вашим пожеланиям и идеям, с радостью отвечу на вопросы.\n\n\nКраткое содержание:\nСтатья рассказывает о технике организации кода при работе с DOM-элементами, используя библиотеку PageObject.js. Эта техника упрощает процесс создания объектов, манипулирующих DOM-элементами и их подключения на страницу.\n\nPageObject.js помогает с первыми двумя шагами создания объектов: отрендерить шаблон и разобрать его на части. Утилита использует селекторы и некоторые функции из jQuery для удобства в чтении и написании кода.\n\nПример использования техники представлен с калькулятором. После того, как отработает функция $.turnToPageObject, у объекта calc появится свойство calc.DOM - неймспейс, заполненный DOM-элементами, которые будут соответствовать указанным селекторам, и еще появится calc.DOM.container - контейнер всего-всего.\n\nСтатья также описывает опции, которые можно использовать при использовании PageObject.js, такие как контейнер, контекст, шаблон, селекторы и другие.\n\nАвтор надеется, что эта техника и описанная выше конвенция будут полезны в проектах других разработчиков.","summary":"Статья рассказывает о технике организации кода при работе с DOM-элементами, используя библиотеку PageObject.js. Эта техника упрощает процесс создания объектов, манипулирующих DOM-элементами и их подключения на страницу.\n\nPageObject.js помогает с первыми двумя шагами создания объектов: отрендерить шаблон и разобрать его на части. Утилита использует селекторы и некоторые функции из jQuery для удобства в чтении и написании кода.\n\nПример использования техники представлен с калькулятором. После того, как отработает функция $.turnToPageObject, у объекта calc появится свойство calc.DOM - неймспейс, заполненный DOM-элементами, которые будут соответствовать указанным селекторам, и еще появится calc.DOM.container - контейнер всего-всего.\n\nСтатья также описывает опции, которые можно использовать при использовании PageObject.js, такие как контейнер, контекст, шаблон, селекторы и другие.\n\nАвтор надеется, что эта техника и описанная выше конвенция будут полезны в проектах других разработчиков."}
{"Text":"Миф про «мобильный» CHACHA20 / Хабр                \n\n02-03-2021\nПри подготовке Методики расчета «Индекса надежности HTTPS» мы перерыли массу тематической литературы и не раз сталкивались с рекомендацией поддерживать на веб-сервере шифронаборы на основе алгоритма шифрования CHACHA20 в целях снижения нагрузки на мобильные клиенты, которые не умеют в аппаратный AES. В этом контексте обычно упоминались процессоры Mediatek и скопом «старые бюджетные мобильные процессоры». Действительно ли CHACHA20 со своим верным спутником POLY1305 позволяют не слишком греться мобильным клиентам и стоит ли его поддерживать на веб-сервере? Давайте это обсудим! CHACHA20 был создан известным специалистом по криптографии Дэниэлом Бернштейном, которого мы любим, в частности, за Curve25519, а также за правозащитную деятельность, благодаря которой только олдфаги помнят, что означало _EXPORT_ в имени шифронабора. Алгоритм неплохо изучен, работает в AEAD-режиме, не имеет известных слабостей и уязвимостей, и является одним из двух алгоритмов шифрования, одобренных IETF для использования в TLS 1.3 (второй – бессмертный AES). Его теоретическая криптостойкость при использовании в TLS оценивается по-разному, в интервале между AES-128 и AES-256 в режиме GCM, что считается достаточным по сегодняшним меркам и на обозримую перспективу. При этом отмечается, что CHACHA20 «быстрее» AES, т.е. «потребляет меньше процессорных ресурсов на обеспечение того же уровня криптостойкости». Эта формулировка не только отдает душком телемаркетинга (при всем уважении к ее автору), но и упускает важную деталь: на процессорах без аппаратной поддержки AES. И тут мы наконец возвращаемся к «бюджетным мобильным процессорам», которые перегреваются под AES, начинают троттлить и требовать жидкого азота (сарказм). Производители процессоров в курсе проблемы и решили ее добавлением соответствующего набора инструкций. В x86-совместимых процессорах это AES-NI, в других – свои названия (и свой набор). И тут мы переходим к самому интересному: поддержке AES процессорами. Intel представил поддержку AES-NI в 2010 году в процессорах архитектуры Westmere, причем далеко не во всех: Atom, Celeron, Pentium и Core i3 она еще долго не полагалась. В поддержке AES-NI без копания в спецификациях можно быть уверенным только начиная с архитектуры Goldmont (Apollo Lake и Denverton), а это уже 2016 год. У AMD это архитектуры Bulldozer (2011) и Jaguar (2013 год), с ARM все сложнее: поддержка AES-инструкций предусмотрена в архитектуре ARMv8-A (2011 год, первое устройство – 2013 год), но фактическое воплощение их в кремнии зависит от производителя процессора и я лично не стал бы так уверенно свистеть про «старые бюджетные мобильные процессоры», скорее стоит говорить о «не флагманских мобильных процессорах» вообще, в т.ч. выпускаемых поныне. Проще говоря, встретить процессор без аппаратной поддержки AES не так уж и сложно. Получается, CHACHA20, действительно, отличная альтернатива AES? Давайте взглянем, например, на результаты этого исследования. На процессорах без поддержки AES CHACHA20 заметно опережает его в производительности, зачастую в разы. К сожалению, замеров температуры нам не показали, но если речь идет о серверном процессоре, очевидно, что разница в потребляемых процессорных ресурсах значима. Ситуация меняется на прямо противоположную, когда речь заходит о процессорах с поддержкой AES. Вряд ли стоит винить в этом CHACHA20, которому никто не предложил персональный набор инструкций в процессоре, а что бывает, когда оба участника играют по одним правилам, мы видели на старых процессорах (напоминаю: AES сливает). Так что, дружно включаем поддержку CHACHA20 на веб-серверах? Почему бы и нет, хотя бы из того соображения, что все яйца в одну корзину не кладут, и если вдруг завтра в самом AES или его реализации в конкретной криптобиблиотеке найдут дыру, мы легким движением руки сможем отключить его «до выяснения», оставшись на CHACHA20, а не судорожно искать, чем заменить AES, да как это включается. Куда менее однозначен вопрос о месте CHACHA20 в нашей жизни списке шифронаборов, предлагаемых веб-сервером для согласования, то есть о его приоритетности. Давайте вспомним, как вообще происходит согласование шифронабора при установлении HTTPS-соединения: клиент передает серверу список поддерживаемых им шифронаборов, в порядке «от балды» и изменить этот порядок можно только через групповые политики Windows и только для Internet Explorer браузеров, использующих SChannel (поправьте, если ошибаюсь). Сервер сравнивает полученный от клиента список со списком поддерживаемых им самим шифронаборов и сообщает клиенту, какой из них он выбрал для защиты соединения. Если на сервере задан приоритет шифронаборов, согласован будет первый совпавший в обоих списках с учетом заданного на сервере приоритета. Если не задан, то админу сервера надо оторвать руки мы погружаемся в область неизведанного теории вероятностей. Приоритетность шифронаборов на сервере обычно задают исходя из принципа максимально доступной защищенности: более стойкие шифронаборы идут в списке первыми, менее – последними. Современный клиент натыкается на стойкий шифронабор и согласовывает его, «устаревший» клиент – проскакивает по списку дальше, к менее стойкому шифронабору и согласовывает его. Все довольны, всё работает, от каждого – по способностям, каждому – по HTTPS. И тут в эту стройную картину мира влезает шифронабор на базе CHACHA20, который мы добавляем из соображений снижения нагрузки на «слабых» с аппаратной точки зрения клиентов, ничего не зная о том, являются ли они одновременно «устаревшими» или нет (т.е. флагманом этого года от третьеразрядной китайской компании или середнячком пятилетней давности от первостатейного бренда). Клиент сообщает, что поддерживает TLS 1.3 и полный комплект соответствующих шифронаборов, как на базе AES, так и на базе CHACHA20. Ваше решение, админ, какой шифронабор согласовываем клиенту? Вот и я о том же… Резюмирую вышесказанное по поводу алгоритма шифрования CHACHA20. Алгоритм вполне себе хорош и годится для использования в TLS. Шифронаборы на его основе поддерживаются только достаточно современным браузерами, так что совсем без AES пока никуда. Выигрыш в производительности от его использования можно получить не только лишь на «старых бюджетных мобильных процессорах», но и на десктопах и серверах. На процессорах с аппаратной поддержкой AES, ситуация меняется на прямо противоположную. При установлении HTTPS-соединения не существует способа узнать, поддерживает ли процессор клиента AES на аппаратном уровне. Соответственно, нет способа узнать, какой шифронабор окажется «быстрее» в каждом конкретном случае.\n","metric":0.47778320309999999793,"metric_input":"Текст статьи:\nМиф про «мобильный» CHACHA20 / Хабр                \n\n02-03-2021\nПри подготовке Методики расчета «Индекса надежности HTTPS» мы перерыли массу тематической литературы и не раз сталкивались с рекомендацией поддерживать на веб-сервере шифронаборы на основе алгоритма шифрования CHACHA20 в целях снижения нагрузки на мобильные клиенты, которые не умеют в аппаратный AES. В этом контексте обычно упоминались процессоры Mediatek и скопом «старые бюджетные мобильные процессоры». Действительно ли CHACHA20 со своим верным спутником POLY1305 позволяют не слишком греться мобильным клиентам и стоит ли его поддерживать на веб-сервере? Давайте это обсудим! CHACHA20 был создан известным специалистом по криптографии Дэниэлом Бернштейном, которого мы любим, в частности, за Curve25519, а также за правозащитную деятельность, благодаря которой только олдфаги помнят, что означало _EXPORT_ в имени шифронабора. Алгоритм неплохо изучен, работает в AEAD-режиме, не имеет известных слабостей и уязвимостей, и является одним из двух алгоритмов шифрования, одобренных IETF для использования в TLS 1.3 (второй – бессмертный AES). Его теоретическая криптостойкость при использовании в TLS оценивается по-разному, в интервале между AES-128 и AES-256 в режиме GCM, что считается достаточным по сегодняшним меркам и на обозримую перспективу. При этом отмечается, что CHACHA20 «быстрее» AES, т.е. «потребляет меньше процессорных ресурсов на обеспечение того же уровня криптостойкости». Эта формулировка не только отдает душком телемаркетинга (при всем уважении к ее автору), но и упускает важную деталь: на процессорах без аппаратной поддержки AES. И тут мы наконец возвращаемся к «бюджетным мобильным процессорам», которые перегреваются под AES, начинают троттлить и требовать жидкого азота (сарказм). Производители процессоров в курсе проблемы и решили ее добавлением соответствующего набора инструкций. В x86-совместимых процессорах это AES-NI, в других – свои названия (и свой набор). И тут мы переходим к самому интересному: поддержке AES процессорами. Intel представил поддержку AES-NI в 2010 году в процессорах архитектуры Westmere, причем далеко не во всех: Atom, Celeron, Pentium и Core i3 она еще долго не полагалась. В поддержке AES-NI без копания в спецификациях можно быть уверенным только начиная с архитектуры Goldmont (Apollo Lake и Denverton), а это уже 2016 год. У AMD это архитектуры Bulldozer (2011) и Jaguar (2013 год), с ARM все сложнее: поддержка AES-инструкций предусмотрена в архитектуре ARMv8-A (2011 год, первое устройство – 2013 год), но фактическое воплощение их в кремнии зависит от производителя процессора и я лично не стал бы так уверенно свистеть про «старые бюджетные мобильные процессоры», скорее стоит говорить о «не флагманских мобильных процессорах» вообще, в т.ч. выпускаемых поныне. Проще говоря, встретить процессор без аппаратной поддержки AES не так уж и сложно. Получается, CHACHA20, действительно, отличная альтернатива AES? Давайте взглянем, например, на результаты этого исследования. На процессорах без поддержки AES CHACHA20 заметно опережает его в производительности, зачастую в разы. К сожалению, замеров температуры нам не показали, но если речь идет о серверном процессоре, очевидно, что разница в потребляемых процессорных ресурсах значима. Ситуация меняется на прямо противоположную, когда речь заходит о процессорах с поддержкой AES. Вряд ли стоит винить в этом CHACHA20, которому никто не предложил персональный набор инструкций в процессоре, а что бывает, когда оба участника играют по одним правилам, мы видели на старых процессорах (напоминаю: AES сливает). Так что, дружно включаем поддержку CHACHA20 на веб-серверах? Почему бы и нет, хотя бы из того соображения, что все яйца в одну корзину не кладут, и если вдруг завтра в самом AES или его реализации в конкретной криптобиблиотеке найдут дыру, мы легким движением руки сможем отключить его «до выяснения», оставшись на CHACHA20, а не судорожно искать, чем заменить AES, да как это включается. Куда менее однозначен вопрос о месте CHACHA20 в нашей жизни списке шифронаборов, предлагаемых веб-сервером для согласования, то есть о его приоритетности. Давайте вспомним, как вообще происходит согласование шифронабора при установлении HTTPS-соединения: клиент передает серверу список поддерживаемых им шифронаборов, в порядке «от балды» и изменить этот порядок можно только через групповые политики Windows и только для Internet Explorer браузеров, использующих SChannel (поправьте, если ошибаюсь). Сервер сравнивает полученный от клиента список со списком поддерживаемых им самим шифронаборов и сообщает клиенту, какой из них он выбрал для защиты соединения. Если на сервере задан приоритет шифронаборов, согласован будет первый совпавший в обоих списках с учетом заданного на сервере приоритета. Если не задан, то админу сервера надо оторвать руки мы погружаемся в область неизведанного теории вероятностей. Приоритетность шифронаборов на сервере обычно задают исходя из принципа максимально доступной защищенности: более стойкие шифронаборы идут в списке первыми, менее – последними. Современный клиент натыкается на стойкий шифронабор и согласовывает его, «устаревший» клиент – проскакивает по списку дальше, к менее стойкому шифронабору и согласовывает его. Все довольны, всё работает, от каждого – по способностям, каждому – по HTTPS. И тут в эту стройную картину мира влезает шифронабор на базе CHACHA20, который мы добавляем из соображений снижения нагрузки на «слабых» с аппаратной точки зрения клиентов, ничего не зная о том, являются ли они одновременно «устаревшими» или нет (т.е. флагманом этого года от третьеразрядной китайской компании или середнячком пятилетней давности от первостатейного бренда). Клиент сообщает, что поддерживает TLS 1.3 и полный комплект соответствующих шифронаборов, как на базе AES, так и на базе CHACHA20. Ваше решение, админ, какой шифронабор согласовываем клиенту? Вот и я о том же… Резюмирую вышесказанное по поводу алгоритма шифрования CHACHA20. Алгоритм вполне себе хорош и годится для использования в TLS. Шифронаборы на его основе поддерживаются только достаточно современным браузерами, так что совсем без AES пока никуда. Выигрыш в производительности от его использования можно получить не только лишь на «старых бюджетных мобильных процессорах», но и на десктопах и серверах. На процессорах с аппаратной поддержкой AES, ситуация меняется на прямо противоположную. При установлении HTTPS-соединения не существует способа узнать, поддерживает ли процессор клиента AES на аппаратном уровне. Соответственно, нет способа узнать, какой шифронабор окажется «быстрее» в каждом конкретном случае.\n\n\nКраткое содержание:\nВ статье обсуждается алгоритм шифрования CHACHA20 и его использование в веб-серверах. Авторы статьи анализируют его преимущества и недостатки, а также рассматривают влияние аппаратной поддержки AES на производительность и температуру процессоров. Они приходят к выводу, что CHACHA20 может быть хорошей альтернативой AES, но его использование должно быть основано на конкретных условиях и характеристиках процессоров. Также авторы статьи обсуждают вопросы приоритетности CHACHA20 в списке шифронаборов и возможности его использования на мобильных устройствах. В целом, статья предоставляет полезную информацию о применении алгоритмов шифрования в веб-серверах и мобильной среде.","summary":"В статье обсуждается алгоритм шифрования CHACHA20 и его использование в веб-серверах. Авторы статьи анализируют его преимущества и недостатки, а также рассматривают влияние аппаратной поддержки AES на производительность и температуру процессоров. Они приходят к выводу, что CHACHA20 может быть хорошей альтернативой AES, но его использование должно быть основано на конкретных условиях и характеристиках процессоров. Также авторы статьи обсуждают вопросы приоритетности CHACHA20 в списке шифронаборов и возможности его использования на мобильных устройствах. В целом, статья предоставляет полезную информацию о применении алгоритмов шифрования в веб-серверах и мобильной среде."}
{"Text":"Нейроинтеллект или миф? / Habr                       \n\n15-08-2016\nЭра нейросетевого искусственного интеллекта в самом разгаре! Настоящий прорыв за последние годы — нейросети распознают не только речь, картинки с животными, сложные сцены, но даже описывают их словами! (NeuralTalk). На нейросетях делают боты, которые пока плохо, но уже что-то отвечают. Нейросети сочиняют музыку и пишут стихи. По сценарию нейросети даже собираются снять фильм! Скоро, пророчат журналисты, совсем скоро они буду управлять машинами, банками, корпорациями, странами. «И все будут летать на дилижансах» — так мечтали в начале прошлого века. Так мечтает сейчас о нейросетевом будущем его создатели. Говорят, что они скоро обгонят человека во всем. Нейросети уже обыграли человека в шахматы, го, в Jaopardy (Своя игра). И поэтому они называют свои творения не меньше, чем Искусственный интеллект. Распознал лошадь на картинке — искусственный интеллект. Почему интеллект? Потому что раньше это умел делать только человек. Странно, почему тогда ранее искусственным интеллектом не назвали калькулятор. Верно ли тогда так называть нейросети? Конечно, нейросеть на порядки сложнее калькулятора. Но, если посмотреть на итог работы нейросети без предвосхищения интеллекта, это просто классификатор! Причем регрессивного характера — множество входов она сводит к выбору нескольких вариантов (кроме ассоциативных сетей и машины Больцмана). Реле, только с более сложным входом. Мне казалось, что интеллект — это как минимум способность рассуждать. И пока ни одна сеть не может построить хотя бы простейший силлогизм, называть ее интеллектом, мягко говоря, рановато. Я уж не говорю о более сложных задачах. Например, У пчелы немногим больше нейронов в голове, но она умеет то, на что ни одна современная нейросеть пока не способна — летать с большой скоростью среди веток и деревьев. Пока Google с трудом, вспотев за 10 лет разработок, водит машины по дорожной разметке. А пчелы — это насекомое, самое начало эволюции нейросети. Мы не достигли даже этого начального уровня, а уже называем сети созданием, которым ознаменован венец эволюции нейросети — интеллектом. Все, и сами ученые, совершают одну и ту же эпистемиологическую ошибку. Они говорят, что сеть обладает интеллектом, потому что распознала лошадь на картинке! Разве она распознала лошадь? Она просто выбрала выход Х, который исследователь назвал «лошадь». Да, она произвела обобщение (классификатор), сведя вариант лошади к выходу «лошадь». Но нейросеть никакого представления о том, что она выбрала, не имеет. У нее нет понятия о «лошади» (семантики), «значение» выхода «лошадь» у нас в голове, а не в сети. Чтобы он был в самой нейросети, она должна выбрать не вход, не слово соответственно распознанной картинке, а все контекстуально связанные слова, то есть весь концепт «лошади». Тогда она, может быть, будет понимать, что лошадь и туманность имеют общее значение только в смысле облака космической пыли. Для нейросети же выбранный выход — это смысловая «точка», не имеющая содержания. Самое слабые достижения нейронных сетей как раз в NLP, там где требуется понимание естественного языка. Потому что мы до сих пор сами, как мне кажется, не очень понимаем, что значит «понимать». А в этом самый важный аспект, который мы хотели бы реализовать в машине. И который могли бы тогда по праву назвать интеллектом. Понимание именно в смысле семантики «лошади». При этом похоже мы приближаемся к порогу понимания того, что сами делаем — сети создают, обучают, но понять как они достигают результата уже не могут по признанию самих создателей. Хотя мы создали только первый кирпичик в здании Интеллекта. Сможем ли мы тогда достичь более серьезного уровня ИИ, который сложнее нынешнего на порядок? Итог Как итог приведу цитату из статьи, только что появившуюся на Хабре, об «успехах» нейросетей в языковом (смысловом) подражании habrahabr.ru/company/payonline/blog/307666 «Есть только одна проблема, которая быстро становится очевидной по мере наблюдения за другими ответами системы. Когда Ли спросил: «Сколько ног у кошки?», система ответила: «Четыре, я полагаю». После этого он сделал другую попытку: «Сколько ног у сороконожки?». Ответ последовал любопытный: «Восемь». По сути, программа Ли не имеет понятия о чем она говорит. Она понимает, что определенные сочетания символов могут встречаться вместе, но не имеет ни малейшего понятия о существовании реального мира. Она не знает, как на самом деле выглядит сороконожка, или как она передвигается. То есть перед нами все еще только иллюзия интеллекта, лишенная того самого здравого смысла, который мы, люди, воспринимаем как нечто само собой разумеющееся. Подобная нестабильность результатов вполне обыденна для систем глубинного обучения. Программа Google, сочиняющая подписи к изображениям, делает странные ошибки. Глядя на дорожный знак, например, она может назвать его набитым едой холодильником». Из комментариев отмечу ZhenyaZero, который очень точно выразил конкретное отличие современных нейросетей от того, как распознаем картинки мы, люди. «Тем не менее вы, несомненно, отличите лошадь с пятой ногой от тигра с пятой ногой. И при описании картинки большинство людей скажет «лошадь с пятой ногой», а не „думаю это кусок торта“. А результаты нейросети на необычных и пограничных вариантах действительно плохо предсказуемы и часто выглядят полностью неадекватными. А также несколько фактов 1) Чтобы увидеть столько разных „лошадок“, ни одному человеку не хватит и всей жизни (с). 2) Ребенку часто достаточно показать один раз картинку нового животного и объяснить в чем его отличие, чтобы он смог узнавать таких животных в следующий раз и в любых позах. 3) Мы подразумеваем „лошадь“ даже когда видим просто уздечку. Выводы статьи вполне очевидны, но важнее понять, что почему такие отличия существуют, и что делать дальше. Ваши предложения.\n","metric":0.5771484375,"metric_input":"Текст статьи:\nНейроинтеллект или миф? / Habr                       \n\n15-08-2016\nЭра нейросетевого искусственного интеллекта в самом разгаре! Настоящий прорыв за последние годы — нейросети распознают не только речь, картинки с животными, сложные сцены, но даже описывают их словами! (NeuralTalk). На нейросетях делают боты, которые пока плохо, но уже что-то отвечают. Нейросети сочиняют музыку и пишут стихи. По сценарию нейросети даже собираются снять фильм! Скоро, пророчат журналисты, совсем скоро они буду управлять машинами, банками, корпорациями, странами. «И все будут летать на дилижансах» — так мечтали в начале прошлого века. Так мечтает сейчас о нейросетевом будущем его создатели. Говорят, что они скоро обгонят человека во всем. Нейросети уже обыграли человека в шахматы, го, в Jaopardy (Своя игра). И поэтому они называют свои творения не меньше, чем Искусственный интеллект. Распознал лошадь на картинке — искусственный интеллект. Почему интеллект? Потому что раньше это умел делать только человек. Странно, почему тогда ранее искусственным интеллектом не назвали калькулятор. Верно ли тогда так называть нейросети? Конечно, нейросеть на порядки сложнее калькулятора. Но, если посмотреть на итог работы нейросети без предвосхищения интеллекта, это просто классификатор! Причем регрессивного характера — множество входов она сводит к выбору нескольких вариантов (кроме ассоциативных сетей и машины Больцмана). Реле, только с более сложным входом. Мне казалось, что интеллект — это как минимум способность рассуждать. И пока ни одна сеть не может построить хотя бы простейший силлогизм, называть ее интеллектом, мягко говоря, рановато. Я уж не говорю о более сложных задачах. Например, У пчелы немногим больше нейронов в голове, но она умеет то, на что ни одна современная нейросеть пока не способна — летать с большой скоростью среди веток и деревьев. Пока Google с трудом, вспотев за 10 лет разработок, водит машины по дорожной разметке. А пчелы — это насекомое, самое начало эволюции нейросети. Мы не достигли даже этого начального уровня, а уже называем сети созданием, которым ознаменован венец эволюции нейросети — интеллектом. Все, и сами ученые, совершают одну и ту же эпистемиологическую ошибку. Они говорят, что сеть обладает интеллектом, потому что распознала лошадь на картинке! Разве она распознала лошадь? Она просто выбрала выход Х, который исследователь назвал «лошадь». Да, она произвела обобщение (классификатор), сведя вариант лошади к выходу «лошадь». Но нейросеть никакого представления о том, что она выбрала, не имеет. У нее нет понятия о «лошади» (семантики), «значение» выхода «лошадь» у нас в голове, а не в сети. Чтобы он был в самой нейросети, она должна выбрать не вход, не слово соответственно распознанной картинке, а все контекстуально связанные слова, то есть весь концепт «лошади». Тогда она, может быть, будет понимать, что лошадь и туманность имеют общее значение только в смысле облака космической пыли. Для нейросети же выбранный выход — это смысловая «точка», не имеющая содержания. Самое слабые достижения нейронных сетей как раз в NLP, там где требуется понимание естественного языка. Потому что мы до сих пор сами, как мне кажется, не очень понимаем, что значит «понимать». А в этом самый важный аспект, который мы хотели бы реализовать в машине. И который могли бы тогда по праву назвать интеллектом. Понимание именно в смысле семантики «лошади». При этом похоже мы приближаемся к порогу понимания того, что сами делаем — сети создают, обучают, но понять как они достигают результата уже не могут по признанию самих создателей. Хотя мы создали только первый кирпичик в здании Интеллекта. Сможем ли мы тогда достичь более серьезного уровня ИИ, который сложнее нынешнего на порядок? Итог Как итог приведу цитату из статьи, только что появившуюся на Хабре, об «успехах» нейросетей в языковом (смысловом) подражании habrahabr.ru/company/payonline/blog/307666 «Есть только одна проблема, которая быстро становится очевидной по мере наблюдения за другими ответами системы. Когда Ли спросил: «Сколько ног у кошки?», система ответила: «Четыре, я полагаю». После этого он сделал другую попытку: «Сколько ног у сороконожки?». Ответ последовал любопытный: «Восемь». По сути, программа Ли не имеет понятия о чем она говорит. Она понимает, что определенные сочетания символов могут встречаться вместе, но не имеет ни малейшего понятия о существовании реального мира. Она не знает, как на самом деле выглядит сороконожка, или как она передвигается. То есть перед нами все еще только иллюзия интеллекта, лишенная того самого здравого смысла, который мы, люди, воспринимаем как нечто само собой разумеющееся. Подобная нестабильность результатов вполне обыденна для систем глубинного обучения. Программа Google, сочиняющая подписи к изображениям, делает странные ошибки. Глядя на дорожный знак, например, она может назвать его набитым едой холодильником». Из комментариев отмечу ZhenyaZero, который очень точно выразил конкретное отличие современных нейросетей от того, как распознаем картинки мы, люди. «Тем не менее вы, несомненно, отличите лошадь с пятой ногой от тигра с пятой ногой. И при описании картинки большинство людей скажет «лошадь с пятой ногой», а не „думаю это кусок торта“. А результаты нейросети на необычных и пограничных вариантах действительно плохо предсказуемы и часто выглядят полностью неадекватными. А также несколько фактов 1) Чтобы увидеть столько разных „лошадок“, ни одному человеку не хватит и всей жизни (с). 2) Ребенку часто достаточно показать один раз картинку нового животного и объяснить в чем его отличие, чтобы он смог узнавать таких животных в следующий раз и в любых позах. 3) Мы подразумеваем „лошадь“ даже когда видим просто уздечку. Выводы статьи вполне очевидны, но важнее понять, что почему такие отличия существуют, и что делать дальше. Ваши предложения.\n\n\nКраткое содержание:\nСтатья обсуждает развитие нейросетевого искусственного интеллекта и его достижения в распознавании речи, изображений и написании музыки и стихов. Однако, несмотря на успехи в этих областях, нейросети все еще не могут достичь уровня интеллекта, который мы ассоциируем с человеческим разумом. Статья также подчеркивает различия между тем, как люди и нейросети воспринимают и описывают изображения, и указывает на необходимость дальнейшего исследования и разработки для достижения более глубокого уровня искусственного интеллекта.","summary":"Статья обсуждает развитие нейросетевого искусственного интеллекта и его достижения в распознавании речи, изображений и написании музыки и стихов. Однако, несмотря на успехи в этих областях, нейросети все еще не могут достичь уровня интеллекта, который мы ассоциируем с человеческим разумом. Статья также подчеркивает различия между тем, как люди и нейросети воспринимают и описывают изображения, и указывает на необходимость дальнейшего исследования и разработки для достижения более глубокого уровня искусственного интеллекта."}
{"Text":"Дружим Docker и dnscrypt-proxy / Хабр                 \n\nDNSCrypt — довольно популярный вариант защиты обычно не шифруемого DNS-трафика от внешних лиц. Клиентский резолвер dnscrypt-proxy поддерживает в том числе протокол DNS-over-HTTPS, позволяя разрешать доменные имена через DoH.\n\nВсе бы ничего, только при использовании dnscrypt-proxy с настройками по умолчанию контейнеры Docker перестают корректно резолвить адреса. Исправим это, не включая слушатель DNS на публичных интерфейсах.\n\n**TL;DR поста**\n\nСоздаем dummy-адаптер, вешаем ему IP из частного диапазона.\n\nЗаставляем dnscrypt-proxy слушать на этом IP, меняем настройки DNS соответственно.\n\nPROFIT\n\n# Суть проблемы #\n\nДля настройки DNS Docker копирует настройки DNS с хоста , копируя файл `/etc/resolv.conf`  с хостовой машины.\n\nПри использовании dnscrypt-proxy и других кастомных резолверов обычно их запускают на `127.0.0.1:53` , записывая этот адрес как `nameserver`  в `resolv.conf` , эта же настройка приходит контейнеру. Так как контейнер находится в другом сетевом пространстве имен, 127.0.0.1 у хоста и контейнера разные.\n\n# Как исправить #\n\nСамым простым вариантом решения является запуск dnscrypt-proxy на публичном адресе хоста и добавление этого же адреса в `/etc/resolv.conf` . Однако в таком случае мы выставляем в сеть DNS-резолвер, чего хочется избежать.\n\nВместо этого мы создадим сетевой адаптер-затычку, через который не идет трафик, но на который можно повесить IP-адрес из частного диапазона; он будет роутабелен из контейнера, поскольку Docker-контейнеры с сетью через бридж используют в качестве шлюза по умолчанию хост. В Linux такой адаптер называется `dummy` .\n\n## Создаем адаптер ##\n\nЕсли в вашей системе не загружен модуль ядра `dummy`  ( `lsmod | grep dummy`  не показывает этот модуль), загрузите его и включите его загрузку на постоянной основе:\n\n```\n# modprobe dummy\n# echo \"dummy\" >> /etc/modules-load.d/net_dummy.conf\n```\n\nДля того, чтобы создать и настроить dummy-адаптер, в современной Linux-системе с iproute2 достаточно выполнить две следующие команды:\n\n```\n# ip link add type dummy name dummy0\n# ip addr add dev dummy0 10.0.197.1/24\n```\n\nСоздание адаптера для его использования на постоянной основе будет разниться в зависимости от используемого вами сетевого конфигуратора. Например, для systemd-networkd понадобятся два конфигурационных файла:\n\n`/etc/systemd/network/50-dummy0.netdev` :\n\n```\n[NetDev]\nName=dummy0\nKind=dummy\nDescription=Dummy network for dnscrypt-proxy\n```\n\n`/etc/systemd/network/50-dummy0.network` :\n\n```\n[Match]\nName=dummy0\n\n[Network]\nDHCP=no\nAddress=10.0.197.1/24\nDefaultRouteOnDevice=false\n```\n\n## Изменяем настройки DNS ##\n\nДля того, чтобы заставить dnscrypt-proxy слушать на новом адресе, потребуется отредактировать `/etc/dnscrypt-proxy/dnscrypt-proxy.toml` . Отредактируем директиву `listen_addresses`  и приведем ее к следующему виду:\n\n```\nlisten_addresses = ['127.0.0.1:53', '[::1]:53', '10.0.197.1:53']\n```\n\nПерезапустим dnscrypt-proxy, после чего в `/etc/resolv.conf`  (либо где в вашем сетевом конфигураторе находятся настройки DNS) оставим в списке серверов имен один: `nameserver 10.0.197.1` .\n\n## Проверяем ##\n\nЗапускаем контейнер:\n\n```\n% docker run -it --rm alpine:3.12\n# cat /etc/resolv.conf\nnameserver 10.0.197.1\n# ping -c 1 ya.ru\n```\n\n## Дополнительно ##\n\nЕсли вы используете файрволл, разрешите входящий трафик на `10.0.197.1:53`  из подсетей, из которых Docker выдает адреса контейнерам.\n\nВ случае, если вы используете в качестве локального резолвера systemd-resolved, направленный на dnscrypt-proxy (например, для кэширования или для использования разных DNS-серверов для разных интерфейсов), тот факт, что systemd-resolved слушает исключительно на `127.0.0.53`  и не позволяет изменение этого адреса, вас смущать не должен: Docker определяет использование systemd-resolved  и вместо содержимого `/etc/resolv.conf`  копирует содержимое `/run/systemd/resolve/resolv.conf` , генерируемое из настроек systemd-resolved.","metric":0.53857421880000000414,"metric_input":"Текст статьи:\nДружим Docker и dnscrypt-proxy / Хабр                 \n\nDNSCrypt — довольно популярный вариант защиты обычно не шифруемого DNS-трафика от внешних лиц. Клиентский резолвер dnscrypt-proxy поддерживает в том числе протокол DNS-over-HTTPS, позволяя разрешать доменные имена через DoH.\n\nВсе бы ничего, только при использовании dnscrypt-proxy с настройками по умолчанию контейнеры Docker перестают корректно резолвить адреса. Исправим это, не включая слушатель DNS на публичных интерфейсах.\n\n**TL;DR поста**\n\nСоздаем dummy-адаптер, вешаем ему IP из частного диапазона.\n\nЗаставляем dnscrypt-proxy слушать на этом IP, меняем настройки DNS соответственно.\n\nPROFIT\n\n# Суть проблемы #\n\nДля настройки DNS Docker копирует настройки DNS с хоста , копируя файл `/etc/resolv.conf`  с хостовой машины.\n\nПри использовании dnscrypt-proxy и других кастомных резолверов обычно их запускают на `127.0.0.1:53` , записывая этот адрес как `nameserver`  в `resolv.conf` , эта же настройка приходит контейнеру. Так как контейнер находится в другом сетевом пространстве имен, 127.0.0.1 у хоста и контейнера разные.\n\n# Как исправить #\n\nСамым простым вариантом решения является запуск dnscrypt-proxy на публичном адресе хоста и добавление этого же адреса в `/etc/resolv.conf` . Однако в таком случае мы выставляем в сеть DNS-резолвер, чего хочется избежать.\n\nВместо этого мы создадим сетевой адаптер-затычку, через который не идет трафик, но на который можно повесить IP-адрес из частного диапазона; он будет роутабелен из контейнера, поскольку Docker-контейнеры с сетью через бридж используют в качестве шлюза по умолчанию хост. В Linux такой адаптер называется `dummy` .\n\n## Создаем адаптер ##\n\nЕсли в вашей системе не загружен модуль ядра `dummy`  ( `lsmod | grep dummy`  не показывает этот модуль), загрузите его и включите его загрузку на постоянной основе:\n\n```\n# modprobe dummy\n# echo \"dummy\" >> /etc/modules-load.d/net_dummy.conf\n```\n\nДля того, чтобы создать и настроить dummy-адаптер, в современной Linux-системе с iproute2 достаточно выполнить две следующие команды:\n\n```\n# ip link add type dummy name dummy0\n# ip addr add dev dummy0 10.0.197.1/24\n```\n\nСоздание адаптера для его использования на постоянной основе будет разниться в зависимости от используемого вами сетевого конфигуратора. Например, для systemd-networkd понадобятся два конфигурационных файла:\n\n`/etc/systemd/network/50-dummy0.netdev` :\n\n```\n[NetDev]\nName=dummy0\nKind=dummy\nDescription=Dummy network for dnscrypt-proxy\n```\n\n`/etc/systemd/network/50-dummy0.network` :\n\n```\n[Match]\nName=dummy0\n\n[Network]\nDHCP=no\nAddress=10.0.197.1/24\nDefaultRouteOnDevice=false\n```\n\n## Изменяем настройки DNS ##\n\nДля того, чтобы заставить dnscrypt-proxy слушать на новом адресе, потребуется отредактировать `/etc/dnscrypt-proxy/dnscrypt-proxy.toml` . Отредактируем директиву `listen_addresses`  и приведем ее к следующему виду:\n\n```\nlisten_addresses = ['127.0.0.1:53', '[::1]:53', '10.0.197.1:53']\n```\n\nПерезапустим dnscrypt-proxy, после чего в `/etc/resolv.conf`  (либо где в вашем сетевом конфигураторе находятся настройки DNS) оставим в списке серверов имен один: `nameserver 10.0.197.1` .\n\n## Проверяем ##\n\nЗапускаем контейнер:\n\n```\n% docker run -it --rm alpine:3.12\n# cat /etc/resolv.conf\nnameserver 10.0.197.1\n# ping -c 1 ya.ru\n```\n\n## Дополнительно ##\n\nЕсли вы используете файрволл, разрешите входящий трафик на `10.0.197.1:53`  из подсетей, из которых Docker выдает адреса контейнерам.\n\nВ случае, если вы используете в качестве локального резолвера systemd-resolved, направленный на dnscrypt-proxy (например, для кэширования или для использования разных DNS-серверов для разных интерфейсов), тот факт, что systemd-resolved слушает исключительно на `127.0.0.53`  и не позволяет изменение этого адреса, вас смущать не должен: Docker определяет использование systemd-resolved  и вместо содержимого `/etc/resolv.conf`  копирует содержимое `/run/systemd/resolve/resolv.conf` , генерируемое из настроек systemd-resolved.\n\nКраткое содержание:\nВ статье рассматривается проблема совместимости Docker и dnscrypt-proxy, популярного решения для защиты DNS-трафика. При использовании dnscrypt-proxy с настройками по умолчанию, контейнеры Docker не могут корректно резолвить адреса. Авторы предлагают решение, которое позволяет использовать dnscrypt-proxy без проблем с контейнерами Docker.\n\nСуть проблемы заключается в том, что dnscrypt-proxy и Docker используют разные IP-адреса для прослушивания DNS-запросов. В то время как dnscrypt-proxy слушает на 127.0.0.1:53, Docker копирует настройки DNS из хостовой системы и использует этот адрес в файле /etc/resolv.conf.\n\nАвторы предлагают создать специальный сетевой адаптер, который не будет передавать трафик, но будет иметь доступ к частному IP-адресу. Затем, они изменяют настройки dnscrypt-proxy для прослушивания на этом адресе. Это позволяет решить проблему совместимости между Docker и dnscrypt-proxy.","summary":"В статье рассматривается проблема совместимости Docker и dnscrypt-proxy, популярного решения для защиты DNS-трафика. При использовании dnscrypt-proxy с настройками по умолчанию, контейнеры Docker не могут корректно резолвить адреса. Авторы предлагают решение, которое позволяет использовать dnscrypt-proxy без проблем с контейнерами Docker.\n\nСуть проблемы заключается в том, что dnscrypt-proxy и Docker используют разные IP-адреса для прослушивания DNS-запросов. В то время как dnscrypt-proxy слушает на 127.0.0.1:53, Docker копирует настройки DNS из хостовой системы и использует этот адрес в файле /etc/resolv.conf.\n\nАвторы предлагают создать специальный сетевой адаптер, который не будет передавать трафик, но будет иметь доступ к частному IP-адресу. Затем, они изменяют настройки dnscrypt-proxy для прослушивания на этом адресе. Это позволяет решить проблему совместимости между Docker и dnscrypt-proxy."}
{"Text":"dock: простая библиотека модульного тестирования кода на С++ / Хабр                                                          \n\n18-01-2017\nХотя и существуют уже библиотеки для юнит-тестирования кода на С++, например, Google Test или Bandit, но они написаны не мной здесь оно, на мой взгляд, как-то переусложнено, по сравнению с тем же JS. Там просто делаешь, например, npm i mocha assert --save-dev и можно приступать к написанию тестов, а здесь же нужно это сделать ручками, а в случае с gtest еще и собрать с помощью cmake ее. Bandit подключается просто, но не умеет в сериализацию результатов в какой-то формат данных, gtest это умеет, но его нужно собирать отдельно. А я не хочу выбирать \"либо то, либо это\". Мне было нужно сделать удобный и простой инструмент под мои задачи. Я хотел получить простую библиотеку без зависимостей, header-only, на несколько файлов, которую можно легко и быстро подключить к своему проекту, удобно внести в нее изменения (если это будет необходимо). Но, самое основное, мне хотелось получать удобные, машиночитаемые отчеты, причем не только в stdout (или xml, как в gtest), но и в любой другой формат, который я захочу. Далее под катом. Как я уже писал выше, библиотека dock header-only, а значит ее подключение максимально простое: #include <iostream> #include <dock/dock.hpp> using namespace dock; int main() { core().run(); return 0; } При сборке, например, в gcc, нужно передать только путь к папке с библиотеками и указать стандарт языка C++14. Я намеренно делаю так, потому что новые проекты я пишу на свежем стандарте, а для поддержки старых есть уже свои готовые библиотеки. Описание тестов тоже сделано предельно простым: using namespace dock; Module(u8\"Some module 1\", [](DOCK_MODULE()) { Test(u8\"Some test 1\", []() { uint8_t value = 0x10; uint8_t expectedValue = 0x10; Assert::isEqual(value, expectedValue); }); Test(u8\"Some test 2\", []() { uint8_t value = 0x10; uint8_t expectedBorder = 0x20; Assert::isLess(value, expectedBorder); }); }); Module(u8\"Some module 2\", [](DOCK_MODULE()) { Test(u8\"Some test 1\", []() { Assert::isTrue(true); }); Test(u8\"Some test 2\", []() { Assert::isTrue(false); }); }); Для удобства тесты группируются в модули. В них передается объект std::function<void(Module*)>, внутри которого описываются непосредственно тесты. Тесты имеют примерно такой же синтаксис, только функциональный объект без параметров. Пока что я не делал проверку на уникальность имени модуля или теста, потому что это было не критично. \"Библиотека\" Assert содержит простой набор методов isTrue,isEquals, isGreater, isLess, которые по умолчанию могут сравнивать объекты через операторы ==, > или <. Если операторов нет, то можно функцию сравнения передать в конце параметром (например, в виде лямбды). static void isTrue(std::function<bool()> fcn); template<typename T> static void isEqual(const T a, const T b, std::function<bool(const T, const T)> compareFcn = defaultEqualsFunction<T>); template<typename T> static void isGreater(const T a, const T b, std::function<bool(const T, const T)> compareFcn = defaultGreaterFunction<T>); template<typename T> static void isLess(const T a, const T b, std::function<bool(const T, const T)> compareFcn = defaultLessFunction<T>); А теперь как раз то, что было нужно мне: удобное преобразование результатов тестирования в необходимый формат данных. Для начала, просто хочется поработать с статистикой ведения проекта, смотреть динамику по тестам и подобные вещи, и мне это удобно делать на JS. Поэтому первый формат, который мне потребовался — JSON. В репозитории есть уже три готовых сериализатора: в JSON, в plain text и вывод в консоль с подсветкой. Использование сериализаторов очень простое: nlohmann::json outJson; JsonSerializer serializer(outJson, 4); core().run(); core().collect(serializer); std::cout << serializer << std::endl; А сам интерфейс сериализатора выглядит следующим образом: class ResultSerializer { public: virtual ~ResultSerializer() = default; virtual void serialize(std::vector<Result>& results) = 0; virtual std::string toString() const = 0; friend std::ostream& operator<<(std::ostream& os, ResultSerializer& s); }; Т.е. выводить результат можем куда угодно, подставить только std::ostream и все. Логика работы сериализатора следующая: Передаем сериализатор движку через collect() и он вызывает метод serialize() с вектором результатов. В операторе << вызывается метод toString(), который выдает строку в std::ostream. Можно сделать два варианта: либо при вызове serialize() сразу создаем нужную строку, а ее потом либо просто возвращаем, либо сохраняем ссылку на результаты и генерируем выдачу непосредственно при выдаче в ostream. В любом случае, остается свобода движения — движок выдает просто std::vector<dock::Result>, а что с ним делать уже дело ваше :). Лицензия свободная (MIT), потому что мне не жалко и будет приятно видеть её использование. Для сериализаторов использовались библиотеки termcolor и JSON for Modern C++, но можно спокойно убрать их вместе с ненужными сериализаторами.\n","metric":0.5595703125,"metric_input":"Текст статьи:\ndock: простая библиотека модульного тестирования кода на С++ / Хабр                                                          \n\n18-01-2017\nХотя и существуют уже библиотеки для юнит-тестирования кода на С++, например, Google Test или Bandit, но они написаны не мной здесь оно, на мой взгляд, как-то переусложнено, по сравнению с тем же JS. Там просто делаешь, например, npm i mocha assert --save-dev и можно приступать к написанию тестов, а здесь же нужно это сделать ручками, а в случае с gtest еще и собрать с помощью cmake ее. Bandit подключается просто, но не умеет в сериализацию результатов в какой-то формат данных, gtest это умеет, но его нужно собирать отдельно. А я не хочу выбирать \"либо то, либо это\". Мне было нужно сделать удобный и простой инструмент под мои задачи. Я хотел получить простую библиотеку без зависимостей, header-only, на несколько файлов, которую можно легко и быстро подключить к своему проекту, удобно внести в нее изменения (если это будет необходимо). Но, самое основное, мне хотелось получать удобные, машиночитаемые отчеты, причем не только в stdout (или xml, как в gtest), но и в любой другой формат, который я захочу. Далее под катом. Как я уже писал выше, библиотека dock header-only, а значит ее подключение максимально простое: #include <iostream> #include <dock/dock.hpp> using namespace dock; int main() { core().run(); return 0; } При сборке, например, в gcc, нужно передать только путь к папке с библиотеками и указать стандарт языка C++14. Я намеренно делаю так, потому что новые проекты я пишу на свежем стандарте, а для поддержки старых есть уже свои готовые библиотеки. Описание тестов тоже сделано предельно простым: using namespace dock; Module(u8\"Some module 1\", [](DOCK_MODULE()) { Test(u8\"Some test 1\", []() { uint8_t value = 0x10; uint8_t expectedValue = 0x10; Assert::isEqual(value, expectedValue); }); Test(u8\"Some test 2\", []() { uint8_t value = 0x10; uint8_t expectedBorder = 0x20; Assert::isLess(value, expectedBorder); }); }); Module(u8\"Some module 2\", [](DOCK_MODULE()) { Test(u8\"Some test 1\", []() { Assert::isTrue(true); }); Test(u8\"Some test 2\", []() { Assert::isTrue(false); }); }); Для удобства тесты группируются в модули. В них передается объект std::function<void(Module*)>, внутри которого описываются непосредственно тесты. Тесты имеют примерно такой же синтаксис, только функциональный объект без параметров. Пока что я не делал проверку на уникальность имени модуля или теста, потому что это было не критично. \"Библиотека\" Assert содержит простой набор методов isTrue,isEquals, isGreater, isLess, которые по умолчанию могут сравнивать объекты через операторы ==, > или <. Если операторов нет, то можно функцию сравнения передать в конце параметром (например, в виде лямбды). static void isTrue(std::function<bool()> fcn); template<typename T> static void isEqual(const T a, const T b, std::function<bool(const T, const T)> compareFcn = defaultEqualsFunction<T>); template<typename T> static void isGreater(const T a, const T b, std::function<bool(const T, const T)> compareFcn = defaultGreaterFunction<T>); template<typename T> static void isLess(const T a, const T b, std::function<bool(const T, const T)> compareFcn = defaultLessFunction<T>); А теперь как раз то, что было нужно мне: удобное преобразование результатов тестирования в необходимый формат данных. Для начала, просто хочется поработать с статистикой ведения проекта, смотреть динамику по тестам и подобные вещи, и мне это удобно делать на JS. Поэтому первый формат, который мне потребовался — JSON. В репозитории есть уже три готовых сериализатора: в JSON, в plain text и вывод в консоль с подсветкой. Использование сериализаторов очень простое: nlohmann::json outJson; JsonSerializer serializer(outJson, 4); core().run(); core().collect(serializer); std::cout << serializer << std::endl; А сам интерфейс сериализатора выглядит следующим образом: class ResultSerializer { public: virtual ~ResultSerializer() = default; virtual void serialize(std::vector<Result>& results) = 0; virtual std::string toString() const = 0; friend std::ostream& operator<<(std::ostream& os, ResultSerializer& s); }; Т.е. выводить результат можем куда угодно, подставить только std::ostream и все. Логика работы сериализатора следующая: Передаем сериализатор движку через collect() и он вызывает метод serialize() с вектором результатов. В операторе << вызывается метод toString(), который выдает строку в std::ostream. Можно сделать два варианта: либо при вызове serialize() сразу создаем нужную строку, а ее потом либо просто возвращаем, либо сохраняем ссылку на результаты и генерируем выдачу непосредственно при выдаче в ostream. В любом случае, остается свобода движения — движок выдает просто std::vector<dock::Result>, а что с ним делать уже дело ваше :). Лицензия свободная (MIT), потому что мне не жалко и будет приятно видеть её использование. Для сериализаторов использовались библиотеки termcolor и JSON for Modern C++, но можно спокойно убрать их вместе с ненужными сериализаторами.\n\n\nКраткое содержание:\nСтатья описывает создание простой и удобной библиотеки модульного тестирования кода на C++, dock, которая предназначена для автоматизации тестирования кода и обеспечения машиночитаемых отчетов. Библиотека header-only, что делает ее подключение простым и быстрым. Она предоставляет простой интерфейс для описания тестов и имеет набор методов для сравнения объектов. Библиотека также имеет возможность преобразования результатов тестирования в различные форматы данных, включая JSON, что позволяет легко работать с данными и визуализировать статистику.","summary":"Статья описывает создание простой и удобной библиотеки модульного тестирования кода на C++, dock, которая предназначена для автоматизации тестирования кода и обеспечения машиночитаемых отчетов. Библиотека header-only, что делает ее подключение простым и быстрым. Она предоставляет простой интерфейс для описания тестов и имеет набор методов для сравнения объектов. Библиотека также имеет возможность преобразования результатов тестирования в различные форматы данных, включая JSON, что позволяет легко работать с данными и визуализировать статистику."}
{"Text":"Биткойн и денежные переводы / Habr             \n\n13-12-2013\nНесмотря на стабильно растущий интерес к криптографической валюте bitcoin как-то по умолчанию считается, что её практическое применение для получения прибыли ограничено использованием биткойнов для операций с нелегальными товарами, организацией сервисов-миксеров и биткойн-бирж, а так же торговлей в интернет-магазинах для таких же энтузиастов криптовалюты и, собственно, наиболее широко обсуждаемой в контексте роста его курса теме — накоплению. А ведь многие свойства этой валюты, и, в первую очередь, гарантируемая анонимность пользователя в системе могла бы очень пригодиться для решения и других задач. Ниже будет рассмотрена одна из них. Денежные переводы В контексте организации денежных переводов мы будем говорить о четырёх основных схемах: Классической (передача наличных) Электронной (банковский перевод) Криптографической (перевод денег в электронной валюте bitcoin) Усиленной криптографической с участием банка-посредника Все схемы рассматриваются в модели двух взаимодействующих физ. (или юр.) лиц, Алисы и Боба, которые хотят осуществить между собой транзакцию на определённую сумму. Классическая схема Описание схемы Алиса встречается с Бобом и передаёт ему деньги лично. Недостатки Невозможность обеспечения гарантированной анонимности участников транзакции Необходимость физического участия лиц проводящих транзакцию Возможность вмешательства третьих лиц Необходимость обеспечения физической безопасности подготовки и проведения транзакции Электронная схема Описание схемы Алиса осуществляет электронный перевод денег на счёт Боба Недостатки По определению отсутствие анонимности Контроль и возможность блокировки транзакции со стороны третьих лиц Криптографическая схема Описание схемы Алиса через электронную биржу осуществляет покупку виртуальной пиринговой криптовалюты BTC в системе bitcoin, после чего переводит деньги на bitcoin-кошелёк Боба. Боб продаёт BTC на бирже, переводя деньги на свой счёт. Система bitcoin обеспечивает гарантированную анонимность производимых транзакций. Недостатки Возможность восстановления истории транзакций (в случае компрометации анонимности как Алисы так и Боба можно восставить всю историю проводимых ими операций). Возможная недостаточная ликвидность биткойн-активов и курсовые риски. Предъявляет высокие требования к технической грамотности Алисы и Боба. Усиленная криптографическая схема с участием банка-посредника Описание схемы Алиса в наличной либо безналичной форме зачисляет деньги на счёт уполномоченной фирмы (будем называть её Чарли). Чарли даёт поручение Банку на рублёвый перевод в пользу продавца (спекулянта) некой промежуточной электронной валюты (схема проверялась с использованием Liberty reserve, прекратившей существование в мае 2013-го года, но для понимания идеи это не имеет значения, так что дальше так же будет упоминаться она), с которой работают биткойн-биржи. На Liberty reserve Чарли покупает биткойны. В рамках альтернативной схемы действий Чарли может дать поручение на валютный перевод в пользу нерезидента — биткойн-биржи — на покупку программного продукта (биткойнов). Биткойны покупаются на случайное число сгенерированных bitcoin-кошельков (всего можно создать 115 792 089 237 316 195 423 570 985 008 687 907 853 269 984 665 640 564 039 457 584 007 913 129 639 936 кошельков). Суммарный объём денег на кошельках будет эквивалентен исходной сумме за вычетом комиссии за перевод. Работа со случайными кошельками гарантирует невозможность восстановления истории транзакций (ту же самую услугу предоставляют сервисы-миксеры). Между данными и любыми другими кошельками проводиться последовательность бессмысленных операций перевода с одного кошелька на другой (внутри системы bitcoin обязательная комиссия отсутствует). Это необходимо для “замешивания” релевантных транзакций в общее их число, что делает невозможным эвристический анализ. Затем Алиса поручает Чарли перевести биткойны на кошелёк(и) Боба или самостоятельно передаёт Бобу свои кошельки (их номера и криптографические ключи). Второй вариант может быть целесообразен в случае подозрения слежки за кошельками. С точки зрения третьих лиц это будет выглядеть так, будто на кошелёк пришли деньги (с биржи) и ушли деньги (на биржу), а то, что кошелек перешел от одного человека к другому — никто не узнает. Боб поручает Чарли продать имеющиеся у него биткойны (по схеме обратной вышеописанной) или продаёт их на бирже сам. Примечания На каждом шаге происходит автоматическое уничтожение далее не используемой информации по предыдущим шагам. Данный подход гарантирует безопасность Алисы и Боба даже в случае вмешательства третьих лиц в работу Чарли. Время проведения транзакции в системе биткойн определяется исключительно фактором верификации данной операции системой. Минимальная и достаточная степень верификации достигается в течение десяти минут. Стопроцентная верификация, являющаяся излишней, занимает порядка часа. Если транзакции проводятся доверяющими друг другу лицами, или одним и тем же лицом между своими счетами операция происходит мгновенно. Схема избавляет от двух главных рисков биткойнов – возможной недостаточной ликвидности биткойн-активов и курсовых скачков. В условиях российского рынка биткойнов основной проблемой является ввод рублей в систему. Постоянное повторение цепочки Алиса – Чарли – спекулянт – биткойны – Боб невыгодно и неудобно. Разумной была бы разовая покупка достаточного количества биткойнов по одной из известных товарных схем с последующим их внутренним оборотом для обслуживания собственной клиентуры. По причине невозможности привязки биткойн-счёта к физическому лицу с точки зрения стороннего наблюдателя операции перевода биткойнов между кошельками, принадлежащими банку (Чарли), и кошельками, принадлежащими любым другим пользователям системы, будут неотличимы. Целесообразным шагом для Чарли было бы учреждение собственной биткойн-биржи. Для этого требуются весьма скромные траты на открытие сайта и оплату труда 13 человек персонала с соответствующим программным и аппаратным обеспечением. Практически бесплатное мероприятие привлечёт стороннюю клиентуру и увеличит оборот. Пример практической реализации Ниже приведена схема решения практической (и наиболее актуальной для России) задачи необлагаемого налогами вывода капитала за границу (в данном примере в США). Деньги Алисы зачисляются на счёт фирмы Чарли в банке-посреднике От имени Чарли производится банковский перевод в счёт приобретения товара, электронной криптовалюты биткойн, у фирмы Mt.Gox Co Ltd (Shibuya, Tokyo, Japan) с оформлением паспорта сделки и проведением всех сопутствующих операций. Данный шаг имеет смысл производить однократно с целью дальнейшего обслуживания всех клиентов посредством приобретённого пула биткойнов. Альтернативная схема приобретения биткойнов в обход законов о валютном регулировании и валютном контроле, представляющая собой двухшаговую операцию покупки карты Master Card (https://www.aurumxchange.com/cards) у фирмы Aurum Xchange (AURUM CAPITAL HOLDINGS INC.) с последующей тратой переведённых на неё денег на приобретение так называемых Mt.Gox Redeemable Code, позволяющих зачислить деньги на счёт в Mt.Gox с комиссией за перевод в 3%. Максимальный месячный лимит одной карты Master Card – 100 т.$. Максимальный месячный лимит Mt.Gox – 500 т. $. Покупка биткойнов на бирже Mt.Gox (комиссия 0.55%) Цепочка транзакций между bitcoin-кошельками, гарантирующая анонимность и неотслеживаемость основного перевода Продажа биткойнов на бирже Mt.Gox (может производиться от имени нескольких пользователей) Аккумуляция и вывод средств на счёт в американском банке банковским переводом от Mt.Gox PS Изучение вопроса практической применимости описанной криптографической схемы денежных переводов с участием банка-посредника для анонимизации самих переводов и оптимизации налогообложения при перечислении средств показало: Высокую заинтересованность в данной услуге потенциальных пользователей (Алисы и Боба). Не менее высокую заинтересованность банков и (почему-то) одной риэлторской конторы в возможности предоставления такой услуги (Чарли). Полную практическую неприменимость и неконкурентоспособность услуги в условиях современной российской конъюнктуры (именно в силу последней, а не недостатков самой схемы).\n","metric":0.60986328130000000414,"metric_input":"Текст статьи:\nБиткойн и денежные переводы / Habr             \n\n13-12-2013\nНесмотря на стабильно растущий интерес к криптографической валюте bitcoin как-то по умолчанию считается, что её практическое применение для получения прибыли ограничено использованием биткойнов для операций с нелегальными товарами, организацией сервисов-миксеров и биткойн-бирж, а так же торговлей в интернет-магазинах для таких же энтузиастов криптовалюты и, собственно, наиболее широко обсуждаемой в контексте роста его курса теме — накоплению. А ведь многие свойства этой валюты, и, в первую очередь, гарантируемая анонимность пользователя в системе могла бы очень пригодиться для решения и других задач. Ниже будет рассмотрена одна из них. Денежные переводы В контексте организации денежных переводов мы будем говорить о четырёх основных схемах: Классической (передача наличных) Электронной (банковский перевод) Криптографической (перевод денег в электронной валюте bitcoin) Усиленной криптографической с участием банка-посредника Все схемы рассматриваются в модели двух взаимодействующих физ. (или юр.) лиц, Алисы и Боба, которые хотят осуществить между собой транзакцию на определённую сумму. Классическая схема Описание схемы Алиса встречается с Бобом и передаёт ему деньги лично. Недостатки Невозможность обеспечения гарантированной анонимности участников транзакции Необходимость физического участия лиц проводящих транзакцию Возможность вмешательства третьих лиц Необходимость обеспечения физической безопасности подготовки и проведения транзакции Электронная схема Описание схемы Алиса осуществляет электронный перевод денег на счёт Боба Недостатки По определению отсутствие анонимности Контроль и возможность блокировки транзакции со стороны третьих лиц Криптографическая схема Описание схемы Алиса через электронную биржу осуществляет покупку виртуальной пиринговой криптовалюты BTC в системе bitcoin, после чего переводит деньги на bitcoin-кошелёк Боба. Боб продаёт BTC на бирже, переводя деньги на свой счёт. Система bitcoin обеспечивает гарантированную анонимность производимых транзакций. Недостатки Возможность восстановления истории транзакций (в случае компрометации анонимности как Алисы так и Боба можно восставить всю историю проводимых ими операций). Возможная недостаточная ликвидность биткойн-активов и курсовые риски. Предъявляет высокие требования к технической грамотности Алисы и Боба. Усиленная криптографическая схема с участием банка-посредника Описание схемы Алиса в наличной либо безналичной форме зачисляет деньги на счёт уполномоченной фирмы (будем называть её Чарли). Чарли даёт поручение Банку на рублёвый перевод в пользу продавца (спекулянта) некой промежуточной электронной валюты (схема проверялась с использованием Liberty reserve, прекратившей существование в мае 2013-го года, но для понимания идеи это не имеет значения, так что дальше так же будет упоминаться она), с которой работают биткойн-биржи. На Liberty reserve Чарли покупает биткойны. В рамках альтернативной схемы действий Чарли может дать поручение на валютный перевод в пользу нерезидента — биткойн-биржи — на покупку программного продукта (биткойнов). Биткойны покупаются на случайное число сгенерированных bitcoin-кошельков (всего можно создать 115 792 089 237 316 195 423 570 985 008 687 907 853 269 984 665 640 564 039 457 584 007 913 129 639 936 кошельков). Суммарный объём денег на кошельках будет эквивалентен исходной сумме за вычетом комиссии за перевод. Работа со случайными кошельками гарантирует невозможность восстановления истории транзакций (ту же самую услугу предоставляют сервисы-миксеры). Между данными и любыми другими кошельками проводиться последовательность бессмысленных операций перевода с одного кошелька на другой (внутри системы bitcoin обязательная комиссия отсутствует). Это необходимо для “замешивания” релевантных транзакций в общее их число, что делает невозможным эвристический анализ. Затем Алиса поручает Чарли перевести биткойны на кошелёк(и) Боба или самостоятельно передаёт Бобу свои кошельки (их номера и криптографические ключи). Второй вариант может быть целесообразен в случае подозрения слежки за кошельками. С точки зрения третьих лиц это будет выглядеть так, будто на кошелёк пришли деньги (с биржи) и ушли деньги (на биржу), а то, что кошелек перешел от одного человека к другому — никто не узнает. Боб поручает Чарли продать имеющиеся у него биткойны (по схеме обратной вышеописанной) или продаёт их на бирже сам. Примечания На каждом шаге происходит автоматическое уничтожение далее не используемой информации по предыдущим шагам. Данный подход гарантирует безопасность Алисы и Боба даже в случае вмешательства третьих лиц в работу Чарли. Время проведения транзакции в системе биткойн определяется исключительно фактором верификации данной операции системой. Минимальная и достаточная степень верификации достигается в течение десяти минут. Стопроцентная верификация, являющаяся излишней, занимает порядка часа. Если транзакции проводятся доверяющими друг другу лицами, или одним и тем же лицом между своими счетами операция происходит мгновенно. Схема избавляет от двух главных рисков биткойнов – возможной недостаточной ликвидности биткойн-активов и курсовых скачков. В условиях российского рынка биткойнов основной проблемой является ввод рублей в систему. Постоянное повторение цепочки Алиса – Чарли – спекулянт – биткойны – Боб невыгодно и неудобно. Разумной была бы разовая покупка достаточного количества биткойнов по одной из известных товарных схем с последующим их внутренним оборотом для обслуживания собственной клиентуры. По причине невозможности привязки биткойн-счёта к физическому лицу с точки зрения стороннего наблюдателя операции перевода биткойнов между кошельками, принадлежащими банку (Чарли), и кошельками, принадлежащими любым другим пользователям системы, будут неотличимы. Целесообразным шагом для Чарли было бы учреждение собственной биткойн-биржи. Для этого требуются весьма скромные траты на открытие сайта и оплату труда 13 человек персонала с соответствующим программным и аппаратным обеспечением. Практически бесплатное мероприятие привлечёт стороннюю клиентуру и увеличит оборот. Пример практической реализации Ниже приведена схема решения практической (и наиболее актуальной для России) задачи необлагаемого налогами вывода капитала за границу (в данном примере в США). Деньги Алисы зачисляются на счёт фирмы Чарли в банке-посреднике От имени Чарли производится банковский перевод в счёт приобретения товара, электронной криптовалюты биткойн, у фирмы Mt.Gox Co Ltd (Shibuya, Tokyo, Japan) с оформлением паспорта сделки и проведением всех сопутствующих операций. Данный шаг имеет смысл производить однократно с целью дальнейшего обслуживания всех клиентов посредством приобретённого пула биткойнов. Альтернативная схема приобретения биткойнов в обход законов о валютном регулировании и валютном контроле, представляющая собой двухшаговую операцию покупки карты Master Card (https://www.aurumxchange.com/cards) у фирмы Aurum Xchange (AURUM CAPITAL HOLDINGS INC.) с последующей тратой переведённых на неё денег на приобретение так называемых Mt.Gox Redeemable Code, позволяющих зачислить деньги на счёт в Mt.Gox с комиссией за перевод в 3%. Максимальный месячный лимит одной карты Master Card – 100 т.$. Максимальный месячный лимит Mt.Gox – 500 т. $. Покупка биткойнов на бирже Mt.Gox (комиссия 0.55%) Цепочка транзакций между bitcoin-кошельками, гарантирующая анонимность и неотслеживаемость основного перевода Продажа биткойнов на бирже Mt.Gox (может производиться от имени нескольких пользователей) Аккумуляция и вывод средств на счёт в американском банке банковским переводом от Mt.Gox PS Изучение вопроса практической применимости описанной криптографической схемы денежных переводов с участием банка-посредника для анонимизации самих переводов и оптимизации налогообложения при перечислении средств показало: Высокую заинтересованность в данной услуге потенциальных пользователей (Алисы и Боба). Не менее высокую заинтересованность банков и (почему-то) одной риэлторской конторы в возможности предоставления такой услуги (Чарли). Полную практическую неприменимость и неконкурентоспособность услуги в условиях современной российской конъюнктуры (именно в силу последней, а не недостатков самой схемы).\n\n\nКраткое содержание:\nВ статье рассматривается возможность использования криптовалюты Bitcoin для денежных переводов. Четыре основные схемы переводов - классическая, электронная, криптографическая и усиленная криптографическая с участием банка-посредника - были рассмотрены в контексте двух взаимодействующих лиц, Алисы и Боба.\n\nУсиленная криптографическая схема с участием банка-посредника была предложена как наиболее безопасная и анонимная. Она включает в себя несколько этапов, таких как зачисление денег на счет уполномоченной фирмы, поручение банку на перевод, покупка биткойнов и перевод их на кошелек Боба. Это позволяет гарантировать безопасность и анонимность участников транзакции, а также избежать рисков недостаточной ликвидности и курсовых скачков.\n\nОднако, в условиях российского рынка биткойнов, данная схема оказалась неприменимой и неконкурентоспособной из-за сложностей с вводом рублей в систему и проблем с налогообложением.\n\nВ целом, статья предоставляет обзор различных методов денежных переводов с использованием Bitcoin, но указывает на сложности и ограничения, связанные с их практическим применением в России.","summary":"В статье рассматривается возможность использования криптовалюты Bitcoin для денежных переводов. Четыре основные схемы переводов - классическая, электронная, криптографическая и усиленная криптографическая с участием банка-посредника - были рассмотрены в контексте двух взаимодействующих лиц, Алисы и Боба.\n\nУсиленная криптографическая схема с участием банка-посредника была предложена как наиболее безопасная и анонимная. Она включает в себя несколько этапов, таких как зачисление денег на счет уполномоченной фирмы, поручение банку на перевод, покупка биткойнов и перевод их на кошелек Боба. Это позволяет гарантировать безопасность и анонимность участников транзакции, а также избежать рисков недостаточной ликвидности и курсовых скачков.\n\nОднако, в условиях российского рынка биткойнов, данная схема оказалась неприменимой и неконкурентоспособной из-за сложностей с вводом рублей в систему и проблем с налогообложением.\n\nВ целом, статья предоставляет обзор различных методов денежных переводов с использованием Bitcoin, но указывает на сложности и ограничения, связанные с их практическим применением в России."}
{"Text":"Ряд Фибоначчи и Мемоизация с примерами на Swift языке / Хабр                                                             \n\n## Ряд Фибоначчи ##\n>\n>\n> **Чи́сла Фибона́ччи**  (вариант написания — **Фибона́чи** [2] ) — элементы числовой последовательности\n>\n>\n>\n> 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, … (последовательность A000045  в OEIS ),\n>\n>\n>\n> в которой первые два числа равны 0 и 1, а каждое последующее число равно сумме двух предыдущих чисел [3] . Названы в честь средневекового математика Леонардо Пизанского (известного как Фибоначчи ) [4] .\n>\n>\n>\n> - Википедия\n>\n>\nРяд Фибоначчи\nРяд Фибоначчи в природе\nРяд Фибоначчи часто упоминается на собеседованиях, потому что в нем демонстрируется множество мощных методов, включая рекурсию. Он является отличным примером того, что мы называем мемоизацией(запоминанием). Понимание ряда Фибоначчи и его работы очень полезно.\n\n Математически ряд Фибоначчи представляет собой последовательность чисел, которые следуют этому уравнению: F(n) = F(n-1) + F(n-2). Эта последовательность встречается в различных интересных контекстах, например, в природе, в раковинах, спиральных структурах и галактиках, а также в дизайне римских полов, и даже брокеры используют ряд Фибоначчи для прогнозирования акций.\n```\nfunc  fib ( \\_  n : Int )  -> Int  {\n    if  n ==  0  {\n        return  0\n    } else  if  n ==  1  {\n        return  1\n    } else  {\n        return  fib(n -  1 ) +  fib(n -  2 )\n    }\n}\n```\nВ коде серия Фибоначчи выглядит следующим образом: для F(0) и F(1) возвращаем их значения напрямую, а затем алгоритм сводится к этой строке - return fib(n-1) + fib(n-2). Это является ключевым моментом алгоритма и демонстрирует использование рекурсии.\n\n Однако у ряда Фибоначчи есть проблема: с увеличением чисел в последовательности алгоритм требует все больше времени для вычисления. Это связано с тем, что время, необходимое для вычислений, растет экспоненциально, и сложность алгоритма составляет O(2^n). Это делает вычисления очень затратными.\n\nНаша задача - найти способ оптимизировать этот алгоритм и сделать его быстрее. Именно здесь на помощь приходит мощная техника, известная как мемоизация, которую важно знать и понимать.\n## Мемоизация ##\nЧтобы понять мемоизацию, нужно рассмотреть проблему с рядом Фибоначчи. Для простых чисел, таких как число до F(5), ряд Фибоначчи вычисляется достаточно быстро. Однако, при вычислении чисел более высокого порядка, повторное вычисление чисел становится очень затратным по времени.\nМемоизация - это техника оптимизации, ускоряющая алгоритмы за счет кэширования или хранения результатов вычислений для их использования в будущих вычислениях. В случае ряда Фибоначчи, мы можем создать словарь (назовем его \"Memo\") для хранения ранее вычисленных чисел. Затем, когда мы вычисляем каждое число ряда Фибоначчи, мы сохраняем результат в массиве и возвращаем его. Таким образом, в следующий раз нам не нужно вычислять это число снова и снова.\n```\nvar  memo =   Int : Int \n\nfunc  fib ( \\_  n : Int )  -> Int  {\n    if  n ==  0  { return  0  }\n    else  if  n ==  1  { return  1  }\n\n    if  let  result =  memo[n] { return  result }\n\n    memo[n] =  fib(n -  1 ) +  fib(n -  2 )\n    return  memo [n] !\n}\n```\nБлагодаря мемоизации, эффективность алгоритма возрастает значительно. Теперь вместо того, чтобы за 19 секунд вычислить ряд Фибоначчи из 30 чисел, мы можем вычислить тысячи чисел. Это увеличение на 3333%. Мы превратили алгоритм из экспоненциального (O(2^n)) в линейный (O(n)).\nИменно поэтому ряд Фибоначчи и мемоизация являются отличными примерами для интервью. Они объединяют рекурсию и мемоизацию, показывая, как сделать затратный алгоритм быстрее. Знание и понимание ряда Фибоначчи и мемоизации помогут вам решать подобные задачи быстро и эффективно.\n```\nimport  UIKit\n\nfunc  fibNaive ( \\_  n : Int )  -> Int  {\n    print (n)\n    if  n ==  0  {\n        return  0\n    } else  if  n ==  1  {\n        return  1\n    } else  {\n        return  fibNaive(n -  1 ) +  fibNaive(n -  2 )\n    }\n}\n\nfibNaive( 20 ) // 20 = 13s / 22 = 54 s\nvar  memo =   Int : Int \n\nfunc  fib ( \\_  n : Int )  -> Int  {\n    if  n ==  0  { return  0 }\n    else  if  n ==  1  { return  1  }\n\n    if  let  result =  memo[n] { return  result }\n\n    memo[n] =  fib(n -  1 ) +  fib(n -  2 )\n\n    return  memo[n] !\n}\n\nfib( 22 ) // 90 max\n```\nЧто касается создания одного из них с нуля, то это очень просто. Представляю к вашему вниманию две реализации: одну наивную, которая ничего не запоминает и не хранит, и вторую, где мы используем мемоизацию и храним наши результаты по мере их вычисления.\n\nНа мгновение остановимся на мемоизированном варианте, чтобы показать, сколько времени требуется для выполнения очень маленького вычисления Фибоначчи для числа 20. Если мы запустим его для простого числа, например, 20, вы увидите, что счетчик работает, пересчитывая одни и те же числа снова и снова. В среднем это занимает около 13 секунд, что не так уж и много. Теперь, если я увеличу его на один или два, это займет еще больше времени. Это экспоненциальный рост, увеличивающийся в размерах, и для очень маленьких приращений числа требуется все больше и больше времени. Именно так работает ряд Фибоначчи, и именно поэтому он так затратен.\nТеперь давайте сравним это с мемоизированной версией и посмотрим, каково это - хранить эти значения. Во втором случае мы будем хранить наши результаты в словаре. Каждый раз, когда мы вычисляем F(n-1) + F(n-2), мы будем хранить его в ключе, представленном N, каким бы ни было число в этот момент (20, 21, 22 и т. д.). Мы просто сохраним этот результат. Затем, по мере выполнения последовательных вычислений, если мы можем извлечь результат из словаря, мы просто вернем его, не вычисляя его снова.\n## Заключение: ##\nЭто удобно не только для рядов Фибоначчи, но и для всего, что связано с дорогостоящими вычислениями, которые можно сохранить, кэшировать и использовать в будущих результатах.\nВ этом и заключается сила мемоизации.\nОна используется для самых разных вещей.\nРяд Фибоначчи - отличный пример. Когда дело доходит до реального интервью.\nЯ слышал, как людей просили воспроизвести ряд Фибоначчи.\nЭто не огромный алгоритм, он сводится к одной строчке и двум случаям: конец равен нулю и единице. Но мой совет - просто запомните эту строчку, потому что она демонстрирует такую вещь как рекурсия.\nКак только вы это поймете, очень просто перейти к мемоизированному примеру, где вы сможете продемонстрировать как взять трудозатратный алгоритм и сделать его более эффективным. Вы знаете - это называется мемоизацией.","metric":0.55615234380000000414,"metric_input":"Текст статьи:\nРяд Фибоначчи и Мемоизация с примерами на Swift языке / Хабр                                                             \n\n## Ряд Фибоначчи ##\n>\n>\n> **Чи́сла Фибона́ччи**  (вариант написания — **Фибона́чи** [2] ) — элементы числовой последовательности\n>\n>\n>\n> 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, … (последовательность A000045  в OEIS ),\n>\n>\n>\n> в которой первые два числа равны 0 и 1, а каждое последующее число равно сумме двух предыдущих чисел [3] . Названы в честь средневекового математика Леонардо Пизанского (известного как Фибоначчи ) [4] .\n>\n>\n>\n> - Википедия\n>\n>\nРяд Фибоначчи\nРяд Фибоначчи в природе\nРяд Фибоначчи часто упоминается на собеседованиях, потому что в нем демонстрируется множество мощных методов, включая рекурсию. Он является отличным примером того, что мы называем мемоизацией(запоминанием). Понимание ряда Фибоначчи и его работы очень полезно.\n\n Математически ряд Фибоначчи представляет собой последовательность чисел, которые следуют этому уравнению: F(n) = F(n-1) + F(n-2). Эта последовательность встречается в различных интересных контекстах, например, в природе, в раковинах, спиральных структурах и галактиках, а также в дизайне римских полов, и даже брокеры используют ряд Фибоначчи для прогнозирования акций.\n```\nfunc  fib ( \\_  n : Int )  -> Int  {\n    if  n ==  0  {\n        return  0\n    } else  if  n ==  1  {\n        return  1\n    } else  {\n        return  fib(n -  1 ) +  fib(n -  2 )\n    }\n}\n```\nВ коде серия Фибоначчи выглядит следующим образом: для F(0) и F(1) возвращаем их значения напрямую, а затем алгоритм сводится к этой строке - return fib(n-1) + fib(n-2). Это является ключевым моментом алгоритма и демонстрирует использование рекурсии.\n\n Однако у ряда Фибоначчи есть проблема: с увеличением чисел в последовательности алгоритм требует все больше времени для вычисления. Это связано с тем, что время, необходимое для вычислений, растет экспоненциально, и сложность алгоритма составляет O(2^n). Это делает вычисления очень затратными.\n\nНаша задача - найти способ оптимизировать этот алгоритм и сделать его быстрее. Именно здесь на помощь приходит мощная техника, известная как мемоизация, которую важно знать и понимать.\n## Мемоизация ##\nЧтобы понять мемоизацию, нужно рассмотреть проблему с рядом Фибоначчи. Для простых чисел, таких как число до F(5), ряд Фибоначчи вычисляется достаточно быстро. Однако, при вычислении чисел более высокого порядка, повторное вычисление чисел становится очень затратным по времени.\nМемоизация - это техника оптимизации, ускоряющая алгоритмы за счет кэширования или хранения результатов вычислений для их использования в будущих вычислениях. В случае ряда Фибоначчи, мы можем создать словарь (назовем его \"Memo\") для хранения ранее вычисленных чисел. Затем, когда мы вычисляем каждое число ряда Фибоначчи, мы сохраняем результат в массиве и возвращаем его. Таким образом, в следующий раз нам не нужно вычислять это число снова и снова.\n```\nvar  memo =   Int : Int \n\nfunc  fib ( \\_  n : Int )  -> Int  {\n    if  n ==  0  { return  0  }\n    else  if  n ==  1  { return  1  }\n\n    if  let  result =  memo[n] { return  result }\n\n    memo[n] =  fib(n -  1 ) +  fib(n -  2 )\n    return  memo [n] !\n}\n```\nБлагодаря мемоизации, эффективность алгоритма возрастает значительно. Теперь вместо того, чтобы за 19 секунд вычислить ряд Фибоначчи из 30 чисел, мы можем вычислить тысячи чисел. Это увеличение на 3333%. Мы превратили алгоритм из экспоненциального (O(2^n)) в линейный (O(n)).\nИменно поэтому ряд Фибоначчи и мемоизация являются отличными примерами для интервью. Они объединяют рекурсию и мемоизацию, показывая, как сделать затратный алгоритм быстрее. Знание и понимание ряда Фибоначчи и мемоизации помогут вам решать подобные задачи быстро и эффективно.\n```\nimport  UIKit\n\nfunc  fibNaive ( \\_  n : Int )  -> Int  {\n    print (n)\n    if  n ==  0  {\n        return  0\n    } else  if  n ==  1  {\n        return  1\n    } else  {\n        return  fibNaive(n -  1 ) +  fibNaive(n -  2 )\n    }\n}\n\nfibNaive( 20 ) // 20 = 13s / 22 = 54 s\nvar  memo =   Int : Int \n\nfunc  fib ( \\_  n : Int )  -> Int  {\n    if  n ==  0  { return  0 }\n    else  if  n ==  1  { return  1  }\n\n    if  let  result =  memo[n] { return  result }\n\n    memo[n] =  fib(n -  1 ) +  fib(n -  2 )\n\n    return  memo[n] !\n}\n\nfib( 22 ) // 90 max\n```\nЧто касается создания одного из них с нуля, то это очень просто. Представляю к вашему вниманию две реализации: одну наивную, которая ничего не запоминает и не хранит, и вторую, где мы используем мемоизацию и храним наши результаты по мере их вычисления.\n\nНа мгновение остановимся на мемоизированном варианте, чтобы показать, сколько времени требуется для выполнения очень маленького вычисления Фибоначчи для числа 20. Если мы запустим его для простого числа, например, 20, вы увидите, что счетчик работает, пересчитывая одни и те же числа снова и снова. В среднем это занимает около 13 секунд, что не так уж и много. Теперь, если я увеличу его на один или два, это займет еще больше времени. Это экспоненциальный рост, увеличивающийся в размерах, и для очень маленьких приращений числа требуется все больше и больше времени. Именно так работает ряд Фибоначчи, и именно поэтому он так затратен.\nТеперь давайте сравним это с мемоизированной версией и посмотрим, каково это - хранить эти значения. Во втором случае мы будем хранить наши результаты в словаре. Каждый раз, когда мы вычисляем F(n-1) + F(n-2), мы будем хранить его в ключе, представленном N, каким бы ни было число в этот момент (20, 21, 22 и т. д.). Мы просто сохраним этот результат. Затем, по мере выполнения последовательных вычислений, если мы можем извлечь результат из словаря, мы просто вернем его, не вычисляя его снова.\n## Заключение: ##\nЭто удобно не только для рядов Фибоначчи, но и для всего, что связано с дорогостоящими вычислениями, которые можно сохранить, кэшировать и использовать в будущих результатах.\nВ этом и заключается сила мемоизации.\nОна используется для самых разных вещей.\nРяд Фибоначчи - отличный пример. Когда дело доходит до реального интервью.\nЯ слышал, как людей просили воспроизвести ряд Фибоначчи.\nЭто не огромный алгоритм, он сводится к одной строчке и двум случаям: конец равен нулю и единице. Но мой совет - просто запомните эту строчку, потому что она демонстрирует такую вещь как рекурсия.\nКак только вы это поймете, очень просто перейти к мемоизированному примеру, где вы сможете продемонстрировать как взять трудозатратный алгоритм и сделать его более эффективным. Вы знаете - это называется мемоизацией.\n\nКраткое содержание:\nРяд Фибоначчи - это последовательность чисел, встречающаяся в природе и использующаяся в различных областях. Он вычисляется с помощью рекурсивной функции, однако с увеличением порядка чисел вычисления становятся затратными. Мемоизация - это техника оптимизации, которая позволяет кэшировать уже вычисленные значения и использовать их в дальнейшем, что значительно ускоряет алгоритм. Знание ряда Фибоначчи и мемоизации полезно при решении задач на собеседованиях.","summary":"Ряд Фибоначчи - это последовательность чисел, встречающаяся в природе и использующаяся в различных областях. Он вычисляется с помощью рекурсивной функции, однако с увеличением порядка чисел вычисления становятся затратными. Мемоизация - это техника оптимизации, которая позволяет кэшировать уже вычисленные значения и использовать их в дальнейшем, что значительно ускоряет алгоритм. Знание ряда Фибоначчи и мемоизации полезно при решении задач на собеседованиях."}
{"Text":"Code Coverage — хочу верить / Habr                              \n\n10-01-2018\nРазработчик обязан знать свои инструменты! Знание инструментов увеличивает продуктивность, эффективность, производительность, потенцию разработчика! Не могу программировать без R#! Подобного рода фразы можно услышать от абсолютно разных людей: фанатиков разработки, продавцов различных утилит, пользователей удобных тулз. Слышит их и мой менеджер, когда мне хочется поэкспериментировать с чем-то новеньким. Правда, инструкция к инструменту обычно не содержит раздел «Противопоказания», не указываются ситуации когда НЕ стоит применять утилиту. Между тем, подобный раздел мог бы сэкономить тонны времени на неудачные эксперименты. Сегодня я пошвыряю камни в огород Code Coverage (CC). Достаточно полезная метрика, под которой лежат несколько скудно документированных граблей. «Есть ложь, есть наглая ложь есть статистика». Создатели SonarQube это великолепно понимают, недаром у SonarQube с десяток CC метрик. Я буду перебивать статистику используя CC от DevExpress, там метрика лишь одна. Проблема 1. Типичный тест не того. Протестируем метод с кучей проверок аргумента: public static Hash Example1_IfThen(int arg) { if(arg == 0) { throw new ArgumentException(\"0 argument\"); } if (arg == 1) { throw new ArgumentException(\"1 argument\"); } if (arg == 2) { throw new ArgumentException(\"2 argument\"); } return new Hash(new Hash(arg + 42) + 13); } ... [TestMethod, ExpectedException(typeof(ArgumentException))] public void Example1_IfThen_0() { Program.Example1_IfThen(0); } [TestMethod, ExpectedException(typeof(ArgumentException))] public void Example1_IfThen_2() { Program.Example1_IfThen(2); } [TestMethod, ExpectedException(typeof(ArgumentException))] public void Example1_IfThen_1() { Program.Example1_IfThen(1); } Метод покрыт тестами на 83%, чего обычно достаточно для авто-билда. Технически спорить не о чем, большая часть кода покрыта тестами, но основной сценарий тестами не затронут. Тестами покрыта наиболее простая часть кода, не наиболее важная. Проблема 2. Замеряем актуальный код, вместо необходимого. public static int Example2_IncompleteLogic(IEnumerable<int> arg) { return arg .Where(elem => elem != 2) .Where(elem => elem != 3) .Count(); } ... [TestMethod] public void OneTestToCoverAll() { // Arrange - collection with one element '4' var arg = new List<int> { 4 }; // Act var result = Program.Example2_IncompleteLogic(arg); // Assert Assert.AreEqual(1, result); } Тестируемый метод не содержит проверки на null аргумент, однако покрытие — 100%. Иногда люди забывают: Code Coverage — это метрика покрытия кода, не метрика закрытия требований; если в методе не достает логики (метод недостаточно сложен для решения своей задачи) — CC это не покажет. 100% покрытия не гарантируют работоспособности программы. Доводя до абсурда: пустой метод элементарно покрывается на 100%. Непустой метод покрывается на 100% тестами без Assert-ов. Проблема 3. Оптимизм. Немного иное проявление предыдущей проблемы. Как видно, один тест покрывает 100% кода. Попробуем переписать наш метод, избавившись от LINQ (для улучшения производительности). var result = 0; foreach(var elem in arg) { if(elem == 2 || elem == 3) { continue; } else { result++; } } return result; Получаем лишь 73% покрытия. Функциональность не изменилась, метрика упала. Мало того, что 100% покрытия не гарантируют работоспособности программы, эти 100% могут быть фейковыми. Вывод: LINQ — г**но результаты CC могут быть завышены, старайтесь проверять покрытие в редакторе. Побочное наблюдение: в данном случае мы можем просто иметь косяк в технической реализации, в теории анонимный метод elem => elem != 2 можно заменить на elem => if (elem != 2) return true else return false;, что пофиксит покрытие оригинального метода до 73%. Правда, такой подход потребует усложнения удобного сейчас UI. Следствие: Используемый инструмент может не обладать всей желаемой функциональностью. Тривиальная вещь, не менее от того верная. Проблема 4. Передача ответственности. public static void MakeEverythingWell() { OuterLib.MakeEverythingWell(); } Метод покрывается на 100% одним тестом. При этом покрытие OuterLib библиотеки лежит на совести того, кто её добавил. Или обновил. Года три назад, до введения CC. До увольнения. Приходится снова констатировать факт: мало того, что 100% покрытия не гарантируют работоспособности программы, эти 100% могут быть фейковыми. Помимо чисто кодовых моментов есть несколько претензий именно к обработке результатов CC Претензия 0, всем известная. 100% покрытия. Нет 100% покрытия — нет одобрения билда. Проблема в том, что первые проценты покрытия получить относительно просто, а вот последние… Особенно, когда часть кода генерируется. Или недостижима (поскольку создана для Васи, который будет её юзать через два дня). Или просто теоретически достижима, а пример подбирать\\высчитывать пару недель (такое бывает при работе с математикой). Короче, большинство команд (из тех кто вообще интегрирует CC в CI) останавливаются на 60\\70\\80 процентах необходимого покрытия. Претензия 1, спорная. Покрытие мертвого кода. На моей памяти схожая проблема особо ярко проявилась в ходе проверки Mirand-ы коллегами из PVS. Комментарии довольно эмоциональны, но часть споров касалась мертвого кода: часть найденных диагностик указывала на (заброшенные) плагины, но не на ядро. Возникает вопрос: нужен ли CodeCoverage для мертвого кода? С одной стороны, мертвый код это проблема, и привлечение внимания к нему приветствуется. С другой стороны, мертвый код не влияет на продакшн, так стоит ли позволять ему влиять на CC метрику? Претензия 2. Важность кода. Расширение проблемы 1. В моем проекте есть два примечательных контроллера: «оплата» и «переговорка». «Оплата» критична для клиента, и я вполне согласен с требованием «80% покрытия», «переговоркой» же пользуются 1.5 анонимуса. В год. И она не менялась уже два года. Вопрос: для чего писать тесты к полумертвой функциональности? Лишь для получения 80% бейджа одобрения автосборки? Претензия 3, невозможная. Метрика как ачивка. Это когда никто не проверяет что именно покрыто. Помните байки про оплату за линии кода? Мне доводилось слышать про людей, которые творили ненужный кода для лучшего покрытия. Претензия 4. Метрика «за бесплатно». Когда руководство скидывает требование «покрывайте код на 80%», и разработчики безропотно соглашаются. Проект при этом — одноразовый. Или прототип. Или дедлайн на носу. Или имеется здоровенный макаронный легаси монстр без единого теста. Покрытие кода тестами требует времени! Если покрытие еще и замерять — время на тесты может и возрасти (хотя может и упасть). Так что если команда не успела сдать проект в срок, но зато достигла 80% покрытия — вина может поделиться между руководством и разработчиками. Вопрос линии раздела вины поднимать не стоит, ибо холивар. Под конец. Еще раз замечу: СС — метрика полезная, хоть и с сюрпризами. Она реально помогает с контролем кода, если нет слепого стремления к цифрам в отчетах.\n","metric":0.58544921880000000414,"metric_input":"Текст статьи:\nCode Coverage — хочу верить / Habr                              \n\n10-01-2018\nРазработчик обязан знать свои инструменты! Знание инструментов увеличивает продуктивность, эффективность, производительность, потенцию разработчика! Не могу программировать без R#! Подобного рода фразы можно услышать от абсолютно разных людей: фанатиков разработки, продавцов различных утилит, пользователей удобных тулз. Слышит их и мой менеджер, когда мне хочется поэкспериментировать с чем-то новеньким. Правда, инструкция к инструменту обычно не содержит раздел «Противопоказания», не указываются ситуации когда НЕ стоит применять утилиту. Между тем, подобный раздел мог бы сэкономить тонны времени на неудачные эксперименты. Сегодня я пошвыряю камни в огород Code Coverage (CC). Достаточно полезная метрика, под которой лежат несколько скудно документированных граблей. «Есть ложь, есть наглая ложь есть статистика». Создатели SonarQube это великолепно понимают, недаром у SonarQube с десяток CC метрик. Я буду перебивать статистику используя CC от DevExpress, там метрика лишь одна. Проблема 1. Типичный тест не того. Протестируем метод с кучей проверок аргумента: public static Hash Example1_IfThen(int arg) { if(arg == 0) { throw new ArgumentException(\"0 argument\"); } if (arg == 1) { throw new ArgumentException(\"1 argument\"); } if (arg == 2) { throw new ArgumentException(\"2 argument\"); } return new Hash(new Hash(arg + 42) + 13); } ... [TestMethod, ExpectedException(typeof(ArgumentException))] public void Example1_IfThen_0() { Program.Example1_IfThen(0); } [TestMethod, ExpectedException(typeof(ArgumentException))] public void Example1_IfThen_2() { Program.Example1_IfThen(2); } [TestMethod, ExpectedException(typeof(ArgumentException))] public void Example1_IfThen_1() { Program.Example1_IfThen(1); } Метод покрыт тестами на 83%, чего обычно достаточно для авто-билда. Технически спорить не о чем, большая часть кода покрыта тестами, но основной сценарий тестами не затронут. Тестами покрыта наиболее простая часть кода, не наиболее важная. Проблема 2. Замеряем актуальный код, вместо необходимого. public static int Example2_IncompleteLogic(IEnumerable<int> arg) { return arg .Where(elem => elem != 2) .Where(elem => elem != 3) .Count(); } ... [TestMethod] public void OneTestToCoverAll() { // Arrange - collection with one element '4' var arg = new List<int> { 4 }; // Act var result = Program.Example2_IncompleteLogic(arg); // Assert Assert.AreEqual(1, result); } Тестируемый метод не содержит проверки на null аргумент, однако покрытие — 100%. Иногда люди забывают: Code Coverage — это метрика покрытия кода, не метрика закрытия требований; если в методе не достает логики (метод недостаточно сложен для решения своей задачи) — CC это не покажет. 100% покрытия не гарантируют работоспособности программы. Доводя до абсурда: пустой метод элементарно покрывается на 100%. Непустой метод покрывается на 100% тестами без Assert-ов. Проблема 3. Оптимизм. Немного иное проявление предыдущей проблемы. Как видно, один тест покрывает 100% кода. Попробуем переписать наш метод, избавившись от LINQ (для улучшения производительности). var result = 0; foreach(var elem in arg) { if(elem == 2 || elem == 3) { continue; } else { result++; } } return result; Получаем лишь 73% покрытия. Функциональность не изменилась, метрика упала. Мало того, что 100% покрытия не гарантируют работоспособности программы, эти 100% могут быть фейковыми. Вывод: LINQ — г**но результаты CC могут быть завышены, старайтесь проверять покрытие в редакторе. Побочное наблюдение: в данном случае мы можем просто иметь косяк в технической реализации, в теории анонимный метод elem => elem != 2 можно заменить на elem => if (elem != 2) return true else return false;, что пофиксит покрытие оригинального метода до 73%. Правда, такой подход потребует усложнения удобного сейчас UI. Следствие: Используемый инструмент может не обладать всей желаемой функциональностью. Тривиальная вещь, не менее от того верная. Проблема 4. Передача ответственности. public static void MakeEverythingWell() { OuterLib.MakeEverythingWell(); } Метод покрывается на 100% одним тестом. При этом покрытие OuterLib библиотеки лежит на совести того, кто её добавил. Или обновил. Года три назад, до введения CC. До увольнения. Приходится снова констатировать факт: мало того, что 100% покрытия не гарантируют работоспособности программы, эти 100% могут быть фейковыми. Помимо чисто кодовых моментов есть несколько претензий именно к обработке результатов CC Претензия 0, всем известная. 100% покрытия. Нет 100% покрытия — нет одобрения билда. Проблема в том, что первые проценты покрытия получить относительно просто, а вот последние… Особенно, когда часть кода генерируется. Или недостижима (поскольку создана для Васи, который будет её юзать через два дня). Или просто теоретически достижима, а пример подбирать\\высчитывать пару недель (такое бывает при работе с математикой). Короче, большинство команд (из тех кто вообще интегрирует CC в CI) останавливаются на 60\\70\\80 процентах необходимого покрытия. Претензия 1, спорная. Покрытие мертвого кода. На моей памяти схожая проблема особо ярко проявилась в ходе проверки Mirand-ы коллегами из PVS. Комментарии довольно эмоциональны, но часть споров касалась мертвого кода: часть найденных диагностик указывала на (заброшенные) плагины, но не на ядро. Возникает вопрос: нужен ли CodeCoverage для мертвого кода? С одной стороны, мертвый код это проблема, и привлечение внимания к нему приветствуется. С другой стороны, мертвый код не влияет на продакшн, так стоит ли позволять ему влиять на CC метрику? Претензия 2. Важность кода. Расширение проблемы 1. В моем проекте есть два примечательных контроллера: «оплата» и «переговорка». «Оплата» критична для клиента, и я вполне согласен с требованием «80% покрытия», «переговоркой» же пользуются 1.5 анонимуса. В год. И она не менялась уже два года. Вопрос: для чего писать тесты к полумертвой функциональности? Лишь для получения 80% бейджа одобрения автосборки? Претензия 3, невозможная. Метрика как ачивка. Это когда никто не проверяет что именно покрыто. Помните байки про оплату за линии кода? Мне доводилось слышать про людей, которые творили ненужный кода для лучшего покрытия. Претензия 4. Метрика «за бесплатно». Когда руководство скидывает требование «покрывайте код на 80%», и разработчики безропотно соглашаются. Проект при этом — одноразовый. Или прототип. Или дедлайн на носу. Или имеется здоровенный макаронный легаси монстр без единого теста. Покрытие кода тестами требует времени! Если покрытие еще и замерять — время на тесты может и возрасти (хотя может и упасть). Так что если команда не успела сдать проект в срок, но зато достигла 80% покрытия — вина может поделиться между руководством и разработчиками. Вопрос линии раздела вины поднимать не стоит, ибо холивар. Под конец. Еще раз замечу: СС — метрика полезная, хоть и с сюрпризами. Она реально помогает с контролем кода, если нет слепого стремления к цифрам в отчетах.\n\n\nКраткое содержание:\nСтатья обсуждает проблемы и ограничения в использовании Code Coverage, метрики покрытия кода тестами. Она подчеркивает, что Code Coverage может быть полезным инструментом для контроля кода, но имеет несколько проблем:\n\n1. **Типичные тесты для простых частей кода:** Тесты часто покрывают наиболее простые части кода, а не наиболее важные. Это может привести к неправильному представлению о качестве кода.\n\n2. **Замерзание актуального кода:** Code Coverage измеряет покрытие кода тестами, но не обязательно означает, что код полностью протестирован. Важно проверять, что код действительно работает правильно.\n\n3. **Оптимизм и изменение кода:** Изменение кода может повлиять на покрытие Code Coverage. Это может привести к завышенным или заниженным результатам.\n\n4. **Передача ответственности:** Code Coverage может покрывать не только основной код, но и сторонние библиотеки или модули. Важно следить за полным покрытием всего кода.\n\n5. **Претензии к Code Coverage:** Есть несколько спорных моментов в использовании Code Coverage, таких как покрытие мертвого кода, важность кода и измерение покрытия без проверки функциональности.\n\nВ целом, статья подчеркивает, что Code Coverage является полезным инструментом, но требует осторожности и критического мышления при его использовании.","summary":"Статья обсуждает проблемы и ограничения в использовании Code Coverage, метрики покрытия кода тестами. Она подчеркивает, что Code Coverage может быть полезным инструментом для контроля кода, но имеет несколько проблем:\n\n1. **Типичные тесты для простых частей кода:** Тесты часто покрывают наиболее простые части кода, а не наиболее важные. Это может привести к неправильному представлению о качестве кода.\n\n2. **Замерзание актуального кода:** Code Coverage измеряет покрытие кода тестами, но не обязательно означает, что код полностью протестирован. Важно проверять, что код действительно работает правильно.\n\n3. **Оптимизм и изменение кода:** Изменение кода может повлиять на покрытие Code Coverage. Это может привести к завышенным или заниженным результатам.\n\n4. **Передача ответственности:** Code Coverage может покрывать не только основной код, но и сторонние библиотеки или модули. Важно следить за полным покрытием всего кода.\n\n5. **Претензии к Code Coverage:** Есть несколько спорных моментов в использовании Code Coverage, таких как покрытие мертвого кода, важность кода и измерение покрытия без проверки функциональности.\n\nВ целом, статья подчеркивает, что Code Coverage является полезным инструментом, но требует осторожности и критического мышления при его использовании."}
{"Text":"Интересные патенты Apple / Хабр                \n\n28-04-2009\nНе так давно я наткнулся на топик «Не все патенты одинаково полезны» про то, как Apple запатентовала управление звуком в браузере и пообщеал написать топик про более интересные/полезные патенты. Поэтому, имея немного свободного времени, решил выполнить обещаное. Заранее хочу сказать, что наличие патента совершенно не говорит о том, что у компании есть промышленный образец или, хотя бы, наработки в этой области. Всегда видимые индикаторы статуса на экране мобильного Во всех портативных устройствах, которые я видел, для сохранения энергии в режиме ожидания используется отключение дисплея. Правда, такой подход может затруднять пользователю определение статуса устройства, а в темное время суток это вообще не представляется возможным без включения подсветки дисплея. Посему пропущеное сообщение, звонок или низкий заряд батареи может остаться без внимания. Для решения этой проблемы, Apple предлагает использование двойной подсветки в системе, где вторичная подсветка малой мощности будет размещена позади основной системы подсветки. По задумке, вторичная подсветка может просвечиваться в прозрачных или полу-прозрачных участках дисплея. Таким образом, на экране можно будет увидеть системную информацию даже тогда, когда основная подсветка выключается. Основная система подсветки может блокировать свет от вторичной, за исключением одного или более участков дисплея. Таким образом, размер и форма индикаторов состояния могут быть определены путем формирования прозрачных или полупрозрачных регионов в основной системе подсветки. Аналогичным образом, можно было бы корректировать цвет статусных индикаторов. Кроме этого, источник света во вторичной системе подсветки мог бы или мигать, или создавать эффект пульсирования. Несомненный плюс такого решения заключается в том, что пользователи смогли бы в любой момент времени увидеть уведомление даже не прикасаясь к устройству. Плюс, в патенте описывалась возможность добавления специального API для разработчиков. Автоматическая настройка громкости звука/рингтона и самоочищающийся 3.5-мм джек Данная разработка подошла бы, в первую очередь, пользователям мобильников. К примеру, в городском шуме он может звонить недостаточно сильно, а в тихом месте очень громкий звонок будет в лучшем случае неуместным. В качестве другого примера можно привести такой: вы работаете с ноутбуком в доме, а после этого переходите в более шумное помещение — в этом случае звук из динамиков будет попросту не слышно. При проигрывании музыки специальный сенсор мог бы замерять звук между песнями и во время их проигрывания для определения громкости окружающей среды. При обнаружении шума в помещении система могла бы усилит громкость динамика, а если в комнате тихо — снизить. В дополнение к патенту, предлагается концепцию джека, который способен прочистить аудиоразъем от накопившейся пыли или мусора. Для этого предлагается в обычный 3,5-мм джек добавить полый канал, через который подавать обычный или сжатый воздух, продувая таким образом разъем. А что, очень полезно — всякое ведь в жизни бывает :) Скрытая веб-камера позади экрана На данный момент веб-камера iSight находится практически на верхней грани MacBook Pro, iMac или Cinema Display (да и вообще в других устройствах). Поэтому при видео-звонке другому человеку при помощи Skype (Google Talk или любого другого приложения) могут возникать некоторые трудности: пользователю приходится смотреть на собеседника (или на монитор), но не в объектив камеры. В этом случае, разговор может показаться немного неестественным. Для решения данной проблемы в одном из мнгочисленных патентов Apple предлагает встроить веб-камеру за дисплей, а дисплей реализовать в виде «сэндвича» из нескольких различных слоёв, которые позволят камере захватить изображение. Насколько я понял, достигается это за счет их отключения. Но поскольку «мерцание» будет происходить достаточно быстро — пользователь не заметит этого. Поэтому любой человек сможет общаться глядя собесенднику прямо в глаза. Ну или почти прямо в глаза :) Скрытая биометрическая защита Согласитесь, звучит несколько футуристично. Обычно такие штуки либо описываются писателями-фантастами в своих романах про далекое или не очень будущее, либо появляются опять-таки в фантастических фильмах. На самом деле, ничего удивительного тут нет: датчики, сканирующие отпечатки пальцев, уже давно существуют — устанавливались они и на мобильные телефоны (даже одно время была реклама какого-то телефона для «реальных пацанов»), и на ноутбуки класса выше среднего. Правда Apple предлагает их прятать от любопытных глаз. В компании не могут не быть обеспокоены тем, что Passcode на iPhone/iPod touch может обеспечить лишь минимальный уровень защиты данных — всего 4 цифры, зато для настоящих параноиков можно включить функцию удаления всех данных после десяти неправильных попыток. Поэтому компания пытается прдумать более серьезные методы предотвращения несанкционированного доступа к устройству и личной информации. И хотят они это сделать не отвлекая пользователя от работы. Как это организовать? Да очень просто. Всего навсего нужно встроить датчики в то место устройства, по которому пользователь точно будет «елозить» пальцем. К примеру, это может быть слайдер разблокировки телефона («Slide to unlock»), безкнопочный тачпад или частоиспользуемые клавиши в ноутбуке — наверное, это клавиша Power :) В патенте предполагается, что в ноутбуки и настольные системы вроде iMac могут быть встроены сенсоры для распознавания лица пользователя, сидящего за компьютером. Кроме того, в компании задумались над распознаванием голоса и даже генетических последовательностей путем сбора образцов ДНК. А в качестве альтернативы биометрическим характеристикам может применяться менее очевидные замки-блокировки: к примеру, наклоняя девайс в определенные стороны или заставляя пользователей iPhone нажимать на иконки в определенной последовательности или использовать специальные жесты для разблокирования устройства. Осталось добавить паззл на 1000 кусочков и диффур :) Magic wand — волшебная палочка для Apple TV Вот это — мое любимое :) С помощью этого контроллера пользователь смог бы управлять курсором на экране телевизора так же, как это делают пользователи компьютеров — в двухмерной плоскости. В отличие от нынешних пятикнопочных пультов Remote, «волшебная палочка» могла бы контролировать множество новых операций и приложений, использующихся в медиа-системе. К примеру, это операции масштабирования, управление приложениями для просмотра фото и видео, прослушивания аудио. Кроме этого, можно было бы попытаться при помощи такого контроллера избавиться от некоторых физических кнопок. Для этого в патенте используются специальные жесты: вращение и перемещение палочки, поворот или любые другие действия. Эти жесты можно применять для технологии CoverFlow, листая медиа-библиотеку, можно уменьшать/увеличивать эскизы фотографий, можно прокручивать фильм или песню и т.д. Еще одним из применений стала бы возможность работать с экранной клавиатурой. Вместо обычной навигации «вверх/вниз/влево/вправо» можно более-менее удобно вводить текст или использовать поиск. В итоге, патент содержит около 64 страниц описания идеи. Ничего не могу сказать на счет удобства, но я уверен, что дети будут от неё просто в восторге! Вот и все, в принципе, что я хотел рассказать. Материал собирался достаточно долго и из разных источников. Посмотрим, как отнесутся читатели к топику. Если что — перенесу в тематический блог.\n","metric":0.53125,"metric_input":"Текст статьи:\nИнтересные патенты Apple / Хабр                \n\n28-04-2009\nНе так давно я наткнулся на топик «Не все патенты одинаково полезны» про то, как Apple запатентовала управление звуком в браузере и пообщеал написать топик про более интересные/полезные патенты. Поэтому, имея немного свободного времени, решил выполнить обещаное. Заранее хочу сказать, что наличие патента совершенно не говорит о том, что у компании есть промышленный образец или, хотя бы, наработки в этой области. Всегда видимые индикаторы статуса на экране мобильного Во всех портативных устройствах, которые я видел, для сохранения энергии в режиме ожидания используется отключение дисплея. Правда, такой подход может затруднять пользователю определение статуса устройства, а в темное время суток это вообще не представляется возможным без включения подсветки дисплея. Посему пропущеное сообщение, звонок или низкий заряд батареи может остаться без внимания. Для решения этой проблемы, Apple предлагает использование двойной подсветки в системе, где вторичная подсветка малой мощности будет размещена позади основной системы подсветки. По задумке, вторичная подсветка может просвечиваться в прозрачных или полу-прозрачных участках дисплея. Таким образом, на экране можно будет увидеть системную информацию даже тогда, когда основная подсветка выключается. Основная система подсветки может блокировать свет от вторичной, за исключением одного или более участков дисплея. Таким образом, размер и форма индикаторов состояния могут быть определены путем формирования прозрачных или полупрозрачных регионов в основной системе подсветки. Аналогичным образом, можно было бы корректировать цвет статусных индикаторов. Кроме этого, источник света во вторичной системе подсветки мог бы или мигать, или создавать эффект пульсирования. Несомненный плюс такого решения заключается в том, что пользователи смогли бы в любой момент времени увидеть уведомление даже не прикасаясь к устройству. Плюс, в патенте описывалась возможность добавления специального API для разработчиков. Автоматическая настройка громкости звука/рингтона и самоочищающийся 3.5-мм джек Данная разработка подошла бы, в первую очередь, пользователям мобильников. К примеру, в городском шуме он может звонить недостаточно сильно, а в тихом месте очень громкий звонок будет в лучшем случае неуместным. В качестве другого примера можно привести такой: вы работаете с ноутбуком в доме, а после этого переходите в более шумное помещение — в этом случае звук из динамиков будет попросту не слышно. При проигрывании музыки специальный сенсор мог бы замерять звук между песнями и во время их проигрывания для определения громкости окружающей среды. При обнаружении шума в помещении система могла бы усилит громкость динамика, а если в комнате тихо — снизить. В дополнение к патенту, предлагается концепцию джека, который способен прочистить аудиоразъем от накопившейся пыли или мусора. Для этого предлагается в обычный 3,5-мм джек добавить полый канал, через который подавать обычный или сжатый воздух, продувая таким образом разъем. А что, очень полезно — всякое ведь в жизни бывает :) Скрытая веб-камера позади экрана На данный момент веб-камера iSight находится практически на верхней грани MacBook Pro, iMac или Cinema Display (да и вообще в других устройствах). Поэтому при видео-звонке другому человеку при помощи Skype (Google Talk или любого другого приложения) могут возникать некоторые трудности: пользователю приходится смотреть на собеседника (или на монитор), но не в объектив камеры. В этом случае, разговор может показаться немного неестественным. Для решения данной проблемы в одном из мнгочисленных патентов Apple предлагает встроить веб-камеру за дисплей, а дисплей реализовать в виде «сэндвича» из нескольких различных слоёв, которые позволят камере захватить изображение. Насколько я понял, достигается это за счет их отключения. Но поскольку «мерцание» будет происходить достаточно быстро — пользователь не заметит этого. Поэтому любой человек сможет общаться глядя собесенднику прямо в глаза. Ну или почти прямо в глаза :) Скрытая биометрическая защита Согласитесь, звучит несколько футуристично. Обычно такие штуки либо описываются писателями-фантастами в своих романах про далекое или не очень будущее, либо появляются опять-таки в фантастических фильмах. На самом деле, ничего удивительного тут нет: датчики, сканирующие отпечатки пальцев, уже давно существуют — устанавливались они и на мобильные телефоны (даже одно время была реклама какого-то телефона для «реальных пацанов»), и на ноутбуки класса выше среднего. Правда Apple предлагает их прятать от любопытных глаз. В компании не могут не быть обеспокоены тем, что Passcode на iPhone/iPod touch может обеспечить лишь минимальный уровень защиты данных — всего 4 цифры, зато для настоящих параноиков можно включить функцию удаления всех данных после десяти неправильных попыток. Поэтому компания пытается прдумать более серьезные методы предотвращения несанкционированного доступа к устройству и личной информации. И хотят они это сделать не отвлекая пользователя от работы. Как это организовать? Да очень просто. Всего навсего нужно встроить датчики в то место устройства, по которому пользователь точно будет «елозить» пальцем. К примеру, это может быть слайдер разблокировки телефона («Slide to unlock»), безкнопочный тачпад или частоиспользуемые клавиши в ноутбуке — наверное, это клавиша Power :) В патенте предполагается, что в ноутбуки и настольные системы вроде iMac могут быть встроены сенсоры для распознавания лица пользователя, сидящего за компьютером. Кроме того, в компании задумались над распознаванием голоса и даже генетических последовательностей путем сбора образцов ДНК. А в качестве альтернативы биометрическим характеристикам может применяться менее очевидные замки-блокировки: к примеру, наклоняя девайс в определенные стороны или заставляя пользователей iPhone нажимать на иконки в определенной последовательности или использовать специальные жесты для разблокирования устройства. Осталось добавить паззл на 1000 кусочков и диффур :) Magic wand — волшебная палочка для Apple TV Вот это — мое любимое :) С помощью этого контроллера пользователь смог бы управлять курсором на экране телевизора так же, как это делают пользователи компьютеров — в двухмерной плоскости. В отличие от нынешних пятикнопочных пультов Remote, «волшебная палочка» могла бы контролировать множество новых операций и приложений, использующихся в медиа-системе. К примеру, это операции масштабирования, управление приложениями для просмотра фото и видео, прослушивания аудио. Кроме этого, можно было бы попытаться при помощи такого контроллера избавиться от некоторых физических кнопок. Для этого в патенте используются специальные жесты: вращение и перемещение палочки, поворот или любые другие действия. Эти жесты можно применять для технологии CoverFlow, листая медиа-библиотеку, можно уменьшать/увеличивать эскизы фотографий, можно прокручивать фильм или песню и т.д. Еще одним из применений стала бы возможность работать с экранной клавиатурой. Вместо обычной навигации «вверх/вниз/влево/вправо» можно более-менее удобно вводить текст или использовать поиск. В итоге, патент содержит около 64 страниц описания идеи. Ничего не могу сказать на счет удобства, но я уверен, что дети будут от неё просто в восторге! Вот и все, в принципе, что я хотел рассказать. Материал собирался достаточно долго и из разных источников. Посмотрим, как отнесутся читатели к топику. Если что — перенесу в тематический блог.\n\n\nКраткое содержание:\nВ статье рассматриваются интересные патенты, предложенные компанией Apple, такие как:\n\n- Использование двойной подсветки для индикации статуса на экране мобильного устройства.\n- Автоматическая настройка громкости звука и рингтона в зависимости от окружающей среды.\n- Разработка самоочищающегося 3.5-мм джека для удаления пыли из аудиоразъема.\n- Встройка веб-камеры за дисплей для видеозвонков с естественным общением.\n- Скрытая биометрическая защита с использованием датчиков отпечатков пальцев или распознавания лица.\n- Использование \"волшебной палочки\" (Magic wand) в Apple TV для управления приложениями и медиа-контентом на экране телевизора.\n\nВсе эти идеи направлены на улучшение пользовательского опыта и безопасности в мобильных устройствах и медиа-системах от Apple.","summary":"В статье рассматриваются интересные патенты, предложенные компанией Apple, такие как:\n\n- Использование двойной подсветки для индикации статуса на экране мобильного устройства.\n- Автоматическая настройка громкости звука и рингтона в зависимости от окружающей среды.\n- Разработка самоочищающегося 3.5-мм джека для удаления пыли из аудиоразъема.\n- Встройка веб-камеры за дисплей для видеозвонков с естественным общением.\n- Скрытая биометрическая защита с использованием датчиков отпечатков пальцев или распознавания лица.\n- Использование \"волшебной палочки\" (Magic wand) в Apple TV для управления приложениями и медиа-контентом на экране телевизора.\n\nВсе эти идеи направлены на улучшение пользовательского опыта и безопасности в мобильных устройствах и медиа-системах от Apple."}
{"Text":"Git: много хуков полезных и разных / Habr                       \n\n30-05-2017\nСтатья является вольным переводом вот этого материала. Любителям длинных и заумных первоисточников можно сразу читать оригинал. Когда перед нами ставится задача при изменении кодбейса, например, в Github-репозитории выполнить пересборку/перезапуск какого-нибудь приложения на каком-то нашем окружении, то первое, что приходит на ум в качестве возможного триггера такой пересборки, это предоставляемый тем же гитхабом механизм веб-хуков: при наступлении какого-либо события с нашим удаленным репозиторием (т.к. появление нового коммита в какой-нибудь его отслеживаемой ветке) гитхаб задействует соответствующий веб-хук и «дернет» указанный в его настройках сервис, который и запустит процесс пересборки/перезапуска нашего приложения. Это стандартный широкоиспользуемый и простой механизм для таких случаев, все так делают, и все такое… Но что, если наше приложение живет на хосте, доступ к которому по каким-то причинам гитхабу не положен? Например, хост находится в закрытой сети или за NAT'ом и недоступен из интернета? В этом случае можно воспользоваться механизмом локальных хуков самого Git, о котором (механизме), как выясняется, мало кто знает даже из тех, кто использует Git уже довольно продолжительное время. Когда мы создаем локальный репозиторий (инициализируем пустой, клонируем удаленный, ...), то в папке .git, что в его корне, или в самом корне, если это bare-репозиторий, присутствует папка hooks. После инициализации репозитория в этой папке сохраняются шаблоны хуков для различных событий, таких как, например: post-merge, post-receive, post-update и др. Полное описание поддерживаемых событий для хуков можно найти, например, тут. Мы воспользуемся этим механизмом и реализуем простенькую push-to-deploy схему для нашего горемычного приложения. Нам понадобится два локальных репозитория. Создадим их, например, по указанным путям: 1. /opt/repo-dev/example-repo/ 2. /opt/repo-remote.git/ Первый репозиторий — это клон нашего удаленного репозитория example-repo на гитхабе. Второй — это bare репозиторий, копия первого, который будет служить нам исключительно для обработки события post-update при появлении обновлений в удаленном репозитории. Итак, как же мы это реализуем? Схема очень проста (предположим, мы отслеживаем ветку test, а приложение наше это node.js, управляемый менеджером pm2): 1. Периодически обновляем первый локальный репозиторий до состояния, полностью соотвествующего состоянию удаленного репозитория. 2. Из первого локального репозитория обновляем второй. 3. Как только HEAD у нас переместился — появился новый коммит — во втором репозитории будет задействован хук post-update, который выполняется при появлении любых изменений в репозитории, и который и выполнит необходимые действия по ребилду и рестарту приложения. Для этого мы делаем следующее: 1. В первом локальном репозитории добавляем remote — второй локальный репозиторий: cd /opt/repo-dev/example-repo/ && git remote add prod /opt/repo-remote.git Теперь мы можем выполнять git push из первого локального репозитория во второй. 2. В папке /opt/repo-remote.git/hooks/ создаем файл post-update и делаем его исполняемым: touch /opt/repo-remote.git/hooks/post-update && chmod +x /opt/repo-remote.git/hooks/post-update Это обычный шел-скрипт, но согласно внутренней конвенции Git без расширения .sh! Давайте добавим в него несколько команд: #!/bin/bash cd /opt/repo-remote.git /usr/bin/git --work-tree=/opt/repo/example-repo/ checkout -f origin/test cd /opt/repo/example-repo/ /usr/bin/npm install /usr/local/bin/pm2 restart all Что делает скрипт? Сначала просто выгружает working tree нашего bare репозитория в папку с нашим работающим приложением, а затем пересобирает зависимости и рестартует сервисы pm2. Как видите, никакой магии. 3. Настраиваем cron, который каждые n минут будет обновлять первый репозиторий из удаленного: git fetch origin && git reset --hard -f origin/test Т.о. теперь наш хост будет являться регулярным инициатором проверки удаленного репозитория — а не появились ли там обновления? Прошу заметить, что обновляем локальный репозиторий мы не путем git pull, а путем git reset --hard. Делается это для того, чтобы исключить необходимость мерджей при определенном содержимом очередного коммита — мы делаем локальный репозиторий полной копией удаленного. 4. Сразу после синхронизации первого локального репозитория с удаленным мы делаем пуш всех изменений в наш псевдо-удаленный второй локальный репозиторий: git push prod test Вот и все. Как только наш псевдо-удаленный локальный репозиторий получает ненулевые изменения, Git дергает свой хук post-update, который выполняет соответствующий скрипт. И мы получаем простенькую рабочую схему push-to-deploy, которую при желании можно и дальше усовершенствовать в соответствии с нашими потребностями. «Зачем городить такую неудобоваримую монструозную схему?!» — спросите вы. Я сначала задавался этим же вопросом, но оказалось, что с существующим перечнем хуков Git'а только так мы сможем вызвать необходимую обработку при любом обновлении нашей ветки в удаленном репозитории. Нужный нам хук post-update предназначен для выполнения на remote репозитории (а часть хуков предназначена для выполнения на локальном репозитории). И мы таким вот не очень изящным способом это псевдо-удаленный репозиторий и сэмулировали. Возможно в скором времени появится еще какой-нибудь более удобный хук, выполняющийся локально, и схему можно будет упростить. Но пока так. В заключение хочу дать несколько советов тем, кто решит это реализовать у себя и, столкнувшись с проблемами неработающего хука или каких-то его частей, будет яростно проклинать меня, Git, автора оригинальной статьи и всех остальных по списку: 1. Помните про специфику работы cron — программы в нем по умолчанию запускаются совсем не в том окружении, которое вы, вероятно, ожидаете. 2. Проверяйте версии ваших утилит (npm, node etc) при вызове их из скриптов и по cron — они могут быть не такими, как при ручном запуске из-за различия путей к исполняемым файлам в переменных окружения, например. А запуск других их версий может приводить к непрогнозируемым результатам. 3. Потратьте 20 минут на просмотр очередной серии Simpsons и возвращайтесь к экспериментам с новыми силами и хорошим настроением Буду рад любым замечаниям и уточнениям по существу. Хорошего всем дня!\n","metric":0.6357421875,"metric_input":"Текст статьи:\nGit: много хуков полезных и разных / Habr                       \n\n30-05-2017\nСтатья является вольным переводом вот этого материала. Любителям длинных и заумных первоисточников можно сразу читать оригинал. Когда перед нами ставится задача при изменении кодбейса, например, в Github-репозитории выполнить пересборку/перезапуск какого-нибудь приложения на каком-то нашем окружении, то первое, что приходит на ум в качестве возможного триггера такой пересборки, это предоставляемый тем же гитхабом механизм веб-хуков: при наступлении какого-либо события с нашим удаленным репозиторием (т.к. появление нового коммита в какой-нибудь его отслеживаемой ветке) гитхаб задействует соответствующий веб-хук и «дернет» указанный в его настройках сервис, который и запустит процесс пересборки/перезапуска нашего приложения. Это стандартный широкоиспользуемый и простой механизм для таких случаев, все так делают, и все такое… Но что, если наше приложение живет на хосте, доступ к которому по каким-то причинам гитхабу не положен? Например, хост находится в закрытой сети или за NAT'ом и недоступен из интернета? В этом случае можно воспользоваться механизмом локальных хуков самого Git, о котором (механизме), как выясняется, мало кто знает даже из тех, кто использует Git уже довольно продолжительное время. Когда мы создаем локальный репозиторий (инициализируем пустой, клонируем удаленный, ...), то в папке .git, что в его корне, или в самом корне, если это bare-репозиторий, присутствует папка hooks. После инициализации репозитория в этой папке сохраняются шаблоны хуков для различных событий, таких как, например: post-merge, post-receive, post-update и др. Полное описание поддерживаемых событий для хуков можно найти, например, тут. Мы воспользуемся этим механизмом и реализуем простенькую push-to-deploy схему для нашего горемычного приложения. Нам понадобится два локальных репозитория. Создадим их, например, по указанным путям: 1. /opt/repo-dev/example-repo/ 2. /opt/repo-remote.git/ Первый репозиторий — это клон нашего удаленного репозитория example-repo на гитхабе. Второй — это bare репозиторий, копия первого, который будет служить нам исключительно для обработки события post-update при появлении обновлений в удаленном репозитории. Итак, как же мы это реализуем? Схема очень проста (предположим, мы отслеживаем ветку test, а приложение наше это node.js, управляемый менеджером pm2): 1. Периодически обновляем первый локальный репозиторий до состояния, полностью соотвествующего состоянию удаленного репозитория. 2. Из первого локального репозитория обновляем второй. 3. Как только HEAD у нас переместился — появился новый коммит — во втором репозитории будет задействован хук post-update, который выполняется при появлении любых изменений в репозитории, и который и выполнит необходимые действия по ребилду и рестарту приложения. Для этого мы делаем следующее: 1. В первом локальном репозитории добавляем remote — второй локальный репозиторий: cd /opt/repo-dev/example-repo/ && git remote add prod /opt/repo-remote.git Теперь мы можем выполнять git push из первого локального репозитория во второй. 2. В папке /opt/repo-remote.git/hooks/ создаем файл post-update и делаем его исполняемым: touch /opt/repo-remote.git/hooks/post-update && chmod +x /opt/repo-remote.git/hooks/post-update Это обычный шел-скрипт, но согласно внутренней конвенции Git без расширения .sh! Давайте добавим в него несколько команд: #!/bin/bash cd /opt/repo-remote.git /usr/bin/git --work-tree=/opt/repo/example-repo/ checkout -f origin/test cd /opt/repo/example-repo/ /usr/bin/npm install /usr/local/bin/pm2 restart all Что делает скрипт? Сначала просто выгружает working tree нашего bare репозитория в папку с нашим работающим приложением, а затем пересобирает зависимости и рестартует сервисы pm2. Как видите, никакой магии. 3. Настраиваем cron, который каждые n минут будет обновлять первый репозиторий из удаленного: git fetch origin && git reset --hard -f origin/test Т.о. теперь наш хост будет являться регулярным инициатором проверки удаленного репозитория — а не появились ли там обновления? Прошу заметить, что обновляем локальный репозиторий мы не путем git pull, а путем git reset --hard. Делается это для того, чтобы исключить необходимость мерджей при определенном содержимом очередного коммита — мы делаем локальный репозиторий полной копией удаленного. 4. Сразу после синхронизации первого локального репозитория с удаленным мы делаем пуш всех изменений в наш псевдо-удаленный второй локальный репозиторий: git push prod test Вот и все. Как только наш псевдо-удаленный локальный репозиторий получает ненулевые изменения, Git дергает свой хук post-update, который выполняет соответствующий скрипт. И мы получаем простенькую рабочую схему push-to-deploy, которую при желании можно и дальше усовершенствовать в соответствии с нашими потребностями. «Зачем городить такую неудобоваримую монструозную схему?!» — спросите вы. Я сначала задавался этим же вопросом, но оказалось, что с существующим перечнем хуков Git'а только так мы сможем вызвать необходимую обработку при любом обновлении нашей ветки в удаленном репозитории. Нужный нам хук post-update предназначен для выполнения на remote репозитории (а часть хуков предназначена для выполнения на локальном репозитории). И мы таким вот не очень изящным способом это псевдо-удаленный репозиторий и сэмулировали. Возможно в скором времени появится еще какой-нибудь более удобный хук, выполняющийся локально, и схему можно будет упростить. Но пока так. В заключение хочу дать несколько советов тем, кто решит это реализовать у себя и, столкнувшись с проблемами неработающего хука или каких-то его частей, будет яростно проклинать меня, Git, автора оригинальной статьи и всех остальных по списку: 1. Помните про специфику работы cron — программы в нем по умолчанию запускаются совсем не в том окружении, которое вы, вероятно, ожидаете. 2. Проверяйте версии ваших утилит (npm, node etc) при вызове их из скриптов и по cron — они могут быть не такими, как при ручном запуске из-за различия путей к исполняемым файлам в переменных окружения, например. А запуск других их версий может приводить к непрогнозируемым результатам. 3. Потратьте 20 минут на просмотр очередной серии Simpsons и возвращайтесь к экспериментам с новыми силами и хорошим настроением Буду рад любым замечаниям и уточнениям по существу. Хорошего всем дня!\n\n\nКраткое содержание:\nСтатья рассказывает о механизме локальных хуков в Git, который позволяет выполнять определенные действия при определенных событиях в локальном репозитории. Описывается схема push-to-deploy, которая позволяет автоматически запускать пересборку и перезапуск приложения при появлении обновлений в удаленном репозитории. Даются советы по настройке и использованию этой схемы.","summary":"Статья рассказывает о механизме локальных хуков в Git, который позволяет выполнять определенные действия при определенных событиях в локальном репозитории. Описывается схема push-to-deploy, которая позволяет автоматически запускать пересборку и перезапуск приложения при появлении обновлений в удаленном репозитории. Даются советы по настройке и использованию этой схемы."}
{"Text":"Удалённый сервер с 100% шифрованием и защитой от маски-шоу / Хабр                 \n\nНа написание сего шедевра меня сподвигла статья\n«Мечта параноика или Еще раз о шифровании»\n. Очень замечательная и полезная за одним исключением — если придут «маски-шоу», то они заберут сервер вместе со всеми флэшками и ключами. Отсюда вопрос — как сделать так, чтобы на сервере вообще не было никаких следов шифрования, ключей и т.п.?\n\nОтвет банален — не хранить их на сервере. И рядом с сервером не хранить. И вообще где-либо в районе досягаемости потенциального злоумышленника.\n\nИдея предлагаемого решения проста:\n\n— На сервере, который нужно защитить (назовём его «рабочий»), установить две системы. Первую — минимальную, на обычный НЕ шифрованный раздел и состоящую только из ядра, консоли и сетевых интерфейсов и не использующую своп. Вторую — на шифрованный раздел по методу\nFeNUMe\n. Шифрованный раздел должен быть зашифрован целиком и полностью, и не содержать никаких заголовков. С точки зрения постороннего наблюдателя это должна быть неформатированная область диска заполненная случайными данными.\n\n— Должен быть второй (назовём его «скрытый») сервер, географически находящийся в другой стране и оформленный на другого человека. Сервер не должен отвечать на ping и должен принимать запросы только с одного единственного IP — IP рабочего сервера. Причём остальные соединения должны отсекаться на уровне firewall — для всего остального мира кроме рабочего сервера скрытый сервер -«чёрная дыра».\n\n— Загрузка рабочего сервера начинается с запуска минимальной (открытой) нешифрованной системы. В ходе загрузки поднимаются сетевые интерфейсы, SSH и ram-диск.\n\n— После загрузки открытой системы она стучится на второй сервер по протоколу HTTP/HTTPS.\n\n— В ответ на стук рабочего севера скрытый сервер заходит в консоль рабочего сервера по протоколу SSH, копирует на ram-диск некий скрипт и файл ключа от скрытого раздела рабочего сервера и запускает скрипт. После чего благополучно отключается.\n\n— Скрипт подключает скрытый раздел (файл ключа у него есть), и запускает оттуда ядро при помощи kexec. Т.е. фактически запускается новая система.\n\n— Всё. Финита, как говорится, ля комедия и полный всеобъемлющий профит.\n\n— Для желающих скрытый сервер можно оборудовать функцией отключения по получению некого сообщения от шлюза SMS-email (фактически функцией отключения по получению SMS). Причём, перед собственным отключением он должен войти в рабочий сервер по SSH и выключить его. Т.е. добавляется функция удалённого отключения обоих серверов.\n\nЧто имеем в результате?\n\nЕсли изъять рабочий сервер, то на нём нет ничего, кроме голой системы, которая после загрузки обращается по некоторому загадочному URL. Большие ничего делать такая изъятая система не будет, так как у неё сменится IP и скрытый сервер будет игнорировать все обращения. Более того, сам факт существования скрытого сервера будет недоказуем, т.к. он отвечает на запросы с одного конкретного IP.\n\nНикаким образом невозможно доказать факт наличия каких-либо полезных данных на рабочем сервере — видна лишь неразмеченная область, заполненная случайными данными.\n\nНикакие ключи не хранятся на рабочем сервере.\n\nНикаким образом невозможно доказать факт применения средств шифрования, т.к. они каждый раз копируются с другого компьютера (со скрытого сервера) и располагаются на ram-диске.\n\nОсновная система рабочего сервера целиком и полностью, вместе с ядром и логами находится на зашифрованном разделе и факт её существования недоказуем.\n\nВладелец сервера (его друзья, родственники, коллеги) в любой момент может отключить скрытый сервер сделав невозможным доступ к данным на рабочем сервере, а сам факт наличия каких-либо данных недоказуемым.\n\nЕсли есть управление по СМС, то в случае маски-шоу, владелец может удалённо выключить оба сервера и после включения, на рабочем сервере не будет ничего кроме голой системы.\n\nМетод полностью и 100% устойчив даже к самому суровому терморектальному криптоанализу. Ибо, если физически отформатировать скрытый сервер или удалить с него файл ключа, то при всём желании владелец никому ничего не сможет показать.\n\nМетод устойчив к сбоям — если тайный сервер неожиданно умер, то владелец может иметь закопанную в тайном месте под заветной липой флэшку с файлами, необходимыми для старта рабочего сервера. А имея эти файлы (скрипт и ключ) никто не запрещает войти на открытую систему рабочего сервера по SSH и запустить зашифрованную систему. Правда, стойкость к терморектальному криптоанализу в таком случае заметно падает.\n\nИз недостатков имеем:\n\n— в случае недоступности скрытого сервера, рабочий сервер стартовать не может, но нормальные сервера перезагружаются редко и если нет инета, то рабочий сервер скорее всего бесполезен;\n\n— никто не запрещает иметь два скрытых сервера (дублирование).\n\nP.S.\n\nТ.к. тут шлют вопросы в почту, то напишу несколько замечаний:\n\n1) Просто использовать скрытый сервер для хранения данных не интересно, т.к. он далеко и пинг до него велик, а канал узок.\n\n2) От скрытого сервера ничего не отдаётся — скрытый сервер заходит по SSH на рабочий сервер. Если что-либо отдавать, то становится понятна схема работы и возникает масса вопросов, плюс, появляется повод давить на владельца т.к. он «препятствует».\n\n3) Обращение к скрытому серверу, ясное дело, по HTTPS, чтобы IP подменить нельзя было. Хотя, на самом деле это не обязательно — мы всё равно принимаем один единственный IP. Плюс, стучащемуся нужно обладать ключами от SSH.\n\n4) Это не панацея и не защита от всего и вся. Если государство захочет кого-то посадить, то оно посадит. И никаких серверов для этого не нужно. И если спецслужбы за кого-то возьмутся, то они своё получат. Это защита сугубо от произвола и бардака — когда из-за конкурентов или малолетних шутников ничего не подозревающий владелец сервера может получить реальный срок.\n\n5) Это относится прежде всего к web-хостингу, когда нет физического доступа к серверу. Для корпоративных серверов, наверное тоже что-то подобное использовать, но не понятно зачем :)\n\n6) В Комментариях все привязались к этому злосчастному паяльнику, как банный лист — к заднице. От паяльника ничего поможет. Даже отсутствие сервера.\n\nP.P.S.\n\nУтомившись спорить со странными людьми из параллельной вселенной добавляю\nвот этот пример\n.","metric":0.44970703130000000414,"metric_input":"Текст статьи:\nУдалённый сервер с 100% шифрованием и защитой от маски-шоу / Хабр                 \n\nНа написание сего шедевра меня сподвигла статья\n«Мечта параноика или Еще раз о шифровании»\n. Очень замечательная и полезная за одним исключением — если придут «маски-шоу», то они заберут сервер вместе со всеми флэшками и ключами. Отсюда вопрос — как сделать так, чтобы на сервере вообще не было никаких следов шифрования, ключей и т.п.?\n\nОтвет банален — не хранить их на сервере. И рядом с сервером не хранить. И вообще где-либо в районе досягаемости потенциального злоумышленника.\n\nИдея предлагаемого решения проста:\n\n— На сервере, который нужно защитить (назовём его «рабочий»), установить две системы. Первую — минимальную, на обычный НЕ шифрованный раздел и состоящую только из ядра, консоли и сетевых интерфейсов и не использующую своп. Вторую — на шифрованный раздел по методу\nFeNUMe\n. Шифрованный раздел должен быть зашифрован целиком и полностью, и не содержать никаких заголовков. С точки зрения постороннего наблюдателя это должна быть неформатированная область диска заполненная случайными данными.\n\n— Должен быть второй (назовём его «скрытый») сервер, географически находящийся в другой стране и оформленный на другого человека. Сервер не должен отвечать на ping и должен принимать запросы только с одного единственного IP — IP рабочего сервера. Причём остальные соединения должны отсекаться на уровне firewall — для всего остального мира кроме рабочего сервера скрытый сервер -«чёрная дыра».\n\n— Загрузка рабочего сервера начинается с запуска минимальной (открытой) нешифрованной системы. В ходе загрузки поднимаются сетевые интерфейсы, SSH и ram-диск.\n\n— После загрузки открытой системы она стучится на второй сервер по протоколу HTTP/HTTPS.\n\n— В ответ на стук рабочего севера скрытый сервер заходит в консоль рабочего сервера по протоколу SSH, копирует на ram-диск некий скрипт и файл ключа от скрытого раздела рабочего сервера и запускает скрипт. После чего благополучно отключается.\n\n— Скрипт подключает скрытый раздел (файл ключа у него есть), и запускает оттуда ядро при помощи kexec. Т.е. фактически запускается новая система.\n\n— Всё. Финита, как говорится, ля комедия и полный всеобъемлющий профит.\n\n— Для желающих скрытый сервер можно оборудовать функцией отключения по получению некого сообщения от шлюза SMS-email (фактически функцией отключения по получению SMS). Причём, перед собственным отключением он должен войти в рабочий сервер по SSH и выключить его. Т.е. добавляется функция удалённого отключения обоих серверов.\n\nЧто имеем в результате?\n\nЕсли изъять рабочий сервер, то на нём нет ничего, кроме голой системы, которая после загрузки обращается по некоторому загадочному URL. Большие ничего делать такая изъятая система не будет, так как у неё сменится IP и скрытый сервер будет игнорировать все обращения. Более того, сам факт существования скрытого сервера будет недоказуем, т.к. он отвечает на запросы с одного конкретного IP.\n\nНикаким образом невозможно доказать факт наличия каких-либо полезных данных на рабочем сервере — видна лишь неразмеченная область, заполненная случайными данными.\n\nНикакие ключи не хранятся на рабочем сервере.\n\nНикаким образом невозможно доказать факт применения средств шифрования, т.к. они каждый раз копируются с другого компьютера (со скрытого сервера) и располагаются на ram-диске.\n\nОсновная система рабочего сервера целиком и полностью, вместе с ядром и логами находится на зашифрованном разделе и факт её существования недоказуем.\n\nВладелец сервера (его друзья, родственники, коллеги) в любой момент может отключить скрытый сервер сделав невозможным доступ к данным на рабочем сервере, а сам факт наличия каких-либо данных недоказуемым.\n\nЕсли есть управление по СМС, то в случае маски-шоу, владелец может удалённо выключить оба сервера и после включения, на рабочем сервере не будет ничего кроме голой системы.\n\nМетод полностью и 100% устойчив даже к самому суровому терморектальному криптоанализу. Ибо, если физически отформатировать скрытый сервер или удалить с него файл ключа, то при всём желании владелец никому ничего не сможет показать.\n\nМетод устойчив к сбоям — если тайный сервер неожиданно умер, то владелец может иметь закопанную в тайном месте под заветной липой флэшку с файлами, необходимыми для старта рабочего сервера. А имея эти файлы (скрипт и ключ) никто не запрещает войти на открытую систему рабочего сервера по SSH и запустить зашифрованную систему. Правда, стойкость к терморектальному криптоанализу в таком случае заметно падает.\n\nИз недостатков имеем:\n\n— в случае недоступности скрытого сервера, рабочий сервер стартовать не может, но нормальные сервера перезагружаются редко и если нет инета, то рабочий сервер скорее всего бесполезен;\n\n— никто не запрещает иметь два скрытых сервера (дублирование).\n\nP.S.\n\nТ.к. тут шлют вопросы в почту, то напишу несколько замечаний:\n\n1) Просто использовать скрытый сервер для хранения данных не интересно, т.к. он далеко и пинг до него велик, а канал узок.\n\n2) От скрытого сервера ничего не отдаётся — скрытый сервер заходит по SSH на рабочий сервер. Если что-либо отдавать, то становится понятна схема работы и возникает масса вопросов, плюс, появляется повод давить на владельца т.к. он «препятствует».\n\n3) Обращение к скрытому серверу, ясное дело, по HTTPS, чтобы IP подменить нельзя было. Хотя, на самом деле это не обязательно — мы всё равно принимаем один единственный IP. Плюс, стучащемуся нужно обладать ключами от SSH.\n\n4) Это не панацея и не защита от всего и вся. Если государство захочет кого-то посадить, то оно посадит. И никаких серверов для этого не нужно. И если спецслужбы за кого-то возьмутся, то они своё получат. Это защита сугубо от произвола и бардака — когда из-за конкурентов или малолетних шутников ничего не подозревающий владелец сервера может получить реальный срок.\n\n5) Это относится прежде всего к web-хостингу, когда нет физического доступа к серверу. Для корпоративных серверов, наверное тоже что-то подобное использовать, но не понятно зачем :)\n\n6) В Комментариях все привязались к этому злосчастному паяльнику, как банный лист — к заднице. От паяльника ничего поможет. Даже отсутствие сервера.\n\nP.P.S.\n\nУтомившись спорить со странными людьми из параллельной вселенной добавляю\nвот этот пример\n.\n\nКраткое содержание:\nВ статье предлагается решение для защиты удаленных серверов с использованием двух систем: открытой и зашифрованной. Открытая система устанавливается на обычный раздел без шифрования, а зашифрованная система использует метод FeNUMe для полного шифрования раздела.\n\nДля обеспечения безопасности, предлагается использовать второй скрытый сервер, расположенный в другой стране. Скрытый сервер принимает запросы только с IP рабочего сервера и отвечает на них через HTTPS. При загрузке рабочего сервера, он сначала подключается к скрытому серверу для получения скриптов и ключа шифрования, а затем запускает зашифрованную систему.\n\nТаким образом, на рабочем сервере не остается никаких следов шифрования или ключей, и его содержимое выглядит как незаполненная область диска. Это делает невозможным доказательство факта наличия данных на сервере или использования шифрования.\n\nВладелец сервера может удаленно отключить скрытый сервер, что делает невозможным доступ к данным на рабочем сервере. Кроме того, если есть управление по СМС, владелец может удаленно выключить оба сервера, чтобы скрыть данные в случае необходимости.\n\nНедостатком данной схемы является то, что в случае недоступности скрытого сервера, рабочий сервер не может быть запущен. Также, использование двух скрытых серверов может обеспечить дополнительную защиту.","summary":"В статье предлагается решение для защиты удаленных серверов с использованием двух систем: открытой и зашифрованной. Открытая система устанавливается на обычный раздел без шифрования, а зашифрованная система использует метод FeNUMe для полного шифрования раздела.\n\nДля обеспечения безопасности, предлагается использовать второй скрытый сервер, расположенный в другой стране. Скрытый сервер принимает запросы только с IP рабочего сервера и отвечает на них через HTTPS. При загрузке рабочего сервера, он сначала подключается к скрытому серверу для получения скриптов и ключа шифрования, а затем запускает зашифрованную систему.\n\nТаким образом, на рабочем сервере не остается никаких следов шифрования или ключей, и его содержимое выглядит как незаполненная область диска. Это делает невозможным доказательство факта наличия данных на сервере или использования шифрования.\n\nВладелец сервера может удаленно отключить скрытый сервер, что делает невозможным доступ к данным на рабочем сервере. Кроме того, если есть управление по СМС, владелец может удаленно выключить оба сервера, чтобы скрыть данные в случае необходимости.\n\nНедостатком данной схемы является то, что в случае недоступности скрытого сервера, рабочий сервер не может быть запущен. Также, использование двух скрытых серверов может обеспечить дополнительную защиту."}
{"Text":"Security Week 32: уязвимость в GRUB2 / Хабр                                                          \n\n29 июля компания Eclypsium опубликовала\nисследование\nновой уязвимости в загрузчике Grub2, массово используемом в системах на базе Linux — от ноутбуков и IoT-устройств до серверов. Суть уязвимости довольно простая: ошибки при обработке текстового конфигурационного файла grub.cfg приводят к переполнению буфера и открывают возможность выполнения произвольного кода. Последствия тоже понятные: взлом системы на ранней стадии загрузки обеспечивает полный контроль над ней, затрудняет обнаружение проблемы. Уязвимости был присвоен идентификатор\nCVE-2020-10713\n. Это не единственная уязвимость, закрытая в Grub2 за последнее время: после уведомления мейнтейнеров популярных дистрибутивов в коде загрузчика было исправлено еще как минимум 7 похожих проблем с рейтингом опасности по CVSS от 5,7 до 6,4. Оригинальная уязвимость по тому же методу оценки получила 8,2 балла.\n\nДля эксплуатации уязвимости атакующему требуется получить высокий уровень доступа к системе. Настолько высокий, что в большинстве сценариев дополнительно взламывать Grub2 вроде бы и не обязательно. Варианты, когда это действительно приносит пользу злоумышленнику, еще предстоит исследовать. Основная проблема данной уязвимости заключается в сложных взаимоотношениях между Grub2 и UEFI Secure Boot — универсальным инструментом инициализации железа, помимо прочего гарантирующим, что на следующих этапах будет запущен только проверенный код с цифровой подписью. Из-за особенностей лицензии, под которой распространяется Grub2, между UEFI и Grub2 «вставлен» посредник, известный как shim — небольшой загрузчик, проверяющий версию Grub2. Перевыпуск этих самых shim, а заодно обновление UEFI огромного числа систем, блокирующее старые shim, — это, если выражаться простыми словами, Гигантская Головная Боль всей индустрии.\n\nТехнические особенности уязвимости достаточно подробно (но без примеров эксплуатации) расписаны в отчете Eclypsium. В коде Grub2, отвечающем за парсинг текстового конфигурационного файла, предусмотрена обработка ситуации, когда входящая строка слишком длинная. Ошибка в коде может привести к тому, что данные все же будут загружены, и произойдет переполнение буфера. В данном случае важнее не уязвимость, а ее последствия. Ранее уже происходили ситуации, когда незащищенную версию Grub (из-за ошибки конфигурации) приходилось добавлять в так называемый\nUEFI Revocation List\n— список shim, которые обновленная система откажется запускать. Этот список нужно как-то доставить на конкретную систему, что может сделать либо поставщик железа, либо разработчик операционной системы, например компания Microsoft, помимо прочего отвечающая за сертификацию загрузочного кода, в том числе и для Linux-систем. Любая ошибка в апдейте, применяемом на ранней стадии, может сделать компьютер или сервер полностью неработоспособным.\n\nИными словами, обновления Grub, закрывающего свежеобнаруженные уязвимости, недостаточно для решения проблемы. Требуется запретить выполнение старых версий Grub2 путем добавления связанных с ними shim в UEFI Revocation List. По данным Eclypsium, более 80 промежуточных загрузчиков потребуется заблокировать, и вот это уже исключит возможность запуска произвольного кода, если обновление так или иначе доберется до конкретной материнской платы с конкретной версией UEFI. И кстати, так как уязвимость была найдена в универсальном, не зависящем от платформы коде, устройства на архитектуре arm64 также подвержены.\n\nКроме того, понадобится обновить системы экстренного восстановления компьютеров или серверов, иначе в будущем может сложиться ситуация, когда инструмент восстановления не сработает. Потребуется протестировать и патчи для Grub, и новые промежуточные загрузчики, и обновления Revocation List. Уже сейчас даже простое обновление Grub2 вызывает проблемы: см.\nотчет\nна Хабре о неработоспособности серверов под управлением CentOS после установки багфикса\nRHSA-2020:3216\n, закрывающего данную уязвимость. Решаться проблема будет долго, и именно поэтому код Grub2 был проверен на наличие похожих уязвимостей, чтобы не пришлось проходить этот процесс несколько раз. С одной стороны, реальной опасности данная уязвимость прямо сейчас не несет. С другой — она включает сложный механизм взаимодействия разработчиков софта и железа, который в данном случае неизбежен. Хорошее подтверждение расхожей фразы о том, что безопасность — это процесс, а не результат.\n\n### Что еще произошло ###\n\nКомпания Garmin устраняет последствия атаки на собственную инфраструктуру, подробно описанной в\nпредыдущем дайджесте\n. По состоянию на 3 августа полностью восстановлены сервисы Garmin Aviation, включая FlyGarmin. Функциональность облачного сервиса Garmin Connect для умных часов компании восстановлена\nчастично\n. Сайт Bleeping Computer, снова со ссылкой на анонимные источники,\nсообщает\nо том, что ключ для расшифровки данных, пострадавших от трояна-вымогателя, компания получила. Как именно — непонятно, но возможных вариантов здесь немного. Эксперты «Лаборатории Касперского» опубликовали\nподробный разбор\nсэмпла WastedLocker, который (скорее всего) атаковал Garmin.\n\nЕще одна крупная атака с вымогательством\nпроизошла\nв компании CWT, занятой организацией командировок. Злоумышленники требуют выкуп в размере 4,5 миллиона долларов.\n\nВ США арестованы подозреваемые во взломе десятков аккаунтов Twitter в\nсередине июля\n. Так быстро идентифицировать взломщиков удалось благодаря их беспечности. Например, один почтовый адрес использовался на хакерском форуме и на бирже криптовалют, а взлом форума позволил сопоставить учетную запись с реальным IP. Подробнее — в\nпубликации\nZDNet и\nна Хабре\n.\n\nУязвимость\nв VoIP-адаптерах Grandstream позволяет взломать их на стадии первоначальной настройки, в том числе с использованием подготовленного SIP-пакета.\n\nВ Zoom\nзакрыли\n«детскую» уязвимость, позволявшую взламывать пароли для доступа к телеконференции. Пароли в Zoom используются шестизначные, а число попыток ввода никак не ограничено — можно было взломать грубой силой, зная только идентификатор встречи (как правило, постоянный).\n\nНайдена свежая десятибалльная\nуязвимость\nв плагине wpDiscuz для обработки комментариев на сайтах под управлением Wordpress с возможностью выполнения произвольного кода. Подвержены 70 тысяч сайтов.","metric":0.51708984380000000414,"metric_input":"Текст статьи:\nSecurity Week 32: уязвимость в GRUB2 / Хабр                                                          \n\n29 июля компания Eclypsium опубликовала\nисследование\nновой уязвимости в загрузчике Grub2, массово используемом в системах на базе Linux — от ноутбуков и IoT-устройств до серверов. Суть уязвимости довольно простая: ошибки при обработке текстового конфигурационного файла grub.cfg приводят к переполнению буфера и открывают возможность выполнения произвольного кода. Последствия тоже понятные: взлом системы на ранней стадии загрузки обеспечивает полный контроль над ней, затрудняет обнаружение проблемы. Уязвимости был присвоен идентификатор\nCVE-2020-10713\n. Это не единственная уязвимость, закрытая в Grub2 за последнее время: после уведомления мейнтейнеров популярных дистрибутивов в коде загрузчика было исправлено еще как минимум 7 похожих проблем с рейтингом опасности по CVSS от 5,7 до 6,4. Оригинальная уязвимость по тому же методу оценки получила 8,2 балла.\n\nДля эксплуатации уязвимости атакующему требуется получить высокий уровень доступа к системе. Настолько высокий, что в большинстве сценариев дополнительно взламывать Grub2 вроде бы и не обязательно. Варианты, когда это действительно приносит пользу злоумышленнику, еще предстоит исследовать. Основная проблема данной уязвимости заключается в сложных взаимоотношениях между Grub2 и UEFI Secure Boot — универсальным инструментом инициализации железа, помимо прочего гарантирующим, что на следующих этапах будет запущен только проверенный код с цифровой подписью. Из-за особенностей лицензии, под которой распространяется Grub2, между UEFI и Grub2 «вставлен» посредник, известный как shim — небольшой загрузчик, проверяющий версию Grub2. Перевыпуск этих самых shim, а заодно обновление UEFI огромного числа систем, блокирующее старые shim, — это, если выражаться простыми словами, Гигантская Головная Боль всей индустрии.\n\nТехнические особенности уязвимости достаточно подробно (но без примеров эксплуатации) расписаны в отчете Eclypsium. В коде Grub2, отвечающем за парсинг текстового конфигурационного файла, предусмотрена обработка ситуации, когда входящая строка слишком длинная. Ошибка в коде может привести к тому, что данные все же будут загружены, и произойдет переполнение буфера. В данном случае важнее не уязвимость, а ее последствия. Ранее уже происходили ситуации, когда незащищенную версию Grub (из-за ошибки конфигурации) приходилось добавлять в так называемый\nUEFI Revocation List\n— список shim, которые обновленная система откажется запускать. Этот список нужно как-то доставить на конкретную систему, что может сделать либо поставщик железа, либо разработчик операционной системы, например компания Microsoft, помимо прочего отвечающая за сертификацию загрузочного кода, в том числе и для Linux-систем. Любая ошибка в апдейте, применяемом на ранней стадии, может сделать компьютер или сервер полностью неработоспособным.\n\nИными словами, обновления Grub, закрывающего свежеобнаруженные уязвимости, недостаточно для решения проблемы. Требуется запретить выполнение старых версий Grub2 путем добавления связанных с ними shim в UEFI Revocation List. По данным Eclypsium, более 80 промежуточных загрузчиков потребуется заблокировать, и вот это уже исключит возможность запуска произвольного кода, если обновление так или иначе доберется до конкретной материнской платы с конкретной версией UEFI. И кстати, так как уязвимость была найдена в универсальном, не зависящем от платформы коде, устройства на архитектуре arm64 также подвержены.\n\nКроме того, понадобится обновить системы экстренного восстановления компьютеров или серверов, иначе в будущем может сложиться ситуация, когда инструмент восстановления не сработает. Потребуется протестировать и патчи для Grub, и новые промежуточные загрузчики, и обновления Revocation List. Уже сейчас даже простое обновление Grub2 вызывает проблемы: см.\nотчет\nна Хабре о неработоспособности серверов под управлением CentOS после установки багфикса\nRHSA-2020:3216\n, закрывающего данную уязвимость. Решаться проблема будет долго, и именно поэтому код Grub2 был проверен на наличие похожих уязвимостей, чтобы не пришлось проходить этот процесс несколько раз. С одной стороны, реальной опасности данная уязвимость прямо сейчас не несет. С другой — она включает сложный механизм взаимодействия разработчиков софта и железа, который в данном случае неизбежен. Хорошее подтверждение расхожей фразы о том, что безопасность — это процесс, а не результат.\n\n### Что еще произошло ###\n\nКомпания Garmin устраняет последствия атаки на собственную инфраструктуру, подробно описанной в\nпредыдущем дайджесте\n. По состоянию на 3 августа полностью восстановлены сервисы Garmin Aviation, включая FlyGarmin. Функциональность облачного сервиса Garmin Connect для умных часов компании восстановлена\nчастично\n. Сайт Bleeping Computer, снова со ссылкой на анонимные источники,\nсообщает\nо том, что ключ для расшифровки данных, пострадавших от трояна-вымогателя, компания получила. Как именно — непонятно, но возможных вариантов здесь немного. Эксперты «Лаборатории Касперского» опубликовали\nподробный разбор\nсэмпла WastedLocker, который (скорее всего) атаковал Garmin.\n\nЕще одна крупная атака с вымогательством\nпроизошла\nв компании CWT, занятой организацией командировок. Злоумышленники требуют выкуп в размере 4,5 миллиона долларов.\n\nВ США арестованы подозреваемые во взломе десятков аккаунтов Twitter в\nсередине июля\n. Так быстро идентифицировать взломщиков удалось благодаря их беспечности. Например, один почтовый адрес использовался на хакерском форуме и на бирже криптовалют, а взлом форума позволил сопоставить учетную запись с реальным IP. Подробнее — в\nпубликации\nZDNet и\nна Хабре\n.\n\nУязвимость\nв VoIP-адаптерах Grandstream позволяет взломать их на стадии первоначальной настройки, в том числе с использованием подготовленного SIP-пакета.\n\nВ Zoom\nзакрыли\n«детскую» уязвимость, позволявшую взламывать пароли для доступа к телеконференции. Пароли в Zoom используются шестизначные, а число попыток ввода никак не ограничено — можно было взломать грубой силой, зная только идентификатор встречи (как правило, постоянный).\n\nНайдена свежая десятибалльная\nуязвимость\nв плагине wpDiscuz для обработки комментариев на сайтах под управлением Wordpress с возможностью выполнения произвольного кода. Подвержены 70 тысяч сайтов.\n\nКраткое содержание:\nВ статье рассказывается о новой уязвимости в загрузчике Grub2, которая может привести к взлому системы на ранней стадии загрузки. Уязвимость была обнаружена компанией Eclypsium и получила идентификатор CVE-2020-10713. Для эксплуатации уязвимости требуется высокий уровень доступа к системе. Основная проблема заключается в сложных взаимоотношениях между Grub2 и UEFI Secure Boot. Требуется обновление UEFI огромного числа систем, что может вызвать Гигантскую Головную Боль для всей индустрии. Также требуется обновление систем экстренного восстановления компьютеров или серверов.","summary":"В статье рассказывается о новой уязвимости в загрузчике Grub2, которая может привести к взлому системы на ранней стадии загрузки. Уязвимость была обнаружена компанией Eclypsium и получила идентификатор CVE-2020-10713. Для эксплуатации уязвимости требуется высокий уровень доступа к системе. Основная проблема заключается в сложных взаимоотношениях между Grub2 и UEFI Secure Boot. Требуется обновление UEFI огромного числа систем, что может вызвать Гигантскую Головную Боль для всей индустрии. Также требуется обновление систем экстренного восстановления компьютеров или серверов."}
{"Text":"Тренды-2016 в аналитических системах / Хабр                 \n\n05-02-2016\nСистемы аналитики приложений, а уж тем более, игр – рынок интересный и бурно развивающийся. Поговорив с клиентами и коллегами по индустрии, мы хотели бы представить вашему вниманию тренды в развитии аналитических систем, как видим их мы в devtodev. Тренд #1. Предиктивная аналитика Предиктивная аналитика – достаточно широкое понятие, и мы вкладываем в него следующие смыслы: Предсказание значения метрик. Аналитические системы многое знают об основных аналитических показателях и связях между ними. Показывая значение метрики (скажем, ARPU) на графике, аналитическая система должна уметь предсказать, как изменится этот показатель в ближайшем будущем, хотя бы в краткосрочном периоде. Такие подсказки помогут клиенту ориентироваться в ситуации и планировать дальнейшие действия. Предсказание поведения пользователей. Система аналитики – это не только метрики и их значения. Это ещё и различные методы анализа: пользовательские сегменты, события и воронки, продвинутые отчёты и так далее. Опять же, система аналитики, если она правильно интегрирована, знает очень многое о поведении пользователей, о том, что влияет на их удержание и монетизацию, а что способствует их уходу. Поэтому хорошая аналитика должна сигнализировать клиенту о том, что пользователь скоро уйдёт из приложения. Тренд #2. Коммуникация с игроком Логичный этап в развитии аналитических систем – это объединение продвинутого аналитического функционала с сервисом для рассылки push-уведомлений. Идеальная схема выглядит так: выделяем пользовательский сегмент, отслеживаем его показатели; если значения показателей нас не устраивают или начинают заваливаться, рассылаем этим пользователям уведомления и поднимаем значения показателей; производим повторный замер. Скажем, система аналитики предсказала (см. тренд #1), что пользователь стал попадать под паттерн “скоро уйдёт” – давно не заходил / снизил регулярность входов / застрял на уровне. Разработчик приложения тут же формирует push-уведомления и рассылает их пользователям из группы риска. При этом не обязательно отправлять сообщение в стиле “Вернись, я все прощу” – можно предложить игроку бонус на следующую покупку или удвоенный ежедневный подарок. Скриншот взят из системы devtodev. Тренд #3. Кроссплатформенная аналитика Здесь аналитические системы просто догоняют игровую индустрию, в которой кроссплатформенность уже называется одним из трендов будущего года. Всё больше игр становятся выходит разом на нескольких платформах. Для нас нет ничего странного в том, что в проектах вроде WarThunder или Minecraft встречаются консольный игрок, ПК-игрок и пользователь, играющий с мобильного устройства. Подобные игры требуют особого подхода к анализу, ведь важно замечать отклонения в работе каждой из платформ и приводить их к балансу. К тому же, кроссплатформенная аналитика помогает делать выводы об эффективности каждой из платформ: сколько стоит привлечение, каковы ROI по платформам и так далее. Тренд #4. Сопоставление с рынком (бенчмаркинг) ARPU = $0,15 – это хорошо или плохо? Сколько процентов игр в этом жанре имеют показатели выше, чем у моего проекта? Аналитическая система должна уметь отвечать на такие вопросы. Как правило, к сервисам аналитики подключены сотни и тысячи игр со всего света, и сравнить какой-либо проект со средними показателями для его жанра нетрудно. Эта информация будет очень полезна разработчику – он сможет обнаружить в проекте узкие места и вовремя их исправить. Поэтому мы считаем, что в 2016 функционал бенчмаркинга станет трендом в аналитике. Пример отчёта по бенчмаркингу, которые мы в devtodev делаем по заказу клиента. Тренд #5. Простота и обучение. Инди-разработчиков становится всё больше, интересные и успешные игры от маленьких студий (в составе от одного до десяти разработчиков) появляются всё чаще. Надо понимать, что у этих ребят нет времени на то, чтобы долго интегрировать аналитику и разбираться, чем отличается 1-day retention от 1-day rolling retention. Аналитические системы должны быть максимально простыми в интеграции и максимально понятными при использовании. Промежуток времени между поступлением данных и принятием решения на их основе должен быть сведён к минимуму. Поэтому мы видим в развитии аналитики явный запрос на простоту интерфейса, на развитие системы сигналов и рекомендаций (“Друг! У тебя падает retention! Последний апдейт туториала значительно ухудшил его эффективность!”). Обучение – тоже тренд следующего года. Становясь проще и ближе к пользователю, аналитический сервис должен давать ему новые знания, рассказывать о подходящих методах и помогать принять верное решение. Например, мы для этого проводим вебинары и создали свой Образовательный центр. Тренд #6, он же вывод. Аналитика с человеческим лицом. Тренды, которые перечислены выше, меняют само понятие аналитической системы. Мы верим, что системы аналитики станут не просто непонятным сборником таблиц и графиков, а другом и советчиком каждого, кто создает игры. Ведь, в конце концов, аналитика существует только для того, чтобы делать жизнь разработчика проще и помогать ему принимать решения о развитии проекта. Насколько всё это возможно прямо сейчас – открытый вопрос. Но мы считаем, что улучшение коммуникации между системой аналитики и ее пользователями, взаимодействие живых аналитиков с разработчиками на уровне советов и рекомендаций точно станет реальностью и закономерным этапом в развитии аналитики.\n","metric":0.6279296875,"metric_input":"Текст статьи:\nТренды-2016 в аналитических системах / Хабр                 \n\n05-02-2016\nСистемы аналитики приложений, а уж тем более, игр – рынок интересный и бурно развивающийся. Поговорив с клиентами и коллегами по индустрии, мы хотели бы представить вашему вниманию тренды в развитии аналитических систем, как видим их мы в devtodev. Тренд #1. Предиктивная аналитика Предиктивная аналитика – достаточно широкое понятие, и мы вкладываем в него следующие смыслы: Предсказание значения метрик. Аналитические системы многое знают об основных аналитических показателях и связях между ними. Показывая значение метрики (скажем, ARPU) на графике, аналитическая система должна уметь предсказать, как изменится этот показатель в ближайшем будущем, хотя бы в краткосрочном периоде. Такие подсказки помогут клиенту ориентироваться в ситуации и планировать дальнейшие действия. Предсказание поведения пользователей. Система аналитики – это не только метрики и их значения. Это ещё и различные методы анализа: пользовательские сегменты, события и воронки, продвинутые отчёты и так далее. Опять же, система аналитики, если она правильно интегрирована, знает очень многое о поведении пользователей, о том, что влияет на их удержание и монетизацию, а что способствует их уходу. Поэтому хорошая аналитика должна сигнализировать клиенту о том, что пользователь скоро уйдёт из приложения. Тренд #2. Коммуникация с игроком Логичный этап в развитии аналитических систем – это объединение продвинутого аналитического функционала с сервисом для рассылки push-уведомлений. Идеальная схема выглядит так: выделяем пользовательский сегмент, отслеживаем его показатели; если значения показателей нас не устраивают или начинают заваливаться, рассылаем этим пользователям уведомления и поднимаем значения показателей; производим повторный замер. Скажем, система аналитики предсказала (см. тренд #1), что пользователь стал попадать под паттерн “скоро уйдёт” – давно не заходил / снизил регулярность входов / застрял на уровне. Разработчик приложения тут же формирует push-уведомления и рассылает их пользователям из группы риска. При этом не обязательно отправлять сообщение в стиле “Вернись, я все прощу” – можно предложить игроку бонус на следующую покупку или удвоенный ежедневный подарок. Скриншот взят из системы devtodev. Тренд #3. Кроссплатформенная аналитика Здесь аналитические системы просто догоняют игровую индустрию, в которой кроссплатформенность уже называется одним из трендов будущего года. Всё больше игр становятся выходит разом на нескольких платформах. Для нас нет ничего странного в том, что в проектах вроде WarThunder или Minecraft встречаются консольный игрок, ПК-игрок и пользователь, играющий с мобильного устройства. Подобные игры требуют особого подхода к анализу, ведь важно замечать отклонения в работе каждой из платформ и приводить их к балансу. К тому же, кроссплатформенная аналитика помогает делать выводы об эффективности каждой из платформ: сколько стоит привлечение, каковы ROI по платформам и так далее. Тренд #4. Сопоставление с рынком (бенчмаркинг) ARPU = $0,15 – это хорошо или плохо? Сколько процентов игр в этом жанре имеют показатели выше, чем у моего проекта? Аналитическая система должна уметь отвечать на такие вопросы. Как правило, к сервисам аналитики подключены сотни и тысячи игр со всего света, и сравнить какой-либо проект со средними показателями для его жанра нетрудно. Эта информация будет очень полезна разработчику – он сможет обнаружить в проекте узкие места и вовремя их исправить. Поэтому мы считаем, что в 2016 функционал бенчмаркинга станет трендом в аналитике. Пример отчёта по бенчмаркингу, которые мы в devtodev делаем по заказу клиента. Тренд #5. Простота и обучение. Инди-разработчиков становится всё больше, интересные и успешные игры от маленьких студий (в составе от одного до десяти разработчиков) появляются всё чаще. Надо понимать, что у этих ребят нет времени на то, чтобы долго интегрировать аналитику и разбираться, чем отличается 1-day retention от 1-day rolling retention. Аналитические системы должны быть максимально простыми в интеграции и максимально понятными при использовании. Промежуток времени между поступлением данных и принятием решения на их основе должен быть сведён к минимуму. Поэтому мы видим в развитии аналитики явный запрос на простоту интерфейса, на развитие системы сигналов и рекомендаций (“Друг! У тебя падает retention! Последний апдейт туториала значительно ухудшил его эффективность!”). Обучение – тоже тренд следующего года. Становясь проще и ближе к пользователю, аналитический сервис должен давать ему новые знания, рассказывать о подходящих методах и помогать принять верное решение. Например, мы для этого проводим вебинары и создали свой Образовательный центр. Тренд #6, он же вывод. Аналитика с человеческим лицом. Тренды, которые перечислены выше, меняют само понятие аналитической системы. Мы верим, что системы аналитики станут не просто непонятным сборником таблиц и графиков, а другом и советчиком каждого, кто создает игры. Ведь, в конце концов, аналитика существует только для того, чтобы делать жизнь разработчика проще и помогать ему принимать решения о развитии проекта. Насколько всё это возможно прямо сейчас – открытый вопрос. Но мы считаем, что улучшение коммуникации между системой аналитики и ее пользователями, взаимодействие живых аналитиков с разработчиками на уровне советов и рекомендаций точно станет реальностью и закономерным этапом в развитии аналитики.\n\n\nКраткое содержание:\nСтатья обсуждает тренды в развитии аналитических систем для приложений и игр. Перечислив тренд #1 - Предиктивная аналитика, авторы объясняют, что аналитические системы должны предсказывать значения метрик и поведение пользователей.\n\nДалее они описывают тренд #2 - Коммуникация с игроком, где говорится о том, как аналитика может использоваться для отправки push-уведомлений пользователям, чтобы помочь им вернуться и увеличить показатели.\n\nТренд #3 - Кроссплатформенная аналитика касается анализа игр, которые выходят на нескольких платформах.\n\nВ тренде #4 - Сопоставление с рынком (бенчмаркинг) говорится о том, как аналитические системы могут помочь разработчикам сравнивать свои показатели с другими играми в их жанре.\n\nПятый тренд - Простота и обучение - подчеркивает важность простоты интеграции и понимания аналитики для инди-разработчиков.\n\nНаконец, шестой и последний тренд - Аналитика с человеческим лицом - говорит о том, что аналитические системы станут более дружественными и обучающими для разработчиков.","summary":"Статья обсуждает тренды в развитии аналитических систем для приложений и игр. Перечислив тренд #1 - Предиктивная аналитика, авторы объясняют, что аналитические системы должны предсказывать значения метрик и поведение пользователей.\n\nДалее они описывают тренд #2 - Коммуникация с игроком, где говорится о том, как аналитика может использоваться для отправки push-уведомлений пользователям, чтобы помочь им вернуться и увеличить показатели.\n\nТренд #3 - Кроссплатформенная аналитика касается анализа игр, которые выходят на нескольких платформах.\n\nВ тренде #4 - Сопоставление с рынком (бенчмаркинг) говорится о том, как аналитические системы могут помочь разработчикам сравнивать свои показатели с другими играми в их жанре.\n\nПятый тренд - Простота и обучение - подчеркивает важность простоты интеграции и понимания аналитики для инди-разработчиков.\n\nНаконец, шестой и последний тренд - Аналитика с человеческим лицом - говорит о том, что аналитические системы станут более дружественными и обучающими для разработчиков."}
{"Text":"Тестирование проектов Django / Хабр                                                  \n\n18-06-2011\nВ предыдущем посте мы бегло рассмотрели некоторые приемы тестирования кода на питоне. Все это применимо также и к Django-проектам, безусловно, но есть достаточное количество подводных камней и просто интересных штук, о которых я попробую рассказать. Краткое содержание поста: тестирование веб-сайтов — это сложно и непонятно юнит-тесты в django тестовая БД и как с ней бороться smoke testing покрытие кода (code coverage) Тестирование веб-сайтов Самый главный подводный айсберг тестирования Django-проектов заключается в том, что недостаточно написать тесты для питонокода. Разваливается верстка, JavaScript живет своей жизнью, веб-сервер не выдержал нагрузки и превратился в тыкву — все эти вещи выявить при помощи тестирования несколько сложнее, чем отследить неверный результат функции. Поэтому проверка работоспособности веб-сайта — это обычно сложное явление, состоящее из нескольких независимых наборов тестов, часть которых (проверка внешнего вида в различных браузерах, например) может предполагать участие оператора. При отсутствии отдела QA роль тестировщика нередко возлагают на конечного пользователя, который потом всячески ругается. Так делать неправильно. Полковник Очевидность пост сдал. Начнем же мы с (относительно) простых и понятных юнит-тестов. Юнит-тесты в Django Юнит-тесты в Django живут в модуле django.utils.unittest и являют собой расширение стандартного модуля unittest из поставки python 2.7 (unittest2). Что добавлено: Тестовый HTTP-клиент. Имитирует работу браузера, может отправлять get- и post-запросы, сохраняет cookies между вызовами. >>> from django.test.client import Client >>> c = Client() >>> response = c.post('/login/', {'username': 'admin', 'password': 'qwerty'}) >>> response.status_code 200 С тестовым клиентом связан ряд ограничений. Например, запросить можно только относительный путь, URL вида http:/ /localhost:8000/ не сработает (по понятным причинам). Расширенный набор проверок. Помимо стандартного набора, класс django.test.TestCase содержит также django-специфичные методы assert*, например: assertContains(response, text, ...) # проверяет, что в ответе сервера содержится указанный текст; assertTemplateUsed(response, template_name, ...) # проверяет, что при рендеринге страницы использовался указанный шаблон; assertRedirects(response, expected_url, ...) # проверяет, было ли перенаправление; и другие полезные вещи. Тестирование почты. Модуль django.core.mail сохраняет в переменной outbox список всех отправленных посредством send_mail() писем. Условное исключение тестов. В случае, если выбранная СУБД не поддерживает (или, наоборот, поддерживает) транзакционность, можно исключить заведомо сломанные тесты при помощи декоратора @skipUnlessDBFeature('supports_transactions') или @skipIfDBFeature('supports_transactions'). Тестирование запускается вот так: $ ./manage.py test [список приложений] По умолчанию прогоняются все тесты для всех приложений, перечисленных в INSTALLED_APPS. Пускалка (на языке оригинала — test runner) найдет юнит- и доктесты в файлах models.py и tests.py внутри каждого приложения. Чтобы импортировать доктесты из других модулей, можно использовать следующую запись: from utils import func_a, func_b __test__ = {\"func_a\": func_a, \"func_b\": func_b} Здесь func_* — функция (или другая сущность), docstring которой нас интересует. Для наблюдателя процесс тестирования выглядит следующим образом: $ ./manage.py test main Creating test database for alias 'default'... .......... Ran 10 tests in 0.790s OK Destroying test database for alias 'default'... Тестовая БД и как с ней бороться Для запуска тестов Django всегда создает новую БД, чтобы исключить вероятность уничтожения данных в рабочем окружении. Если в settings.py не указано иное, тестовая БД предваряется словом test_. Применимо к MySQL, привилегии обычно задаются как-то так: GRANT ALL PRIVILEGES ON `project`.* TO 'user'@'localhost'; GRANT ALL PRIVILEGES ON `test_project`.* TO 'user'@'localhost'; Создавать саму БД test_project при этом не нужно. Хозяйке на заметку. Все работает быстрее, если добавить в конфиг MySQL строку [mysqld] skip-sync-frm=OFF Умозрительно, что сразу после создания никаких полезных данных в БД нет. Чтобы не порождать тестовый набор данных внутри каждого теста в отдельности, можно сделать это один раз и сохранить в fixture: $ ./manage.py dumpdata > app/fixtures/test_data.json В коде: class HelloTestCase(TestCase): fixtures = ['test_data.json', 'moar_data.json'] И еще. Старайтесь использовать для разработки и тестирования ту же СУБД, что и на production-сервере. Это сделает ваш сон на 28%* спокойнее. * научно доказано, что 87.56% статистики берется с потолка. Smoke testing В среде радиолюбителей термин smoke test означает буквально следующее: подключаем к свежесобранной схеме питание и наблюдаем, в каком месте из нее пошел дым. Если дым не пошел, можно приступать к более наукообразной проверке правильности работы схемы. Описанный подход практикуют также при тестировании приложений. Применимо к Django имеет определенный смысл описывать в tests.py точки входа из URLconf, например, так: urls.py urlpatterns = patterns(None, url(r'^registration/$', registration, name='registration'), url(r'^login/$', ..., name='login'), url(r'^logout/$', logout_then_login, name='logout'), ) tests.py from django import test from django.core.urlresolvers import reverse __test__ = {\"urls\": \"\"\" >>> c = test.Client() >>> c.get(reverse('registration')).status_code 200 >>> c.get(reverse('login')).status_code 200 >>> c.get(reverse('logout')).status_code 302 \"\"\"} Безусловно, такая проверка не заменит функционального тестирования регистрации и логина. Полковник Очевидность пост принял. Покрытие кода (code coverage) Покрытие кода — это метрика, показывающая, какой объем исходного кода был протестирован относительно всего объема полезного исходного кода в приложении. Низкое покрытие кода указывает на отсутствие тестов. Хозяйке на заметку-2. Высокое покрытие кода не говорит об отсутствии ошибок (ни в коде, ни в тестах), это вымысел. Для измерения покрытия кода на питоне существует coverage.py. Гугл помнит много попыток подружить coverage.py и Django, есть даже тикет #4501 (ему четыре года). И сразу ложка дегтя: с Django 1.3 (и dev-версией) ни одно готовое решение для code coverage, похоже, не работает (поправьте меня, если это не так). Что, впрочем, не помешает нам запустить coverage.py руками. $ coverage run --source=main,users manage.py test main users $ coverage html # генерация отчета Перечислим только интересующие нас модули (ключ --source); если не указать, там будет в том числе django, mysqldb и половина стандартной поставки питона. После этого в папке htmlcov (путь по умолчанию) можно наблюдать детальный отчет по каждой строке кода, покрытие по модулям и суммарное по проекту. В следующем выпуске: статический анализ как превентивная мера, тестирование верстки и JS, нагрузочное тестирование.\n","metric":0.5869140625,"metric_input":"Текст статьи:\nТестирование проектов Django / Хабр                                                  \n\n18-06-2011\nВ предыдущем посте мы бегло рассмотрели некоторые приемы тестирования кода на питоне. Все это применимо также и к Django-проектам, безусловно, но есть достаточное количество подводных камней и просто интересных штук, о которых я попробую рассказать. Краткое содержание поста: тестирование веб-сайтов — это сложно и непонятно юнит-тесты в django тестовая БД и как с ней бороться smoke testing покрытие кода (code coverage) Тестирование веб-сайтов Самый главный подводный айсберг тестирования Django-проектов заключается в том, что недостаточно написать тесты для питонокода. Разваливается верстка, JavaScript живет своей жизнью, веб-сервер не выдержал нагрузки и превратился в тыкву — все эти вещи выявить при помощи тестирования несколько сложнее, чем отследить неверный результат функции. Поэтому проверка работоспособности веб-сайта — это обычно сложное явление, состоящее из нескольких независимых наборов тестов, часть которых (проверка внешнего вида в различных браузерах, например) может предполагать участие оператора. При отсутствии отдела QA роль тестировщика нередко возлагают на конечного пользователя, который потом всячески ругается. Так делать неправильно. Полковник Очевидность пост сдал. Начнем же мы с (относительно) простых и понятных юнит-тестов. Юнит-тесты в Django Юнит-тесты в Django живут в модуле django.utils.unittest и являют собой расширение стандартного модуля unittest из поставки python 2.7 (unittest2). Что добавлено: Тестовый HTTP-клиент. Имитирует работу браузера, может отправлять get- и post-запросы, сохраняет cookies между вызовами. >>> from django.test.client import Client >>> c = Client() >>> response = c.post('/login/', {'username': 'admin', 'password': 'qwerty'}) >>> response.status_code 200 С тестовым клиентом связан ряд ограничений. Например, запросить можно только относительный путь, URL вида http:/ /localhost:8000/ не сработает (по понятным причинам). Расширенный набор проверок. Помимо стандартного набора, класс django.test.TestCase содержит также django-специфичные методы assert*, например: assertContains(response, text, ...) # проверяет, что в ответе сервера содержится указанный текст; assertTemplateUsed(response, template_name, ...) # проверяет, что при рендеринге страницы использовался указанный шаблон; assertRedirects(response, expected_url, ...) # проверяет, было ли перенаправление; и другие полезные вещи. Тестирование почты. Модуль django.core.mail сохраняет в переменной outbox список всех отправленных посредством send_mail() писем. Условное исключение тестов. В случае, если выбранная СУБД не поддерживает (или, наоборот, поддерживает) транзакционность, можно исключить заведомо сломанные тесты при помощи декоратора @skipUnlessDBFeature('supports_transactions') или @skipIfDBFeature('supports_transactions'). Тестирование запускается вот так: $ ./manage.py test [список приложений] По умолчанию прогоняются все тесты для всех приложений, перечисленных в INSTALLED_APPS. Пускалка (на языке оригинала — test runner) найдет юнит- и доктесты в файлах models.py и tests.py внутри каждого приложения. Чтобы импортировать доктесты из других модулей, можно использовать следующую запись: from utils import func_a, func_b __test__ = {\"func_a\": func_a, \"func_b\": func_b} Здесь func_* — функция (или другая сущность), docstring которой нас интересует. Для наблюдателя процесс тестирования выглядит следующим образом: $ ./manage.py test main Creating test database for alias 'default'... .......... Ran 10 tests in 0.790s OK Destroying test database for alias 'default'... Тестовая БД и как с ней бороться Для запуска тестов Django всегда создает новую БД, чтобы исключить вероятность уничтожения данных в рабочем окружении. Если в settings.py не указано иное, тестовая БД предваряется словом test_. Применимо к MySQL, привилегии обычно задаются как-то так: GRANT ALL PRIVILEGES ON `project`.* TO 'user'@'localhost'; GRANT ALL PRIVILEGES ON `test_project`.* TO 'user'@'localhost'; Создавать саму БД test_project при этом не нужно. Хозяйке на заметку. Все работает быстрее, если добавить в конфиг MySQL строку [mysqld] skip-sync-frm=OFF Умозрительно, что сразу после создания никаких полезных данных в БД нет. Чтобы не порождать тестовый набор данных внутри каждого теста в отдельности, можно сделать это один раз и сохранить в fixture: $ ./manage.py dumpdata > app/fixtures/test_data.json В коде: class HelloTestCase(TestCase): fixtures = ['test_data.json', 'moar_data.json'] И еще. Старайтесь использовать для разработки и тестирования ту же СУБД, что и на production-сервере. Это сделает ваш сон на 28%* спокойнее. * научно доказано, что 87.56% статистики берется с потолка. Smoke testing В среде радиолюбителей термин smoke test означает буквально следующее: подключаем к свежесобранной схеме питание и наблюдаем, в каком месте из нее пошел дым. Если дым не пошел, можно приступать к более наукообразной проверке правильности работы схемы. Описанный подход практикуют также при тестировании приложений. Применимо к Django имеет определенный смысл описывать в tests.py точки входа из URLconf, например, так: urls.py urlpatterns = patterns(None, url(r'^registration/$', registration, name='registration'), url(r'^login/$', ..., name='login'), url(r'^logout/$', logout_then_login, name='logout'), ) tests.py from django import test from django.core.urlresolvers import reverse __test__ = {\"urls\": \"\"\" >>> c = test.Client() >>> c.get(reverse('registration')).status_code 200 >>> c.get(reverse('login')).status_code 200 >>> c.get(reverse('logout')).status_code 302 \"\"\"} Безусловно, такая проверка не заменит функционального тестирования регистрации и логина. Полковник Очевидность пост принял. Покрытие кода (code coverage) Покрытие кода — это метрика, показывающая, какой объем исходного кода был протестирован относительно всего объема полезного исходного кода в приложении. Низкое покрытие кода указывает на отсутствие тестов. Хозяйке на заметку-2. Высокое покрытие кода не говорит об отсутствии ошибок (ни в коде, ни в тестах), это вымысел. Для измерения покрытия кода на питоне существует coverage.py. Гугл помнит много попыток подружить coverage.py и Django, есть даже тикет #4501 (ему четыре года). И сразу ложка дегтя: с Django 1.3 (и dev-версией) ни одно готовое решение для code coverage, похоже, не работает (поправьте меня, если это не так). Что, впрочем, не помешает нам запустить coverage.py руками. $ coverage run --source=main,users manage.py test main users $ coverage html # генерация отчета Перечислим только интересующие нас модули (ключ --source); если не указать, там будет в том числе django, mysqldb и половина стандартной поставки питона. После этого в папке htmlcov (путь по умолчанию) можно наблюдать детальный отчет по каждой строке кода, покрытие по модулям и суммарное по проекту. В следующем выпуске: статический анализ как превентивная мера, тестирование верстки и JS, нагрузочное тестирование.\n\n\nКраткое содержание:\nТестирование веб-сайтов на Django-проектах может быть сложным из-за необходимости проверки работоспособности как питонокода, так и внешнего вида, а также взаимодействия с базой данных и веб-сервером.\n\nДля тестирования питонокода можно использовать юнит-тесты, которые имитируют работу браузера и отправляют HTTP-запросы.\n\nТестовая БД создается при запуске тестов, чтобы исключить повреждение основной БД.\n\nТакже можно использовать smoke testing для проверки основных точек входа в приложение.\n\nПокрытие кода (code coverage) - метрика, показывающая, какой объем кода был протестирован. Однако, высокое покрытие не гарантирует отсутствия ошибок.\n\nВ следующем выпуске статьи будут рассмотрены статический анализ, тестирование верстки и нагрузочное тестирование Django-проектов.","summary":"Тестирование веб-сайтов на Django-проектах может быть сложным из-за необходимости проверки работоспособности как питонокода, так и внешнего вида, а также взаимодействия с базой данных и веб-сервером.\n\nДля тестирования питонокода можно использовать юнит-тесты, которые имитируют работу браузера и отправляют HTTP-запросы.\n\nТестовая БД создается при запуске тестов, чтобы исключить повреждение основной БД.\n\nТакже можно использовать smoke testing для проверки основных точек входа в приложение.\n\nПокрытие кода (code coverage) - метрика, показывающая, какой объем кода был протестирован. Однако, высокое покрытие не гарантирует отсутствия ошибок.\n\nВ следующем выпуске статьи будут рассмотрены статический анализ, тестирование верстки и нагрузочное тестирование Django-проектов."}
{"Text":"История о том, как невидимая рука рынка перевела меня на удаленку / Habr               \n\nКогда за плечами 10 лет сисадминской рутины и душа требует перемен, снова приходится полагаться на невидимую руку рынка. Очередное собеседование по телефону с рекрутером. Стандартный поток вопросов, пара из них на английском языке. И опять «мы вам перезвоним». Как ни странно, в этот раз перезвонили. Техническое собеседование. Подробно рассказал о текущих обязанностях и функционале.\n\nЕще более удивительно – перезвонили и в третий раз, предложили переехать в другой город, даже два на выбор. Хотя в вакансии была указана «возможность удаленной работы». Но что ж, можно и переехать, вопрос как обычно в мотивации. На этот раз не пришлось тратить много времени на размышления. Так как вслед за переездом мне поступило необычное (для меня) предложение – поехать на месяц в другой город, поработать там и потом вернуться к себе на удаленную работу. Челлендж был принят.\n\nЭто становилось по-настоящему интересным. Сменить на время обстановку, посмотреть другой город, а потом уже работать по удаленке — в этом было ощущение перспективы. К тому же путешествие обещало быть бесплатным: сказали, что оформят его как командировку.\n\nДолго и мучительно тянулись дни до оффера, параллельно пришлось отменить несколько довольно интересных предложений (естественно, с мучениями по поводу «а если там кинут»), и вот наконец день «Д» настал.\n\nСразу после оффера я почувствовал мощь невидимой руки, когда она хочет и может делать большие дела. Билеты – «не беспокойтесь». Жилье – «не беспокойтесь». Проезд: «вот вам схема, вот карта, вот расписание служебного транспорта».\n\nНевидимая рука понесла меня в аэропорт, потом в снятую компанией квартиру. Новый город, новое всё, гугл-карты, оформление на работу – ощущение нереальности всего этого не оставляло меня почти неделю. Постоянно ожидался какой-то подвох, типа скажут «ну, пошутили и хватит».\n\nБесплатный транспорт до работы, опенспейс — все это создавало ощущение чего-то европейского и непривычного. Опенспейс для меня лично скорее минус (как-то некомфортно сидеть в маленькой, по плечи, коробке), но с учетом того, что все это будет длиться не дольше месяца, не так уж и важно.\n\nКакова бы ни была внешняя сторона, но гораздо важнее, что у компании внутри. «Сложно» — это первое впечатление. Сложная структура управления, сложные регламенты, дикое количество инструкций, процессов, информации. В первые дни просто было ощущение перегрузки (и это при том, что я очень долгое время провел в академической среде и обработка информации для меня- вполне обыденный процесс). Для новичков дело непривычное, но со временем стало проще. Тем более тут есть грамотный вводный курс для новых сотрудников, где информация подается дозированно, в виде небольших заданий, в течение примерно трех месяцев.\n\nВторое впечатление — очень серьезное отношение к системе информационной безопасности. В отличие от большинства российских компаний, здесь она не просто внедрена, но и работает. Я примерно представляю, какими усилиями этого добились, и это действительно впечатляет. Серьезное внимание уделено непрерывности бизнеса – подробно расписаны процессы, как и что делать в случае отключения электроэнергии, интернет, аварий.\n\nПроцесс перехода на удаленку был поэтапным и постепенным, но однозначно радует его оперативность. Месяц работы оффлайн оказался полезным и позволил не только ознакомиться со структурой компании, внутренними правилами, ожиданиями коллег и руководства, но и плавно перейти на удаленный формат, поскольку в компании уже сложился действующий алгоритм такого перехода.\n\nКак удаленщик, я по максимуму изучил все, что относится к этой теме. Заранее (как и любой админ, наученный горьким опытом) проверил инструкции на практике. По возвращению домой они уже пару раз меня выручали.\n\nТакже касательно безопасности, как удаленщику, мне пришлось продумать и заполнить планы действия на случай отключения электроэнергии, интернета и даже серьезных аварий в квартире.\n\nСреди прочего при переходе на удаленку работодатель задает ряд вопросов: есть ли у вас комфортное помещение для работы, есть ли у вас огнетушитель, если нет, когда вы думаете его приобрести? То же самое и с аптечкой первой помощи. Не скажу однозначно, но мне показалось это странноватым, но тем не менее терпимым.\n\n#### Теперь непосредственно о процессе удаленной работы ####\n\nСперва пришлось столкнуться с рядом непривычных ощущений. Очень странно никуда не ехать утром, сначала это довольно сильно расслабляет (проспать работу почти невозможно). Потом появляется ощущение какого-то бесконечного больничного, когда вроде ты и дома, а вроде как, и не особо свободен. Кроме того, вечером поначалу нет ощущения «возврата домой». Постепенно с этим всем свыкаешься, кое-что перестаешь замечать, но кое-что остается. В целом дома находиться (особенно на выходных) становится труднее, хочется «хоть куда, но подальше отсюда».\n\nТакже не получается побороть ощущение оторванности (в коллективе есть всегда какие-то разговоры, слухи, взаимоотношения, пересуды). А тут ты «на отшибе», общаешься по формальным поводам и кажется, что кое-что важное проходит мимо. Иногда очень тянет расслабиться, и нужно жестко напоминать себе, что ты на работе, хоть офис и не видишь.\n\nА, ну и мне пришлось работать в другом часовом поясе, что тоже добавляет поначалу неудобств. Но это легко лечится установкой часов на рабочем ПК на часовой пояс офиса, я привык за пару дней.\n\nНо плюсов, естественно, тоже немало, они начинают ощущаться, когда (и если) удастся побороть вышеописанное. Экономится огромное количество времени (дорога, обед), денег на питание, не надо париться об одежде на каждый день (а это тоже экономия и времени и денег), очень тихо и спокойно после опенспейса.\n\nИсключительно сидячая работа (это же норма для хорошего сисадмина, да?) для здоровья не особо полезна, но как по мне, так это лучше, чем в любую погоду лазить по ржавым железкам без страховки, что было привычным делом на прошлом месте работы.\n\nВ общем период адаптации, как, впрочем, и всегда, не оказался линейным. Пришлось лицом к лицу столкнуться со спецификой удаленки. Прочувствовать все pro и contra. Выводы пока делать рановато. Но одно могу сказать точно: электричество и интернет становятся очень важными ресурсами, поскольку работа должна продолжаться, а отвечаешь за это лично ТЫ. Пару раз уже были отключения, они реально напрягли, несмотря на изученные и проверенные раньше аварийные планы.\n\nНо самое интересное, как всегда — впереди: что сделает дальше невидимая рука?","metric":0.61572265630000000414,"metric_input":"Текст статьи:\nИстория о том, как невидимая рука рынка перевела меня на удаленку / Habr               \n\nКогда за плечами 10 лет сисадминской рутины и душа требует перемен, снова приходится полагаться на невидимую руку рынка. Очередное собеседование по телефону с рекрутером. Стандартный поток вопросов, пара из них на английском языке. И опять «мы вам перезвоним». Как ни странно, в этот раз перезвонили. Техническое собеседование. Подробно рассказал о текущих обязанностях и функционале.\n\nЕще более удивительно – перезвонили и в третий раз, предложили переехать в другой город, даже два на выбор. Хотя в вакансии была указана «возможность удаленной работы». Но что ж, можно и переехать, вопрос как обычно в мотивации. На этот раз не пришлось тратить много времени на размышления. Так как вслед за переездом мне поступило необычное (для меня) предложение – поехать на месяц в другой город, поработать там и потом вернуться к себе на удаленную работу. Челлендж был принят.\n\nЭто становилось по-настоящему интересным. Сменить на время обстановку, посмотреть другой город, а потом уже работать по удаленке — в этом было ощущение перспективы. К тому же путешествие обещало быть бесплатным: сказали, что оформят его как командировку.\n\nДолго и мучительно тянулись дни до оффера, параллельно пришлось отменить несколько довольно интересных предложений (естественно, с мучениями по поводу «а если там кинут»), и вот наконец день «Д» настал.\n\nСразу после оффера я почувствовал мощь невидимой руки, когда она хочет и может делать большие дела. Билеты – «не беспокойтесь». Жилье – «не беспокойтесь». Проезд: «вот вам схема, вот карта, вот расписание служебного транспорта».\n\nНевидимая рука понесла меня в аэропорт, потом в снятую компанией квартиру. Новый город, новое всё, гугл-карты, оформление на работу – ощущение нереальности всего этого не оставляло меня почти неделю. Постоянно ожидался какой-то подвох, типа скажут «ну, пошутили и хватит».\n\nБесплатный транспорт до работы, опенспейс — все это создавало ощущение чего-то европейского и непривычного. Опенспейс для меня лично скорее минус (как-то некомфортно сидеть в маленькой, по плечи, коробке), но с учетом того, что все это будет длиться не дольше месяца, не так уж и важно.\n\nКакова бы ни была внешняя сторона, но гораздо важнее, что у компании внутри. «Сложно» — это первое впечатление. Сложная структура управления, сложные регламенты, дикое количество инструкций, процессов, информации. В первые дни просто было ощущение перегрузки (и это при том, что я очень долгое время провел в академической среде и обработка информации для меня- вполне обыденный процесс). Для новичков дело непривычное, но со временем стало проще. Тем более тут есть грамотный вводный курс для новых сотрудников, где информация подается дозированно, в виде небольших заданий, в течение примерно трех месяцев.\n\nВторое впечатление — очень серьезное отношение к системе информационной безопасности. В отличие от большинства российских компаний, здесь она не просто внедрена, но и работает. Я примерно представляю, какими усилиями этого добились, и это действительно впечатляет. Серьезное внимание уделено непрерывности бизнеса – подробно расписаны процессы, как и что делать в случае отключения электроэнергии, интернет, аварий.\n\nПроцесс перехода на удаленку был поэтапным и постепенным, но однозначно радует его оперативность. Месяц работы оффлайн оказался полезным и позволил не только ознакомиться со структурой компании, внутренними правилами, ожиданиями коллег и руководства, но и плавно перейти на удаленный формат, поскольку в компании уже сложился действующий алгоритм такого перехода.\n\nКак удаленщик, я по максимуму изучил все, что относится к этой теме. Заранее (как и любой админ, наученный горьким опытом) проверил инструкции на практике. По возвращению домой они уже пару раз меня выручали.\n\nТакже касательно безопасности, как удаленщику, мне пришлось продумать и заполнить планы действия на случай отключения электроэнергии, интернета и даже серьезных аварий в квартире.\n\nСреди прочего при переходе на удаленку работодатель задает ряд вопросов: есть ли у вас комфортное помещение для работы, есть ли у вас огнетушитель, если нет, когда вы думаете его приобрести? То же самое и с аптечкой первой помощи. Не скажу однозначно, но мне показалось это странноватым, но тем не менее терпимым.\n\n#### Теперь непосредственно о процессе удаленной работы ####\n\nСперва пришлось столкнуться с рядом непривычных ощущений. Очень странно никуда не ехать утром, сначала это довольно сильно расслабляет (проспать работу почти невозможно). Потом появляется ощущение какого-то бесконечного больничного, когда вроде ты и дома, а вроде как, и не особо свободен. Кроме того, вечером поначалу нет ощущения «возврата домой». Постепенно с этим всем свыкаешься, кое-что перестаешь замечать, но кое-что остается. В целом дома находиться (особенно на выходных) становится труднее, хочется «хоть куда, но подальше отсюда».\n\nТакже не получается побороть ощущение оторванности (в коллективе есть всегда какие-то разговоры, слухи, взаимоотношения, пересуды). А тут ты «на отшибе», общаешься по формальным поводам и кажется, что кое-что важное проходит мимо. Иногда очень тянет расслабиться, и нужно жестко напоминать себе, что ты на работе, хоть офис и не видишь.\n\nА, ну и мне пришлось работать в другом часовом поясе, что тоже добавляет поначалу неудобств. Но это легко лечится установкой часов на рабочем ПК на часовой пояс офиса, я привык за пару дней.\n\nНо плюсов, естественно, тоже немало, они начинают ощущаться, когда (и если) удастся побороть вышеописанное. Экономится огромное количество времени (дорога, обед), денег на питание, не надо париться об одежде на каждый день (а это тоже экономия и времени и денег), очень тихо и спокойно после опенспейса.\n\nИсключительно сидячая работа (это же норма для хорошего сисадмина, да?) для здоровья не особо полезна, но как по мне, так это лучше, чем в любую погоду лазить по ржавым железкам без страховки, что было привычным делом на прошлом месте работы.\n\nВ общем период адаптации, как, впрочем, и всегда, не оказался линейным. Пришлось лицом к лицу столкнуться со спецификой удаленки. Прочувствовать все pro и contra. Выводы пока делать рановато. Но одно могу сказать точно: электричество и интернет становятся очень важными ресурсами, поскольку работа должна продолжаться, а отвечаешь за это лично ТЫ. Пару раз уже были отключения, они реально напрягли, несмотря на изученные и проверенные раньше аварийные планы.\n\nНо самое интересное, как всегда — впереди: что сделает дальше невидимая рука?\n\nКраткое содержание:\nВ статье рассказывается о том, как автор перешел на удаленную работу после телефонного собеседования с рекрутером. Автор подробно рассказал о своих обязанностях и функционале, а затем ему предложили переехать в другой город для работы. Он согласился на предложение, и ему предложили поработать в другом городе на месяц, после чего вернуться к удаленной работе. Автор чувствовал мощь невидимой руки, которая помогала ему в этом процессе. Он также описал свои впечатления от новой работы, включая сложную структуру управления, серьезное отношение к информационной безопасности и процесс перехода на удаленную работу. Автор также поделился своими впечатлениями от удаленной работы, включая непривычные ощущения и плюсы и минусы такого формата работы.","summary":"В статье рассказывается о том, как автор перешел на удаленную работу после телефонного собеседования с рекрутером. Автор подробно рассказал о своих обязанностях и функционале, а затем ему предложили переехать в другой город для работы. Он согласился на предложение, и ему предложили поработать в другом городе на месяц, после чего вернуться к удаленной работе. Автор чувствовал мощь невидимой руки, которая помогала ему в этом процессе. Он также описал свои впечатления от новой работы, включая сложную структуру управления, серьезное отношение к информационной безопасности и процесс перехода на удаленную работу. Автор также поделился своими впечатлениями от удаленной работы, включая непривычные ощущения и плюсы и минусы такого формата работы."}
{"Text":"Unreal Engine 4 — шейдер горения / Хабр               \n\n05-01-2019\nЭто небольшой шейдер, который появился, когда я думал о различных методах применения flowmap. Существуют много эффектов перехода/растворения, но большинство из них выглядит довольно статично, так как они используют статические текстуры. Данный шейдер далек от совершенства, но плавное движение делает его визуально привлекательным как плавный эффект горения. Сам шейдер чрезвычайно прост, ниже я опишу процесс его создания. Как и с большинством моих шейдеров, я обернул его в функцию материала, чтобы его можно было использовать с любыми существующими материалами. Примечание. Вы можете открыть изображения в новой вкладке, чтобы посмотреть их в более высоком разрешении. Текстуры Я использовал следующие текстуры в шейдере, первая, это простой noise, генерируемый в Substance Designer, вторая, flowmap, которую я нарисовал с помощью FlowMap Painter. Настройка FlowMap Flowmapping — это метод, с помощью которого мы используем 2D векторы, хранящиеся в виде текстуры, чтобы искажать UV текстуры в течении определенного времени. Эта анимация потом зацикливается, чтобы создать иллюзию потока. Обычно такое используют в потоках воды, чтобы создать впечатление, что вода течет вокруг камней и т.п. Но, конечно, его можно использовать и для множества других вещей. В моем шейдере я использую его для создания эффекта горения материала. Если вы хотите больше понять о flowmap, рекомендую почитать статью на Polycount и посмотреть урок от Simonschreibt, в котором он отлично объясняет работу с flowmap. Мои вариант настроек ниже Как вы можете видеть, у нас есть параметры для управления силой искажения, тайлингом текстур, а также направлением потока. Чтобы управлять эффектом, я использую выход из красного канала из карты шума, если вы посмотрите на узел Lerp, вы увидите следующий результат: SphereMask Этот шаг может быть реализован несколькими способами, в конце концов, все, что ему нужно, это значение масштаба градиента, чтобы управлять нашим эффектом. В этом случае я использую SphereMask, управляемую положением blueprint’а, чтобы перемещать нашу маску. Двигая blueprint вокруг, я могу влиять на то, какие области сжигаются, а тaк же анимировать радиус сферы, чтобы создавать эффект сгорания. Если вы хотите узнать, как настроить набор параметров материала для передачи положения BP в шейдер, рекомендую почитать о Wormhole шейдере, где я подробно это описываю. Создать SphereMask легко, так как UE4 уже имеет встроенную функцию В этом случае радиус указан в мировых координатах (World Space), чтобы это увидеть, попробуйте изменить значение радиуса. По умолчанию расположение SphereMask равно {0, 0, 0}, поэтому вы заметите, что сфера появляется в центре меша. Следующим шагом будет вычитание результата flowmap шума из SphereMask, чтобы мы получили рисунок, подобный горению, по мере масштабирования маски. Обратите внимание, что вы можете настроить параметр Hardness у SphereMask, чтобы контролировать величину разброса при переходе Hardness: 0.2 Hardness: 0.5 Теперь мы можем взять результат и умножить его на Opacity_Mask и вернуть в наш канал Opacity материала, чтобы получить итоговую маску. Также вы можете сделать такой эффект без настройки flowmap, используя только статическую текстуру. Разница лишь в том, что используя flowmap вы добьетесь более плавной анимации. Ниже вы можете увидеть разницу с flowmap без flowmap Edge Glow и Charring Чтобы сделать горящие края и получить обугленный черноватый контур, мы используем технику, которую я применял во многих своих шейдерах, где мы выставляем градиент маски в 0, и создаем маску края, используя функцию расстояния. Ниже вы можете увидеть эти настройки. Единственное, что следует здесь отметить, это то, что мы выбираем 0.45 для контура свечения и 0.5 для контура обугливания, чтобы они были смещены относительно друг друга, и обугливание появилось вокруг свечения. Затем мы создаем параметр из ширины, чтобы контролировать разброс по мере необходимости. Если вы поставите превью на каждую из этих нод, вы увидите следующее EmberGlow Charring Теперь берем результат из EmberGlow и умножаем на цвет пламени, далее добавляем его к цвету Emissive и возвращаем на вход нашего материала. В качестве альтернативы вы можете использовать результат из секции EmberGlow в качестве Diffuse данных для отображения цветов на основе градиента. Данная техника называется градиентным отображением, и в UE4 теперь есть функция CurveAtlass, которая позволяет вам определять кривые цвета и получать к ним доступ внутри шейдера. Учтите, что это экспериментальная функция, и я столкнулся с несколькими вылетами в попытках ее использования в моих шейдерах, так что советую пока воздержаться от ее применения. Затем мы берем результат из секции Charring и умножаем его на Basecolor из входных данных функции, чтобы получить значение Final Basecolor. Теперь вы можете видеть как чернеют края, перед тем как начать гореть На этом мы закончили. Теперь самая крутая часть — мы можем перемещать наш blueprint вокруг, чтобы изменять область горения. К примеру в игре мы можем использовать расположение факела игрока, чтобы управлять этим эффектом. В результате это будет выглядеть так, как будто он что-то сжигает. И мы можем управлять параметром Hardness у SphereMask и радиусом, чтобы получить нужный нам вид. Вот так это будет выглядеть с разными настройками. Кроме того, вы можете анимировать радиус, чтобы это выглядело так, как будто он сгорает со временем. Пример проекта с данным шейдером можете взять из репозитория github\n","metric":0.48168945309999999793,"metric_input":"Текст статьи:\nUnreal Engine 4 — шейдер горения / Хабр               \n\n05-01-2019\nЭто небольшой шейдер, который появился, когда я думал о различных методах применения flowmap. Существуют много эффектов перехода/растворения, но большинство из них выглядит довольно статично, так как они используют статические текстуры. Данный шейдер далек от совершенства, но плавное движение делает его визуально привлекательным как плавный эффект горения. Сам шейдер чрезвычайно прост, ниже я опишу процесс его создания. Как и с большинством моих шейдеров, я обернул его в функцию материала, чтобы его можно было использовать с любыми существующими материалами. Примечание. Вы можете открыть изображения в новой вкладке, чтобы посмотреть их в более высоком разрешении. Текстуры Я использовал следующие текстуры в шейдере, первая, это простой noise, генерируемый в Substance Designer, вторая, flowmap, которую я нарисовал с помощью FlowMap Painter. Настройка FlowMap Flowmapping — это метод, с помощью которого мы используем 2D векторы, хранящиеся в виде текстуры, чтобы искажать UV текстуры в течении определенного времени. Эта анимация потом зацикливается, чтобы создать иллюзию потока. Обычно такое используют в потоках воды, чтобы создать впечатление, что вода течет вокруг камней и т.п. Но, конечно, его можно использовать и для множества других вещей. В моем шейдере я использую его для создания эффекта горения материала. Если вы хотите больше понять о flowmap, рекомендую почитать статью на Polycount и посмотреть урок от Simonschreibt, в котором он отлично объясняет работу с flowmap. Мои вариант настроек ниже Как вы можете видеть, у нас есть параметры для управления силой искажения, тайлингом текстур, а также направлением потока. Чтобы управлять эффектом, я использую выход из красного канала из карты шума, если вы посмотрите на узел Lerp, вы увидите следующий результат: SphereMask Этот шаг может быть реализован несколькими способами, в конце концов, все, что ему нужно, это значение масштаба градиента, чтобы управлять нашим эффектом. В этом случае я использую SphereMask, управляемую положением blueprint’а, чтобы перемещать нашу маску. Двигая blueprint вокруг, я могу влиять на то, какие области сжигаются, а тaк же анимировать радиус сферы, чтобы создавать эффект сгорания. Если вы хотите узнать, как настроить набор параметров материала для передачи положения BP в шейдер, рекомендую почитать о Wormhole шейдере, где я подробно это описываю. Создать SphereMask легко, так как UE4 уже имеет встроенную функцию В этом случае радиус указан в мировых координатах (World Space), чтобы это увидеть, попробуйте изменить значение радиуса. По умолчанию расположение SphereMask равно {0, 0, 0}, поэтому вы заметите, что сфера появляется в центре меша. Следующим шагом будет вычитание результата flowmap шума из SphereMask, чтобы мы получили рисунок, подобный горению, по мере масштабирования маски. Обратите внимание, что вы можете настроить параметр Hardness у SphereMask, чтобы контролировать величину разброса при переходе Hardness: 0.2 Hardness: 0.5 Теперь мы можем взять результат и умножить его на Opacity_Mask и вернуть в наш канал Opacity материала, чтобы получить итоговую маску. Также вы можете сделать такой эффект без настройки flowmap, используя только статическую текстуру. Разница лишь в том, что используя flowmap вы добьетесь более плавной анимации. Ниже вы можете увидеть разницу с flowmap без flowmap Edge Glow и Charring Чтобы сделать горящие края и получить обугленный черноватый контур, мы используем технику, которую я применял во многих своих шейдерах, где мы выставляем градиент маски в 0, и создаем маску края, используя функцию расстояния. Ниже вы можете увидеть эти настройки. Единственное, что следует здесь отметить, это то, что мы выбираем 0.45 для контура свечения и 0.5 для контура обугливания, чтобы они были смещены относительно друг друга, и обугливание появилось вокруг свечения. Затем мы создаем параметр из ширины, чтобы контролировать разброс по мере необходимости. Если вы поставите превью на каждую из этих нод, вы увидите следующее EmberGlow Charring Теперь берем результат из EmberGlow и умножаем на цвет пламени, далее добавляем его к цвету Emissive и возвращаем на вход нашего материала. В качестве альтернативы вы можете использовать результат из секции EmberGlow в качестве Diffuse данных для отображения цветов на основе градиента. Данная техника называется градиентным отображением, и в UE4 теперь есть функция CurveAtlass, которая позволяет вам определять кривые цвета и получать к ним доступ внутри шейдера. Учтите, что это экспериментальная функция, и я столкнулся с несколькими вылетами в попытках ее использования в моих шейдерах, так что советую пока воздержаться от ее применения. Затем мы берем результат из секции Charring и умножаем его на Basecolor из входных данных функции, чтобы получить значение Final Basecolor. Теперь вы можете видеть как чернеют края, перед тем как начать гореть На этом мы закончили. Теперь самая крутая часть — мы можем перемещать наш blueprint вокруг, чтобы изменять область горения. К примеру в игре мы можем использовать расположение факела игрока, чтобы управлять этим эффектом. В результате это будет выглядеть так, как будто он что-то сжигает. И мы можем управлять параметром Hardness у SphereMask и радиусом, чтобы получить нужный нам вид. Вот так это будет выглядеть с разными настройками. Кроме того, вы можете анимировать радиус, чтобы это выглядело так, как будто он сгорает со временем. Пример проекта с данным шейдером можете взять из репозитория github\n\n\nКраткое содержание:\nСтатья описывает создание шейдера для Unreal Engine 4, который имитирует эффект горения. Шейдер использует технику flowmap для создания плавного движения и использует SphereMask для управления областью горения. Также используются градиенты для создания эффекта горящих краев и обугливания. Шейдер может быть использован для создания реалистичных эффектов в играх.","summary":"Статья описывает создание шейдера для Unreal Engine 4, который имитирует эффект горения. Шейдер использует технику flowmap для создания плавного движения и использует SphereMask для управления областью горения. Также используются градиенты для создания эффекта горящих краев и обугливания. Шейдер может быть использован для создания реалистичных эффектов в играх."}
{"Text":"Как повысить отказоустойчивость биллинга: Опыт «Гидры» / Хабр                                                         \n\nМы уже писали о том,\nкакие проблемы\nмогут возникнуть у компании при самостоятельной разработке сложных систем. В комментариях читатели просили нас подробнее рассказать о технической составляющей нашего проекта — биллинга для операторов связи «\nГидра\n».\n\nВыполняя эту просьбу, сегодня мы расскажем о том, как работали над повышением отказоустойчивости системы.\n\n#### Компоненты биллинга: где надежность важнее всего ####\n\nБиллинг — это система, которая должна работать в режиме 24x7 все 365 дней в году. Поэтому резервирование всех его компонентов просто необходимо. Повнимательнее рассмотрим, из чего состоит система биллинга.\n\n#### Данные ####\n\nИнформация о потребленных абонентами услугах, балансы лицевых счетов, детализация платежей и списаний. Это самая ценная информация бизнеса, а значит, спасать ее нужно в первую очередь.\n\n#### Ядро ####\n\nЯдро — часть системы, в которой ведутся все операции с данными. В «Гидре» оно интегрировано с данными — бизнес-логика находится прямо в СУБД в виде хранимых процедур. Модифицировать данные напрямую внешним приложениям запрещено, это можно делать только через API. Поэтому чтобы данные были доступны, ядро должно сохранять работоспособность.\n\n#### AAA-сервер (Authentication, Authorization, Accounting) ####\n\nЭлемент, который отвечает за аутентификацию, авторизацию и учет важной информации о потребленных абонентами услугах. Сетевое оборудование взаимодействует с AAA-сервером (сервером предоставления доступа к услугам). В случае отказа ААА-сервера, как правило, абоненты сталкиваются с невозможностью открытия новых сессий, подключения новых услуг и отключения старых.\n\nКроме того, в таких ситуациях может теряться информация об уже оказанных услугах — это не только приводит к неудобствам для клиентов, но и чревато «бесплатным» оказанием услуг во время сбоя биллинга. Таким образом, ААА-сервер — это один из наиболее чувствительных компонентов всей системы.\n\n#### Платежный шлюз ####\n\nПринимает информацию о проведенных платежах из различных платежных систем. Отказ шлюза приводит к задержкам в прохождении информации об оплатах, совершенных клиентами на свои лицевые счета в биллинге.\n\nВ некоторых случаях это может негативно сказываться на качестве обслуживания — например, если у всех абонентов расчетный день наступает первого числа месяца, и сбой случился именно в этот день. Отказ платежного шлюза в это время приводит к тому, что абоненты не могут пополнить свой лицевой счет и восстановить доступ к услугам, если их отключило за неуплату.\n\nРаботоспособность платежного шлюза зависит от работоспособности ядра, поскольку в случае недоступности ядра он не сможет сообщить остальным компонентам биллинга о поступлении средств на счет. Однако даже несколько часов простоя вряд ли приведут к серьезным проблемам, поскольку практически все платежные системы автоматически проводят платежи повторно, если предыдущие запросы получали в ответ ошибку, и предпринимать что-либо помимо восстановления самого сервиса не требуется.\n\n#### Личный кабинет абонента и веб-панель управления ####\n\nПри «падении» панели управления биллингом и личного кабинета пользователей сам биллинг сохраняет работоспособность, однако сотрудники компании и ее клиенты временно теряют возможность работы с системой.\n\n#### Что и как резервировать? ####\n\nВыше мы рассмотрели основные компоненты биллинга и их важность с точки зрения обеспечения отказоустойчивости. Теперь поговорим о шагах, которые мы предприняли для повышения надежности работы системы.\n\nНачнем с описания общих подходов. Компоненты биллинга делятся на две большие категории: те, что хранят состояние и те, что не хранят. Состояние традиционно хранится в базе данных, их у нас две, при этом в нормальном режиме работы они активно обмениваются данными.\n\n#### Резервирование основной БД ####\n\nОсновная база хранит данные и ядро системы. Мы используем вырожденную\nтрехуровневую архитектуру\n— бизнес-логика (хранимые процедуры и функции в Oracle) и данные (таблицы) тесно интегрированы в СУБД, в качестве клиента выступает браузер («тонкий клиент»), а веб-интерфейс служит программной оболочкой к ядру. Так как в качестве РСУБД используется Oracle Database, то подходы к резервированию основной БД мы используем рекомендованные производителем:\n\n**Периодическое снятие резервной копии** . Самый простой путь, который, тем не менее, сопряжен с определенными сложностями — восстановление данных при его использовании может занимать длительное время, а по завершении этого процесса информация будет неактуальной. Для снятия резервных копий в Oracle возможно использование утилит expdp  — для небольших инсталляций (они просты в использовании, данные восстанавливаются долго), и универсального менеджера RMAN , который позволяет снимать полные и инкрементальные бэкапы по расписанию и разархивировать их.\n\n**Копирование redo-логов СУБД на standby-сервер** . Этот метод позволяет организовать «горячее» резервирование, при котором изменения с рабочей БД применяются на standby-сервере. В случае сбоя рабочего сервера обслуживание автоматически или вручную переводится на резервный сервер с актуальными данными.\n\nДля ускорения переключения на резервный сервер мы используем небольшую хитрость. У основного и резервного серверов есть свои IP-адреса. Над ними есть IP-алиас, который в нормальном режиме «висит» на основном сервере. В случае его отказа этот алиас можно быстро «перевесить» на резервный сервер — пользователи не заметят разницы.\n\n#### Резервирование приложений ####\n\nВсе приложения, за исключением AAA-сервера (о нем речь пойдет позже), хранят состояние в основной БД, поэтому на резервном сервере нужно держать только сами приложения нужных версий, их конфигурационные файлы, а также скрипты запуска.\n\nПриложения настроены на работу с резервной БД и в любой момент готовы принять входящий запрос, который будет обработан после перевода резервной базы данных в активный режим.\n\n#### Резервирование AAA-сервера ####\n\nAAA-сервер — наиболее чувствительный к перебоям компонент системы. Строго говоря он не относится к биллингу, а работает с ним в связке. Как только сервер доступа оказывается недоступен, абоненты замечают это незамедлительно. Допустимое время простоя — несколько минут, иначе абоненты «завалят» звонками колл-центр любого размера.\n\nКак правило, AAA-сервер размещается на отдельной физической машине поближе к абонентам, чтобы не зависеть от доступности сервера с основной БД. Это очень полезно для территориально распределенных сетей, в которых риски перерыва связи ААА-сервера с ядром высоки.\n\nСам сервер доступа резвервируется в двух режимах: автоматическом и ручном. Для автоматического резервирования необходимо иметь минимум три физических сервера (для решения проблемы разрыва сети, netsplit). Для ручного режима достаточно двух серверов, но в таком случае решение о переключении нагрузки необходимо принимать дежурному сотруднику.\n\nРасмотрим пример инсталляции с автоматическим переключением (automatic failover):\n\nКаждый AAA-сервер состоит из трех компонентов:\n\nБаза данных с профилями абонентов и данными о потребленных услугах.\n\nНаше приложение под кодовым названием HARD. Оно отвечает на HTTP-запросы, которые идут от следующего компонента.\n\nFreeRADIUS — непосредственно сервер, реализующий стандартный AAA-протокол — RADIUS. Он непосредственно общается с абонентской сетью и переводит запросы из бинарного формата в обычный HTTP+JSON для HARD.\n\nБазы данных всех AAA-серверов (это MongoDB) объединены в группу с одним основным узлом (master) и двумя подчиненными (slave). Все запросы из абонентской сети идут на один AAA-сервер, при этом необязательно и даже нежелательно, чтобы им был сервер с основной БД.\n\nПосмотрим, что случится, если что-то пойдет не так, и один из компонентов откажет:\n\nЕсли пропадает связь с ядром биллинга, то в БД сервера доступа перестают поступать обновления, однако все параметры аутентификации и авторизации для абонентов остаются доступны серверу. Если у вас был доступ в интернет до аварии, то после аварии он у вас останется, вы ничего не заметите.\n\nЕсли откажет AAA-сервер под номером 1, то сеть начнет обращаться к следующему по порядку серверу. Это стандартная схема поддержки отказоустойчивости для современных BRAS.\n\nЕсли откажет сервер 2, то перестанет быть доступен основной узел с базой данных. В этом случае через некоторое время (порядка одной минуты) среди оставшихся будет выбран новый основной узел. После этого ядро биллинга будет общаться именно с этим сервером.\n\nОтказ третьего сервера не вызовет никаких дополнительных действий.\n\nТаким образом, любые неглобальные сбои не приведут к потере доступа абонетов к услугам. Что бы ни произошло, пользователи, скорее всего, ничего не заметят.\n\n#### Заключение ####\n\nНаш проект развивается уже 8 лет, и за это время мы поучаствовали более чем в 80 проектах внедрения. Мы постоянно работаем над тем, чтобы сделать систему более надежной и удобной для клиентов.\n\nВ следующих постах мы продолжим рассказывать об архитектуре «Гидры», технологиях и подходах, которые используются при ее разработке.\n\nПодписывайтесь на\nнаш блог\n, чтобы не пропустить ничего интересного!","metric":0.64599609380000000414,"metric_input":"Текст статьи:\nКак повысить отказоустойчивость биллинга: Опыт «Гидры» / Хабр                                                         \n\nМы уже писали о том,\nкакие проблемы\nмогут возникнуть у компании при самостоятельной разработке сложных систем. В комментариях читатели просили нас подробнее рассказать о технической составляющей нашего проекта — биллинга для операторов связи «\nГидра\n».\n\nВыполняя эту просьбу, сегодня мы расскажем о том, как работали над повышением отказоустойчивости системы.\n\n#### Компоненты биллинга: где надежность важнее всего ####\n\nБиллинг — это система, которая должна работать в режиме 24x7 все 365 дней в году. Поэтому резервирование всех его компонентов просто необходимо. Повнимательнее рассмотрим, из чего состоит система биллинга.\n\n#### Данные ####\n\nИнформация о потребленных абонентами услугах, балансы лицевых счетов, детализация платежей и списаний. Это самая ценная информация бизнеса, а значит, спасать ее нужно в первую очередь.\n\n#### Ядро ####\n\nЯдро — часть системы, в которой ведутся все операции с данными. В «Гидре» оно интегрировано с данными — бизнес-логика находится прямо в СУБД в виде хранимых процедур. Модифицировать данные напрямую внешним приложениям запрещено, это можно делать только через API. Поэтому чтобы данные были доступны, ядро должно сохранять работоспособность.\n\n#### AAA-сервер (Authentication, Authorization, Accounting) ####\n\nЭлемент, который отвечает за аутентификацию, авторизацию и учет важной информации о потребленных абонентами услугах. Сетевое оборудование взаимодействует с AAA-сервером (сервером предоставления доступа к услугам). В случае отказа ААА-сервера, как правило, абоненты сталкиваются с невозможностью открытия новых сессий, подключения новых услуг и отключения старых.\n\nКроме того, в таких ситуациях может теряться информация об уже оказанных услугах — это не только приводит к неудобствам для клиентов, но и чревато «бесплатным» оказанием услуг во время сбоя биллинга. Таким образом, ААА-сервер — это один из наиболее чувствительных компонентов всей системы.\n\n#### Платежный шлюз ####\n\nПринимает информацию о проведенных платежах из различных платежных систем. Отказ шлюза приводит к задержкам в прохождении информации об оплатах, совершенных клиентами на свои лицевые счета в биллинге.\n\nВ некоторых случаях это может негативно сказываться на качестве обслуживания — например, если у всех абонентов расчетный день наступает первого числа месяца, и сбой случился именно в этот день. Отказ платежного шлюза в это время приводит к тому, что абоненты не могут пополнить свой лицевой счет и восстановить доступ к услугам, если их отключило за неуплату.\n\nРаботоспособность платежного шлюза зависит от работоспособности ядра, поскольку в случае недоступности ядра он не сможет сообщить остальным компонентам биллинга о поступлении средств на счет. Однако даже несколько часов простоя вряд ли приведут к серьезным проблемам, поскольку практически все платежные системы автоматически проводят платежи повторно, если предыдущие запросы получали в ответ ошибку, и предпринимать что-либо помимо восстановления самого сервиса не требуется.\n\n#### Личный кабинет абонента и веб-панель управления ####\n\nПри «падении» панели управления биллингом и личного кабинета пользователей сам биллинг сохраняет работоспособность, однако сотрудники компании и ее клиенты временно теряют возможность работы с системой.\n\n#### Что и как резервировать? ####\n\nВыше мы рассмотрели основные компоненты биллинга и их важность с точки зрения обеспечения отказоустойчивости. Теперь поговорим о шагах, которые мы предприняли для повышения надежности работы системы.\n\nНачнем с описания общих подходов. Компоненты биллинга делятся на две большие категории: те, что хранят состояние и те, что не хранят. Состояние традиционно хранится в базе данных, их у нас две, при этом в нормальном режиме работы они активно обмениваются данными.\n\n#### Резервирование основной БД ####\n\nОсновная база хранит данные и ядро системы. Мы используем вырожденную\nтрехуровневую архитектуру\n— бизнес-логика (хранимые процедуры и функции в Oracle) и данные (таблицы) тесно интегрированы в СУБД, в качестве клиента выступает браузер («тонкий клиент»), а веб-интерфейс служит программной оболочкой к ядру. Так как в качестве РСУБД используется Oracle Database, то подходы к резервированию основной БД мы используем рекомендованные производителем:\n\n**Периодическое снятие резервной копии** . Самый простой путь, который, тем не менее, сопряжен с определенными сложностями — восстановление данных при его использовании может занимать длительное время, а по завершении этого процесса информация будет неактуальной. Для снятия резервных копий в Oracle возможно использование утилит expdp  — для небольших инсталляций (они просты в использовании, данные восстанавливаются долго), и универсального менеджера RMAN , который позволяет снимать полные и инкрементальные бэкапы по расписанию и разархивировать их.\n\n**Копирование redo-логов СУБД на standby-сервер** . Этот метод позволяет организовать «горячее» резервирование, при котором изменения с рабочей БД применяются на standby-сервере. В случае сбоя рабочего сервера обслуживание автоматически или вручную переводится на резервный сервер с актуальными данными.\n\nДля ускорения переключения на резервный сервер мы используем небольшую хитрость. У основного и резервного серверов есть свои IP-адреса. Над ними есть IP-алиас, который в нормальном режиме «висит» на основном сервере. В случае его отказа этот алиас можно быстро «перевесить» на резервный сервер — пользователи не заметят разницы.\n\n#### Резервирование приложений ####\n\nВсе приложения, за исключением AAA-сервера (о нем речь пойдет позже), хранят состояние в основной БД, поэтому на резервном сервере нужно держать только сами приложения нужных версий, их конфигурационные файлы, а также скрипты запуска.\n\nПриложения настроены на работу с резервной БД и в любой момент готовы принять входящий запрос, который будет обработан после перевода резервной базы данных в активный режим.\n\n#### Резервирование AAA-сервера ####\n\nAAA-сервер — наиболее чувствительный к перебоям компонент системы. Строго говоря он не относится к биллингу, а работает с ним в связке. Как только сервер доступа оказывается недоступен, абоненты замечают это незамедлительно. Допустимое время простоя — несколько минут, иначе абоненты «завалят» звонками колл-центр любого размера.\n\nКак правило, AAA-сервер размещается на отдельной физической машине поближе к абонентам, чтобы не зависеть от доступности сервера с основной БД. Это очень полезно для территориально распределенных сетей, в которых риски перерыва связи ААА-сервера с ядром высоки.\n\nСам сервер доступа резвервируется в двух режимах: автоматическом и ручном. Для автоматического резервирования необходимо иметь минимум три физических сервера (для решения проблемы разрыва сети, netsplit). Для ручного режима достаточно двух серверов, но в таком случае решение о переключении нагрузки необходимо принимать дежурному сотруднику.\n\nРасмотрим пример инсталляции с автоматическим переключением (automatic failover):\n\nКаждый AAA-сервер состоит из трех компонентов:\n\nБаза данных с профилями абонентов и данными о потребленных услугах.\n\nНаше приложение под кодовым названием HARD. Оно отвечает на HTTP-запросы, которые идут от следующего компонента.\n\nFreeRADIUS — непосредственно сервер, реализующий стандартный AAA-протокол — RADIUS. Он непосредственно общается с абонентской сетью и переводит запросы из бинарного формата в обычный HTTP+JSON для HARD.\n\nБазы данных всех AAA-серверов (это MongoDB) объединены в группу с одним основным узлом (master) и двумя подчиненными (slave). Все запросы из абонентской сети идут на один AAA-сервер, при этом необязательно и даже нежелательно, чтобы им был сервер с основной БД.\n\nПосмотрим, что случится, если что-то пойдет не так, и один из компонентов откажет:\n\nЕсли пропадает связь с ядром биллинга, то в БД сервера доступа перестают поступать обновления, однако все параметры аутентификации и авторизации для абонентов остаются доступны серверу. Если у вас был доступ в интернет до аварии, то после аварии он у вас останется, вы ничего не заметите.\n\nЕсли откажет AAA-сервер под номером 1, то сеть начнет обращаться к следующему по порядку серверу. Это стандартная схема поддержки отказоустойчивости для современных BRAS.\n\nЕсли откажет сервер 2, то перестанет быть доступен основной узел с базой данных. В этом случае через некоторое время (порядка одной минуты) среди оставшихся будет выбран новый основной узел. После этого ядро биллинга будет общаться именно с этим сервером.\n\nОтказ третьего сервера не вызовет никаких дополнительных действий.\n\nТаким образом, любые неглобальные сбои не приведут к потере доступа абонетов к услугам. Что бы ни произошло, пользователи, скорее всего, ничего не заметят.\n\n#### Заключение ####\n\nНаш проект развивается уже 8 лет, и за это время мы поучаствовали более чем в 80 проектах внедрения. Мы постоянно работаем над тем, чтобы сделать систему более надежной и удобной для клиентов.\n\nВ следующих постах мы продолжим рассказывать об архитектуре «Гидры», технологиях и подходах, которые используются при ее разработке.\n\nПодписывайтесь на\nнаш блог\n, чтобы не пропустить ничего интересного!\n\nКраткое содержание:\nВ статье рассказывается о том, как повысить отказоустойчивость биллинга для операторов связи. Основные компоненты биллинга, такие как данные, ядро, AAA-сервер и платежный шлюз, были рассмотрены с точки зрения их важности для обеспечения надежности системы.\n\nДля повышения отказоустойчивости были предложены различные подходы к резервированию компонентов биллинга. Основная база данных была резервирована с использованием вырожденной трехуровневой архитектуры и рекомендованных производителем подходов. Приложения были резервированы, за исключением AAA-сервера, который был резервирован в автоматическом и ручном режимах.\n\nВ статье также были описаны шаги по резервированию AAA-сервера и его компонентов. Любые неглобальные сбои не приведут к потере доступа абонентов к услугам. Что бы ни произошло, пользователи, скорее всего, ничего не заметят.\n\nПроект \"Гидра\" развивается уже 8 лет и за это время поучаствовал более чем в 80 проектах внедрения. Система биллинга постоянно совершенствуется, чтобы стать более надежной и удобной для клиентов. В следующих постах будут представлены дополнительные материалы о технологиях и подходах, используемых при разработке системы.","summary":"В статье рассказывается о том, как повысить отказоустойчивость биллинга для операторов связи. Основные компоненты биллинга, такие как данные, ядро, AAA-сервер и платежный шлюз, были рассмотрены с точки зрения их важности для обеспечения надежности системы.\n\nДля повышения отказоустойчивости были предложены различные подходы к резервированию компонентов биллинга. Основная база данных была резервирована с использованием вырожденной трехуровневой архитектуры и рекомендованных производителем подходов. Приложения были резервированы, за исключением AAA-сервера, который был резервирован в автоматическом и ручном режимах.\n\nВ статье также были описаны шаги по резервированию AAA-сервера и его компонентов. Любые неглобальные сбои не приведут к потере доступа абонентов к услугам. Что бы ни произошло, пользователи, скорее всего, ничего не заметят.\n\nПроект \"Гидра\" развивается уже 8 лет и за это время поучаствовал более чем в 80 проектах внедрения. Система биллинга постоянно совершенствуется, чтобы стать более надежной и удобной для клиентов. В следующих постах будут представлены дополнительные материалы о технологиях и подходах, используемых при разработке системы."}
{"Text":"Из Беларуси решили уехать 45% стартапов / Habr             \n\nСтартап-хаб Imaguru провёл\nопрос\nоснователей стартап-компаний по поводу планов переезда из страны на фоне политического, экономического кризиса и правового дефолта.\n\nОказалось, что 45% белорусских стартапов приняли решение уехать или уже сделали это. Сейчас привлекательные условия для IT-мигрантов из Беларуси предоставляют Польша и Украина (по специальному\nуказу\nпрезидента Зеленского). Белорусский МИД\nнегодует\nпо этому поводу.\n\nТем временем из СИЗО КГБ под подписку о невыезде\nотпустили\nсотрудников PandaDoc — бухгалтера Юлию Шардыко и HR Владислава Михолапа. Ранее на домашний арест\nвышел\nдиректор минского офиса Дмитрий Рабцевич. Однако менеджер по продукту Виктор Кувшинов пока за решёткой. Также в Минске\nзадержан\nоснователь Bamboo Group Владислав Герчиков.\n\nНекоторые результаты опроса Imaguru:\n\n**Планируете ли вы релоцировать вашу компанию в другую страну?**\n\n**В какую страну вы релоцировали (планируете релоцировать) бизнес?**\n\nВ опросе приняли участие 208 основателей стартапов.\n\n«На отрасли, которая до недавнего времени динамично развивалась, кризис отразился самым серьёзным образом. Отключение интернета, невыносимая рабочая обстановка, запугивание и преследование сотрудников, обыски, жестокое обращение силовиков с гражданами, мирно высказывавшими свою позицию, вынуждают многие компании переехать. В изгнании компании продолжают поддерживать инициативы гражданского общества», — сказала Татьяна Маринич, основатель стартап-хаба Imaguru и член Координационного Совета.\n\nОбщие потери стартап-среды Беларуси из-за политического кризиса оцениваются в \\$15 млн. Политико-экономический кризис в августе-сентябре оказал влияние на деятельность большинства стартап-компаний (71%). Подавляющее большинство сказало, что это влияние для них оказалось сильнее, чем пандемия коронавируса.\n\nСреди наиболее частых последствий политического кризиса — снижение эффективности работы персонала (80%), невозможность выполнять свою работу из-за блокировок интернета (33%), задержания сотрудников (32%), участие персонала в акциях протеста (26%), необходимость принятия срочных мер по релокации бизнеса (22%).\n\nСреди самых негативных последствий для бизнеса компании отметили неработающую законодательную систему и отсутствие правовой защиты (56%), насилие и убийства мирных граждан (56%), потерю доверия власти и политический кризис (47%), уголовные дела против сотрудников стартап-компаний (например, дело PandaDoc) (47%), а также вероятность ситуации, при которой неизвестные в масках могут ворваться в офис, арестовать сотрудников и изъять технику без законных оснований (45%).\n\nПри этом белорусские стартапы активно участвуют в гражданских инициативах. В числе самых популярных мер — благотворительность (43%), гражданские инициативы (41%) и участие в акциях протеста (37%).\n\nКроме Польши и Украины, льготные условия для релокейта белорусским айтишникам собирается предложить Литва. Правительство этой страны сейчас\nрассматривает план помощи жителям Беларуси\n. Также намечается учредить стипендии для белорусских студентов, пригласить предприятия информационных технологий (ИТ) и специалистов этого сектора, упростить выдачу виз гражданам Беларуси. Министр экономики и инноваций Римантас Синкявичюс сказал, что министерство получило обращение 21 ИТ-компании из Беларуси, желающих работать в Литве. Около половины из них — крупные и могли бы обосноваться в Литве, а не в других странах Европы, однако сталкиваются с трудностями при учреждении офисов: банки руководствуются законом о превенции отмывания денег и не позволяют открывать счета — а это необходимое условие для регистрации предприятий в Литве. «Если нельзя открыть счёт в банке, нельзя зарегистрировать предприятие, если нельзя зарегистрировать предприятие — нельзя пригласить работников, потому что нет работодателя (...) Я считаю, что в решении, которое мы намерены принять, было бы неплохо записать и поручение выработать поправки к законам, которые разрешили бы зарубежным юридическим лицам открывать счета в литовских банках», — сказал министр.\n\nP. S. Тем временем десятки белорусских ИТ компаний уже переехали в Украину благодаря проекту IT Relocate Belarus, который реализует команда Минздрава,\nнаписал\n21 октября 2020 года замминистра цифровой трансформации Александр Борняков: «Мы создали IT Relocate для тех, кто хочет переехать сам или перевезти свою команду из Беларуси. Компания Wargaming перевезла сотни сотрудников в Украину. Ещё одна белорусская компания PandaDoc открывает офис в Киеве. Украина готова предоставить белорусским экспертам возможности для работы по упрощённой креативной системе. Преимущества Украины: виза на 90 дней в течение 180 дней, нет языкового барьера, 4G во всех больших городах, 5600+ сервисных и технологических компаний, доступная аренда дома и офиса».\n\n>\n> ### Минутка заботы от НЛО ###\n>\n>\n>\n>\n\nЭтот материал мог вызвать противоречивые чувства, поэтому перед написанием комментария освежите в памяти кое-что важное:\n>\n>\n>\n>\n> **Как написать комментарий и выжить**\n>\n>\n> Не пишите оскорбительных комментариев, не переходите на личности.\n>\n> Воздержитесь от нецензурной лексики и токсичного поведения (даже в завуалированной форме).\n>\n> Для сообщения о комментариях, нарушающих правила сайта, используйте кнопку «Пожаловаться» (если доступна) или форму обратной связи .\n>\n>\n>\n>\n\nЧто делать, если: минусуют карму  | заблокировали аккаунт\n>\n>\n>\n>\n\n→ Кодекс авторов Хабра  и хабраэтикет\n>\n>\n\n→ Полная версия правил сайта\n>\n>","metric":0.626953125,"metric_input":"Текст статьи:\nИз Беларуси решили уехать 45% стартапов / Habr             \n\nСтартап-хаб Imaguru провёл\nопрос\nоснователей стартап-компаний по поводу планов переезда из страны на фоне политического, экономического кризиса и правового дефолта.\n\nОказалось, что 45% белорусских стартапов приняли решение уехать или уже сделали это. Сейчас привлекательные условия для IT-мигрантов из Беларуси предоставляют Польша и Украина (по специальному\nуказу\nпрезидента Зеленского). Белорусский МИД\nнегодует\nпо этому поводу.\n\nТем временем из СИЗО КГБ под подписку о невыезде\nотпустили\nсотрудников PandaDoc — бухгалтера Юлию Шардыко и HR Владислава Михолапа. Ранее на домашний арест\nвышел\nдиректор минского офиса Дмитрий Рабцевич. Однако менеджер по продукту Виктор Кувшинов пока за решёткой. Также в Минске\nзадержан\nоснователь Bamboo Group Владислав Герчиков.\n\nНекоторые результаты опроса Imaguru:\n\n**Планируете ли вы релоцировать вашу компанию в другую страну?**\n\n**В какую страну вы релоцировали (планируете релоцировать) бизнес?**\n\nВ опросе приняли участие 208 основателей стартапов.\n\n«На отрасли, которая до недавнего времени динамично развивалась, кризис отразился самым серьёзным образом. Отключение интернета, невыносимая рабочая обстановка, запугивание и преследование сотрудников, обыски, жестокое обращение силовиков с гражданами, мирно высказывавшими свою позицию, вынуждают многие компании переехать. В изгнании компании продолжают поддерживать инициативы гражданского общества», — сказала Татьяна Маринич, основатель стартап-хаба Imaguru и член Координационного Совета.\n\nОбщие потери стартап-среды Беларуси из-за политического кризиса оцениваются в \\$15 млн. Политико-экономический кризис в августе-сентябре оказал влияние на деятельность большинства стартап-компаний (71%). Подавляющее большинство сказало, что это влияние для них оказалось сильнее, чем пандемия коронавируса.\n\nСреди наиболее частых последствий политического кризиса — снижение эффективности работы персонала (80%), невозможность выполнять свою работу из-за блокировок интернета (33%), задержания сотрудников (32%), участие персонала в акциях протеста (26%), необходимость принятия срочных мер по релокации бизнеса (22%).\n\nСреди самых негативных последствий для бизнеса компании отметили неработающую законодательную систему и отсутствие правовой защиты (56%), насилие и убийства мирных граждан (56%), потерю доверия власти и политический кризис (47%), уголовные дела против сотрудников стартап-компаний (например, дело PandaDoc) (47%), а также вероятность ситуации, при которой неизвестные в масках могут ворваться в офис, арестовать сотрудников и изъять технику без законных оснований (45%).\n\nПри этом белорусские стартапы активно участвуют в гражданских инициативах. В числе самых популярных мер — благотворительность (43%), гражданские инициативы (41%) и участие в акциях протеста (37%).\n\nКроме Польши и Украины, льготные условия для релокейта белорусским айтишникам собирается предложить Литва. Правительство этой страны сейчас\nрассматривает план помощи жителям Беларуси\n. Также намечается учредить стипендии для белорусских студентов, пригласить предприятия информационных технологий (ИТ) и специалистов этого сектора, упростить выдачу виз гражданам Беларуси. Министр экономики и инноваций Римантас Синкявичюс сказал, что министерство получило обращение 21 ИТ-компании из Беларуси, желающих работать в Литве. Около половины из них — крупные и могли бы обосноваться в Литве, а не в других странах Европы, однако сталкиваются с трудностями при учреждении офисов: банки руководствуются законом о превенции отмывания денег и не позволяют открывать счета — а это необходимое условие для регистрации предприятий в Литве. «Если нельзя открыть счёт в банке, нельзя зарегистрировать предприятие, если нельзя зарегистрировать предприятие — нельзя пригласить работников, потому что нет работодателя (...) Я считаю, что в решении, которое мы намерены принять, было бы неплохо записать и поручение выработать поправки к законам, которые разрешили бы зарубежным юридическим лицам открывать счета в литовских банках», — сказал министр.\n\nP. S. Тем временем десятки белорусских ИТ компаний уже переехали в Украину благодаря проекту IT Relocate Belarus, который реализует команда Минздрава,\nнаписал\n21 октября 2020 года замминистра цифровой трансформации Александр Борняков: «Мы создали IT Relocate для тех, кто хочет переехать сам или перевезти свою команду из Беларуси. Компания Wargaming перевезла сотни сотрудников в Украину. Ещё одна белорусская компания PandaDoc открывает офис в Киеве. Украина готова предоставить белорусским экспертам возможности для работы по упрощённой креативной системе. Преимущества Украины: виза на 90 дней в течение 180 дней, нет языкового барьера, 4G во всех больших городах, 5600+ сервисных и технологических компаний, доступная аренда дома и офиса».\n\n>\n> ### Минутка заботы от НЛО ###\n>\n>\n>\n>\n\nЭтот материал мог вызвать противоречивые чувства, поэтому перед написанием комментария освежите в памяти кое-что важное:\n>\n>\n>\n>\n> **Как написать комментарий и выжить**\n>\n>\n> Не пишите оскорбительных комментариев, не переходите на личности.\n>\n> Воздержитесь от нецензурной лексики и токсичного поведения (даже в завуалированной форме).\n>\n> Для сообщения о комментариях, нарушающих правила сайта, используйте кнопку «Пожаловаться» (если доступна) или форму обратной связи .\n>\n>\n>\n>\n\nЧто делать, если: минусуют карму  | заблокировали аккаунт\n>\n>\n>\n>\n\n→ Кодекс авторов Хабра  и хабраэтикет\n>\n>\n\n→ Полная версия правил сайта\n>\n>\n\nКраткое содержание:\nБелорусский стартап-хаб Imaguru провел опрос основателей стартап-компаний о планах переезда из страны на фоне политического, экономического кризиса и правового дефолта. 45% белорусских стартапов приняли решение уехать или уже сделали это. Белорусские стартапы активно участвуют в гражданских инициативах. Литва собирается предложить льготные условия для релокейта белорусским айтишникам.","summary":"Белорусский стартап-хаб Imaguru провел опрос основателей стартап-компаний о планах переезда из страны на фоне политического, экономического кризиса и правового дефолта. 45% белорусских стартапов приняли решение уехать или уже сделали это. Белорусские стартапы активно участвуют в гражданских инициативах. Литва собирается предложить льготные условия для релокейта белорусским айтишникам."}
{"Text":"Установка PHP на ubuntu / Habr                        \n\n20-08-2009\nЗачем? Это самый первый вопрос, который задаст большинство. Отвечу: 1) Дефолтный php без зачастую нужных вещей вроде pcntl, и к тому же с вкомпиленным генератором неведомой совокупительной фигни Suhosin Patch. 2) Сборка модулей, которых нет в репозитории. 3) Нет руководств такого типа. Нет, действительно нету, лишь короткие руководства, которые с оговоркой можно назвать логом ./configure && make && make install, и по которым сложно что-то собрать из-за наличия требований дополнительных библиотек. 4) Я не буду писать про фан. Просто не буду, т.к. мы не балуемся, а собираем продукт для работы. Многие найдут это плюсом, но не я. 5) Внезапно возник вопрос сборки своего модуля. Навыки сборки под никс очень помогли в сборке под винду, где и был написан экстеншн. Можно конечно было сразу писать под никсами… Но ставить и настраиватькастомизировать никсы только ради одного проекта(в консоли на удаленном сервере разрабатывать как-то не особо приятно) мне не хотелось. Начинаем установку Создаем каталог и переходим в него: mkdir build cd build Сначала ситавим из репозитория компилер, набор утилит для сборки, wget, и набор библиотек, которые нам понадобятся для сборки PHP(ну и zlib для apache, если собираем с mod_deflate): sudo apt-get install build-essential wget autoconf zlib1g-dev libxml2-dev libcurl4-openssl-dev libpng-dev libmcrypt-dev libmhash-dev libmysqlclient15-dev libtidy-dev libmemcache-dev На девственной ubuntu-server apt-get потребует скачать около 85мб. Устанавливать мы будем в /opt, чтобы не путать с файлами дистрибутива. Приступим к сборке апача Комментировать ничего не буду, т.к. тут все прозрачно и в комментариях не нуждается. wget http://www.sai.msu.su/apache/httpd/httpd-2.2.12.tar.bz2 bunzip2 httpd-2.2.12.tar.bz2 tar -xf httpd-2.2.12.tar cd httpd-2.2.12/ ./configure --prefix=/opt/apache2 --exec-prefix=/opt/apache2 --enable-mods-shared=all --enable-suexec make sudo make install cd .. Устанавливаем PHP Да, именно его модули требуют кучу зависимостей. Я конфигурировал для MySQL/SQLite СУБД, для других собирайте сами. Не нужно бояться добавить сюда лишнего — на производительность это не повлияет. wget http://ru2.php.net/distributions/php-5.2.10.tar.bz2 bunzip2 php-5.2.10.tar.bz2 tar -xf php-5.2.10.tar cd php-5.2.10 ./configure --prefix=/opt/php5.2 --with-apxs2=/opt/apache2/bin/apxs --enable-fastcgi --with-curl --enable-ftp --with-gd --enable-mbstring --with-mcrypt --with-mhash --with-mysql --with-sqlite --with-mysqli --enable-pcntl --with-pdo-mysql --enable-shmop --enable-soap --enable-sockets --enable-sqlite-utf8 --with-tidy --enable-zip --enable-mbregex --enable-ctype make sudo make install cd .. Пара слов о опциях. PHP-разработчик обязан знать, что значат эти модули(опция начиная с --with-curl), а тот, кто не разработчик, но кому волею судьбы пришлось настраивать сервер, пусть просто поверит, что они более-менее оптимальны и ничего необходимого типа pdo или mysqli не вырезано, как иногда бывает. Это же касается и конфига php.ini. Правим конфиг апача sudo vim /opt/apache2/conf/httpd.conf Я заменяю значение DocumentRoot на \"/var/www\", добавляю в DirectoryIndex index.php, добавляю AddType application/x-httpd-php .php Создаем phpinfo.php с содержимым <? phpinfo() ?> и наслаждаемся видом ненастроенного php(запускаем апач через /opt/apache2/bin/apachectl start). Настраиваем php Копируем дефолтный php.ini, редактируем его: sudo cp php.ini-dist /opt/php5.2/lib/php.ini sudo vim /opt/php5.2/lib/php.ini Правим эти параметры(я даже не буду заикаться насчет того, что они значат): error_reporting = E_ALL display_errors включаем для дебага, на боевой раскладке же выключаем. log_errors = On post_max_size = 64M(для типичных задач это более чем) magic_quotes_gpc = Off(и почему они по молчанию включены? Ума не приложу) include_path = \".:/opt/php5.2/lib/php\"(сюда я положил ZF и прочие интересные вещи) upload_max_filesize = 64M(равен post_max_size) Подскажу, что в виме поиском занимается \"?\" Автоматический запуск Для начала уберем дефолтный апач из запуска, нам поможет удобная утилитка rcconf(ее использовать нагляднее, чем update-rc.d, привык ее юзать): sudo apt-get install rcconf sudo rcconf Убираем галочку. Ну и останавливаем апач, чтобы не перезагружаться: sudo /etc/init.d/apache2 stop Теперь в rc.local добавляем строку запуска нашего апача, что позволит запускаться ему при старте: sudo vim /etc/rc.local Строка запуска выглядит так: /opt/apache2/bin/apachectl start Можно же все сделать и правильно, через добавление скрипта, но я ограничусь этим. Устанавливаем XCache Сборку комментировать не буду: wget http://xcache.lighttpd.net/pub/Releases/1.2.2/xcache-1.2.2.tar.bz2 bunzip2 xcache-1.2.2.tar.bz2 tar -xf xcache-1.2.2.tar cd xcache-1.2.2 /opt/php5.2/bin/phpize ./configure --enable-xcache --prefix=/opt/xcache --enable-xcache-optimizer --with-php-config=/opt/php5.2/bin/php-config make sudo make install Копируем конфиг(был досадный глюк с sudo и правами, поэтому мне пришлось сделать через sudo su) и админку. sudo su cat xcache.ini >> /opt/php5.2/lib/php.ini exit sudo cp -R admin/ /var/www/xcache Снова правим php.ini: sudo vim /opt/php5.2/lib/php.ini Комментируем zend_extension_ts, выставляем xcache.admin.user в имя админа, xcache.admin.pass в md5 пароля, xcache.size в 64M, xcache.optimizer в On, путь zend_extension выставляем в \"/opt/php5.2/lib/php/extensions/no-debug-non-zts-20060613/xcache.so\". Если путь выставлен правильно, то в phpinfo() мы увидим параметры xcache, а статистику сможем посмотреть в админке. Если этого нету, то смотрим логи апача и ищем ошибку. Ставим memcache sudo /opt/php5.2/bin/pecl install memcache Добавляем в php.ini extension=memcache.so, путь прописывать не надо, она лежит по дефолтному. Ставить из pear на никсах легко, это вам не windows =) nginx Меняем порт у апача, вводим в гугле nginx reverse proxy, читаем, делаем. Много раз переписывать одно и то же нет не только желания, но и смысла. Автозагрузка аналогичная апачу. Update: Использование Nginx Как Reverse-Proxy Сервера На Загруженных Сайтах, nginx как reverse proxy В следующих сериях 1. Хоррор «Сборка минимального PHP под Windows при помощи VS 2008». Сборка экстеншенов: helloworld. Если хватит кармы, то будет завтра-послезавтра, т.к. уже написана. 2. Сборка экстеншенов на linux. Реализация TEA. 2009-09-01\n","metric":0.373046875,"metric_input":"Текст статьи:\nУстановка PHP на ubuntu / Habr                        \n\n20-08-2009\nЗачем? Это самый первый вопрос, который задаст большинство. Отвечу: 1) Дефолтный php без зачастую нужных вещей вроде pcntl, и к тому же с вкомпиленным генератором неведомой совокупительной фигни Suhosin Patch. 2) Сборка модулей, которых нет в репозитории. 3) Нет руководств такого типа. Нет, действительно нету, лишь короткие руководства, которые с оговоркой можно назвать логом ./configure && make && make install, и по которым сложно что-то собрать из-за наличия требований дополнительных библиотек. 4) Я не буду писать про фан. Просто не буду, т.к. мы не балуемся, а собираем продукт для работы. Многие найдут это плюсом, но не я. 5) Внезапно возник вопрос сборки своего модуля. Навыки сборки под никс очень помогли в сборке под винду, где и был написан экстеншн. Можно конечно было сразу писать под никсами… Но ставить и настраиватькастомизировать никсы только ради одного проекта(в консоли на удаленном сервере разрабатывать как-то не особо приятно) мне не хотелось. Начинаем установку Создаем каталог и переходим в него: mkdir build cd build Сначала ситавим из репозитория компилер, набор утилит для сборки, wget, и набор библиотек, которые нам понадобятся для сборки PHP(ну и zlib для apache, если собираем с mod_deflate): sudo apt-get install build-essential wget autoconf zlib1g-dev libxml2-dev libcurl4-openssl-dev libpng-dev libmcrypt-dev libmhash-dev libmysqlclient15-dev libtidy-dev libmemcache-dev На девственной ubuntu-server apt-get потребует скачать около 85мб. Устанавливать мы будем в /opt, чтобы не путать с файлами дистрибутива. Приступим к сборке апача Комментировать ничего не буду, т.к. тут все прозрачно и в комментариях не нуждается. wget http://www.sai.msu.su/apache/httpd/httpd-2.2.12.tar.bz2 bunzip2 httpd-2.2.12.tar.bz2 tar -xf httpd-2.2.12.tar cd httpd-2.2.12/ ./configure --prefix=/opt/apache2 --exec-prefix=/opt/apache2 --enable-mods-shared=all --enable-suexec make sudo make install cd .. Устанавливаем PHP Да, именно его модули требуют кучу зависимостей. Я конфигурировал для MySQL/SQLite СУБД, для других собирайте сами. Не нужно бояться добавить сюда лишнего — на производительность это не повлияет. wget http://ru2.php.net/distributions/php-5.2.10.tar.bz2 bunzip2 php-5.2.10.tar.bz2 tar -xf php-5.2.10.tar cd php-5.2.10 ./configure --prefix=/opt/php5.2 --with-apxs2=/opt/apache2/bin/apxs --enable-fastcgi --with-curl --enable-ftp --with-gd --enable-mbstring --with-mcrypt --with-mhash --with-mysql --with-sqlite --with-mysqli --enable-pcntl --with-pdo-mysql --enable-shmop --enable-soap --enable-sockets --enable-sqlite-utf8 --with-tidy --enable-zip --enable-mbregex --enable-ctype make sudo make install cd .. Пара слов о опциях. PHP-разработчик обязан знать, что значат эти модули(опция начиная с --with-curl), а тот, кто не разработчик, но кому волею судьбы пришлось настраивать сервер, пусть просто поверит, что они более-менее оптимальны и ничего необходимого типа pdo или mysqli не вырезано, как иногда бывает. Это же касается и конфига php.ini. Правим конфиг апача sudo vim /opt/apache2/conf/httpd.conf Я заменяю значение DocumentRoot на \"/var/www\", добавляю в DirectoryIndex index.php, добавляю AddType application/x-httpd-php .php Создаем phpinfo.php с содержимым <? phpinfo() ?> и наслаждаемся видом ненастроенного php(запускаем апач через /opt/apache2/bin/apachectl start). Настраиваем php Копируем дефолтный php.ini, редактируем его: sudo cp php.ini-dist /opt/php5.2/lib/php.ini sudo vim /opt/php5.2/lib/php.ini Правим эти параметры(я даже не буду заикаться насчет того, что они значат): error_reporting = E_ALL display_errors включаем для дебага, на боевой раскладке же выключаем. log_errors = On post_max_size = 64M(для типичных задач это более чем) magic_quotes_gpc = Off(и почему они по молчанию включены? Ума не приложу) include_path = \".:/opt/php5.2/lib/php\"(сюда я положил ZF и прочие интересные вещи) upload_max_filesize = 64M(равен post_max_size) Подскажу, что в виме поиском занимается \"?\" Автоматический запуск Для начала уберем дефолтный апач из запуска, нам поможет удобная утилитка rcconf(ее использовать нагляднее, чем update-rc.d, привык ее юзать): sudo apt-get install rcconf sudo rcconf Убираем галочку. Ну и останавливаем апач, чтобы не перезагружаться: sudo /etc/init.d/apache2 stop Теперь в rc.local добавляем строку запуска нашего апача, что позволит запускаться ему при старте: sudo vim /etc/rc.local Строка запуска выглядит так: /opt/apache2/bin/apachectl start Можно же все сделать и правильно, через добавление скрипта, но я ограничусь этим. Устанавливаем XCache Сборку комментировать не буду: wget http://xcache.lighttpd.net/pub/Releases/1.2.2/xcache-1.2.2.tar.bz2 bunzip2 xcache-1.2.2.tar.bz2 tar -xf xcache-1.2.2.tar cd xcache-1.2.2 /opt/php5.2/bin/phpize ./configure --enable-xcache --prefix=/opt/xcache --enable-xcache-optimizer --with-php-config=/opt/php5.2/bin/php-config make sudo make install Копируем конфиг(был досадный глюк с sudo и правами, поэтому мне пришлось сделать через sudo su) и админку. sudo su cat xcache.ini >> /opt/php5.2/lib/php.ini exit sudo cp -R admin/ /var/www/xcache Снова правим php.ini: sudo vim /opt/php5.2/lib/php.ini Комментируем zend_extension_ts, выставляем xcache.admin.user в имя админа, xcache.admin.pass в md5 пароля, xcache.size в 64M, xcache.optimizer в On, путь zend_extension выставляем в \"/opt/php5.2/lib/php/extensions/no-debug-non-zts-20060613/xcache.so\". Если путь выставлен правильно, то в phpinfo() мы увидим параметры xcache, а статистику сможем посмотреть в админке. Если этого нету, то смотрим логи апача и ищем ошибку. Ставим memcache sudo /opt/php5.2/bin/pecl install memcache Добавляем в php.ini extension=memcache.so, путь прописывать не надо, она лежит по дефолтному. Ставить из pear на никсах легко, это вам не windows =) nginx Меняем порт у апача, вводим в гугле nginx reverse proxy, читаем, делаем. Много раз переписывать одно и то же нет не только желания, но и смысла. Автозагрузка аналогичная апачу. Update: Использование Nginx Как Reverse-Proxy Сервера На Загруженных Сайтах, nginx как reverse proxy В следующих сериях 1. Хоррор «Сборка минимального PHP под Windows при помощи VS 2008». Сборка экстеншенов: helloworld. Если хватит кармы, то будет завтра-послезавтра, т.к. уже написана. 2. Сборка экстеншенов на linux. Реализация TEA. 2009-09-01\n\n\nКраткое содержание:\nУстановка PHP на Ubuntu включает следующие шаги:\n\n1. Создание каталога и переход в него.\n2. Установка компилятора, утилит для сборки, wget и набора библиотек.\n3. Сборка Apache.\n4. Установка PHP.\n5. Настройка PHP, включая редактирование php.ini и добавление параметров.\n6. Установка XCache.\n7. Настройка XCache, включая добавление конфигурации и админки.\n8. Установка Memcache.\n9. Добавление Memcache в php.ini.\n10. Установка Nginx.\n11. Настройка Nginx, включая изменение порта и использование в качестве обратного прокси-сервера.\n12. Автозагрузка Nginx.","summary":"Установка PHP на Ubuntu включает следующие шаги:\n\n1. Создание каталога и переход в него.\n2. Установка компилятора, утилит для сборки, wget и набора библиотек.\n3. Сборка Apache.\n4. Установка PHP.\n5. Настройка PHP, включая редактирование php.ini и добавление параметров.\n6. Установка XCache.\n7. Настройка XCache, включая добавление конфигурации и админки.\n8. Установка Memcache.\n9. Добавление Memcache в php.ini.\n10. Установка Nginx.\n11. Настройка Nginx, включая изменение порта и использование в качестве обратного прокси-сервера.\n12. Автозагрузка Nginx."}
{"Text":"Нейрочип, позволяющий читать мысли парализованных людей / Habr             \n\n23-12-2017\nСогласно данным Всемирной Организации Здравоохранения, каждый год во всем мире от 250000 до 500000 человек теряет возможность двигаться в результате травм спинного мозга, в последствие чего становятся инвалидами. В результате последних экспериментов, ученым удалось достигнуть колоссальных результатов, которые дают надежду людям покинуть инвалидное кресло и вернуться к полноценной жизни. Первые попытки ученых были более 20 лет назад. Этой темой начал заниматься профессор Грегуар Куртин с фондом Кристофера Рива. Он создал команду из нейробиологов, хирургов, физиотерапевтов и инженеров, которые пытались решить проблему. Результаты экспериментов успешно провели на мышах и обезьянах. Но в связи со смертью Кристофера Рива, исследования затянулись. И лишь совсем недавно швейцарцы отпраздновали успех. Опыт проводился на двух макаках, у которых был частично перерзан спинной мозг. Одна обезьяна с парализовано правой ногой научилась ходить за 6 дней, другая за две недели. Со временем, ученые доработали технологии, которые можно было использовать для реабилитации людей. И в настоящее время швейцарским нейрофизиологам удалось изобрести устройство, дающее возможность понимать мысли и желания людей, скованных после ДТП или тяжелых болезней. Ученые долгое время пытались помочь людям с «синдромом изоляции», доказывая, что пациенты все так же могут мыслить. Это заболевание представляет собой заключительный этап развития медленно прогрессирующих нейродегенеративных заболеваний. Примером могут стать такие заболевания нервной системы как, болезнь Альцгеймера, Кортикобазальная дегенерация, «болезнь Лу Герига», в следствие травмирования мозга, интоксикации, а также злоупотребление препаратами. В то время как болезнь прогрессирует, больной как будто теряет себя, испытывает трудность в передвижении, частые судороги в области рук и ног. Завершающим этапом становится трудность глотать и дышать, и говорить. Известный немецкий невролог Нильс Бирбаумер из Германии с группой специалистов провел эксперимент, в качестве объектов выступили четыре пациента имеющие диагноз БАС, находящиеся на том этапе, когда затрудняется возможность дышать. Цель этого исследования: понять и доказать, что люди с таким заболеванием имеют возможность мыслить и помочь им общаться с окружающим миром. Благодаря нейрочипу и мозговому интерфейсу —технология, с помощью которой мозг передает информацию на компьютер, а он в свою очередь преобразует данные в понятный компьютеру язык. В эксперименте специалисты применили магнитно-резонансный томограф для отслеживания работы мозга, и инфракрасный спектроскоп, дающий возможность контролировать активность нервных клеток и количество потребляемого ими кислорода. После долгого изучения человеческого тела, ученые пришли к выводу, что наше тело имеет прозрачность, которая способна пропускать тепловое излучение, варьирующееся в среднем 800 нанометров, которые как раз и потребляет гемоглобин. Следовательно, прибор показывает наиболее темный цвет, если клетка получает от эритроцитов наибольшее количество кислорода. Группа ученых выяснила, что при наблюдении за работой мозга у здоровых людей есть некие особенности реакции мозга на простые вопросы. Дело в том, что концентрация выброса гемоглобина меняется в зависимости от ответа «да»/ «нет». При положительном ответе доля увеличивается, пятно приобретает цвет ближе к красному. А при отрицательном ответе не изменяется или снижается, то есть становится сине-зеленым. Нильс Бирбаумер с группой ученных опирались именно на эту идею, и разработали специальную программу, которая подобные сигналы могла переводить в ответы «да» и «нет». Данный эксперимент провели на четырех парализованных людях разного пола, опекуны которых были согласны. Результаты ученых поразили. Потому что с помощью разработанной программы участникам эксперимента корректно удавалось отвечать на вопросы, узнавать своих близких людей, пытаться поговорить с родственниками. Сигналы, полученные с помощью программы, помогли парализованным людям поговорить, чему собственно они были очень довольны. Важен тот факт, что многие из участников находились на искусственной вентиляции легких, что доказывало невероятность и уникальность данной разработки. Результаты эксперимента смогли подтвердить то, что даже в парализованном состоянии человеку хочется общаться с людьми, у них есть желание и надежда на восстановление. Ученые утверждают, что эксперимент доказывает, что дальнейшее изучение и развитие поможет развить полноценное формирование речевого сигнала по определенным параметрам, а также устройств, которые смогут помочь парализованным людям учиться двигаться самостоятельно. Рассмотрим еще один пример американских ученых из Баттельской исследовательской организации, которая несколько лет назад смогла совершить открытие в области лечения парализованного человека. В 2010 году молодой человек по имени Ян Бурхарт в результате несчастного случая оказался прикованным к инвалидному креслу. Он не был инвалидом с рождения, и хотел вновь научиться управлять своим телом, именно поэтому он согласился на операцию, которая была экспериментом. В американском медицинском центре в штате Огайо Яну Бурхарту была проведена операция по имплантации специального чипа в мозг, который разработали ученые. В течение пары недель Яну требовалось на экране монитора научиться управлять и шевелить виртуально рукой. В его главные задачи входило умение сосредоточиться мысленно, чтобы суметь выполнить определенное действие. Тренировка прошла успешно. После тренировочной программы на компьютере, Яна начали подключать к специальным устройствам. На руку был надет специальный браслет, стимулирующий мышцы руки с помощью электродов. А имплантированный чип в голове подсоединили к специальному устройству проводами, позволяющими подавать от импульсов мозга в машину определенные сигналы, которые преобразовывались в самой машине, и уже затем направлялись сигналами в браслеты. На картинке наглядно виден сам процесс. После различных попыток, Яну начало удаваться разжимать пальцы на руке. Затем он смог самостоятельно двигать запястье руки, поднимать и опускать его. Достижениями этого эксперимента стало то, что Ян смог ранее парализованной рукой подержать ложку в руке. Это стало удивительным достижением в медицине и науке. Благодаря этому, ученые продолжают свои исследования и эксперименты, для того, чтобы в будущем помочь парализованным людям полноценно научиться работать своими конечностями. Безусловно, технологии нужно дорабатывать, скорость передачи информации должна быть очень высокой, чтобы человек мог управлять своими движениями в режиме реального времени, а не зависал. А устройство должно быть миниатюрным и надежным. По мнению профессора Грегуара Куртина, такие технологии в биоэлектронном лечении, которые помогут парализованным людям вернуться к полноценной жизни, будут доступны к 2020 году. Но уже сейчас такие разработки открывают большие возможности и дают надежду миллионам людей на самостоятельное движение и помощь в синтезировании их речи.\n","metric":0.53955078130000000414,"metric_input":"Текст статьи:\nНейрочип, позволяющий читать мысли парализованных людей / Habr             \n\n23-12-2017\nСогласно данным Всемирной Организации Здравоохранения, каждый год во всем мире от 250000 до 500000 человек теряет возможность двигаться в результате травм спинного мозга, в последствие чего становятся инвалидами. В результате последних экспериментов, ученым удалось достигнуть колоссальных результатов, которые дают надежду людям покинуть инвалидное кресло и вернуться к полноценной жизни. Первые попытки ученых были более 20 лет назад. Этой темой начал заниматься профессор Грегуар Куртин с фондом Кристофера Рива. Он создал команду из нейробиологов, хирургов, физиотерапевтов и инженеров, которые пытались решить проблему. Результаты экспериментов успешно провели на мышах и обезьянах. Но в связи со смертью Кристофера Рива, исследования затянулись. И лишь совсем недавно швейцарцы отпраздновали успех. Опыт проводился на двух макаках, у которых был частично перерзан спинной мозг. Одна обезьяна с парализовано правой ногой научилась ходить за 6 дней, другая за две недели. Со временем, ученые доработали технологии, которые можно было использовать для реабилитации людей. И в настоящее время швейцарским нейрофизиологам удалось изобрести устройство, дающее возможность понимать мысли и желания людей, скованных после ДТП или тяжелых болезней. Ученые долгое время пытались помочь людям с «синдромом изоляции», доказывая, что пациенты все так же могут мыслить. Это заболевание представляет собой заключительный этап развития медленно прогрессирующих нейродегенеративных заболеваний. Примером могут стать такие заболевания нервной системы как, болезнь Альцгеймера, Кортикобазальная дегенерация, «болезнь Лу Герига», в следствие травмирования мозга, интоксикации, а также злоупотребление препаратами. В то время как болезнь прогрессирует, больной как будто теряет себя, испытывает трудность в передвижении, частые судороги в области рук и ног. Завершающим этапом становится трудность глотать и дышать, и говорить. Известный немецкий невролог Нильс Бирбаумер из Германии с группой специалистов провел эксперимент, в качестве объектов выступили четыре пациента имеющие диагноз БАС, находящиеся на том этапе, когда затрудняется возможность дышать. Цель этого исследования: понять и доказать, что люди с таким заболеванием имеют возможность мыслить и помочь им общаться с окружающим миром. Благодаря нейрочипу и мозговому интерфейсу —технология, с помощью которой мозг передает информацию на компьютер, а он в свою очередь преобразует данные в понятный компьютеру язык. В эксперименте специалисты применили магнитно-резонансный томограф для отслеживания работы мозга, и инфракрасный спектроскоп, дающий возможность контролировать активность нервных клеток и количество потребляемого ими кислорода. После долгого изучения человеческого тела, ученые пришли к выводу, что наше тело имеет прозрачность, которая способна пропускать тепловое излучение, варьирующееся в среднем 800 нанометров, которые как раз и потребляет гемоглобин. Следовательно, прибор показывает наиболее темный цвет, если клетка получает от эритроцитов наибольшее количество кислорода. Группа ученых выяснила, что при наблюдении за работой мозга у здоровых людей есть некие особенности реакции мозга на простые вопросы. Дело в том, что концентрация выброса гемоглобина меняется в зависимости от ответа «да»/ «нет». При положительном ответе доля увеличивается, пятно приобретает цвет ближе к красному. А при отрицательном ответе не изменяется или снижается, то есть становится сине-зеленым. Нильс Бирбаумер с группой ученных опирались именно на эту идею, и разработали специальную программу, которая подобные сигналы могла переводить в ответы «да» и «нет». Данный эксперимент провели на четырех парализованных людях разного пола, опекуны которых были согласны. Результаты ученых поразили. Потому что с помощью разработанной программы участникам эксперимента корректно удавалось отвечать на вопросы, узнавать своих близких людей, пытаться поговорить с родственниками. Сигналы, полученные с помощью программы, помогли парализованным людям поговорить, чему собственно они были очень довольны. Важен тот факт, что многие из участников находились на искусственной вентиляции легких, что доказывало невероятность и уникальность данной разработки. Результаты эксперимента смогли подтвердить то, что даже в парализованном состоянии человеку хочется общаться с людьми, у них есть желание и надежда на восстановление. Ученые утверждают, что эксперимент доказывает, что дальнейшее изучение и развитие поможет развить полноценное формирование речевого сигнала по определенным параметрам, а также устройств, которые смогут помочь парализованным людям учиться двигаться самостоятельно. Рассмотрим еще один пример американских ученых из Баттельской исследовательской организации, которая несколько лет назад смогла совершить открытие в области лечения парализованного человека. В 2010 году молодой человек по имени Ян Бурхарт в результате несчастного случая оказался прикованным к инвалидному креслу. Он не был инвалидом с рождения, и хотел вновь научиться управлять своим телом, именно поэтому он согласился на операцию, которая была экспериментом. В американском медицинском центре в штате Огайо Яну Бурхарту была проведена операция по имплантации специального чипа в мозг, который разработали ученые. В течение пары недель Яну требовалось на экране монитора научиться управлять и шевелить виртуально рукой. В его главные задачи входило умение сосредоточиться мысленно, чтобы суметь выполнить определенное действие. Тренировка прошла успешно. После тренировочной программы на компьютере, Яна начали подключать к специальным устройствам. На руку был надет специальный браслет, стимулирующий мышцы руки с помощью электродов. А имплантированный чип в голове подсоединили к специальному устройству проводами, позволяющими подавать от импульсов мозга в машину определенные сигналы, которые преобразовывались в самой машине, и уже затем направлялись сигналами в браслеты. На картинке наглядно виден сам процесс. После различных попыток, Яну начало удаваться разжимать пальцы на руке. Затем он смог самостоятельно двигать запястье руки, поднимать и опускать его. Достижениями этого эксперимента стало то, что Ян смог ранее парализованной рукой подержать ложку в руке. Это стало удивительным достижением в медицине и науке. Благодаря этому, ученые продолжают свои исследования и эксперименты, для того, чтобы в будущем помочь парализованным людям полноценно научиться работать своими конечностями. Безусловно, технологии нужно дорабатывать, скорость передачи информации должна быть очень высокой, чтобы человек мог управлять своими движениями в режиме реального времени, а не зависал. А устройство должно быть миниатюрным и надежным. По мнению профессора Грегуара Куртина, такие технологии в биоэлектронном лечении, которые помогут парализованным людям вернуться к полноценной жизни, будут доступны к 2020 году. Но уже сейчас такие разработки открывают большие возможности и дают надежду миллионам людей на самостоятельное движение и помощь в синтезировании их речи.\n\n\nКраткое содержание:\nСтатья обсуждает последние достижения в области нейрочипов и мозговых интерфейсов, которые могут помочь парализованным людям читать мысли и общаться. Ученые разработали устройство, которое может понимать мысли и желания людей, скованных после ДТП или тяжелых болезней. Эксперименты показали, что даже в парализованном состоянии человеку хочется общаться с людьми и есть желание на восстановление.","summary":"Статья обсуждает последние достижения в области нейрочипов и мозговых интерфейсов, которые могут помочь парализованным людям читать мысли и общаться. Ученые разработали устройство, которое может понимать мысли и желания людей, скованных после ДТП или тяжелых болезней. Эксперименты показали, что даже в парализованном состоянии человеку хочется общаться с людьми и есть желание на восстановление."}
{"Text":"Взгляд изнутри на удаленную разработку, или почему программирование — процесс не линейный / Habr                           \n\n19-10-2015\nДавно сюда не писал. Но за последнее время на тостере уже в который раз спрашивают, как контролировать работу удаленного разработчика. Что ж, я — тот самый удаленный разработчик. Удаленнее некуда. Мне много раз приходилось бывать и в роли постановщика задач, и зачастую этот опыт был успешным. На самом деле, для комфортной работы с программистами достаточно даже минимального понимания того, что происходит «под капотом». Палю тему. Азы 1. Задачи ставятся в таск-менеджер. В письменном виде, а не диктуются по телефону или скайпу. Если вы хотите, чтобы ваш удаленный разработчик не тратил свои силы и время (которые он мог бы направить на работу), то спросите у него, какой таск-менеджер ему удобно использовать. Наверняка он обоснованно пользуется именно им несколько лет, и может поделиться и с вами полезными навыками. Убедитесь, что вы сформулировали задачу. Если задача велика — вместе с разработчиком разделите ее на части, совместно запишите промежуточные цели и критерии. Работа может проходить в условиях неявных исходных данных и целей, но на каждой итерации критерии успеха должны быть очевидны. Нормальная продолжительность итерации — одна неделя. Максимальная — три недели. Количество задач, выставляемых на итерацию, можно обсуждать с разработчиком. Изучите прогресс предыдущей итерации при планировании следующей. 2. Комиты заливаются в удаленный репозиторий, который, как правило, имеет веб-интерфейс. Это означает, что вы можете просматривать текущий прогресс интерактивно. Для этого не нужно иметь специальных знаний, эта работа доступна среднестатистическому менеджеру с базовыми навыками «Оператор ЭВМ». Конечно, такой режим работы требует доверительных отношений с разработчиком. Доверия можно добиться не увещеваниями, а позитивным опытом работы. 3. Можно попросить разработчика периодически делать скринкасты новой функциональности, скринить или выгружать результаты тестов и присылать вам. 4. Намного проще и дешевле пойти и погуглить интересующий вас вопрос, прежде чем просить разработчика объяснять вам что-то голосом. Подробности Во-первых, контролировать работу просто. По результатам. Есть конкретные поставленные задачи? Есть ясные приоритеты? Любую задачу можно решить в течение недели. Если задача сильно объемная, она должна быть разбита на меньшие фрагменты, решение каждого из которых скорее всего занимает один рабочий день. Таким образом, эффективность контроля прямо зависит от качества постановки задачи. Во-вторых, мы, программисты, делаем модули. На 10 среднестатистических модулей приходятся примерно от 5 до 10 сложных задач, и 90 очень тривиальных. В течении недели удается сделать от 3 до 10 модулей. В течении дня — от нуля до 5. Сейчас попробую объяснить, почему так. Тривиальные задачи часто бывают объемными. Они легко программируются и их программирование даже автоматизируется, но именно из-за монотонности они дольше отлаживаются. Вообще, на отладку уходит 60% рабочего времени. И это нормально. 60% времени на то, чтобы заставить работать свеженаписаный код, и чтобы добиться от него нужных свойств, включая производительность и безопасность (если есть дополнительные требования в этой связи). Сложные задачи несколько выбиваются из мейнстрима. Сложные — не значит объемные. Просто решения именно этой задачи в текущем контексте еще не было. Или было, но портировать долго или не целесообразно по иным причинам. Обычно функции в удачной реализации имеют небольшой размер, буквально одна-две сотни строк. Но до этого удачного решения нужно еще прийти. Идти до решения можно от одного до трех дней, или дольше. Экспериментами, тестами, пробами и безусловными ошибками. Часто без мгновенного результата. Более эффективный путь — отвлекаться от компьютера вообще, и обдумывать задачи на свежем воздухе. Я увлекаюсь рыбалкой и плотничаю на даче. Лучшее из того, что я придумывал, приходило в голову на расстоянии нескольких десятков километров от компьютеров. Будучи близко знакомым с десятками коллег, много раз обсуждал с ними этот вопрос. Все сталкивались с этим, и все понимают, что это нормально. Именно поэтому бессмысленно требовать от разработчика присутствия в реальном времени, и использования средств онлайн-контроля. Присутствовать-то он будет, но будет ли он продуктивен? Это вопрос. У меня есть множество знакомых руководителей, которые рассказывали похожие истории про программистов, которые ходят на работу в костюме и по часам, но по факту работают медленно и слишком часто делают «не то». В-третьих, заниматься консультированием — сложнее, чем разрабатывать программы. И дело тут вовсе не в интравертной специфике труда. Консультирование — сложный труд. Если вы хотите получить качественный результат — не надо часами выносить мозги, глупейшими вопросами и обсуждениями. Если вы не компетентны в каких-то вопросах — есть ли тут вина разработчика? Хотите получить какие-то новые знания? А почему мы должны платить за ваше обучение своим временем? Если вы хотите получить качественную консультацию — подготовьтесь к ней. Запишите вопросы списком и отправьте их на почту в конце недели. Не нужно звонить и задавать их по одному. Если вы нашли ответы на вопросы — удалите их из списка. Список пуст? Великолепно. Значит действительно не о чем говорить. И это тоже нормально. Обычно программисты работают небольшими сессиями, по 2-4 часа. Таким образом, в течение дня получается провести всего 2 или 3 сессии. Если вы отвлекаете спонтанным звонком — теряется полдня. Программисты работают непосредственно мозгами, поэтому требуют несколько деликатного общения. Не стоит им беседами мозг выносить. То, что вам кажется эффектным и убедительным, вашему собеседнику может показаться глупым и скучным. Обычно это беседы про перспективы, сотрудничество, новые и старые технологии. Напротив, интересным будет обсуждение реального пользовательского опыта и конкретных планов. Вне зависимости от того, помните вы о своих звонках или нет — вы влияете на работу удаленного разработчика. Можете влиять и позитивно. Это просто: регламентируйте общение и не делайте осознанных глупостей, оправдываясь оговоркой «я же в этом не разбираюсь». Или разберитесь, или честно формулируйте критерии, понятные вам.\n","metric":0.6220703125,"metric_input":"Текст статьи:\nВзгляд изнутри на удаленную разработку, или почему программирование — процесс не линейный / Habr                           \n\n19-10-2015\nДавно сюда не писал. Но за последнее время на тостере уже в который раз спрашивают, как контролировать работу удаленного разработчика. Что ж, я — тот самый удаленный разработчик. Удаленнее некуда. Мне много раз приходилось бывать и в роли постановщика задач, и зачастую этот опыт был успешным. На самом деле, для комфортной работы с программистами достаточно даже минимального понимания того, что происходит «под капотом». Палю тему. Азы 1. Задачи ставятся в таск-менеджер. В письменном виде, а не диктуются по телефону или скайпу. Если вы хотите, чтобы ваш удаленный разработчик не тратил свои силы и время (которые он мог бы направить на работу), то спросите у него, какой таск-менеджер ему удобно использовать. Наверняка он обоснованно пользуется именно им несколько лет, и может поделиться и с вами полезными навыками. Убедитесь, что вы сформулировали задачу. Если задача велика — вместе с разработчиком разделите ее на части, совместно запишите промежуточные цели и критерии. Работа может проходить в условиях неявных исходных данных и целей, но на каждой итерации критерии успеха должны быть очевидны. Нормальная продолжительность итерации — одна неделя. Максимальная — три недели. Количество задач, выставляемых на итерацию, можно обсуждать с разработчиком. Изучите прогресс предыдущей итерации при планировании следующей. 2. Комиты заливаются в удаленный репозиторий, который, как правило, имеет веб-интерфейс. Это означает, что вы можете просматривать текущий прогресс интерактивно. Для этого не нужно иметь специальных знаний, эта работа доступна среднестатистическому менеджеру с базовыми навыками «Оператор ЭВМ». Конечно, такой режим работы требует доверительных отношений с разработчиком. Доверия можно добиться не увещеваниями, а позитивным опытом работы. 3. Можно попросить разработчика периодически делать скринкасты новой функциональности, скринить или выгружать результаты тестов и присылать вам. 4. Намного проще и дешевле пойти и погуглить интересующий вас вопрос, прежде чем просить разработчика объяснять вам что-то голосом. Подробности Во-первых, контролировать работу просто. По результатам. Есть конкретные поставленные задачи? Есть ясные приоритеты? Любую задачу можно решить в течение недели. Если задача сильно объемная, она должна быть разбита на меньшие фрагменты, решение каждого из которых скорее всего занимает один рабочий день. Таким образом, эффективность контроля прямо зависит от качества постановки задачи. Во-вторых, мы, программисты, делаем модули. На 10 среднестатистических модулей приходятся примерно от 5 до 10 сложных задач, и 90 очень тривиальных. В течении недели удается сделать от 3 до 10 модулей. В течении дня — от нуля до 5. Сейчас попробую объяснить, почему так. Тривиальные задачи часто бывают объемными. Они легко программируются и их программирование даже автоматизируется, но именно из-за монотонности они дольше отлаживаются. Вообще, на отладку уходит 60% рабочего времени. И это нормально. 60% времени на то, чтобы заставить работать свеженаписаный код, и чтобы добиться от него нужных свойств, включая производительность и безопасность (если есть дополнительные требования в этой связи). Сложные задачи несколько выбиваются из мейнстрима. Сложные — не значит объемные. Просто решения именно этой задачи в текущем контексте еще не было. Или было, но портировать долго или не целесообразно по иным причинам. Обычно функции в удачной реализации имеют небольшой размер, буквально одна-две сотни строк. Но до этого удачного решения нужно еще прийти. Идти до решения можно от одного до трех дней, или дольше. Экспериментами, тестами, пробами и безусловными ошибками. Часто без мгновенного результата. Более эффективный путь — отвлекаться от компьютера вообще, и обдумывать задачи на свежем воздухе. Я увлекаюсь рыбалкой и плотничаю на даче. Лучшее из того, что я придумывал, приходило в голову на расстоянии нескольких десятков километров от компьютеров. Будучи близко знакомым с десятками коллег, много раз обсуждал с ними этот вопрос. Все сталкивались с этим, и все понимают, что это нормально. Именно поэтому бессмысленно требовать от разработчика присутствия в реальном времени, и использования средств онлайн-контроля. Присутствовать-то он будет, но будет ли он продуктивен? Это вопрос. У меня есть множество знакомых руководителей, которые рассказывали похожие истории про программистов, которые ходят на работу в костюме и по часам, но по факту работают медленно и слишком часто делают «не то». В-третьих, заниматься консультированием — сложнее, чем разрабатывать программы. И дело тут вовсе не в интравертной специфике труда. Консультирование — сложный труд. Если вы хотите получить качественный результат — не надо часами выносить мозги, глупейшими вопросами и обсуждениями. Если вы не компетентны в каких-то вопросах — есть ли тут вина разработчика? Хотите получить какие-то новые знания? А почему мы должны платить за ваше обучение своим временем? Если вы хотите получить качественную консультацию — подготовьтесь к ней. Запишите вопросы списком и отправьте их на почту в конце недели. Не нужно звонить и задавать их по одному. Если вы нашли ответы на вопросы — удалите их из списка. Список пуст? Великолепно. Значит действительно не о чем говорить. И это тоже нормально. Обычно программисты работают небольшими сессиями, по 2-4 часа. Таким образом, в течение дня получается провести всего 2 или 3 сессии. Если вы отвлекаете спонтанным звонком — теряется полдня. Программисты работают непосредственно мозгами, поэтому требуют несколько деликатного общения. Не стоит им беседами мозг выносить. То, что вам кажется эффектным и убедительным, вашему собеседнику может показаться глупым и скучным. Обычно это беседы про перспективы, сотрудничество, новые и старые технологии. Напротив, интересным будет обсуждение реального пользовательского опыта и конкретных планов. Вне зависимости от того, помните вы о своих звонках или нет — вы влияете на работу удаленного разработчика. Можете влиять и позитивно. Это просто: регламентируйте общение и не делайте осознанных глупостей, оправдываясь оговоркой «я же в этом не разбираюсь». Или разберитесь, или честно формулируйте критерии, понятные вам.\n\n\nКраткое содержание:\nСтатья обсуждает удаленную разработку и дает советы по эффективному контролю и взаимодействию с удаленными разработчиками. Автор делится своим опытом и предлагает следующие методы:\n\n1. Постановка задач в таск-менеджере и четкое формулирование задачи.\n2. Использование удобного для разработчика таск-менеджера.\n3. Совместное определение промежуточных целей и критериев успеха.\n4. Интерактивный просмотр прогресса через веб-интерфейс удаленного репозитория.\n5. Периодическое предоставление скринкастов новой функциональности и результатов тестов.\n6. Использование гугла для поиска информации, прежде чем обращаться к разработчику.\n7. Оценка эффективности контроля по результатам работы.\n8. Разделение сложных задач на более мелкие фрагменты.\n9. Учет времени на отладку и автоматизацию тривиальных задач.\n10. Эффективное консультирование: подготовка вопросов, ограничение времени звонков, деликатное общение.\n\nАвтор подчеркивает, что программисты работают небольшими сессиями, поэтому важно планировать общение и не отвлекать их глупыми вопросами.","summary":"Статья обсуждает удаленную разработку и дает советы по эффективному контролю и взаимодействию с удаленными разработчиками. Автор делится своим опытом и предлагает следующие методы:\n\n1. Постановка задач в таск-менеджере и четкое формулирование задачи.\n2. Использование удобного для разработчика таск-менеджера.\n3. Совместное определение промежуточных целей и критериев успеха.\n4. Интерактивный просмотр прогресса через веб-интерфейс удаленного репозитория.\n5. Периодическое предоставление скринкастов новой функциональности и результатов тестов.\n6. Использование гугла для поиска информации, прежде чем обращаться к разработчику.\n7. Оценка эффективности контроля по результатам работы.\n8. Разделение сложных задач на более мелкие фрагменты.\n9. Учет времени на отладку и автоматизацию тривиальных задач.\n10. Эффективное консультирование: подготовка вопросов, ограничение времени звонков, деликатное общение.\n\nАвтор подчеркивает, что программисты работают небольшими сессиями, поэтому важно планировать общение и не отвлекать их глупыми вопросами."}
{"Text":"Астрономы считают, что спутники связи SpaceX, OneWeb и других компаний угрожают будущему астрономии / Habr               \n\n16-11-2019\nНесколько дней назад компания Илона Маска SpaceX успешно запустила Falcon 9 с 60 спутниками Starlink. Они уже вышли на орбиту, проблем с работой устройств нет. Со спутниками все хорошо, но астрономы опасаются что в скором будущем астрономические наблюдения при помощи телескопов будет сложно проводить из-за влияния систем связи на орбите . Проблема в том, что спутники отражают солнечный свет и очень хорошо видны с Земли. Пока их несколько десятков, астрономам они не мешают. Но SpaceX заявила, что в скором будущем количество аппаратов будет увеличено на 30 000. И в этом случае у астрономов появятся значительные проблемы. С момента выхода на орбиту первых спутников астрономы пытаются понять, насколько значительным будет уровень светового загрязнения. Илон Маск и представители SpaceX пока не комментируют этот вопрос. Whoa, it worked!! — Elon Musk (@elonmusk) October 22, 2019 В октябре Маск сообщил, что его компания запросила разрешения у Федеральной комиссии по связи США разрешения на увеличение количества спутников с 12 000 (здесь разрешение уже получено) до 42 000. Соответственно, количество аппаратов на орбите Земли увеличится многократно — в 8 раз по сравнению с текущим количеством всех спутников, которые находятся в околоземном космическом пространстве . Джеймс Лоэнталь, астроном из Колледжа Смит, заявил, что когда впервые увидел вереницу новых спутников Starlink, вышедших на орбиту Земли, сразу понял, что ночное небо никогда не будет прежним. После того, как аппараты вышли на расчетные орбиты, они стали невидимы для жителей городов, где уровень светового загрязнения и так очень высокий. Но, конечно, астрономы все это видят. SpaceX сотрудничает с астрономическим сообществом. В частности, компания пытается минимизировать «шум» от радиоизлучения своих аппаратов, астрономы работают с тем же спектром частот, что и Starlink. И эта проблема решается, в отличие от светового загрязнения. Представитель SpaceX как-то заявил, что для снижения яркости спутников их стороны, которые обращены к Земле, будут окрашивать в черный цвет. По мнению ряда астрономов, это не поможет. В частности, Энтони Тайсон, ученый из Калифорнийского университета, утверждает, что для астрономических наблюдений за очень тусклыми объектами Starlink представляют собой серьезную проблему — в какой бы цвет их ни окрашивали. Тайсон знает, о чем говорит, он является сотрудником на Large Synoptic Survey Telescope — огромном телескопе, который будет сканировать все небо каждые три дня. Этот телескоп, по словам ученых, должен прояснить ряд моментов с темной энергией, темной материей, историей происхождения нашей галактики и других объектов. И вот как раз на этот телескоп спутники будут оказывать очень сильное негативное влияние. Модель, построенная Тайсоном, показала, что любые объекты с характеристиками Starlink будут попадать в «поле зрения» телескопа. В частности, они станут причиной появления различных световых артефактов на фотографиях. Более того, при работе телескопа в режиме длительной экспозиции на снимках будут видны яркие световые полосы, которые закроют телескопу пространство за ними. Конечно, компьютерные технологии дают возможность убрать большую часть артефактов. И если бы спутников было 12 000, с проблемой можно было бы справиться. Но когда их станет 42 000, то артефактов станет настолько много, что их уже невозможно будет убрать. Другой астроном, Патрик Сейтзер, из Мичиганского университета, заявил, что все это — очень пугающие ученых перспективы. Кроме того, спутники усложнят наблюдение и мониторинг опасных астероидов, которые периодически приближаются к Земле. Строить прогнозы и модели движения этих астероидов будет сложно. Team is go for launch of 60 Starlink sats tomorrow—heaviest payload to date, first re-flight of a fairing, and first Falcon 9 to fly a fourth mission. Watching 1 sat that may not orbit raise; if not, 100% of its components will quickly burn up in Earth’s atmosphere pic.twitter.com/OrI8L0ntFK — SpaceX (@SpaceX) November 11, 2019 Но и это только часть проблемы. Ведь спутники запускает не только SpaceX. О своих планах по созданию сетей связи с использованием тысяч и тысяч аппаратов на орбите рассказывали Amazon, Telesat, OneWeb и другие компании. Лоуэнталь считает, что все это угрожает не просто астрономическим наблюдениям, но самой астрономии. С ним согласны его коллеги. Что касается орбитальных телескопов, с ними все хорошо, но вот работа астрономов, наблюдающих за космосом с поверхности Земли, усложнится. Стоит отметить, что ни в одной стране мира нет законов, которые регулировали бы уровень светового загрязнения неба. Международных законов в этой области тоже нет, так что пока компании, которые хотят запускать спутники, могут делать что им угодно, согласовывая, конечно, свои действия с регуляторами вроде Федеральной комиссии по связи США. В целом, многие астрономы считают, что астрономия попала в зависимость от действий корпораций. И будущее наблюдений за космосом в значительной степени зависит от доброй воли этих компаний.\n","metric":0.6650390625,"metric_input":"Текст статьи:\nАстрономы считают, что спутники связи SpaceX, OneWeb и других компаний угрожают будущему астрономии / Habr               \n\n16-11-2019\nНесколько дней назад компания Илона Маска SpaceX успешно запустила Falcon 9 с 60 спутниками Starlink. Они уже вышли на орбиту, проблем с работой устройств нет. Со спутниками все хорошо, но астрономы опасаются что в скором будущем астрономические наблюдения при помощи телескопов будет сложно проводить из-за влияния систем связи на орбите . Проблема в том, что спутники отражают солнечный свет и очень хорошо видны с Земли. Пока их несколько десятков, астрономам они не мешают. Но SpaceX заявила, что в скором будущем количество аппаратов будет увеличено на 30 000. И в этом случае у астрономов появятся значительные проблемы. С момента выхода на орбиту первых спутников астрономы пытаются понять, насколько значительным будет уровень светового загрязнения. Илон Маск и представители SpaceX пока не комментируют этот вопрос. Whoa, it worked!! — Elon Musk (@elonmusk) October 22, 2019 В октябре Маск сообщил, что его компания запросила разрешения у Федеральной комиссии по связи США разрешения на увеличение количества спутников с 12 000 (здесь разрешение уже получено) до 42 000. Соответственно, количество аппаратов на орбите Земли увеличится многократно — в 8 раз по сравнению с текущим количеством всех спутников, которые находятся в околоземном космическом пространстве . Джеймс Лоэнталь, астроном из Колледжа Смит, заявил, что когда впервые увидел вереницу новых спутников Starlink, вышедших на орбиту Земли, сразу понял, что ночное небо никогда не будет прежним. После того, как аппараты вышли на расчетные орбиты, они стали невидимы для жителей городов, где уровень светового загрязнения и так очень высокий. Но, конечно, астрономы все это видят. SpaceX сотрудничает с астрономическим сообществом. В частности, компания пытается минимизировать «шум» от радиоизлучения своих аппаратов, астрономы работают с тем же спектром частот, что и Starlink. И эта проблема решается, в отличие от светового загрязнения. Представитель SpaceX как-то заявил, что для снижения яркости спутников их стороны, которые обращены к Земле, будут окрашивать в черный цвет. По мнению ряда астрономов, это не поможет. В частности, Энтони Тайсон, ученый из Калифорнийского университета, утверждает, что для астрономических наблюдений за очень тусклыми объектами Starlink представляют собой серьезную проблему — в какой бы цвет их ни окрашивали. Тайсон знает, о чем говорит, он является сотрудником на Large Synoptic Survey Telescope — огромном телескопе, который будет сканировать все небо каждые три дня. Этот телескоп, по словам ученых, должен прояснить ряд моментов с темной энергией, темной материей, историей происхождения нашей галактики и других объектов. И вот как раз на этот телескоп спутники будут оказывать очень сильное негативное влияние. Модель, построенная Тайсоном, показала, что любые объекты с характеристиками Starlink будут попадать в «поле зрения» телескопа. В частности, они станут причиной появления различных световых артефактов на фотографиях. Более того, при работе телескопа в режиме длительной экспозиции на снимках будут видны яркие световые полосы, которые закроют телескопу пространство за ними. Конечно, компьютерные технологии дают возможность убрать большую часть артефактов. И если бы спутников было 12 000, с проблемой можно было бы справиться. Но когда их станет 42 000, то артефактов станет настолько много, что их уже невозможно будет убрать. Другой астроном, Патрик Сейтзер, из Мичиганского университета, заявил, что все это — очень пугающие ученых перспективы. Кроме того, спутники усложнят наблюдение и мониторинг опасных астероидов, которые периодически приближаются к Земле. Строить прогнозы и модели движения этих астероидов будет сложно. Team is go for launch of 60 Starlink sats tomorrow—heaviest payload to date, first re-flight of a fairing, and first Falcon 9 to fly a fourth mission. Watching 1 sat that may not orbit raise; if not, 100% of its components will quickly burn up in Earth’s atmosphere pic.twitter.com/OrI8L0ntFK — SpaceX (@SpaceX) November 11, 2019 Но и это только часть проблемы. Ведь спутники запускает не только SpaceX. О своих планах по созданию сетей связи с использованием тысяч и тысяч аппаратов на орбите рассказывали Amazon, Telesat, OneWeb и другие компании. Лоуэнталь считает, что все это угрожает не просто астрономическим наблюдениям, но самой астрономии. С ним согласны его коллеги. Что касается орбитальных телескопов, с ними все хорошо, но вот работа астрономов, наблюдающих за космосом с поверхности Земли, усложнится. Стоит отметить, что ни в одной стране мира нет законов, которые регулировали бы уровень светового загрязнения неба. Международных законов в этой области тоже нет, так что пока компании, которые хотят запускать спутники, могут делать что им угодно, согласовывая, конечно, свои действия с регуляторами вроде Федеральной комиссии по связи США. В целом, многие астрономы считают, что астрономия попала в зависимость от действий корпораций. И будущее наблюдений за космосом в значительной степени зависит от доброй воли этих компаний.\n\n\nКраткое содержание:\nСтатья обсуждает опасения астрономов по поводу влияния спутников связи на астрономические наблюдения. Запуск спутников Starlink компанией SpaceX вызвал беспокойство, так как количество спутников на орбите может увеличиться до 42 000, что может создать проблемы для астрономических наблюдений. Спутники отражают солнечный свет и могут создавать световое загрязнение, мешающее наблюдениям за очень тусклыми объектами. Это может привести к появлению световых артефактов на фотографиях и ярким световым полосам на длительных экспозициях телескопов. Кроме того, спутники могут усложнить наблюдение и мониторинг опасных астероидов. Астрономы считают, что астрономия попала в зависимость от действий корпораций, и будущее наблюдений за космосом зависит от доброй воли компаний.","summary":"Статья обсуждает опасения астрономов по поводу влияния спутников связи на астрономические наблюдения. Запуск спутников Starlink компанией SpaceX вызвал беспокойство, так как количество спутников на орбите может увеличиться до 42 000, что может создать проблемы для астрономических наблюдений. Спутники отражают солнечный свет и могут создавать световое загрязнение, мешающее наблюдениям за очень тусклыми объектами. Это может привести к появлению световых артефактов на фотографиях и ярким световым полосам на длительных экспозициях телескопов. Кроме того, спутники могут усложнить наблюдение и мониторинг опасных астероидов. Астрономы считают, что астрономия попала в зависимость от действий корпораций, и будущее наблюдений за космосом зависит от доброй воли компаний."}
{"Text":"Symfony Components, Event Dispatcher (теория, часть 1) / Habr                        \n\n13-04-2010\nПривет. Данный перевод задумывается как первая (всего будет две) часть документации по компоненту Event Dispatcher. Этот компонент входит в семейство Symfony компонент, но в то же время он независим и его можно использовать не подключая фреймворк, что делает его еще более ценным. Перевод еще можно воспринимать как обзор легковесной реализации паттерна Наблюдатель (Observer) в php, который призван усилить взаимодействие между классами. Хочу сказать что семейство компонент сейчас активно перерабатывается для совместимости с версией PHP >= 5.3, и планируется использовать с новой версией фреймворка Symfony 2. Код новой версии компоненты можно посмотреть здесь. Названия и суть методов в новой редакции почти не поменялись, так что материал будет полезен и изучающим код компонент под PHP 5.3. Итак начнем. Event Dispatcher Component — что это? Symfony Event Dispatcher — это PHP библиотека, представляющая собой легковесную реализацию шаблона проектирования Наблюдатель (Observer). Это хороший путь сделать ваш код гибче. Это также хороший путь сделать код пригодным для расширения сторонними разработчиками (разработка плагинов). Сторонний код прислушивается к специфическим событиям путем создания обратных вызовов (callbacks), а диспетчер делает вызовы когда ваш код извещает эти события. Очень быстрый Главное преимущество компонента Event Dispatcher в Symfony это быть настолько быстрым насколько это возможно. Нет надобности объявлять интерфейсы или расширять сложные классы, события представляют собой простые строки, а код оповещения очень легковесный. Добавляйте любое количество обработчиков и вызовов без дополнительных проблем. Вступление Объектно ориентированный подход проделал длинный путь чтобы код ваших проектов был расширяемым. Созданием классов с четко определенным функционалом вы делаете код более гибким. Если пользователь хочет изменить поведение класса, он может расширить его используя субкласс для переопределения поведения. Но если пользователь хочет распространить эти изменения на других пользователей, которые сделали свои субклассы для изменения поведения, код наследования становится неоднозначным. Как пример из жизни, может быть вы захотите предусмотреть систему плагинов для своего класса. У плагина должна быть возможность добавлять методы, или делать что-то перед началом или по окончании работы метода, без взаимодействия с другими плагинами. Эту проблему нелегко решить путем единичного наследования, а множественное наследование (если бы оно было возможно в PHP) имеет свои недостатки. Главная цель компонента Symfony Event Dispatcher — это позволить объектам общаться вместе не зная друг друга. Это становится возможным благодаря центральному объекту, диспетчеру (dispatcher). Объекты (обработчики, listeners) могут связываться с диспетчером для прослушивания специфических событий, и некоторые другие могут посылать события диспетчеру. Как только событие послано, диспетчер запустит соответствующие обработчики. События В отличие от многих других реализаций Наблюдателя, вы не должны создавать класс для создания нового события. Все события, конечно же, остаются объектами, но все события есть экземплярами встроенного класса sfEvent. Примечание: конечно же вы можете расширить класс sfEvent для расширения события далее, или включить некоторые ограничения, но в большинстве случаев это добавит новый неоправданный уровень сложности. Событие однозначно идентифицируется строкой. По соглашению, лучше всего использовать буквы в нижнем регистре, цифры и нижние подчеркивания (_) для имен событий. Кроме того, для лучшей организации ваших событий, хорошим соглашением будет префиксировать имена событий пространством имен, за которым следует точка (.). Вот примеры хорошо названных событий: change_culture response.filter_content Как вы наверное заметили, имена событий содержат слово для индикации того что должно произойти при наступлении события. Диспетчер Диспетчер это объект, ответственный за контроль регистра обработчиков и их вызов, когда происходит одно из событий. По умолчанию, класс диспетчера это sfEventDispatcher: $dispatcher = new sfEventDispatcher(); Объекты событий Объект события, класса sfEvent, сохраняет информацию об объявляемом событии. Его конструктор принимает три аргумента: Контекст (субъект) события (в большинстве случаев это объект объявляющий событие, но может быть и null); Имя события; Массив параметров для передачи их обработчикам (по умолчанию пустой массив). Чаще всего событие вызывается в контексте объекта, первый аргумент почти всегда $this: $ event = new sfEvent($ this , 'user.change_culture' , array( 'culture' => $culture)); У объекта события есть несколько методов для получения информации о событии: getName(): возвращает идентификатор события; getSubject(): возвращает объект субьекта (контекст), пристыкованного к событию; getParameters(): возвращает массив параметров события. Объект события может быть также использован как массив для получения параметров: echo $ event [ 'culture' ]; Добавление обработчиков Очевидно, вам нужно присоединить некоторые обработчики к диспетчеру перед тем как он может быть полезным. Обращение к методу диспетчера connect() ассоциирует PHP callable с событием. Метод connect() принимает два аргумента: Имя события; PHP callable для вызова, когда событие произойдет. Примечание: PHP callable это переменная PHP, которая может быть использована функцией call_user_func() и возвращает true когда передается в функцию is_callable(). Строка представляет функцию, а массив может представлять метод объекта или метод класса. $dispatcher->connect( 'user.change_culture' , $callable); Как только обработчик зарегистрирован с помощью диспетчера событий, он ждет вызова соответствующего события. Диспетчер событий хранит запись всех обработчиков событий, и знает какой из них вызвать когда произойдет событие. Примечание: обработчики вызываются диспетчером событий в том порядке, котором вы их присоединяли. Для предыдущего примера, $callable будет вызвано диспетчером тогда, когда user.change_culture событие будет объявлено объектом. Когда вызываются обработчики, диспетчер передает им объект sfEvent как параметр. То есть, обработчик получает объект события как свой первый аргумент. Объявление событий Событие может быть объявлено одним из трех методов: notify(); notifyUntil(); filter(); notify() Метод notify() запускает в оборот все обработчики. $dispatcher->notify($ event ); Используя метод notify(), вы можете быть уверены что все зарегистрированные обработчики объявленного события были выполнены но ни один из них не может возвратить значение субъекту. notifyUntil() В некоторых случаях, вам нужно позволить обработчику остановить событие и препятствовать тому чтобы другие обработчики узнали о происшедшем событии. В этом случае, вам нужно использовать notifyUntil() вместо notify(). Тогда диспетчер вызовет все обработчики пока один из них не вернет true, и после этого остановит реакцию на событие: $dispatcher->notifyUntil($ event ); Обработчик, который остановит цепочку может также вызвать метод setReturnValue() для возврата субъекту некоторого значения. Тот кто вызвал событие может проверить что обработчик обработал событие путем вызова метода isProcessed(): if ($ event ->isProcessed()) { $ret = $ event ->getReturnValue(); // ... } filter() Метод filter() требует все обработчики фильтровать заданное значение, передаваемое создателем события во втором аргументе, и получаемое обработчиком как второй аргумент: $dispatcher->filter($ event , $response->getContent()); Все обработчики получают значение и они должны возвратить отфильтрованное значение, которое они изменили или нет. Все обработчики гарантированно будут вызваны. Тот кто объявил событие может получить отфильтрованное значение вызвав метод getReturnValue(): $ret = $ event ->getReturnValue(); Далее во второй части я планирую перевод практических примеров использования Event Dispatcher. А затем, возможно, будет еще топик по использованию компонента в реальном проекте.\n","metric":0.1927490233999999969,"metric_input":"Текст статьи:\nSymfony Components, Event Dispatcher (теория, часть 1) / Habr                        \n\n13-04-2010\nПривет. Данный перевод задумывается как первая (всего будет две) часть документации по компоненту Event Dispatcher. Этот компонент входит в семейство Symfony компонент, но в то же время он независим и его можно использовать не подключая фреймворк, что делает его еще более ценным. Перевод еще можно воспринимать как обзор легковесной реализации паттерна Наблюдатель (Observer) в php, который призван усилить взаимодействие между классами. Хочу сказать что семейство компонент сейчас активно перерабатывается для совместимости с версией PHP >= 5.3, и планируется использовать с новой версией фреймворка Symfony 2. Код новой версии компоненты можно посмотреть здесь. Названия и суть методов в новой редакции почти не поменялись, так что материал будет полезен и изучающим код компонент под PHP 5.3. Итак начнем. Event Dispatcher Component — что это? Symfony Event Dispatcher — это PHP библиотека, представляющая собой легковесную реализацию шаблона проектирования Наблюдатель (Observer). Это хороший путь сделать ваш код гибче. Это также хороший путь сделать код пригодным для расширения сторонними разработчиками (разработка плагинов). Сторонний код прислушивается к специфическим событиям путем создания обратных вызовов (callbacks), а диспетчер делает вызовы когда ваш код извещает эти события. Очень быстрый Главное преимущество компонента Event Dispatcher в Symfony это быть настолько быстрым насколько это возможно. Нет надобности объявлять интерфейсы или расширять сложные классы, события представляют собой простые строки, а код оповещения очень легковесный. Добавляйте любое количество обработчиков и вызовов без дополнительных проблем. Вступление Объектно ориентированный подход проделал длинный путь чтобы код ваших проектов был расширяемым. Созданием классов с четко определенным функционалом вы делаете код более гибким. Если пользователь хочет изменить поведение класса, он может расширить его используя субкласс для переопределения поведения. Но если пользователь хочет распространить эти изменения на других пользователей, которые сделали свои субклассы для изменения поведения, код наследования становится неоднозначным. Как пример из жизни, может быть вы захотите предусмотреть систему плагинов для своего класса. У плагина должна быть возможность добавлять методы, или делать что-то перед началом или по окончании работы метода, без взаимодействия с другими плагинами. Эту проблему нелегко решить путем единичного наследования, а множественное наследование (если бы оно было возможно в PHP) имеет свои недостатки. Главная цель компонента Symfony Event Dispatcher — это позволить объектам общаться вместе не зная друг друга. Это становится возможным благодаря центральному объекту, диспетчеру (dispatcher). Объекты (обработчики, listeners) могут связываться с диспетчером для прослушивания специфических событий, и некоторые другие могут посылать события диспетчеру. Как только событие послано, диспетчер запустит соответствующие обработчики. События В отличие от многих других реализаций Наблюдателя, вы не должны создавать класс для создания нового события. Все события, конечно же, остаются объектами, но все события есть экземплярами встроенного класса sfEvent. Примечание: конечно же вы можете расширить класс sfEvent для расширения события далее, или включить некоторые ограничения, но в большинстве случаев это добавит новый неоправданный уровень сложности. Событие однозначно идентифицируется строкой. По соглашению, лучше всего использовать буквы в нижнем регистре, цифры и нижние подчеркивания (_) для имен событий. Кроме того, для лучшей организации ваших событий, хорошим соглашением будет префиксировать имена событий пространством имен, за которым следует точка (.). Вот примеры хорошо названных событий: change_culture response.filter_content Как вы наверное заметили, имена событий содержат слово для индикации того что должно произойти при наступлении события. Диспетчер Диспетчер это объект, ответственный за контроль регистра обработчиков и их вызов, когда происходит одно из событий. По умолчанию, класс диспетчера это sfEventDispatcher: $dispatcher = new sfEventDispatcher(); Объекты событий Объект события, класса sfEvent, сохраняет информацию об объявляемом событии. Его конструктор принимает три аргумента: Контекст (субъект) события (в большинстве случаев это объект объявляющий событие, но может быть и null); Имя события; Массив параметров для передачи их обработчикам (по умолчанию пустой массив). Чаще всего событие вызывается в контексте объекта, первый аргумент почти всегда $this: $ event = new sfEvent($ this , 'user.change_culture' , array( 'culture' => $culture)); У объекта события есть несколько методов для получения информации о событии: getName(): возвращает идентификатор события; getSubject(): возвращает объект субьекта (контекст), пристыкованного к событию; getParameters(): возвращает массив параметров события. Объект события может быть также использован как массив для получения параметров: echo $ event [ 'culture' ]; Добавление обработчиков Очевидно, вам нужно присоединить некоторые обработчики к диспетчеру перед тем как он может быть полезным. Обращение к методу диспетчера connect() ассоциирует PHP callable с событием. Метод connect() принимает два аргумента: Имя события; PHP callable для вызова, когда событие произойдет. Примечание: PHP callable это переменная PHP, которая может быть использована функцией call_user_func() и возвращает true когда передается в функцию is_callable(). Строка представляет функцию, а массив может представлять метод объекта или метод класса. $dispatcher->connect( 'user.change_culture' , $callable); Как только обработчик зарегистрирован с помощью диспетчера событий, он ждет вызова соответствующего события. Диспетчер событий хранит запись всех обработчиков событий, и знает какой из них вызвать когда произойдет событие. Примечание: обработчики вызываются диспетчером событий в том порядке, котором вы их присоединяли. Для предыдущего примера, $callable будет вызвано диспетчером тогда, когда user.change_culture событие будет объявлено объектом. Когда вызываются обработчики, диспетчер передает им объект sfEvent как параметр. То есть, обработчик получает объект события как свой первый аргумент. Объявление событий Событие может быть объявлено одним из трех методов: notify(); notifyUntil(); filter(); notify() Метод notify() запускает в оборот все обработчики. $dispatcher->notify($ event ); Используя метод notify(), вы можете быть уверены что все зарегистрированные обработчики объявленного события были выполнены но ни один из них не может возвратить значение субъекту. notifyUntil() В некоторых случаях, вам нужно позволить обработчику остановить событие и препятствовать тому чтобы другие обработчики узнали о происшедшем событии. В этом случае, вам нужно использовать notifyUntil() вместо notify(). Тогда диспетчер вызовет все обработчики пока один из них не вернет true, и после этого остановит реакцию на событие: $dispatcher->notifyUntil($ event ); Обработчик, который остановит цепочку может также вызвать метод setReturnValue() для возврата субъекту некоторого значения. Тот кто вызвал событие может проверить что обработчик обработал событие путем вызова метода isProcessed(): if ($ event ->isProcessed()) { $ret = $ event ->getReturnValue(); // ... } filter() Метод filter() требует все обработчики фильтровать заданное значение, передаваемое создателем события во втором аргументе, и получаемое обработчиком как второй аргумент: $dispatcher->filter($ event , $response->getContent()); Все обработчики получают значение и они должны возвратить отфильтрованное значение, которое они изменили или нет. Все обработчики гарантированно будут вызваны. Тот кто объявил событие может получить отфильтрованное значение вызвав метод getReturnValue(): $ret = $ event ->getReturnValue(); Далее во второй части я планирую перевод практических примеров использования Event Dispatcher. А затем, возможно, будет еще топик по использованию компонента в реальном проекте.\n\n\nКраткое содержание:\nSymfony Event Dispatcher Component — это PHP библиотека, представляющая собой легковесную реализацию шаблона проектирования Наблюдатель (Observer). Это хороший путь сделать ваш код гибче. Это также хороший путь сделать код пригодным для расширения сторонними разработчиками (разработка плагинов). Сторонний код прислушивается к специфическим событиям путем создания обратных вызовов (callbacks), а диспетчер делает вызовы когда ваш код извещает эти события. Очень быстрый Главное преимущество компонента Event Dispatcher в Symfony это быть настолько быстрым насколько это возможно. Нет надобности объявлять интерфейсы или расширять сложные классы, события представляют собой простые строки, а код оповещения очень легковесный. Добавляйте любое количество обработчиков и вызовов без дополнительных проблем. Вступление Объектно ориентированный подход проделал длинный путь чтобы код ваших проектов был расширяемым. Созданием классов с четко определенным функционалом вы делаете код более гибким. Если пользователь хочет изменить поведение класса, он может расширить его используя субкласс для переопределения поведения. Но если пользователь хочет распространить эти изменения на других пользователей, которые сделали свои субклассы для изменения поведения, код наследования становится неоднозначным. Как пример из жизни, может быть вы захотите предусмотреть систему плагинов для своего класса. У плагина должна быть возможность добавлять методы, или делать что-то перед началом или по окончании работы метода, без взаимодействия с другими плагинами. Эту проблему нелегко решить путем единичного наследования, а множественное наследование (если бы оно было возможно в PHP) имеет свои недостатки. Главная цель компонента Symfony Event Dispatcher — это позволить объектам общаться вместе не зная друг друга. Это становится возможным благодаря центральному объекту, диспетчеру (dispatcher). Объекты (обработчики, listeners) могут связываться с диспетчером для прослушивания специфических событий, и некоторые другие могут посылать события диспетчеру. Как только событие послано, диспетчер запустит соответствующие обработчики. События В отличие от многих других реализаций Наблюдателя, вы не должны создавать класс для создания нового события. Все события, конечно же, остаются объектами, но все","summary":"Symfony Event Dispatcher Component — это PHP библиотека, представляющая собой легковесную реализацию шаблона проектирования Наблюдатель (Observer). Это хороший путь сделать ваш код гибче. Это также хороший путь сделать код пригодным для расширения сторонними разработчиками (разработка плагинов). Сторонний код прислушивается к специфическим событиям путем создания обратных вызовов (callbacks), а диспетчер делает вызовы когда ваш код извещает эти события. Очень быстрый Главное преимущество компонента Event Dispatcher в Symfony это быть настолько быстрым насколько это возможно. Нет надобности объявлять интерфейсы или расширять сложные классы, события представляют собой простые строки, а код оповещения очень легковесный. Добавляйте любое количество обработчиков и вызовов без дополнительных проблем. Вступление Объектно ориентированный подход проделал длинный путь чтобы код ваших проектов был расширяемым. Созданием классов с четко определенным функционалом вы делаете код более гибким. Если пользователь хочет изменить поведение класса, он может расширить его используя субкласс для переопределения поведения. Но если пользователь хочет распространить эти изменения на других пользователей, которые сделали свои субклассы для изменения поведения, код наследования становится неоднозначным. Как пример из жизни, может быть вы захотите предусмотреть систему плагинов для своего класса. У плагина должна быть возможность добавлять методы, или делать что-то перед началом или по окончании работы метода, без взаимодействия с другими плагинами. Эту проблему нелегко решить путем единичного наследования, а множественное наследование (если бы оно было возможно в PHP) имеет свои недостатки. Главная цель компонента Symfony Event Dispatcher — это позволить объектам общаться вместе не зная друг друга. Это становится возможным благодаря центральному объекту, диспетчеру (dispatcher). Объекты (обработчики, listeners) могут связываться с диспетчером для прослушивания специфических событий, и некоторые другие могут посылать события диспетчеру. Как только событие послано, диспетчер запустит соответствующие обработчики. События В отличие от многих других реализаций Наблюдателя, вы не должны создавать класс для создания нового события. Все события, конечно же, остаются объектами, но все"}
{"Text":"Провалы футурологов. Идеи из фильмов, которые не осуществились / Habr             \n\n19-06-2018\nПрогресс — это прекрасно. Современные технологии упрощают жизнь человека, делают ее безопаснее, а при должном уровне фантазии — еще и интереснее. Фантасты часто создают на страницах своих произведений мир, повествующий нам не только о сложных ситуациях и моральном выборе людей из будущего, но и о гаджетах, новых технологиях, которые, по их предположениям, могут использоваться через годы. Мы сейчас живем в то время, которое и было для писателей XX века будущим. Его они и пытались описать. Интересно сравнить их предположения с тем, что мы имеем сейчас. Мобильные устройства, беспроводная связь, лазеры на службе медицины и ВПК, умные протезы, неплохо прокачавшиеся со времен ноги Долговязого Джона Сильвера и прочее. Бывает, что фантасты, авторы комиксов и фильмов рисовали в своем воображении устройства, которые представлялись им вполне себе возможными уже лет через 20, и при этом очень удобными и полезными. Однако реальность оказалась такова, что создать такой девайс людям будущего (то есть нам с вами) оказалось не под силу. Работая в области информационных технологий, мы с вами постоянно сталкиваемся с новыми идеями. У нас в ЛАНИТ есть специальные команды, отвечающие за акселерацию внешних и внутренних стартапов. Тема изобретательства, инноваций, поиска жизнеспособных проектов нам очень близка. Поэтому сегодня мы поговорим об устройствах, которые наверняка пригодились бы каждому, но которые современная наука не в силах воплотить в жизнь. Пока. Ховерборд («Назад в будущее-2») Легендарная левитирующая доска, лишь однажды попав на глаза зрителю при первом просмотре фильма, почти наверняка остается в памяти навсегда. Еще бы! Кто бы отказался от такой штуки: плюсов по сравнению с обычным скейтом или лонгбордом хоть отбавляй. Ховерборд и выглядит круто, и не так требователен к поверхности (читай – вообще не требователен, у Марти вполне себе получалось рассекать и на водной глади), и впечатления от езды на нем наверняка отличные. Почему его оказалось сложно создать в реальности? Потому что промышленное (да и не только промышленное) производство антигравитаторов пока не ведется. Однако это не мешает энтузиастам предпринимать попытки сделать такую штуку из подручных средств. Джейми Хайнеман и ребята из «Разрушителей мифов» объединили мощный насос для уборки листьев и доску для серфинга. Но это тоже из разряда судна на воздушной подушке. Ближе всего к оригиналу подошел Lexus, создав свой Lexus Hoverboard. Правда, сделали его по технологии сверхпроводника и магнитов, охлаждаемых жидким азотом, что позволяет кататься только по конкретной поверхности. Но выглядит эффектно, не спорим. Lightsaber (Star Wars) Не столь важно, за кого вы болели в «Звездных Войнах» и чьи взгляды на жизнь разделяли. Световой меч вам наверняка хотелось, и без разницы, какого он был бы цвета. К сожалению, рабочего экземпляра пока тоже нет. Сама конструкция и свойства лазера подвергались частой критике, мол, лазеры — это лучи, и при попытке дуэли на таких мечах они будут просто проходить один сквозь другой, да и луч он на то и луч, чтобы стремиться в бесконечность, а не заканчиваться на строго определенной длине клинка. Но мы верим, что когда-нибудь кто-нибудь все же осилит. Световой меч смог стать поистине легендарной штукой — тонны сувенирной продукции, сотни отсылок из других фильмов и сериалов и многое другое. Плащ Бэтмена Одна из составляющих многофункционального костюма Брюса Уэйна — прекрасный плащ с эффектом памяти. С его помощью Бэтмену удавалось планировать с большой высоты без особого риска вывихнуть лодыжку, а также уклоняться от пуль. Электрический импульс от перчаток изменял форму и свойства плаща под нужды супергероя. Говоря об умных плащах, стоит вспомнить еще плащ Спауна, тоже неоднократно спасавший ему жизнь. А также плащ левитации Доктора Стрейнджа. Чего-то подобного в ближайшее время мы вряд ли дождемся, пока максимум, что предлагает наука и техника из этой области, это использование генно-модифицированных шелкопрядов, из нитей которых создают экспериментальную броню для солдат США. Костюм Железного Человека Самая крутая суперсила — деньги. Раз уж мы заговорили о Брюсе Уэйне, будет странно не вспомнить не менее богатого гения — Энтони Эдварда Старка. Одна из версий его прекрасного костюма была хороша своей компактностью — вся мощь Железного Человека в небольшом чемоданчике. К слову, из всех описанных в этой статье гаджетов именно костюм Старка пока ближе всего к реальности. Фактически это сильно прокачанный экзоскелет, которые сейчас создают как и гиганты вроде DARPA, так и энтузиасты, которые в школе вместо гуманитарных классов решили выбрать физмат. Главная проблема — источник питания. Если представить, что кому-то удастся собрать экзоскелет с такими же способностями, как у Железного Человека, включая возможность летать и использовать оружие, энергии ему потребуется, прямо скажем, немало. То есть без толстенного силового кабеля за спиной или миниатюрного ядерного реактора особо не побегаешь. И если Тони смог в домашних условиях создать новый химический элемент, лишь слегка попортив мебель, то у нас с этим пока проблемы. Фил Колсон и Тони Старк находят правильное применение щита Капитана Америки Голографические интерфейсы Чтобы не отходить далеко от темы Железного Человека, вспомним его рабочий интерфейс. Здесь та же ситуация, что и в «Особом мнении» или старом космическом сериале LEXX. Выглядит голографический интерфейс, без сомнения, великолепно. Но на текущем этапе развития технологий добиться подобной интуитивной отдачи от прикосновения руки человека к голографическому элементу дико сложно. Даже если мы говорим о какой-то простой картинке, которую можно подвигать жестами, чтобы лучше рассмотреть, опыт взаимодействия оставляет желать лучшего. Агенты Колсон и Уорд пытаются работать с голографическим столом (Marvel's agents of s.h.i.e.l.d.). А уж в таких сложных системах, как в «Железном Человеке» или «Особом мнении», где взаимодействие идет с сотней элементов, которые перекрывают другу друга в разных плоскостях. И даже если это когда-нибудь осилят, всегда останется проблема с управлением — рука будет перекрывать элементы, и это взаимодействие тоже придется как-то отрабатывать. У героя Тома Круза (Джона Андертона), правда, были специальные перчатки, но это не панацея. Работы в этой области пока ведутся для создания интерактивных обучающих и справочных материалов. Нейрализатор («Люди в черном») По сюжету фильма это одна из разработок специального подразделения, чтобы аккуратно стирать память гражданским, которые увидели то, что видеть не следовало. На сегодняшний день аналогов прибора или медикаментов, позволяющих стирать память без какого-то вреда для головного мозга человека, не придумано (или мы просто не помним, что видели что-то подобное). Но штуковина, без сомнения, нужная. Представьте, сколько раз можно было бы защищать свою диссертацию. Или быстренько исправить ситуацию, в которой вы на вопрос «А меня не полнит это платье?» случайно ответили честно. Пульт перемотки жизни и машины времени Машины времени, пульты перемотки жизни, девайсы, останавливающие время, — все это пока бесконечно далеко от реальности и, скорее всего, так и останется лишь хорошей функциональной фантазией, которая сможет дать жизнь еще сотням книг и фильмов про временные парадоксы. Уменьшители / увеличители предметов От «Дорогая, я уменьшил детей» и «Внутреннего пространства» до «Человека-Муравья» — сфера применения гаджетов, позволяющих в мгновение ока сделать из человека его миниатюрную копию или же гиганта, довольно велика. Текущий уровень развития технологий не предусматривает столь кардинального (и быстрого) взаимодействия с объектами материального мира. Увы. А как было бы здорово — не получилось припарковать свой крузак у торгового центра, пикнул кнопочкой и прицепил крузак на брелок для ключей. Должно быть дико удобно. Особенно «любителям» параллельной парковки. Что радует — так это то, что какими бы странными, наивными и чересчур оптимистичными ни казались нам идеи фантастов прошлого, в том числе и они двигают сегодняшний прогресс, спасибо энтузиастам и тем, кто хочет воплотить это в жизнь. Вперед к новым открытиям! Активные разработки экзоскелетов, что для военных нужд, что для медицинских, совершенствование систем управления, голосовые ассистенты, миниатюрные роботы, дроны, многофункциональные спутники, беспилотные автомобили — мы живем в прекрасное время, которое фантасты еще полвека назад могли только вообразить. Становление и бета-тестирование всех этих технологий происходит прямо на наших глазах. Мы движемся вперед. И это главное. P.S. Кстати, у этой статьи скоро будет продолжение. В новом посте мы собрали для вас идеи футурологов, которые осуществились в отличие от перечисленных в этом материале.\n","metric":0.53955078130000000414,"metric_input":"Текст статьи:\nПровалы футурологов. Идеи из фильмов, которые не осуществились / Habr             \n\n19-06-2018\nПрогресс — это прекрасно. Современные технологии упрощают жизнь человека, делают ее безопаснее, а при должном уровне фантазии — еще и интереснее. Фантасты часто создают на страницах своих произведений мир, повествующий нам не только о сложных ситуациях и моральном выборе людей из будущего, но и о гаджетах, новых технологиях, которые, по их предположениям, могут использоваться через годы. Мы сейчас живем в то время, которое и было для писателей XX века будущим. Его они и пытались описать. Интересно сравнить их предположения с тем, что мы имеем сейчас. Мобильные устройства, беспроводная связь, лазеры на службе медицины и ВПК, умные протезы, неплохо прокачавшиеся со времен ноги Долговязого Джона Сильвера и прочее. Бывает, что фантасты, авторы комиксов и фильмов рисовали в своем воображении устройства, которые представлялись им вполне себе возможными уже лет через 20, и при этом очень удобными и полезными. Однако реальность оказалась такова, что создать такой девайс людям будущего (то есть нам с вами) оказалось не под силу. Работая в области информационных технологий, мы с вами постоянно сталкиваемся с новыми идеями. У нас в ЛАНИТ есть специальные команды, отвечающие за акселерацию внешних и внутренних стартапов. Тема изобретательства, инноваций, поиска жизнеспособных проектов нам очень близка. Поэтому сегодня мы поговорим об устройствах, которые наверняка пригодились бы каждому, но которые современная наука не в силах воплотить в жизнь. Пока. Ховерборд («Назад в будущее-2») Легендарная левитирующая доска, лишь однажды попав на глаза зрителю при первом просмотре фильма, почти наверняка остается в памяти навсегда. Еще бы! Кто бы отказался от такой штуки: плюсов по сравнению с обычным скейтом или лонгбордом хоть отбавляй. Ховерборд и выглядит круто, и не так требователен к поверхности (читай – вообще не требователен, у Марти вполне себе получалось рассекать и на водной глади), и впечатления от езды на нем наверняка отличные. Почему его оказалось сложно создать в реальности? Потому что промышленное (да и не только промышленное) производство антигравитаторов пока не ведется. Однако это не мешает энтузиастам предпринимать попытки сделать такую штуку из подручных средств. Джейми Хайнеман и ребята из «Разрушителей мифов» объединили мощный насос для уборки листьев и доску для серфинга. Но это тоже из разряда судна на воздушной подушке. Ближе всего к оригиналу подошел Lexus, создав свой Lexus Hoverboard. Правда, сделали его по технологии сверхпроводника и магнитов, охлаждаемых жидким азотом, что позволяет кататься только по конкретной поверхности. Но выглядит эффектно, не спорим. Lightsaber (Star Wars) Не столь важно, за кого вы болели в «Звездных Войнах» и чьи взгляды на жизнь разделяли. Световой меч вам наверняка хотелось, и без разницы, какого он был бы цвета. К сожалению, рабочего экземпляра пока тоже нет. Сама конструкция и свойства лазера подвергались частой критике, мол, лазеры — это лучи, и при попытке дуэли на таких мечах они будут просто проходить один сквозь другой, да и луч он на то и луч, чтобы стремиться в бесконечность, а не заканчиваться на строго определенной длине клинка. Но мы верим, что когда-нибудь кто-нибудь все же осилит. Световой меч смог стать поистине легендарной штукой — тонны сувенирной продукции, сотни отсылок из других фильмов и сериалов и многое другое. Плащ Бэтмена Одна из составляющих многофункционального костюма Брюса Уэйна — прекрасный плащ с эффектом памяти. С его помощью Бэтмену удавалось планировать с большой высоты без особого риска вывихнуть лодыжку, а также уклоняться от пуль. Электрический импульс от перчаток изменял форму и свойства плаща под нужды супергероя. Говоря об умных плащах, стоит вспомнить еще плащ Спауна, тоже неоднократно спасавший ему жизнь. А также плащ левитации Доктора Стрейнджа. Чего-то подобного в ближайшее время мы вряд ли дождемся, пока максимум, что предлагает наука и техника из этой области, это использование генно-модифицированных шелкопрядов, из нитей которых создают экспериментальную броню для солдат США. Костюм Железного Человека Самая крутая суперсила — деньги. Раз уж мы заговорили о Брюсе Уэйне, будет странно не вспомнить не менее богатого гения — Энтони Эдварда Старка. Одна из версий его прекрасного костюма была хороша своей компактностью — вся мощь Железного Человека в небольшом чемоданчике. К слову, из всех описанных в этой статье гаджетов именно костюм Старка пока ближе всего к реальности. Фактически это сильно прокачанный экзоскелет, которые сейчас создают как и гиганты вроде DARPA, так и энтузиасты, которые в школе вместо гуманитарных классов решили выбрать физмат. Главная проблема — источник питания. Если представить, что кому-то удастся собрать экзоскелет с такими же способностями, как у Железного Человека, включая возможность летать и использовать оружие, энергии ему потребуется, прямо скажем, немало. То есть без толстенного силового кабеля за спиной или миниатюрного ядерного реактора особо не побегаешь. И если Тони смог в домашних условиях создать новый химический элемент, лишь слегка попортив мебель, то у нас с этим пока проблемы. Фил Колсон и Тони Старк находят правильное применение щита Капитана Америки Голографические интерфейсы Чтобы не отходить далеко от темы Железного Человека, вспомним его рабочий интерфейс. Здесь та же ситуация, что и в «Особом мнении» или старом космическом сериале LEXX. Выглядит голографический интерфейс, без сомнения, великолепно. Но на текущем этапе развития технологий добиться подобной интуитивной отдачи от прикосновения руки человека к голографическому элементу дико сложно. Даже если мы говорим о какой-то простой картинке, которую можно подвигать жестами, чтобы лучше рассмотреть, опыт взаимодействия оставляет желать лучшего. Агенты Колсон и Уорд пытаются работать с голографическим столом (Marvel's agents of s.h.i.e.l.d.). А уж в таких сложных системах, как в «Железном Человеке» или «Особом мнении», где взаимодействие идет с сотней элементов, которые перекрывают другу друга в разных плоскостях. И даже если это когда-нибудь осилят, всегда останется проблема с управлением — рука будет перекрывать элементы, и это взаимодействие тоже придется как-то отрабатывать. У героя Тома Круза (Джона Андертона), правда, были специальные перчатки, но это не панацея. Работы в этой области пока ведутся для создания интерактивных обучающих и справочных материалов. Нейрализатор («Люди в черном») По сюжету фильма это одна из разработок специального подразделения, чтобы аккуратно стирать память гражданским, которые увидели то, что видеть не следовало. На сегодняшний день аналогов прибора или медикаментов, позволяющих стирать память без какого-то вреда для головного мозга человека, не придумано (или мы просто не помним, что видели что-то подобное). Но штуковина, без сомнения, нужная. Представьте, сколько раз можно было бы защищать свою диссертацию. Или быстренько исправить ситуацию, в которой вы на вопрос «А меня не полнит это платье?» случайно ответили честно. Пульт перемотки жизни и машины времени Машины времени, пульты перемотки жизни, девайсы, останавливающие время, — все это пока бесконечно далеко от реальности и, скорее всего, так и останется лишь хорошей функциональной фантазией, которая сможет дать жизнь еще сотням книг и фильмов про временные парадоксы. Уменьшители / увеличители предметов От «Дорогая, я уменьшил детей» и «Внутреннего пространства» до «Человека-Муравья» — сфера применения гаджетов, позволяющих в мгновение ока сделать из человека его миниатюрную копию или же гиганта, довольно велика. Текущий уровень развития технологий не предусматривает столь кардинального (и быстрого) взаимодействия с объектами материального мира. Увы. А как было бы здорово — не получилось припарковать свой крузак у торгового центра, пикнул кнопочкой и прицепил крузак на брелок для ключей. Должно быть дико удобно. Особенно «любителям» параллельной парковки. Что радует — так это то, что какими бы странными, наивными и чересчур оптимистичными ни казались нам идеи фантастов прошлого, в том числе и они двигают сегодняшний прогресс, спасибо энтузиастам и тем, кто хочет воплотить это в жизнь. Вперед к новым открытиям! Активные разработки экзоскелетов, что для военных нужд, что для медицинских, совершенствование систем управления, голосовые ассистенты, миниатюрные роботы, дроны, многофункциональные спутники, беспилотные автомобили — мы живем в прекрасное время, которое фантасты еще полвека назад могли только вообразить. Становление и бета-тестирование всех этих технологий происходит прямо на наших глазах. Мы движемся вперед. И это главное. P.S. Кстати, у этой статьи скоро будет продолжение. В новом посте мы собрали для вас идеи футурологов, которые осуществились в отличие от перечисленных в этом материале.\n\n\nКраткое содержание:\nВ статье рассматриваются идеи из фантастических фильмов и комиксов, которые не были реализованы в реальности.\n\n- Ховерборд (Назад в будущее-2) - устройство для левитации, которое пока не удалось создать.\n- Световой меч (Star Wars) - лазер, который не может функционировать как меч из-за свойств лазера.\n- Плащ Бэтмена - плащ с эффектом памяти, который пока не может изменять форму и свойства.\n- Костюм Железного Человека - экзоскелет, который требует много энергии.\n- Голографические интерфейсы - голографические элементы, которые сложно взаимодействовать с ними.\n- Нейрализатор (\"Люди в черном\") - устройство для стирания памяти, которое не существует на данный момент.\n- Пульт перемотки жизни и машины времени - устройства, которые пока не могут перематывать время.\n- Уменьшители / увеличители предметов - гаджеты для изменения размеров объектов, которые не реализованы.\n\nВ целом, статья призывает не останавливаться на достигнутом и продолжать развивать технологии, даже если некоторые идеи из фантастики пока не осуществились.","summary":"В статье рассматриваются идеи из фантастических фильмов и комиксов, которые не были реализованы в реальности.\n\n- Ховерборд (Назад в будущее-2) - устройство для левитации, которое пока не удалось создать.\n- Световой меч (Star Wars) - лазер, который не может функционировать как меч из-за свойств лазера.\n- Плащ Бэтмена - плащ с эффектом памяти, который пока не может изменять форму и свойства.\n- Костюм Железного Человека - экзоскелет, который требует много энергии.\n- Голографические интерфейсы - голографические элементы, которые сложно взаимодействовать с ними.\n- Нейрализатор (\"Люди в черном\") - устройство для стирания памяти, которое не существует на данный момент.\n- Пульт перемотки жизни и машины времени - устройства, которые пока не могут перематывать время.\n- Уменьшители / увеличители предметов - гаджеты для изменения размеров объектов, которые не реализованы.\n\nВ целом, статья призывает не останавливаться на достигнутом и продолжать развивать технологии, даже если некоторые идеи из фантастики пока не осуществились."}
{"Text":"Топ-10 ключевых анонсов от Microsoft в 2016 году / Habr             \n\n23-12-2016\nТрадиционно провожаем уходящий 2016 год подборкой ключевых анонсов от корпорации Microsoft. Под катом мы собрали десять самых ярких и запоминающихся новостей. 1. Стратегия демократизации искусственного интеллекта Выступая на Microsoft Ignite 2016 в конце сентября, Сатья Наделла озвучил амбициозную цель, которую поставила перед собой компания: «Демократизация искусственного интеллекта». Она заключается в том, чтобы сделать технологию AI доступной и понятной для всех. Сатья также поделился уникальным на тот момент кейсом с Uber. Компания внедрила новую технологию Real-Time ID Check для повышения уровня безопасности водителей и пассажиров, реализованную на основе Microsoft Cognitive Services. Её суть заключается в том, что каждый водитель устанавливает мобильное приложение, которое распознаёт его с помощью камеры телефона и только после успешного прохождения идентификации водитель сможет приступить к работе. Стоит отметить, что первые шаги в направлении новой стратегии в России Microsoft сделали в начале октября — русский язык появился в Skype Translator. Осенью появились ещё две новости о крупнейших партнёрствах в области AI: Facebook, Amazon, IBM, Google и Microsoft объявили о появлении партнерской программы для совместной работы в области развития искусственного интеллекта, Partnership on AI. Microsoft и OpenAI стали партнерами. Напоминаем, что OpenAI — компания, которая занимается исследованиями искусственного интеллекта, и была основана совместно Илоном Маском, Сэмом Олтманом, Грегом Брокманом и Ильей Суцкевером. Также OpenAI выбрала Microsoft Azure в качестве своей основной облачной платформы для совершенствования их исследований, а также создания новых инструментов и технологий, которые возможны только в облаке. В середине декабря Microsoft поделились первыми результатами по достижению своей цели: анонс Cortana Devices SDK, говорящие боты в Skype, обновления Microsoft Bot Framework, партнерство с Harman Kardon для создания уникальной акустической системы с применением AI. Буквально несколько дней назад компания объявила о появлении в открытом доступе набора данных для обучения компьютеров письменной речи. Он получил название Microsoft Machine Reading Comprehension (MS MARCO) и содержит в себе более 100 тысяч англоязычных вопросов и ответов на них, собранных с помощью анонимных реальных запросов в Bing и Cortana. 2. Microsoft приобретает компанию Xamarin В начале года Microsoft официально объявили о приобретении Xamarin — крупнейшей платформы для разработки мобильных приложений. В рамках последующих конференций (Build 2016, Xamarin Evolve 2016 и Connect(); //2016) прозвучало еще несколько ключевых анонсов этого года: Xamarin стал абсолютно бесплатным для всех редакций Visual Studio. Анонсирован Xamarin SDK в Open Source, а также проект Mono, являющийся ядром инструментов Xamarin, перевыпущен под открытой лицензией MIT. Появилась поддержка Appium в Xamarin Test Cloud. 3. Microsoft присоединилась к Linux Foundation Microsoft стала платиновым участником некоммерческого консорциума развития Linux, Linux Foundation. В число платиновых участников входят такие компании как Cisco, Fujitsu, HP, Huawei, IBM, Intel, NEC, Oracle, Qualcomm и Samsung. С одной стороны, эта новость многих могла шокировать, но с другой — стоит обратить внимание, что в последнее время появляется все больше новостей от Microsoft о публикации исходного кода PowerShell, Visual Studio Code, JavaScript-движок Edge, анонс Linux-версии SQL Server, а также в начале года Ubuntu интегрировали в Windows 10. 4. Запуск Microsoft Bot Framework + Azure Bot-as-a-Service на Linux В рамках конференции Build 2016 был анонсирован новый инструмент для создания ботов, Microsoft Bot Framework, включая API. Он включает в себя Bot Builder SDK, Microsoft Cognitive Services и Bot Connector, который позволяет привязать бота к одному или нескольким каналам общения, например Skype, Slack, Telegram и другие. Также приятной новостью стал запуск превью-версии Azure Bot-as-a-Service на Linux с поддержкой контейнеров и ASP.NET Core. 5. Юбилейное обновление Windows 10 2 августа Юбилейное обновление Windows 10 стало доступно на устройствах по всему миру. Среди всех функций можно выделить: Windows Ink, который позволяет быстро и легко создавать заметки, рисунки или наброски ваших идей на экране. Повышенный уровень безопасности с Windows Defender и защищенный доступ к приложениям и сайтам с помощью Windows Hello. Улучшенная энергоэффективность с новым Microsoft Edge и появление расширений. 6. C# 7.0: масштабное обновление языка программирования В 2000 году группой разработчиков компании Microsoft был представлен новый язык программирования С#, который сочетал в себе всю выразительность объектно-ориентированных языков программирования, вроде Java, и мощность С++. В этом году появилась новость о новой версии языка C# 7.0, часть функций которого стала общедоступна с выходом Visual Studio “15” Preview 4. Полный набор фич вы сможете найти в статье. Напоминаем, что в начале сентября мы делились с вами интересными фактами про С#. 7. Visual Studio для Mac, превью-версия Visual Studio 2017 и Visual Studio Mobile Center Preview В этом году одна из самых масштабных конференций для разработчиков Connect(); //2016 была насыщена новостями. Ключевым анонсом можно назвать выход Visual Studio для Mac. Это полноценная среда разработки, предназначенная для создания мобильных приложений, облачных сервисов и приложений для Mac. Первый превью-релиз включает поддержку создания нативных iOS и Android приложений на C# и F# с помощью Xamarin, а также создание бэкэнда с помощью Azure сервисов, и, естественно, поддержку ASP.NET Core. Стала доступна превью-версия Visual Studio 2017 Release Candidate. Visual Studio 2017 RC содержит в себе, как исправления, так и новую функциональность, ранее не доступную в предыдущих версиях. А так же этот выпуск Visual Studio 2017 RC включает в себя возможность разрабатывать production решения. Для мобильных разработчиков главной новостью стал анонс Visual Studio Mobile Center Preview. Mobile Center работает с iOS и Android приложениями, написанными на Objective-C, Swift, Java, Xamarin, React Native и собирает вместе облачные сервисы и инструменты для обеспечения жизненного цикла создания приложений для разработчиков, цель которых быстрая разработка качественных мобильных приложений. Инструмент предоставляет возможности для разработки, сборки, тестирования, мониторинга и развертывания приложений на устройства и облегчает разработчикам возможность масштабировать приложения на все платформы и реализовывать в них самые новые и популярные функции. 8. Microsoft HoloLens для разработчиков В начале 2016 года cтартовали поставки Microsoft HoloLens Dev Kit. Мы уже делились с вами руководством по написанию Hello World для HoloLens, а также техническими характеристиками устройства. Осенью HoloLens удостоились престижной награды Popular Mechanics’ 2016 Breakthrough Awards, которая присуждается «новаторам в мире технологий за последний год, тем, кто вышел за границы и сделали мир немного лучше». В данный момент существует несколько российских компаний, которые занимаются активной разработкой под HoloLens. В том числе, HoloGroup, которые выпустили первое русскоязычное приложение дополненной реальности, а также Zengalt, которые стали первой российской компанией, выпустившей приложение под HoloLens. 9. Microsoft стала компанией №1 по открытому коду на GitHub В середине 2016 года появилась новость о том, что Microsoft опередила Facebook и стала компанией №1 по открытому коду на GitHub! Первым об этом написал один из пользователей Reddit. 10. Вышел новый сервис для общения – Microsoft Teams Корпорация Microsoft представила новое приложение для коммуникаций в Office 365 – Microsoft Teams. Оно организовано по принципу чата и расширяет возможности совместной работы в Microsoft Office 365. Также было анонсировано Microsoft Teams Developer Preview. С помощью функции Tabs можно настраивать отдельные темы для общения, также она предоставляет быстрый доступ к часто используемым документам и приложениям. Tabs могут быть созданы для таких сервисов Office 365, как OneNote, SharePoint и Planner, или сторонних решений. Аналогично Exchange новое пространство Microsoft Teams поддерживает уведомления и обновления от сторонних сервисов, например, Twitter или GitHub. Также осуществлена поддержка Microsoft Bot Framework, что позволяет встроить в корпоративную рабочую среду интеллектуальные сервисы. Напоминаем, что мы собрали самые яркие анонсы. В комментариях, вы можете дополнить список или привести свой. :) Доброго Кода!\n","metric":0.6962890625,"metric_input":"Текст статьи:\nТоп-10 ключевых анонсов от Microsoft в 2016 году / Habr             \n\n23-12-2016\nТрадиционно провожаем уходящий 2016 год подборкой ключевых анонсов от корпорации Microsoft. Под катом мы собрали десять самых ярких и запоминающихся новостей. 1. Стратегия демократизации искусственного интеллекта Выступая на Microsoft Ignite 2016 в конце сентября, Сатья Наделла озвучил амбициозную цель, которую поставила перед собой компания: «Демократизация искусственного интеллекта». Она заключается в том, чтобы сделать технологию AI доступной и понятной для всех. Сатья также поделился уникальным на тот момент кейсом с Uber. Компания внедрила новую технологию Real-Time ID Check для повышения уровня безопасности водителей и пассажиров, реализованную на основе Microsoft Cognitive Services. Её суть заключается в том, что каждый водитель устанавливает мобильное приложение, которое распознаёт его с помощью камеры телефона и только после успешного прохождения идентификации водитель сможет приступить к работе. Стоит отметить, что первые шаги в направлении новой стратегии в России Microsoft сделали в начале октября — русский язык появился в Skype Translator. Осенью появились ещё две новости о крупнейших партнёрствах в области AI: Facebook, Amazon, IBM, Google и Microsoft объявили о появлении партнерской программы для совместной работы в области развития искусственного интеллекта, Partnership on AI. Microsoft и OpenAI стали партнерами. Напоминаем, что OpenAI — компания, которая занимается исследованиями искусственного интеллекта, и была основана совместно Илоном Маском, Сэмом Олтманом, Грегом Брокманом и Ильей Суцкевером. Также OpenAI выбрала Microsoft Azure в качестве своей основной облачной платформы для совершенствования их исследований, а также создания новых инструментов и технологий, которые возможны только в облаке. В середине декабря Microsoft поделились первыми результатами по достижению своей цели: анонс Cortana Devices SDK, говорящие боты в Skype, обновления Microsoft Bot Framework, партнерство с Harman Kardon для создания уникальной акустической системы с применением AI. Буквально несколько дней назад компания объявила о появлении в открытом доступе набора данных для обучения компьютеров письменной речи. Он получил название Microsoft Machine Reading Comprehension (MS MARCO) и содержит в себе более 100 тысяч англоязычных вопросов и ответов на них, собранных с помощью анонимных реальных запросов в Bing и Cortana. 2. Microsoft приобретает компанию Xamarin В начале года Microsoft официально объявили о приобретении Xamarin — крупнейшей платформы для разработки мобильных приложений. В рамках последующих конференций (Build 2016, Xamarin Evolve 2016 и Connect(); //2016) прозвучало еще несколько ключевых анонсов этого года: Xamarin стал абсолютно бесплатным для всех редакций Visual Studio. Анонсирован Xamarin SDK в Open Source, а также проект Mono, являющийся ядром инструментов Xamarin, перевыпущен под открытой лицензией MIT. Появилась поддержка Appium в Xamarin Test Cloud. 3. Microsoft присоединилась к Linux Foundation Microsoft стала платиновым участником некоммерческого консорциума развития Linux, Linux Foundation. В число платиновых участников входят такие компании как Cisco, Fujitsu, HP, Huawei, IBM, Intel, NEC, Oracle, Qualcomm и Samsung. С одной стороны, эта новость многих могла шокировать, но с другой — стоит обратить внимание, что в последнее время появляется все больше новостей от Microsoft о публикации исходного кода PowerShell, Visual Studio Code, JavaScript-движок Edge, анонс Linux-версии SQL Server, а также в начале года Ubuntu интегрировали в Windows 10. 4. Запуск Microsoft Bot Framework + Azure Bot-as-a-Service на Linux В рамках конференции Build 2016 был анонсирован новый инструмент для создания ботов, Microsoft Bot Framework, включая API. Он включает в себя Bot Builder SDK, Microsoft Cognitive Services и Bot Connector, который позволяет привязать бота к одному или нескольким каналам общения, например Skype, Slack, Telegram и другие. Также приятной новостью стал запуск превью-версии Azure Bot-as-a-Service на Linux с поддержкой контейнеров и ASP.NET Core. 5. Юбилейное обновление Windows 10 2 августа Юбилейное обновление Windows 10 стало доступно на устройствах по всему миру. Среди всех функций можно выделить: Windows Ink, который позволяет быстро и легко создавать заметки, рисунки или наброски ваших идей на экране. Повышенный уровень безопасности с Windows Defender и защищенный доступ к приложениям и сайтам с помощью Windows Hello. Улучшенная энергоэффективность с новым Microsoft Edge и появление расширений. 6. C# 7.0: масштабное обновление языка программирования В 2000 году группой разработчиков компании Microsoft был представлен новый язык программирования С#, который сочетал в себе всю выразительность объектно-ориентированных языков программирования, вроде Java, и мощность С++. В этом году появилась новость о новой версии языка C# 7.0, часть функций которого стала общедоступна с выходом Visual Studio “15” Preview 4. Полный набор фич вы сможете найти в статье. Напоминаем, что в начале сентября мы делились с вами интересными фактами про С#. 7. Visual Studio для Mac, превью-версия Visual Studio 2017 и Visual Studio Mobile Center Preview В этом году одна из самых масштабных конференций для разработчиков Connect(); //2016 была насыщена новостями. Ключевым анонсом можно назвать выход Visual Studio для Mac. Это полноценная среда разработки, предназначенная для создания мобильных приложений, облачных сервисов и приложений для Mac. Первый превью-релиз включает поддержку создания нативных iOS и Android приложений на C# и F# с помощью Xamarin, а также создание бэкэнда с помощью Azure сервисов, и, естественно, поддержку ASP.NET Core. Стала доступна превью-версия Visual Studio 2017 Release Candidate. Visual Studio 2017 RC содержит в себе, как исправления, так и новую функциональность, ранее не доступную в предыдущих версиях. А так же этот выпуск Visual Studio 2017 RC включает в себя возможность разрабатывать production решения. Для мобильных разработчиков главной новостью стал анонс Visual Studio Mobile Center Preview. Mobile Center работает с iOS и Android приложениями, написанными на Objective-C, Swift, Java, Xamarin, React Native и собирает вместе облачные сервисы и инструменты для обеспечения жизненного цикла создания приложений для разработчиков, цель которых быстрая разработка качественных мобильных приложений. Инструмент предоставляет возможности для разработки, сборки, тестирования, мониторинга и развертывания приложений на устройства и облегчает разработчикам возможность масштабировать приложения на все платформы и реализовывать в них самые новые и популярные функции. 8. Microsoft HoloLens для разработчиков В начале 2016 года cтартовали поставки Microsoft HoloLens Dev Kit. Мы уже делились с вами руководством по написанию Hello World для HoloLens, а также техническими характеристиками устройства. Осенью HoloLens удостоились престижной награды Popular Mechanics’ 2016 Breakthrough Awards, которая присуждается «новаторам в мире технологий за последний год, тем, кто вышел за границы и сделали мир немного лучше». В данный момент существует несколько российских компаний, которые занимаются активной разработкой под HoloLens. В том числе, HoloGroup, которые выпустили первое русскоязычное приложение дополненной реальности, а также Zengalt, которые стали первой российской компанией, выпустившей приложение под HoloLens. 9. Microsoft стала компанией №1 по открытому коду на GitHub В середине 2016 года появилась новость о том, что Microsoft опередила Facebook и стала компанией №1 по открытому коду на GitHub! Первым об этом написал один из пользователей Reddit. 10. Вышел новый сервис для общения – Microsoft Teams Корпорация Microsoft представила новое приложение для коммуникаций в Office 365 – Microsoft Teams. Оно организовано по принципу чата и расширяет возможности совместной работы в Microsoft Office 365. Также было анонсировано Microsoft Teams Developer Preview. С помощью функции Tabs можно настраивать отдельные темы для общения, также она предоставляет быстрый доступ к часто используемым документам и приложениям. Tabs могут быть созданы для таких сервисов Office 365, как OneNote, SharePoint и Planner, или сторонних решений. Аналогично Exchange новое пространство Microsoft Teams поддерживает уведомления и обновления от сторонних сервисов, например, Twitter или GitHub. Также осуществлена поддержка Microsoft Bot Framework, что позволяет встроить в корпоративную рабочую среду интеллектуальные сервисы. Напоминаем, что мы собрали самые яркие анонсы. В комментариях, вы можете дополнить список или привести свой. :) Доброго Кода!\n\n\nКраткое содержание:\nMicrosoft приобрела компанию Xamarin, присоединилась к Linux Foundation, выпустила юбилейное обновление Windows 10, представила новые инструменты для создания ботов и приложений, а также стала компанией №1 по открытому коду на GitHub. Кроме того, были анонсированы новые сервисы для общения, такие как Microsoft Teams.","summary":"Microsoft приобрела компанию Xamarin, присоединилась к Linux Foundation, выпустила юбилейное обновление Windows 10, представила новые инструменты для создания ботов и приложений, а также стала компанией №1 по открытому коду на GitHub. Кроме того, были анонсированы новые сервисы для общения, такие как Microsoft Teams."}
{"Text":"5.8 млн IOPS: зачем так много? / Блог компании Kingston Technology / Хабр                                                         \n\nПривет Хабр! Наборы данных для Big Data и машинного обучения экспоненциально растут и надо успевать их обрабатывать. Наш пост о еще одной инновационной технологии в области высокопроизводительных вычислений (HPC, High Performance Computing), показанной на стенде Kingston на\nSupercomputing-2019\n. Это применение Hi-End систем хранения данных (СХД) в серверах с графическими процессорами (GPU) и технологией шины GPUDirect Storage. Благодаря прямому обмену данными между СХД и GPU, минуя CPU, на порядок ускоряется загрузка данных в GPU-ускорители, поэтому приложения Big Data выполняются на максимуме производительности, которую обеспечивают GPU. В свою очередь, разработчиков HPC-систем интересуют достижения в области СХД с высочайшей скоростью ввода/вывода — таких, какие выпускает Kingston.\n\n## Производительность GPU опережает загрузку данных ##\n\nС тех пор, как в 2007 году была создана CUDA — программно-аппаратная архитектура параллельных вычислений на основе GPU для разработки приложений общего назначения, аппаратные возможности самих GPU выросли невероятно. Сегодня GPU находят все большее применение в области HPC-приложений, таких как большие данные (Big Data), машинное обучение (ML, machine learning) и глубокое изучение (DL, deep learning).\n\nОтметим, что несмотря на схожесть терминов, два последних — это алгоритмически разные задачи. ML обучает компьютер на основе структурированных данных, а DL — на основе отклика от нейронной сети. Пример, помогающий понять различия, довольно прост. Предположим, что компьютер должен отличать фото кошек и собак, которые загружаются с СХД. Для ML следует подать набор изображений с множеством тегов, каждый из которых определяет какую-то одну особенность животного. Для DL достаточно загрузить намного большее число изображений, но всего лишь с одним тегом «это кошка» или «это собака». DL очень похоже на то, как учат маленьких детей — им просто показывают изображения собак и кошек в книжках и в жизни (чаще всего, даже не объясняя детальное различие), а мозг ребенка сам начинает определять тип животного после некоторого критического числа картинок для сравнения (по оценкам, речь идет всего о сотне-другой показов за все время раннего детства). Алгоритмы DL еще не настолько совершенны: чтобы также успешно могла работать над определением образов нейронная сеть, необходимо подать и обработать в GPU миллионы изображений.\n\nИтог предисловия: на базе GPU можно строить HPC-приложения в области Big Data, ML и DL, но существует проблема — наборы данных настолько велики, что время, затрачиваемое на загрузку данных из системы хранения в GPU, начинает снижать общую производительность приложения. Иными словами, быстрые графические процессоры остаются недогруженными ввиду медленного ввода-вывода данных, поступающих от других подсистем. Разница в скорости ввода/вывода GPU и шины к CPU/СХД может быть на порядок.\n\n## Как работает технология GPUDirect Storage? ##\n\nПроцесс ввода-вывода контролируется CPU, также как и процесс загрузки данных из хранилища в графические процессоры для последующей обработки. Отсюда возник запрос на технологию, которая обеспечила бы прямой доступ между GPU и NVMe-дисками для быстрого взаимодействия друг с другом. Первой такую технология предложила NVIDIA и назвала ее GPUDirect Storage. По сути, это разновидность ранее разработанной ими технологии GPUDirect RDMA (Remote Direct Memory Address).\n\n_Дженсен Хуанг, генеральный директор NVIDIA, презентует GPUDirect Storage как разновидность GPUDirect RDMA на выставке SС-19. Источник: NVIDIA_\n\nРазница между GPUDirect RDMA и GPUDirect Storage — в устройствах, между которыми осуществляется адресация. Технология GPUDirect RDMA переназначена для перемещения данных непосредственно между входной картой сетевого интерфейса (NIC) и памятью GPU, а GPUDirect Storage обеспечивает прямой путь передачи данных между локальным или удаленным хранилищем, таким как NVMe или NVMe через Fabric (NVMe-oF) и памятью GPU.\n\nОба варианта, GPUDirect RDMA и GPUDirect Storage, избегают лишних перемещений данных через буфер в памяти CPU и позволяют механизму прямого доступа к памяти (DMA) перемещать данные от сетевой карты или хранилища сразу в память GPU или из нее — и все это без нагрузки на центральный процессор. Для GPUDirect Storage местоположение хранилища не имеет значения: это может быть NVME-диск внутри юнита с GPU, внутри стойки или подключен по сети как NVMe-oF.\n\n_Схема работы GPUDirect Storage. Источник: NVIDIA_\n\n## Hi-End СХД на NVMe востребованы на рынке HPC-приложений ##\n\nПонимая, что с появлением GPUDirect Storage интерес крупных клиентов будет обращен на предложение систем хранения со скоростью ввода/вывода, соответствующей пропускной способности GPU, на выставке SC-19 Kingston показал демо системы, состоящей из СХД на базе NVMe-дисков и юнита с GPU, в которой проводился анализ тысяч спутниковых снимков в секунду. О такой СХД на базе 10 накопителей DC1000M U.2 NVMe мы уже писали\nв репортаже с выставки суперкомпьютеров\n.\n\n_СХД на базе 10 накопителей DC1000M U.2 NVMe достойно дополняет сервер с графическими ускорителями. Источник: Kingston_\n\nТакая СХД выполняется в виде стоечного юнита 1U или больше и может масштабироваться в зависимости от числа дисков DC1000M U.2 NVMe, где каждый емкостью 3.84-7.68 ТБ. DC1000M является первой моделью NVMe SSD в форм-факторе U.2 в линейке накопителей Kingston для дата-центров. Он обладает рейтингом выносливости (DWPD, Drive writes per day), позволяющим перезаписывать данные на полную емкость один раз в день в течение гарантированного срока службы накопителя.\n\nВ тесте fio v3.13 на операционной системе Ubuntu 18.04.3 LTS, Linux kernel 5.0.0-31-generic выставочный образец СХД показал скорость чтения (Sustained Read) 5.8 млн IOPS при устойчивой пропускной способности (Sustained Bandwidth) 23.8 Гбит/с.\n\nАриэль Перес, бизнес-менеджер SSD в Kingston, так охарактеризовал новые СХД: «Мы готовы снабдить следующее поколение серверов SSD-решениями U.2 NVMe, чтобы устранить многие узкие места в передаче данных, которые традиционно были связаны с системой хранения. Сочетание накопителей NVMe SSD и нашей премиальной оперативной памяти Server Premier DRAM делает Kingston одним из самых полных в отрасли поставщиков комплексных решений для обработки данных».\n\n_Тест gfio v3.13 показал пропускную способность 23.8 Гбит/с для демонстрационной СХД на дисках DC1000M U.2 NVMe. Источник: Kingston_\n\nКак будет выглядеть типичная система для HPC-приложений, где реализована технология GPUDirect Storage или аналогичная ей? Это архитектура с физическим разделением функциональных блоков в пределах стойки: один-два юнита на оперативную память, еще несколько на вычислительные узлы GPU и CPU и один или несколько юнитов под СХД.\n\nС анонсом GPUDirect Storage и возможным появлением аналогичных технологий у других вендоров GPU, для Kingston расширяется спрос на СХД, рассчитанные на применение в высокопроизводительных вычислениях. Маркером будет скорость чтения данных из СХД, сопоставимая с пропускной способностью 40- или 100-Гбитных сетевых карт на входе в вычислительный юнит с GPU. Таким образом, ультраскоростные СХД, в том числе внешние NVMe через Fabric, из экзотики станут мэйнстримом для HPC-приложений. Кроме науки и финансовых расчетов, они найдут применение во многих других практических областях, таких как системы безопасности уровня мегаполиса Safe City или центров наблюдения на транспорте, где требуется скорость распознавания и идентификации на уровне миллионов HD-изображений в секунду», — обозначил рыночную нишу топовых СХД\n\nДополнительную информацию о продуктах Kingston можно найти на\nофициальном сайте\nкомпании.","metric":0.42919921880000000414,"metric_input":"Текст статьи:\n5.8 млн IOPS: зачем так много? / Блог компании Kingston Technology / Хабр                                                         \n\nПривет Хабр! Наборы данных для Big Data и машинного обучения экспоненциально растут и надо успевать их обрабатывать. Наш пост о еще одной инновационной технологии в области высокопроизводительных вычислений (HPC, High Performance Computing), показанной на стенде Kingston на\nSupercomputing-2019\n. Это применение Hi-End систем хранения данных (СХД) в серверах с графическими процессорами (GPU) и технологией шины GPUDirect Storage. Благодаря прямому обмену данными между СХД и GPU, минуя CPU, на порядок ускоряется загрузка данных в GPU-ускорители, поэтому приложения Big Data выполняются на максимуме производительности, которую обеспечивают GPU. В свою очередь, разработчиков HPC-систем интересуют достижения в области СХД с высочайшей скоростью ввода/вывода — таких, какие выпускает Kingston.\n\n## Производительность GPU опережает загрузку данных ##\n\nС тех пор, как в 2007 году была создана CUDA — программно-аппаратная архитектура параллельных вычислений на основе GPU для разработки приложений общего назначения, аппаратные возможности самих GPU выросли невероятно. Сегодня GPU находят все большее применение в области HPC-приложений, таких как большие данные (Big Data), машинное обучение (ML, machine learning) и глубокое изучение (DL, deep learning).\n\nОтметим, что несмотря на схожесть терминов, два последних — это алгоритмически разные задачи. ML обучает компьютер на основе структурированных данных, а DL — на основе отклика от нейронной сети. Пример, помогающий понять различия, довольно прост. Предположим, что компьютер должен отличать фото кошек и собак, которые загружаются с СХД. Для ML следует подать набор изображений с множеством тегов, каждый из которых определяет какую-то одну особенность животного. Для DL достаточно загрузить намного большее число изображений, но всего лишь с одним тегом «это кошка» или «это собака». DL очень похоже на то, как учат маленьких детей — им просто показывают изображения собак и кошек в книжках и в жизни (чаще всего, даже не объясняя детальное различие), а мозг ребенка сам начинает определять тип животного после некоторого критического числа картинок для сравнения (по оценкам, речь идет всего о сотне-другой показов за все время раннего детства). Алгоритмы DL еще не настолько совершенны: чтобы также успешно могла работать над определением образов нейронная сеть, необходимо подать и обработать в GPU миллионы изображений.\n\nИтог предисловия: на базе GPU можно строить HPC-приложения в области Big Data, ML и DL, но существует проблема — наборы данных настолько велики, что время, затрачиваемое на загрузку данных из системы хранения в GPU, начинает снижать общую производительность приложения. Иными словами, быстрые графические процессоры остаются недогруженными ввиду медленного ввода-вывода данных, поступающих от других подсистем. Разница в скорости ввода/вывода GPU и шины к CPU/СХД может быть на порядок.\n\n## Как работает технология GPUDirect Storage? ##\n\nПроцесс ввода-вывода контролируется CPU, также как и процесс загрузки данных из хранилища в графические процессоры для последующей обработки. Отсюда возник запрос на технологию, которая обеспечила бы прямой доступ между GPU и NVMe-дисками для быстрого взаимодействия друг с другом. Первой такую технология предложила NVIDIA и назвала ее GPUDirect Storage. По сути, это разновидность ранее разработанной ими технологии GPUDirect RDMA (Remote Direct Memory Address).\n\n_Дженсен Хуанг, генеральный директор NVIDIA, презентует GPUDirect Storage как разновидность GPUDirect RDMA на выставке SС-19. Источник: NVIDIA_\n\nРазница между GPUDirect RDMA и GPUDirect Storage — в устройствах, между которыми осуществляется адресация. Технология GPUDirect RDMA переназначена для перемещения данных непосредственно между входной картой сетевого интерфейса (NIC) и памятью GPU, а GPUDirect Storage обеспечивает прямой путь передачи данных между локальным или удаленным хранилищем, таким как NVMe или NVMe через Fabric (NVMe-oF) и памятью GPU.\n\nОба варианта, GPUDirect RDMA и GPUDirect Storage, избегают лишних перемещений данных через буфер в памяти CPU и позволяют механизму прямого доступа к памяти (DMA) перемещать данные от сетевой карты или хранилища сразу в память GPU или из нее — и все это без нагрузки на центральный процессор. Для GPUDirect Storage местоположение хранилища не имеет значения: это может быть NVME-диск внутри юнита с GPU, внутри стойки или подключен по сети как NVMe-oF.\n\n_Схема работы GPUDirect Storage. Источник: NVIDIA_\n\n## Hi-End СХД на NVMe востребованы на рынке HPC-приложений ##\n\nПонимая, что с появлением GPUDirect Storage интерес крупных клиентов будет обращен на предложение систем хранения со скоростью ввода/вывода, соответствующей пропускной способности GPU, на выставке SC-19 Kingston показал демо системы, состоящей из СХД на базе NVMe-дисков и юнита с GPU, в которой проводился анализ тысяч спутниковых снимков в секунду. О такой СХД на базе 10 накопителей DC1000M U.2 NVMe мы уже писали\nв репортаже с выставки суперкомпьютеров\n.\n\n_СХД на базе 10 накопителей DC1000M U.2 NVMe достойно дополняет сервер с графическими ускорителями. Источник: Kingston_\n\nТакая СХД выполняется в виде стоечного юнита 1U или больше и может масштабироваться в зависимости от числа дисков DC1000M U.2 NVMe, где каждый емкостью 3.84-7.68 ТБ. DC1000M является первой моделью NVMe SSD в форм-факторе U.2 в линейке накопителей Kingston для дата-центров. Он обладает рейтингом выносливости (DWPD, Drive writes per day), позволяющим перезаписывать данные на полную емкость один раз в день в течение гарантированного срока службы накопителя.\n\nВ тесте fio v3.13 на операционной системе Ubuntu 18.04.3 LTS, Linux kernel 5.0.0-31-generic выставочный образец СХД показал скорость чтения (Sustained Read) 5.8 млн IOPS при устойчивой пропускной способности (Sustained Bandwidth) 23.8 Гбит/с.\n\nАриэль Перес, бизнес-менеджер SSD в Kingston, так охарактеризовал новые СХД: «Мы готовы снабдить следующее поколение серверов SSD-решениями U.2 NVMe, чтобы устранить многие узкие места в передаче данных, которые традиционно были связаны с системой хранения. Сочетание накопителей NVMe SSD и нашей премиальной оперативной памяти Server Premier DRAM делает Kingston одним из самых полных в отрасли поставщиков комплексных решений для обработки данных».\n\n_Тест gfio v3.13 показал пропускную способность 23.8 Гбит/с для демонстрационной СХД на дисках DC1000M U.2 NVMe. Источник: Kingston_\n\nКак будет выглядеть типичная система для HPC-приложений, где реализована технология GPUDirect Storage или аналогичная ей? Это архитектура с физическим разделением функциональных блоков в пределах стойки: один-два юнита на оперативную память, еще несколько на вычислительные узлы GPU и CPU и один или несколько юнитов под СХД.\n\nС анонсом GPUDirect Storage и возможным появлением аналогичных технологий у других вендоров GPU, для Kingston расширяется спрос на СХД, рассчитанные на применение в высокопроизводительных вычислениях. Маркером будет скорость чтения данных из СХД, сопоставимая с пропускной способностью 40- или 100-Гбитных сетевых карт на входе в вычислительный юнит с GPU. Таким образом, ультраскоростные СХД, в том числе внешние NVMe через Fabric, из экзотики станут мэйнстримом для HPC-приложений. Кроме науки и финансовых расчетов, они найдут применение во многих других практических областях, таких как системы безопасности уровня мегаполиса Safe City или центров наблюдения на транспорте, где требуется скорость распознавания и идентификации на уровне миллионов HD-изображений в секунду», — обозначил рыночную нишу топовых СХД\n\nДополнительную информацию о продуктах Kingston можно найти на\nофициальном сайте\nкомпании.\n\nКраткое содержание:\nВ статье рассказывается о применении инновационной технологии Hi-End систем хранения данных (СХД) в серверах с графическими процессорами (GPU) и технологией шины GPUDirect Storage. Технология GPUDirect Storage обеспечивает прямой доступ между GPU и NVMe-дисками для быстрого взаимодействия друг с другом. На выставке SC-19 Kingston показал демо системы, состоящей из СХД на базе NVMe-дисков и юнита с GPU, в которой проводился анализ тысяч спутниковых снимков в секунду. Такая СХД выполняется в виде стоечного юнита 1U или больше и может масштабироваться в зависимости от числа дисков DC1000M U.2 NVMe. В тесте fio v3.13 на операционной системе Ubuntu 18.04.3 LTS, Linux kernel 5.0.0-31-generic выставочный образец СХД показал скорость чтения (Sustained Read) 5.8 млн IOPS при устойчивой пропускной способности (Sustained Bandwidth) 23.8 Гбит/с. Ариэль Перес, бизнес-менеджер SSD в Kingston, так охарактеризовал новые СХД: «Мы готовы снабдить следующее поколение серверов SSD-решениями U.2 NVMe, чтобы устранить многие узкие места в передаче данных, которые традиционно были связаны с системой хранения. Сочетание накопителей NVMe SSD и нашей премиальной оперативной памяти Server Premier DRAM делает Kingston одним из самых полных в отрасли поставщиков комплексных решений для обработки данных».","summary":"В статье рассказывается о применении инновационной технологии Hi-End систем хранения данных (СХД) в серверах с графическими процессорами (GPU) и технологией шины GPUDirect Storage. Технология GPUDirect Storage обеспечивает прямой доступ между GPU и NVMe-дисками для быстрого взаимодействия друг с другом. На выставке SC-19 Kingston показал демо системы, состоящей из СХД на базе NVMe-дисков и юнита с GPU, в которой проводился анализ тысяч спутниковых снимков в секунду. Такая СХД выполняется в виде стоечного юнита 1U или больше и может масштабироваться в зависимости от числа дисков DC1000M U.2 NVMe. В тесте fio v3.13 на операционной системе Ubuntu 18.04.3 LTS, Linux kernel 5.0.0-31-generic выставочный образец СХД показал скорость чтения (Sustained Read) 5.8 млн IOPS при устойчивой пропускной способности (Sustained Bandwidth) 23.8 Гбит/с. Ариэль Перес, бизнес-менеджер SSD в Kingston, так охарактеризовал новые СХД: «Мы готовы снабдить следующее поколение серверов SSD-решениями U.2 NVMe, чтобы устранить многие узкие места в передаче данных, которые традиционно были связаны с системой хранения. Сочетание накопителей NVMe SSD и нашей премиальной оперативной памяти Server Premier DRAM делает Kingston одним из самых полных в отрасли поставщиков комплексных решений для обработки данных»."}
{"Text":"Может ли автоматизированное тестирование заменить ручное? / Habr             \n\n03-07-2021\nЭтот, казалось бы, глупый вопрос задают с завидной регулярностью. Казалось бы уже давно все должно быть понятно, но нет. Disclaimer: данная статья написана с учетом опыта разработки в определенной (хоть и самой массовой) сфере ПО, а именно e-commerce. В других сферах правила игры могут разительно отличаться. Я работал в тестировании 3 года, в автоматизации 7 лет, и в разработке - все оставшееся время, и вскоре я буду выступать на Национальной Конференции по Тестированию в Великобритании с ответом на этот вопрос. Но конференция еще не скоро, а многим, видно, интересно узнать ответ уже сейчас. Мне кажется, уместно ответить на сам вопрос, и попробовать разобраться с контекстом данного вопроса. Если вам проще смотреть видео, чем читать текст, то видео на тему можно посмотреть вот тут: Начнем с самого вопроса И для начала стоит определить термины. Что такое тестирование? Вам на выбор два определения. Определение первое, из сертификации ISQTB: \"The process consisting of all lifecycle activities, both static and dynamic, concerned with planning, preparation and evaluation of a component or system and related work products to determine that they satisfy specified requirements, to demonstrate that they are fit for purpose and to detect defects.\" [1] Что можно вольно перевести следующим образом: \"Процесс состоящий из всех активностей, статичных и динамичных, имеющих отношение к планированию, подготовке и оценке компонентов или систем а также производных продуктов для определения соответствуют ли они заданным требованиям, для демонстрации пригодности для применения и поиска дефектов\" Второе определение из методологии Rapid Software Testing, которую продвигают Майкл Болтон и Джеймс Бах: \"Testing is the process of evaluating a product by learning about it through experiencing, exploring, and experimenting, which includes to some degree: questioning, study, modeling, observation, inference, etc.\" [2] Вольный перевод: \"Тестирование - это процесс оценки продукта посредством изучения через опыт, исследование, постановку экспериментов, которое включает в себя постановку вопросов, изучение, моделирование, наблюдение, деструктивные методы оценки, и так далее.\" Что тогда есть автоматизированное тестирование? Я вижу два варианта. Первый - это тестирование (в полном соответствии с определениями выше), выполняемое автоматически. Как не трудно догадаться, это просто невозможно, потому что: Проблема останова (теоретически невозможно написать программу достоверно валидирующую другую программу) Закон Эшби (для надежной автоматизации тестирования, автоматизация должна быть более сложной, чем тестируемый продукт) Дилемма лица, принимающего решения (кого увольнять\\сажать\\расстреливать в случае критической ошибки, например как в Боинг МАХ) Второй вариант будет подразумевать некое подможество тестирования, например подмножество тестирования в виде автоматизированной регрессии. В таком случае ответ очевиден - оно не может заменить тестирование, потому что не является полноценной заменой. Я считаю следующую аналогию полезной: у вашей машины есть много диагностирующих лампочек (проверь двигатель, долей масла, накачай колеса). Если ни одна из лампочек не горит, значит ли это, что машина исправна, и ее можно не возить на ТО? Нет, не значит. Поэтому если кто-то говорит, что \"мы отказались от ручного тестирования\" это не значит, что они смогли автоматизировать тестирование. Просто они готовы пойти на риск (и да, это тоже вполне годная стратегия обеспечения качества) Так что с этим мы разобрались. Автоматизированное тестирование не заменит ручное (или, что звучит более корректно - тестирование невозможно полностью автоматизировать). Теперь перейдем к контексту вопроса Почему (зачем) человек задает данный вопрос? Опять таки, несколько вариантов: это тестировщик, у которого нет навыков разработки\\автоматизации, и который хочет убедиться, что его работе ничего не угрожает; это автоматизатор тестирования, который во всем видит тесты, которые надо автоматизировать; это менеджер, который пытается сделать стратегические кадровые решения. Тестировщикам рано расслабляться, потому что тестирование - это всего-лишь один из способов управления рисками, и альтернативные способы развиваются очень быстро. Если хотите, тестировщикам нужно бояться не автоматизации. Автоматизаторам расслабляться тоже рано, вот тут, например, я достаточно обоснованно показал, почему я не считаю автоматизацию необходимой специализацией, и со мной согласились Alan Page и James Whittaker. Если уж на то пошло, возможно специализация автоматизатора придет в упадок быстрее специализации тестировщика. Менеджеру, принимающему кадровые решения, я лишь посоветую перестать пытаться сделать из процесса разработки ПО конвейер, где у каждого своя узкая специализация. Разработка ПО (во всяком случае такого ПО, с которым я хотел бы работать) во многом - исследовательская работа, в которой, в отличие от массового производства, количество идентичных операций ничтожно мало. С точки зрения теории управления и технологии, речь идет о мелкосерийном и единичном производстве, в котором специализация только вредит. Будущее за генералистами. Что же из этого вынести для себя? Мой (субъективный и ничем особо не обоснованный) прогноз: Спрос на \"чистых\" тестировщиков будет снижаться, граница между тестировщиками и автоматизаторами продолжит стираться Спрос на автоматизаторов продолжит расти, но в долгосрочной перспективе (и тут больше моя личная надежда, чем прогноз), начнет снижаться быстрее, чем спрос на тестировщиков Спрос на \"универсалов\" и \"T-shaped\" специалистов продолжит расти. [1] https://glossary.istqb.org/app/en/search/testing [2] https://www.satisfice.com/blog/archives/856 Приглашаю вас на мой youtube канал, где мы обсуждаем эту и другие релевантные темы.\n","metric":0.61279296880000000414,"metric_input":"Текст статьи:\nМожет ли автоматизированное тестирование заменить ручное? / Habr             \n\n03-07-2021\nЭтот, казалось бы, глупый вопрос задают с завидной регулярностью. Казалось бы уже давно все должно быть понятно, но нет. Disclaimer: данная статья написана с учетом опыта разработки в определенной (хоть и самой массовой) сфере ПО, а именно e-commerce. В других сферах правила игры могут разительно отличаться. Я работал в тестировании 3 года, в автоматизации 7 лет, и в разработке - все оставшееся время, и вскоре я буду выступать на Национальной Конференции по Тестированию в Великобритании с ответом на этот вопрос. Но конференция еще не скоро, а многим, видно, интересно узнать ответ уже сейчас. Мне кажется, уместно ответить на сам вопрос, и попробовать разобраться с контекстом данного вопроса. Если вам проще смотреть видео, чем читать текст, то видео на тему можно посмотреть вот тут: Начнем с самого вопроса И для начала стоит определить термины. Что такое тестирование? Вам на выбор два определения. Определение первое, из сертификации ISQTB: \"The process consisting of all lifecycle activities, both static and dynamic, concerned with planning, preparation and evaluation of a component or system and related work products to determine that they satisfy specified requirements, to demonstrate that they are fit for purpose and to detect defects.\" [1] Что можно вольно перевести следующим образом: \"Процесс состоящий из всех активностей, статичных и динамичных, имеющих отношение к планированию, подготовке и оценке компонентов или систем а также производных продуктов для определения соответствуют ли они заданным требованиям, для демонстрации пригодности для применения и поиска дефектов\" Второе определение из методологии Rapid Software Testing, которую продвигают Майкл Болтон и Джеймс Бах: \"Testing is the process of evaluating a product by learning about it through experiencing, exploring, and experimenting, which includes to some degree: questioning, study, modeling, observation, inference, etc.\" [2] Вольный перевод: \"Тестирование - это процесс оценки продукта посредством изучения через опыт, исследование, постановку экспериментов, которое включает в себя постановку вопросов, изучение, моделирование, наблюдение, деструктивные методы оценки, и так далее.\" Что тогда есть автоматизированное тестирование? Я вижу два варианта. Первый - это тестирование (в полном соответствии с определениями выше), выполняемое автоматически. Как не трудно догадаться, это просто невозможно, потому что: Проблема останова (теоретически невозможно написать программу достоверно валидирующую другую программу) Закон Эшби (для надежной автоматизации тестирования, автоматизация должна быть более сложной, чем тестируемый продукт) Дилемма лица, принимающего решения (кого увольнять\\сажать\\расстреливать в случае критической ошибки, например как в Боинг МАХ) Второй вариант будет подразумевать некое подможество тестирования, например подмножество тестирования в виде автоматизированной регрессии. В таком случае ответ очевиден - оно не может заменить тестирование, потому что не является полноценной заменой. Я считаю следующую аналогию полезной: у вашей машины есть много диагностирующих лампочек (проверь двигатель, долей масла, накачай колеса). Если ни одна из лампочек не горит, значит ли это, что машина исправна, и ее можно не возить на ТО? Нет, не значит. Поэтому если кто-то говорит, что \"мы отказались от ручного тестирования\" это не значит, что они смогли автоматизировать тестирование. Просто они готовы пойти на риск (и да, это тоже вполне годная стратегия обеспечения качества) Так что с этим мы разобрались. Автоматизированное тестирование не заменит ручное (или, что звучит более корректно - тестирование невозможно полностью автоматизировать). Теперь перейдем к контексту вопроса Почему (зачем) человек задает данный вопрос? Опять таки, несколько вариантов: это тестировщик, у которого нет навыков разработки\\автоматизации, и который хочет убедиться, что его работе ничего не угрожает; это автоматизатор тестирования, который во всем видит тесты, которые надо автоматизировать; это менеджер, который пытается сделать стратегические кадровые решения. Тестировщикам рано расслабляться, потому что тестирование - это всего-лишь один из способов управления рисками, и альтернативные способы развиваются очень быстро. Если хотите, тестировщикам нужно бояться не автоматизации. Автоматизаторам расслабляться тоже рано, вот тут, например, я достаточно обоснованно показал, почему я не считаю автоматизацию необходимой специализацией, и со мной согласились Alan Page и James Whittaker. Если уж на то пошло, возможно специализация автоматизатора придет в упадок быстрее специализации тестировщика. Менеджеру, принимающему кадровые решения, я лишь посоветую перестать пытаться сделать из процесса разработки ПО конвейер, где у каждого своя узкая специализация. Разработка ПО (во всяком случае такого ПО, с которым я хотел бы работать) во многом - исследовательская работа, в которой, в отличие от массового производства, количество идентичных операций ничтожно мало. С точки зрения теории управления и технологии, речь идет о мелкосерийном и единичном производстве, в котором специализация только вредит. Будущее за генералистами. Что же из этого вынести для себя? Мой (субъективный и ничем особо не обоснованный) прогноз: Спрос на \"чистых\" тестировщиков будет снижаться, граница между тестировщиками и автоматизаторами продолжит стираться Спрос на автоматизаторов продолжит расти, но в долгосрочной перспективе (и тут больше моя личная надежда, чем прогноз), начнет снижаться быстрее, чем спрос на тестировщиков Спрос на \"универсалов\" и \"T-shaped\" специалистов продолжит расти. [1] https://glossary.istqb.org/app/en/search/testing [2] https://www.satisfice.com/blog/archives/856 Приглашаю вас на мой youtube канал, где мы обсуждаем эту и другие релевантные темы.\n\n\nКраткое содержание:\nСтатья обсуждает вопрос о возможности автоматизации тестирования заменить ручное тестирование. Автор рассматривает два определения тестирования и анализирует проблемы, связанные с автоматизацией. Он также сравнивает автоматизацию с диагностическими лампочками в автомобиле и делает вывод, что автоматизация не может полностью заменить ручное тестирование.\n\nАвтор также обсуждает контекст вопроса и причины, по которым люди задают его. Он подчеркивает, что тестирование является лишь одним из способов управления рисками, и что автоматизация не представляет угрозы для тестировщиков. Автор также выражает свои опасения по поводу автоматизации как специализации и рекомендует менеджерам отказаться от попыток сделать разработку ПО конвейерным процессом.\n\nВ заключение автор делает прогноз о снижении спроса на \"чистых\" тестировщиков и росте спроса на \"универсалов\" и \"T-shaped\" специалистов. Он надеется, что автоматизация будет развиваться медленнее, чем спрос на тестировщиков, и что в долгосрочной перспективе будет расти спрос на специалистов, которые могут выполнять обе роли.","summary":"Статья обсуждает вопрос о возможности автоматизации тестирования заменить ручное тестирование. Автор рассматривает два определения тестирования и анализирует проблемы, связанные с автоматизацией. Он также сравнивает автоматизацию с диагностическими лампочками в автомобиле и делает вывод, что автоматизация не может полностью заменить ручное тестирование.\n\nАвтор также обсуждает контекст вопроса и причины, по которым люди задают его. Он подчеркивает, что тестирование является лишь одним из способов управления рисками, и что автоматизация не представляет угрозы для тестировщиков. Автор также выражает свои опасения по поводу автоматизации как специализации и рекомендует менеджерам отказаться от попыток сделать разработку ПО конвейерным процессом.\n\nВ заключение автор делает прогноз о снижении спроса на \"чистых\" тестировщиков и росте спроса на \"универсалов\" и \"T-shaped\" специалистов. Он надеется, что автоматизация будет развиваться медленнее, чем спрос на тестировщиков, и что в долгосрочной перспективе будет расти спрос на специалистов, которые могут выполнять обе роли."}
{"Text":"История одного сообщества или как поменять мышление / Хабр                 \n\nПрочитав много различных статей от разных хабрапользователей в разделе «Бизнес студии», решил поделится своей историей.\n\nРаботу веб-разработчиком в веб-студии, организацию собственного бизнеса в данной области я считаю одним из самых сложных и порой неблагодарных занятий в IT сфере. Постоянный демпинг на рынке, недопонимание задач заказчиками, огромная конкуренция, борьба за оптимизацию бизнес-процессов – наверное, только малая часть того, с чем сталкивается ежедневно руководитель веб-студии.\n\n#### Как все начиналось или история одной веб-студии ####\n\nНачал я работать в этом бизнесе в далеком 2004 году. Как, наверное, в большинства начинающих веб-разработчиков, в самом начале почти все приходилось делать самостоятельно, принимать заказы, искать клиентов, заниматься разработкой сайтов. Все это делалось в домашних условиях. Временами было так сложно, что я не раз хотел бросить это все и заняться менее стрессовым занятием. С другой стороны эти поиски помимо всех трудностей позволяли чувствовать себя независимым, а для меня это было главным в то время, поэтому, наверное, и остался в этом бизнесе…\n\nТолько спустя два года у меня появилась команда единомышленников, я взял кредит, мы арендовали офис, купили оборудование и начали работать уже как официальная компания. В данную компанию мы вложили душу, знания, все свои идеи и возможности, свято верили в то, что теперь уже ничто нам не помешает на пути к успеху, но, к сожалению, трудностей только приумножилось…\n\n#### Ищем решение либо делаем свое ####\n\nОсновой работы любой веб-студии является правильная стратегия и хорошие инструменты для оптимизации этих решений. Конечно, как небольшая студия мы сразу же принялись изучать различные бесплатные решения (Joomla, Drupal, Wordpress) и работать с ним. И вроде все бы ничего, да вот только главную нашу задачу – оптимизацию работы и сокращение потерь времени на реализацию проектов мы решить так и не смогли.\n\nИ вот, после долгих размышлений – решено – создаем свой продукт, на котором будем делать проекты, оцениваем все за и против, выбираем Фреймворк, оцениваем время и принимаемся за работу… Оцениваем, оцениваем, а не учитываем одного самого главного – бросая все силы на разработку продукта, мы теряем еще больше времени и ресурсов, которые могли использовать для выполнения заказов нашим клиентам и непосредственного заработка денег. Берем очередные кредиты, одалживаем деньги, начинаются проблемы с своевременными выплатами зарплат, еще до этого наступает кризис 2008 года и взятый в долларах кредит приумножается в два раза…\n\nВсе это, конечно же, влияет на позитивное мышление в команде, в связи с чем, разработка еще более задерживается…\n\nВ конце концов, мы готовы к запуску своего детища ImageCMS, но возникает вопрос – запускать ли продукт в бесплатном виде (что, в силу затраченных средств, для меня лично выглядит как что-то немыслимое) или как платное решение. Поскольку дела в компании начали идти немного лучше, а главная цель создания данного инструмента была в оптимизации работы компании, команда переубеждает запускать продукт как OpenSource CMS.\n\nПоскольку система изначально создавалась как движок для корпоративных сайтов и порталов, а сфера Интернет-коммерции начала развиваться очень быстро, было принято еще одно непростое решение – написать на базе ImageCMS расширение для создания Интернет-магазинов… И снова год разработок и тестов.\n\nПосле запуска ImageCMS Shop меня уже никто не смог переубедить в целесообразности запуска системы на бесплатной основе: нужны постоянные вложения в развитие – теперь уже двух версий системы, да и система у нас получилась отличная ))) Единственное – принято было проводить лояльную ценовую политику, чтобы дать возможность нашим партнерам продолжать работать с ImageCMS, не переходя на другие движки для создания Интернет-магазинов, также мы сразу ввели партнерскую программу скидок, которая вообще решила этот вопрос и все остались довольными.\n\n#### Меняем мышление или не только в деньгах счастье ####\n\nМы уже запустили много версий нашего продукта, существенно расширили штат нашей команды и уже неплохо развиваемся в сфере веб-разработок – видим четкую перспективу в этом деле.\n\nЕдинственное, что не дает покоя – это не совсем верное видение, которое за годы сформировалось в нашей команде: работают различные отделы – Продажи, Разработки, Продвижения, Стартапы, но все скорее по отдельности, нет правильной цели, все нацелено лишь на получение прямой прибыли…\n\nИ, как мне кажется, решение создано – после опроса всех сотрудников и поиска нового позиционирования компании, мы решили двигаться дальше как сообщество, а за основу взять уже существующее сообщество ImageCMS. Сообщив участникам сообщества ImageCMS данную идею, мы получили множество радостных отзывов и всестороннюю поддержку. Принято решение полностью изменить официальный сайт системы, добавить много новых возможностей для участников сообщества и партнеров, выделить ресурсы в виде менеджеров, программистов для поддержки сообщества, а также привлечь всех программистов нашей компании к развитию этого направления.\n\nВместе с сообществом ImageCMS мы планируем не только развивать систему, но и всю сферу веб-разработок в целом – создавать полезные инструменты, организовывать конференции, делится секретами успеха, оптимизировать нашу работу в целом и искать решения как делать прибыльным наше общее дело, как для разработчиков, так и для клиентов.\n\n#### P.S. Рекомендую ####\n\nК сожалению, у меня нет возможности уделять много времени чтению, но хочу выделить, на мой взгляд, очень достойные материалы, которые изменили мое видение:\n\nРичард Брэнсон «К черту все берись и делай»\n\nРичард Брэнсон «Обнаженный бизнес»\n\nУолтер Айзексон «Стив Джобс»\n\nТони Шей «Доставляя счаcтье»\n\nСайт сообщества ImageCMS (пока еще 2.0):\nhttp://www.imagecms.net","metric":0.48413085940000000207,"metric_input":"Текст статьи:\nИстория одного сообщества или как поменять мышление / Хабр                 \n\nПрочитав много различных статей от разных хабрапользователей в разделе «Бизнес студии», решил поделится своей историей.\n\nРаботу веб-разработчиком в веб-студии, организацию собственного бизнеса в данной области я считаю одним из самых сложных и порой неблагодарных занятий в IT сфере. Постоянный демпинг на рынке, недопонимание задач заказчиками, огромная конкуренция, борьба за оптимизацию бизнес-процессов – наверное, только малая часть того, с чем сталкивается ежедневно руководитель веб-студии.\n\n#### Как все начиналось или история одной веб-студии ####\n\nНачал я работать в этом бизнесе в далеком 2004 году. Как, наверное, в большинства начинающих веб-разработчиков, в самом начале почти все приходилось делать самостоятельно, принимать заказы, искать клиентов, заниматься разработкой сайтов. Все это делалось в домашних условиях. Временами было так сложно, что я не раз хотел бросить это все и заняться менее стрессовым занятием. С другой стороны эти поиски помимо всех трудностей позволяли чувствовать себя независимым, а для меня это было главным в то время, поэтому, наверное, и остался в этом бизнесе…\n\nТолько спустя два года у меня появилась команда единомышленников, я взял кредит, мы арендовали офис, купили оборудование и начали работать уже как официальная компания. В данную компанию мы вложили душу, знания, все свои идеи и возможности, свято верили в то, что теперь уже ничто нам не помешает на пути к успеху, но, к сожалению, трудностей только приумножилось…\n\n#### Ищем решение либо делаем свое ####\n\nОсновой работы любой веб-студии является правильная стратегия и хорошие инструменты для оптимизации этих решений. Конечно, как небольшая студия мы сразу же принялись изучать различные бесплатные решения (Joomla, Drupal, Wordpress) и работать с ним. И вроде все бы ничего, да вот только главную нашу задачу – оптимизацию работы и сокращение потерь времени на реализацию проектов мы решить так и не смогли.\n\nИ вот, после долгих размышлений – решено – создаем свой продукт, на котором будем делать проекты, оцениваем все за и против, выбираем Фреймворк, оцениваем время и принимаемся за работу… Оцениваем, оцениваем, а не учитываем одного самого главного – бросая все силы на разработку продукта, мы теряем еще больше времени и ресурсов, которые могли использовать для выполнения заказов нашим клиентам и непосредственного заработка денег. Берем очередные кредиты, одалживаем деньги, начинаются проблемы с своевременными выплатами зарплат, еще до этого наступает кризис 2008 года и взятый в долларах кредит приумножается в два раза…\n\nВсе это, конечно же, влияет на позитивное мышление в команде, в связи с чем, разработка еще более задерживается…\n\nВ конце концов, мы готовы к запуску своего детища ImageCMS, но возникает вопрос – запускать ли продукт в бесплатном виде (что, в силу затраченных средств, для меня лично выглядит как что-то немыслимое) или как платное решение. Поскольку дела в компании начали идти немного лучше, а главная цель создания данного инструмента была в оптимизации работы компании, команда переубеждает запускать продукт как OpenSource CMS.\n\nПоскольку система изначально создавалась как движок для корпоративных сайтов и порталов, а сфера Интернет-коммерции начала развиваться очень быстро, было принято еще одно непростое решение – написать на базе ImageCMS расширение для создания Интернет-магазинов… И снова год разработок и тестов.\n\nПосле запуска ImageCMS Shop меня уже никто не смог переубедить в целесообразности запуска системы на бесплатной основе: нужны постоянные вложения в развитие – теперь уже двух версий системы, да и система у нас получилась отличная ))) Единственное – принято было проводить лояльную ценовую политику, чтобы дать возможность нашим партнерам продолжать работать с ImageCMS, не переходя на другие движки для создания Интернет-магазинов, также мы сразу ввели партнерскую программу скидок, которая вообще решила этот вопрос и все остались довольными.\n\n#### Меняем мышление или не только в деньгах счастье ####\n\nМы уже запустили много версий нашего продукта, существенно расширили штат нашей команды и уже неплохо развиваемся в сфере веб-разработок – видим четкую перспективу в этом деле.\n\nЕдинственное, что не дает покоя – это не совсем верное видение, которое за годы сформировалось в нашей команде: работают различные отделы – Продажи, Разработки, Продвижения, Стартапы, но все скорее по отдельности, нет правильной цели, все нацелено лишь на получение прямой прибыли…\n\nИ, как мне кажется, решение создано – после опроса всех сотрудников и поиска нового позиционирования компании, мы решили двигаться дальше как сообщество, а за основу взять уже существующее сообщество ImageCMS. Сообщив участникам сообщества ImageCMS данную идею, мы получили множество радостных отзывов и всестороннюю поддержку. Принято решение полностью изменить официальный сайт системы, добавить много новых возможностей для участников сообщества и партнеров, выделить ресурсы в виде менеджеров, программистов для поддержки сообщества, а также привлечь всех программистов нашей компании к развитию этого направления.\n\nВместе с сообществом ImageCMS мы планируем не только развивать систему, но и всю сферу веб-разработок в целом – создавать полезные инструменты, организовывать конференции, делится секретами успеха, оптимизировать нашу работу в целом и искать решения как делать прибыльным наше общее дело, как для разработчиков, так и для клиентов.\n\n#### P.S. Рекомендую ####\n\nК сожалению, у меня нет возможности уделять много времени чтению, но хочу выделить, на мой взгляд, очень достойные материалы, которые изменили мое видение:\n\nРичард Брэнсон «К черту все берись и делай»\n\nРичард Брэнсон «Обнаженный бизнес»\n\nУолтер Айзексон «Стив Джобс»\n\nТони Шей «Доставляя счаcтье»\n\nСайт сообщества ImageCMS (пока еще 2.0):\nhttp://www.imagecms.net\n\nКраткое содержание:\nСтатья рассказывает о истории одного сообщества, начиная с 2004 года, когда автор начал работать в веб-студии, и заканчивая созданием собственного продукта ImageCMS. Автор делится своим опытом и трудностями, с которыми он столкнулся в бизнесе, а также рассказывает о создании сообщества ImageCMS, которое планирует развивать систему и сферу веб-разработок в целом.","summary":"Статья рассказывает о истории одного сообщества, начиная с 2004 года, когда автор начал работать в веб-студии, и заканчивая созданием собственного продукта ImageCMS. Автор делится своим опытом и трудностями, с которыми он столкнулся в бизнесе, а также рассказывает о создании сообщества ImageCMS, которое планирует развивать систему и сферу веб-разработок в целом."}
{"Text":"SSO используя Jasig CAS Server 4.0.0. Часть 1 / Habr                        \n\n10-07-2014\nПредставьте, что у вас есть web-проект, который состоит из двух или более частей, (part1.example.com) и вторая часть (part2.example.com). Теперь перед вам стоит задача сделать Аутентификацию пользователя, но так чтобы когда пользователь зайдя один раз в любую из частей web-проекта, например, в первую, то во второй он уже должен быть аутентифицирован. Процесс такой Аутентификацию пользователя называют Технология единого входа или SSO (Single Sign-On). На Хабре уже есть подобная статья Jasig CAS — сервер аутентификации, но она немного устарела так как там используется CAS Server 3.* и на мое мнение она не достаточно детально изложена для людей которые впервые пытаются реализовать SSO используя Jasig CAS (Central Authentication Service), но теория там довольно хорошо описана, поэтому рекомендую также просмотреть. Обратите внимание, что мы будем конфигурировать Jasig CAS Server который будет аутентифицировать пользователя и отдавать web-клиентам. В дальнейшем планирую написать о реализации клиента для этого сервера. Создание Maven проекта и добавление зависимостей Так как это туториал буду все по порядку объяснять. Создайте простой Maven проект и в pom.xml добавьте следующие зависимости: <properties> <cas.version>4.0.0</cas.version> </properties> <dependencies> <dependency> <groupId>org.jasig.cas</groupId> <artifactId>cas-server-webapp</artifactId> <version>${cas.version}</version> <type>war</type> <scope>runtime</scope> </dependency> <dependency> <groupId>org.jasig.cas</groupId> <artifactId>cas-server-support-generic</artifactId> <version>${cas.version}</version> <type>jar</type> </dependency> </dependencies> cas-server-webapp — это стандартый сервер, который мы будем позже конфигурировать под себя; cas-server-support-generic — а эта библиотека нам и позволит конфигурировать сервер под себя. А также подключим плагин сборки: <plugin> <artifactId>maven-war-plugin</artifactId> <configuration> <warName>cas</warName> </configuration> </plugin> Он позволит собрать наш проект с контекстом /cas. После того как Maven скачает CAS у вас в проекте появится новая папка в которой уже будет сборка сервера CAS 4.0.0 по умолчанию. Теперь можно будет задеплоить и запустить, но прежде нам нужно выполнить небольшую конфигурацию Tomcat так как деплоить мы будем на него. Конфигурация Tomcat и генерация ключей Я не хочу повторяться, поэтому укажу ссылку на более детальное описание тому материалу, что будет ниже вот ссылка на описание. Первым делом нужно сгенерировать сертификат и зарегистрировать его, для этого будем использовать стандартный keytool который есть в каждом JDK. 1. Генерируем Java Key Store: keytool -genkey -alias casdc -keypass 12345678 -keystore ssoServer.jks -storepass 12345678 -validity 365 2. Теперь создаем на основании уже сгененированого хранилища сертификат: keytool -export -alias casdc -file casServerPublic.cer -keystore ssoServer.jks -storepass 12345678 3. И полученый сертификат импортируем в хранилище довереных сертификатов: keytool -import -alias casdc -file casServerPublic.cer -keypass 12345678 -keystore \"C:/Program Files/Java/jdk1.7.0/jre/lib/security/cacerts\" -storepass changeit После этого заходим в папку ./apache-tomcat-8.0.9/conf и открываем на редактирование файл server.xml находим там строку: Define a SSL HTTP/1.1 Connector on port 8443 и после этой строки, по умолчанию это примерно 83 строка, вставляем следующий фрахмет xml: <Connector port=\"8443\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\" SSLEnabled=\"true\" maxThreads=\"150\" scheme=\"https\" secure=\"true\" clientAuth=\"false\" sslProtocol=\"TLS\" keyAlias=\"casdc\" keystoreFile=\"C:/keys/ssoServer.jks\" keystorePass=\"12345678\" /> Обратите внимание что сгенерированное хранилище ssoServer.jks я поместил в папку C:/keys/ssoServer.jks и после указал к нему полный путь. keyAlias — это алиас, который мы указывали на первом шаге генерации хранилища. keystorePass — пароль к нашему хранилищу ключей. Таким образом мы сконфигурировали HTTPS, который так необходимый CAS серверу. После этого можна запустить Tomcat и заделоить наш проект. После перейдите по ссылке: https://localhost:8443/cas/ — это cas сервер со стандартной конфигурацией. Вы должны увидеть следующее: Если у вас появляется красная полоска с тестом, то это значит что вы сделали что-то не так, и HTTPS не включен. И для того чтобы проверить что CAS сервер удачно запустился и работает, давайте залогинимся. Для этого вам нужно ввести стандартные логин и пароль: Логин: casuser Пароль: Mellon После удачного входа вы должны увидеть: Поздравляю CAS сервер мы запустили. В следующем посте я расскажу как кастомизировать AuthenticationHandler, а именно возможность получения пользователя с БД или другой внешней системы. Строго не судите, это мой первый прототип на данную тематику, и если будут замечания, с радостью выслушаю. Также я его постоянно улучшаю и о новых достижениях буду писать, конечно же если это будет кому-то интересно. Исходники проекта на Github: https://github.com/alexbarchuk/local-cas Источник: https://wiki.jasig.org/display/CASUM/Configuring\n","metric":0.568359375,"metric_input":"Текст статьи:\nSSO используя Jasig CAS Server 4.0.0. Часть 1 / Habr                        \n\n10-07-2014\nПредставьте, что у вас есть web-проект, который состоит из двух или более частей, (part1.example.com) и вторая часть (part2.example.com). Теперь перед вам стоит задача сделать Аутентификацию пользователя, но так чтобы когда пользователь зайдя один раз в любую из частей web-проекта, например, в первую, то во второй он уже должен быть аутентифицирован. Процесс такой Аутентификацию пользователя называют Технология единого входа или SSO (Single Sign-On). На Хабре уже есть подобная статья Jasig CAS — сервер аутентификации, но она немного устарела так как там используется CAS Server 3.* и на мое мнение она не достаточно детально изложена для людей которые впервые пытаются реализовать SSO используя Jasig CAS (Central Authentication Service), но теория там довольно хорошо описана, поэтому рекомендую также просмотреть. Обратите внимание, что мы будем конфигурировать Jasig CAS Server который будет аутентифицировать пользователя и отдавать web-клиентам. В дальнейшем планирую написать о реализации клиента для этого сервера. Создание Maven проекта и добавление зависимостей Так как это туториал буду все по порядку объяснять. Создайте простой Maven проект и в pom.xml добавьте следующие зависимости: <properties> <cas.version>4.0.0</cas.version> </properties> <dependencies> <dependency> <groupId>org.jasig.cas</groupId> <artifactId>cas-server-webapp</artifactId> <version>${cas.version}</version> <type>war</type> <scope>runtime</scope> </dependency> <dependency> <groupId>org.jasig.cas</groupId> <artifactId>cas-server-support-generic</artifactId> <version>${cas.version}</version> <type>jar</type> </dependency> </dependencies> cas-server-webapp — это стандартый сервер, который мы будем позже конфигурировать под себя; cas-server-support-generic — а эта библиотека нам и позволит конфигурировать сервер под себя. А также подключим плагин сборки: <plugin> <artifactId>maven-war-plugin</artifactId> <configuration> <warName>cas</warName> </configuration> </plugin> Он позволит собрать наш проект с контекстом /cas. После того как Maven скачает CAS у вас в проекте появится новая папка в которой уже будет сборка сервера CAS 4.0.0 по умолчанию. Теперь можно будет задеплоить и запустить, но прежде нам нужно выполнить небольшую конфигурацию Tomcat так как деплоить мы будем на него. Конфигурация Tomcat и генерация ключей Я не хочу повторяться, поэтому укажу ссылку на более детальное описание тому материалу, что будет ниже вот ссылка на описание. Первым делом нужно сгенерировать сертификат и зарегистрировать его, для этого будем использовать стандартный keytool который есть в каждом JDK. 1. Генерируем Java Key Store: keytool -genkey -alias casdc -keypass 12345678 -keystore ssoServer.jks -storepass 12345678 -validity 365 2. Теперь создаем на основании уже сгененированого хранилища сертификат: keytool -export -alias casdc -file casServerPublic.cer -keystore ssoServer.jks -storepass 12345678 3. И полученый сертификат импортируем в хранилище довереных сертификатов: keytool -import -alias casdc -file casServerPublic.cer -keypass 12345678 -keystore \"C:/Program Files/Java/jdk1.7.0/jre/lib/security/cacerts\" -storepass changeit После этого заходим в папку ./apache-tomcat-8.0.9/conf и открываем на редактирование файл server.xml находим там строку: Define a SSL HTTP/1.1 Connector on port 8443 и после этой строки, по умолчанию это примерно 83 строка, вставляем следующий фрахмет xml: <Connector port=\"8443\" protocol=\"org.apache.coyote.http11.Http11NioProtocol\" SSLEnabled=\"true\" maxThreads=\"150\" scheme=\"https\" secure=\"true\" clientAuth=\"false\" sslProtocol=\"TLS\" keyAlias=\"casdc\" keystoreFile=\"C:/keys/ssoServer.jks\" keystorePass=\"12345678\" /> Обратите внимание что сгенерированное хранилище ssoServer.jks я поместил в папку C:/keys/ssoServer.jks и после указал к нему полный путь. keyAlias — это алиас, который мы указывали на первом шаге генерации хранилища. keystorePass — пароль к нашему хранилищу ключей. Таким образом мы сконфигурировали HTTPS, который так необходимый CAS серверу. После этого можна запустить Tomcat и заделоить наш проект. После перейдите по ссылке: https://localhost:8443/cas/ — это cas сервер со стандартной конфигурацией. Вы должны увидеть следующее: Если у вас появляется красная полоска с тестом, то это значит что вы сделали что-то не так, и HTTPS не включен. И для того чтобы проверить что CAS сервер удачно запустился и работает, давайте залогинимся. Для этого вам нужно ввести стандартные логин и пароль: Логин: casuser Пароль: Mellon После удачного входа вы должны увидеть: Поздравляю CAS сервер мы запустили. В следующем посте я расскажу как кастомизировать AuthenticationHandler, а именно возможность получения пользователя с БД или другой внешней системы. Строго не судите, это мой первый прототип на данную тематику, и если будут замечания, с радостью выслушаю. Также я его постоянно улучшаю и о новых достижениях буду писать, конечно же если это будет кому-то интересно. Исходники проекта на Github: https://github.com/alexbarchuk/local-cas Источник: https://wiki.jasig.org/display/CASUM/Configuring\n\n\nКраткое содержание:\nСтатья рассказывает о настройке сервера аутентификации Jasig CAS для технологии единого входа (SSO) в web-проекте. Сервер CAS Server 4.0.0 будет аутентифицировать пользователей и передавать их данные web-клиентам. Для настройки сервера создается Maven проект, в который добавляются зависимости и плагин сборки. Затем Tomcat конфигурируется для деплоя и запуска сервера. После этого генерируются сертификаты и SSL-ключи для работы сервера по протоколу HTTPS. По ссылке https://localhost:8443/cas/ можно проверить работу сервера. В следующем посте описывается кастомизация AuthenticationHandler для получения пользователей из базы данных или другой внешней системы.","summary":"Статья рассказывает о настройке сервера аутентификации Jasig CAS для технологии единого входа (SSO) в web-проекте. Сервер CAS Server 4.0.0 будет аутентифицировать пользователей и передавать их данные web-клиентам. Для настройки сервера создается Maven проект, в который добавляются зависимости и плагин сборки. Затем Tomcat конфигурируется для деплоя и запуска сервера. После этого генерируются сертификаты и SSL-ключи для работы сервера по протоколу HTTPS. По ссылке https://localhost:8443/cas/ можно проверить работу сервера. В следующем посте описывается кастомизация AuthenticationHandler для получения пользователей из базы данных или другой внешней системы."}
{"Text":"Гибридный мозг. Нейрохакинг на практике / Хабр                                                          \n\nИсследователи из Стэнфордского университета трансплантировали человеческие нейроны  в мозг крыс и наблюдали, как они превращаются в гибридные мозговые цепи. После ученые тщательно изучали поведение грызунов. Фактически, эксперимент позволил увидеть, как мозг крысы превращается в живую лабораторию для изучения когнитивных расстройств.\n\nКогда речь заходит об исследовании человеческого тела, именно мозг считается сложнейшим органом для изучения. Один из способов его изучения — выращивание лабораторных органоидов . В их основе лежат стволовые клетки, которые подвергаются воздействию факторов роста, стимулирующих созревание и развитие в иные типы клеток мозга, которые затем собираются в трехмерные структуры.\nСамо собой, речь не идет об исследовании сознания. Но такие структуры служат ученым моделью для изучения таких расстройств, как эпилепсия, аутизм, шизофрения. Вместе с этим они также помогают:\nТестировать работоспособность лекарственных препаратов .\n\nИсследовать то, как мозг отращивает с нуля сеть кровеносных сосудов .\n\nОпределять природу и механизмы возникновения электрических сигналов  в мозге.\n\nВыращивать примитивные глаза, способные воспринимать свет .\nВ прошлом году Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины, озвучил  результаты по выращиванию в лаборатории 20-месячных органоидов мозга. До этой работы считалось, что выращенный в лаборатории мозг не способен созревать дальше стадии, равной стадии развития плода.\nИсследование доказало, что эти органоиды могут созревать так же, как человеческий мозг, следуя внутренним биологическим часам, чтобы достичь постнатальной зрелости на временной шкале, параллельной развитию in vivo. Но проблема в том, что такие участки мозга, выращенные в «чашке Петри», крайне сложно исследовать.\n>\n>\n> Мы создавали все более сложные системы, используя органоиды и их комбинации, называемые ассемблоидами. Но нейроны в лабораторных чашках все еще отстают в своем развитии по сравнению с тем, что происходит в естественно развивающемся человеческом мозгу. Многочисленные проблемы, такие как нехватка питательных веществ и факторов роста, эндотелиальных клеток, формирующих кровеносные сосуды, или сенсорные раздражители, препятствуют развитию клеток в лабораторной чашке.\n>\n>\n>\n> Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины.\n>\n>\nВ своей последней работе Паска и его команда трансплантировали мозговые органоиды, напоминающие кору головного мозга человека, почти 100 молодым крысам. Крысам было два или три дня от роду, что эквивалентно человеческому младенчеству. Цель имплантации в том, чтобы органоиды могли образовывать связи и эволюционировать вместе с мозгом крысы.\nВскоре эндотелиальные клетки крыс мигрировали в ткани мозга человека, чтобы сформировать кровеносные сосуды, снабжая их питательными веществами и сигнализируя о способности избавляться от отходов жизнедеятельности. Иммунные клетки в мозге крыс последовали этому примеру, освоившись в трансплантированной ткани. После этого имплантированные органоиды не только выжили, но и выросли до такой степени, что заняли около трети полушария мозга крысы.\nОтдельные нейроны из органоидов также быстро росли, закрепившись в мозгу крысы, чтобы сформировать связи с естественными участками мозга грызунов, в том числе с областью таламуса, которая отвечает за передачу сенсорной информации от тела.\n>\n>\n> Это соединение могло обеспечивать передачу сигналов, необходимых для оптимального созревания и интеграции нейронов человека.\n>\n>\n>\n> Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины.\n>\n>\nЗатем ученые обратили внимание на болезни, создав органоид, используя клетки кожи, полученные от пациента с синдромом Тимоти, состоянием мозга, связанным с аутизмом и эпилепсией. Этот органоид был трансплантирован в одну сторону мозга крысы, а органоид, созданный из клеток здорового субъекта, был трансплантирован в другую сторону, выступая контрольной выборкой. Через 5-6 месяцев созревания, ученые выявили значительные различия в электрической активности. Нейроны участка от пациента с синдромом Тимоти были намного меньше и имели меньше сигнальных структур, называемых дендритами.\n>\n>\n> Мы многое узнали о синдроме Тимоти, изучая органоиды, выращенные в чашке. Но только с трансплантацией мы смогли увидеть различия, связанные с активностью нейронов.\n>\n>\n>\n> Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины.\n>\n>\nНо самое поразительное открытие было сделано в результате экспериментов по способности гибридного мозга крыс обрабатывать сенсорную информацию. Потоки воздуха направлялись на усы крыc. В ответ на раздражитель активизировались человеческие нейроны.\n>\n>\n> В другом эксперименте участвовали органоиды, модифицированные для реагирования на синий свет, который подавался через ультратонкие волоконно-оптические кабели. Импульсы синего света использовались для активации этих нейронов. После активации нейронов от синего спектра, крысам давали воду. Это происходило в течение 15-дневного «тренировочного периода», во время которого крыс учили, что активация этих нейронов означает приближение награды, побуждая их бежать за наградой к водосточному желобу. То, что крысы научились связывать стимуляцию синим светом с доступностью воды, показывает, что имплантированная человеческая ткань может функционировать как часть схемы поиска вознаграждения в мозгу крысы.\n>\n>\n>\n> Исследователи показывают, что нейроны человека при активации влияют на поведение крыс. Человеческие клетки буквально функционально связаны с крысиным мозгом.\n>\n>\n>\n> Д-р Юрген Кноблих, научный директор Австрийского института молекулярной биотехнологии, не участвовавший в исследовании.\n>\n>\nЭто самая передовая схема человеческого мозга, когда-либо созданная из клеток человека. Такая биоплатформа позволяет контролировать и наблюдать за поведением крыс открывая новые захватывающие возможности для изучения нервно-психических расстройств.","metric":0.5791015625,"metric_input":"Текст статьи:\nГибридный мозг. Нейрохакинг на практике / Хабр                                                          \n\nИсследователи из Стэнфордского университета трансплантировали человеческие нейроны  в мозг крыс и наблюдали, как они превращаются в гибридные мозговые цепи. После ученые тщательно изучали поведение грызунов. Фактически, эксперимент позволил увидеть, как мозг крысы превращается в живую лабораторию для изучения когнитивных расстройств.\n\nКогда речь заходит об исследовании человеческого тела, именно мозг считается сложнейшим органом для изучения. Один из способов его изучения — выращивание лабораторных органоидов . В их основе лежат стволовые клетки, которые подвергаются воздействию факторов роста, стимулирующих созревание и развитие в иные типы клеток мозга, которые затем собираются в трехмерные структуры.\nСамо собой, речь не идет об исследовании сознания. Но такие структуры служат ученым моделью для изучения таких расстройств, как эпилепсия, аутизм, шизофрения. Вместе с этим они также помогают:\nТестировать работоспособность лекарственных препаратов .\n\nИсследовать то, как мозг отращивает с нуля сеть кровеносных сосудов .\n\nОпределять природу и механизмы возникновения электрических сигналов  в мозге.\n\nВыращивать примитивные глаза, способные воспринимать свет .\nВ прошлом году Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины, озвучил  результаты по выращиванию в лаборатории 20-месячных органоидов мозга. До этой работы считалось, что выращенный в лаборатории мозг не способен созревать дальше стадии, равной стадии развития плода.\nИсследование доказало, что эти органоиды могут созревать так же, как человеческий мозг, следуя внутренним биологическим часам, чтобы достичь постнатальной зрелости на временной шкале, параллельной развитию in vivo. Но проблема в том, что такие участки мозга, выращенные в «чашке Петри», крайне сложно исследовать.\n>\n>\n> Мы создавали все более сложные системы, используя органоиды и их комбинации, называемые ассемблоидами. Но нейроны в лабораторных чашках все еще отстают в своем развитии по сравнению с тем, что происходит в естественно развивающемся человеческом мозгу. Многочисленные проблемы, такие как нехватка питательных веществ и факторов роста, эндотелиальных клеток, формирующих кровеносные сосуды, или сенсорные раздражители, препятствуют развитию клеток в лабораторной чашке.\n>\n>\n>\n> Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины.\n>\n>\nВ своей последней работе Паска и его команда трансплантировали мозговые органоиды, напоминающие кору головного мозга человека, почти 100 молодым крысам. Крысам было два или три дня от роду, что эквивалентно человеческому младенчеству. Цель имплантации в том, чтобы органоиды могли образовывать связи и эволюционировать вместе с мозгом крысы.\nВскоре эндотелиальные клетки крыс мигрировали в ткани мозга человека, чтобы сформировать кровеносные сосуды, снабжая их питательными веществами и сигнализируя о способности избавляться от отходов жизнедеятельности. Иммунные клетки в мозге крыс последовали этому примеру, освоившись в трансплантированной ткани. После этого имплантированные органоиды не только выжили, но и выросли до такой степени, что заняли около трети полушария мозга крысы.\nОтдельные нейроны из органоидов также быстро росли, закрепившись в мозгу крысы, чтобы сформировать связи с естественными участками мозга грызунов, в том числе с областью таламуса, которая отвечает за передачу сенсорной информации от тела.\n>\n>\n> Это соединение могло обеспечивать передачу сигналов, необходимых для оптимального созревания и интеграции нейронов человека.\n>\n>\n>\n> Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины.\n>\n>\nЗатем ученые обратили внимание на болезни, создав органоид, используя клетки кожи, полученные от пациента с синдромом Тимоти, состоянием мозга, связанным с аутизмом и эпилепсией. Этот органоид был трансплантирован в одну сторону мозга крысы, а органоид, созданный из клеток здорового субъекта, был трансплантирован в другую сторону, выступая контрольной выборкой. Через 5-6 месяцев созревания, ученые выявили значительные различия в электрической активности. Нейроны участка от пациента с синдромом Тимоти были намного меньше и имели меньше сигнальных структур, называемых дендритами.\n>\n>\n> Мы многое узнали о синдроме Тимоти, изучая органоиды, выращенные в чашке. Но только с трансплантацией мы смогли увидеть различия, связанные с активностью нейронов.\n>\n>\n>\n> Серджиу Паска, профессор психиатрии и поведенческих наук в Стэнфордской школе медицины.\n>\n>\nНо самое поразительное открытие было сделано в результате экспериментов по способности гибридного мозга крыс обрабатывать сенсорную информацию. Потоки воздуха направлялись на усы крыc. В ответ на раздражитель активизировались человеческие нейроны.\n>\n>\n> В другом эксперименте участвовали органоиды, модифицированные для реагирования на синий свет, который подавался через ультратонкие волоконно-оптические кабели. Импульсы синего света использовались для активации этих нейронов. После активации нейронов от синего спектра, крысам давали воду. Это происходило в течение 15-дневного «тренировочного периода», во время которого крыс учили, что активация этих нейронов означает приближение награды, побуждая их бежать за наградой к водосточному желобу. То, что крысы научились связывать стимуляцию синим светом с доступностью воды, показывает, что имплантированная человеческая ткань может функционировать как часть схемы поиска вознаграждения в мозгу крысы.\n>\n>\n>\n> Исследователи показывают, что нейроны человека при активации влияют на поведение крыс. Человеческие клетки буквально функционально связаны с крысиным мозгом.\n>\n>\n>\n> Д-р Юрген Кноблих, научный директор Австрийского института молекулярной биотехнологии, не участвовавший в исследовании.\n>\n>\nЭто самая передовая схема человеческого мозга, когда-либо созданная из клеток человека. Такая биоплатформа позволяет контролировать и наблюдать за поведением крыс открывая новые захватывающие возможности для изучения нервно-психических расстройств.\n\nКраткое содержание:\nСтатья описывает исследование, в котором ученые трансплантировали человеческие нейроны в мозг крыс и наблюдали, как они формируют гибридные мозговые цепи. Эксперимент позволил изучить, как мозг крысы превращается в живую лабораторию для изучения когнитивных расстройств.\n\nВ статье также говорится о том, что выращенные в лаборатории органоиды мозга могут созревать так же, как человеческий мозг, следуя внутренним биологическим часам. Однако, такие структуры все еще сложно исследовать из-за их ограниченной среды.\n\nТакже в статье говорится о том, что ученые использовали трансплантацию органоидов для изучения нервно-психических расстройств, таких как аутизм и эпилепсия, и о том, что это позволило им наблюдать за поведением крыс и контролировать их реакции на различные стимулы.","summary":"Статья описывает исследование, в котором ученые трансплантировали человеческие нейроны в мозг крыс и наблюдали, как они формируют гибридные мозговые цепи. Эксперимент позволил изучить, как мозг крысы превращается в живую лабораторию для изучения когнитивных расстройств.\n\nВ статье также говорится о том, что выращенные в лаборатории органоиды мозга могут созревать так же, как человеческий мозг, следуя внутренним биологическим часам. Однако, такие структуры все еще сложно исследовать из-за их ограниченной среды.\n\nТакже в статье говорится о том, что ученые использовали трансплантацию органоидов для изучения нервно-психических расстройств, таких как аутизм и эпилепсия, и о том, что это позволило им наблюдать за поведением крыс и контролировать их реакции на различные стимулы."}
{"Text":"4 способа увеличить конверсию лендинга и повысить продажи / Хабр               \n\n07-11-2016\nЛендинг — продающая страница, на которую приходит пользователь. Структура посадочной напрямую влияет на продажи. Мы собрали правила по созданию хорошего лендинга и примеры успешных редизайнов, которые помогли увеличить конверсию страниц. Ключевые элементы классического лендинга: Цепляющий заголовок Призыв к действию (Call to Action или сокращенно – CTA) Описание товара и его преимуществ Отзывы счастливых покупателей Гарантии продавцов Рассказываем, как заставить эти элементы работать. 4 правила хорошего лендинга Одна страница – одна цель Attention ratio — это соотношение между количеством гиперссылок на странице и целями страницы. У хорошего лендинга attention ratio составляет 1:1. Это означает, что лендинг и все его элементы – от заголовка до дизайнерских решений — должны быть сосредоточены на достижении одной конкретной цели — побудить пользователя к действию: купить, скачать, подписаться. Компания Unbounce, специализирующаяся на создании «продающих страниц», провела сплит-тестирование и выяснила: сокращение количества гиперссылок на посадочной с 10 до 1-й увеличивает CTR на 31%. Формула «одна страница – одна цель» подталкивает пользователя в нужном направлении. Если лендинг предлагает несколько различных вариантов действия, посетителю сложно сделать выбор. Напротив, если вы предлагаете конкретное решение проблемы, вы помогаете своим потенциальным клиентам принять это решение. Вот яркий пример лендинга, который нарушает эту заповедь. Не надо так: Покажите товар Используйте в лендинге крупные изображения товара высокого разрешения. В тех случаях, где это уместно, добавьте функцию приближения (zoom), чтобы посетители могли детально рассмотреть товар. Компания Mall.cz провела сплит-тестирование двух посадочных страниц. Лендинг с более крупными изображениями товара увеличивает продажи на 9,46%. Версия 2 увеличила конверсию на 9,46% Видео также может улучшить конверсию. При этом известный онлайн-маркетолог Нил Патель советует ограничить длительность ролика максимум 5-ю минутами – на большее пользователя все равно не хватит. Завоюйте доверие пользователей По данным Dimensional Research, 90% потребителей решают купить товар после прочтения отзывов в интернете. Пользователи доверяют мнению людей, уже попробовавших услугу. Не тратьте время посетителей на поиски отзывов в сети, а разместите их на своем лендинге. Британская компания Express Watches включила в посадочную отзывы “счастливых покупателей”. Конверсия продаж на лендинге увеличилась на 58,29%. Другой пример — ContentVerve. Перед компанией стояла задача побудить посетителей скачать бесплатную книгу. Они протестировали два варианта лендинга. Изначальный вариант страницы выглядел так: Так выглядел лендинг после изменений: Два из четырех отзывов поднялись на «первый экран». Это почти косметическое изменение привело к увеличению количества скачиваний на 64,53%. Повысить доверие покупателей к компании поможет размещение на продающей странице информации о наградах, выдержек из публикаций в прессе, мнений экспертов. Важно также убедить посетителя в аутентичности продаваемого товара. Express Watches, торгующей часами японской марки Seiko, провела тест, который помог увеличить конверсию на 107%. Контрольная страница выглядела так: Эта же часть страница, после изменений: Единственное, что они сделали: справа от изображения продукта плашку с обещанием лучшей цены заменили на сине-черную с текстом «Авторизованный дилер Seiko». В результате компания получила фантастический рост конверсии. Call to action Элемент лендинга, к которому можно применить правило «чем больше – тем лучше» – кнопка с призывом к действию (CTA). Она должна выделяться на странице и содержать ясный и понятный посетителю призыв. В идеале CTA должен соответствовать рекламному объявлению или ключевым словам в поисковой выдаче, по которым посетитель пришел на посадочную. Автор курса по созданию лендингов Оли Гарднер из Unbounce разбирает ошибку с кнопкой CTA на примере сайта по доставке цветов. Рекламное объявление сайта в Adwords: При этом лендинг выглядит так: Оли Гарднер обращает внимание на «три магических слова», которые можно обнаружить в рекламном объявлении компании: «next», «day», «delivery» («доставка на следующий день»). При этом на лендинге этих слов нет. Вместо них, мы видим «Flowers, Plants and Gifts» («Цветы, растения и подарки»). Месседж из рекламного объявления никак не соответствует месседжу лендинга. А значит посетитель, пришедший из поисковика, потратит много времени, чтобы найти то, что хочет. Скорее всего, он просто уйдет с сайта. Вот как должен выглядеть правильный лендинг этой компании: Главный месседж теперь занимает доминирующее положение на странице. Кроме того, появились кнопки заказа и отзывы покупателей. Как редизайн увеличил конверсию на 77% Еще один пример от Оли Гарднера – редизайн посадочной по продаже курса об искусстве лендинга. Изначально страница выглядела так: После редизайна: Что изменилось: Верх страницы стал мультибрендовым, что повышает доверие пользователей Заголовок обращается непосредственно к посетителю с вопросом, на который ниже дается ответ Появилось персональное обращение к клиентам от автора курса В результате этих изменений конверсия страницы увеличилась на 77%.\n","metric":0.63720703130000000414,"metric_input":"Текст статьи:\n4 способа увеличить конверсию лендинга и повысить продажи / Хабр               \n\n07-11-2016\nЛендинг — продающая страница, на которую приходит пользователь. Структура посадочной напрямую влияет на продажи. Мы собрали правила по созданию хорошего лендинга и примеры успешных редизайнов, которые помогли увеличить конверсию страниц. Ключевые элементы классического лендинга: Цепляющий заголовок Призыв к действию (Call to Action или сокращенно – CTA) Описание товара и его преимуществ Отзывы счастливых покупателей Гарантии продавцов Рассказываем, как заставить эти элементы работать. 4 правила хорошего лендинга Одна страница – одна цель Attention ratio — это соотношение между количеством гиперссылок на странице и целями страницы. У хорошего лендинга attention ratio составляет 1:1. Это означает, что лендинг и все его элементы – от заголовка до дизайнерских решений — должны быть сосредоточены на достижении одной конкретной цели — побудить пользователя к действию: купить, скачать, подписаться. Компания Unbounce, специализирующаяся на создании «продающих страниц», провела сплит-тестирование и выяснила: сокращение количества гиперссылок на посадочной с 10 до 1-й увеличивает CTR на 31%. Формула «одна страница – одна цель» подталкивает пользователя в нужном направлении. Если лендинг предлагает несколько различных вариантов действия, посетителю сложно сделать выбор. Напротив, если вы предлагаете конкретное решение проблемы, вы помогаете своим потенциальным клиентам принять это решение. Вот яркий пример лендинга, который нарушает эту заповедь. Не надо так: Покажите товар Используйте в лендинге крупные изображения товара высокого разрешения. В тех случаях, где это уместно, добавьте функцию приближения (zoom), чтобы посетители могли детально рассмотреть товар. Компания Mall.cz провела сплит-тестирование двух посадочных страниц. Лендинг с более крупными изображениями товара увеличивает продажи на 9,46%. Версия 2 увеличила конверсию на 9,46% Видео также может улучшить конверсию. При этом известный онлайн-маркетолог Нил Патель советует ограничить длительность ролика максимум 5-ю минутами – на большее пользователя все равно не хватит. Завоюйте доверие пользователей По данным Dimensional Research, 90% потребителей решают купить товар после прочтения отзывов в интернете. Пользователи доверяют мнению людей, уже попробовавших услугу. Не тратьте время посетителей на поиски отзывов в сети, а разместите их на своем лендинге. Британская компания Express Watches включила в посадочную отзывы “счастливых покупателей”. Конверсия продаж на лендинге увеличилась на 58,29%. Другой пример — ContentVerve. Перед компанией стояла задача побудить посетителей скачать бесплатную книгу. Они протестировали два варианта лендинга. Изначальный вариант страницы выглядел так: Так выглядел лендинг после изменений: Два из четырех отзывов поднялись на «первый экран». Это почти косметическое изменение привело к увеличению количества скачиваний на 64,53%. Повысить доверие покупателей к компании поможет размещение на продающей странице информации о наградах, выдержек из публикаций в прессе, мнений экспертов. Важно также убедить посетителя в аутентичности продаваемого товара. Express Watches, торгующей часами японской марки Seiko, провела тест, который помог увеличить конверсию на 107%. Контрольная страница выглядела так: Эта же часть страница, после изменений: Единственное, что они сделали: справа от изображения продукта плашку с обещанием лучшей цены заменили на сине-черную с текстом «Авторизованный дилер Seiko». В результате компания получила фантастический рост конверсии. Call to action Элемент лендинга, к которому можно применить правило «чем больше – тем лучше» – кнопка с призывом к действию (CTA). Она должна выделяться на странице и содержать ясный и понятный посетителю призыв. В идеале CTA должен соответствовать рекламному объявлению или ключевым словам в поисковой выдаче, по которым посетитель пришел на посадочную. Автор курса по созданию лендингов Оли Гарднер из Unbounce разбирает ошибку с кнопкой CTA на примере сайта по доставке цветов. Рекламное объявление сайта в Adwords: При этом лендинг выглядит так: Оли Гарднер обращает внимание на «три магических слова», которые можно обнаружить в рекламном объявлении компании: «next», «day», «delivery» («доставка на следующий день»). При этом на лендинге этих слов нет. Вместо них, мы видим «Flowers, Plants and Gifts» («Цветы, растения и подарки»). Месседж из рекламного объявления никак не соответствует месседжу лендинга. А значит посетитель, пришедший из поисковика, потратит много времени, чтобы найти то, что хочет. Скорее всего, он просто уйдет с сайта. Вот как должен выглядеть правильный лендинг этой компании: Главный месседж теперь занимает доминирующее положение на странице. Кроме того, появились кнопки заказа и отзывы покупателей. Как редизайн увеличил конверсию на 77% Еще один пример от Оли Гарднера – редизайн посадочной по продаже курса об искусстве лендинга. Изначально страница выглядела так: После редизайна: Что изменилось: Верх страницы стал мультибрендовым, что повышает доверие пользователей Заголовок обращается непосредственно к посетителю с вопросом, на который ниже дается ответ Появилось персональное обращение к клиентам от автора курса В результате этих изменений конверсия страницы увеличилась на 77%.\n\n\nКраткое содержание:\nСтатья обсуждает важность создания эффективного лендинга для увеличения конверсии и продаж. Ключевые элементы лендинга включают цепляющий заголовок, призыв к действию, описание товара и отзывы, а также гарантии продавцов. 4 правила хорошего лендинга включают: 1) Одна страница - одна цель; 2) Сокращение количества гиперссылок увеличивает CTR; 3) Показывать крупные изображения товара повышает продажи; 4) Размещать отзывы и информацию о компании на лендинге для повышения доверия пользователей. Элемент лендинга, который можно усилить, - это кнопка с призывом к действию (CTA). Автор статьи приводит примеры успешных редизайнов лендингов, которые увеличили конверсию.","summary":"Статья обсуждает важность создания эффективного лендинга для увеличения конверсии и продаж. Ключевые элементы лендинга включают цепляющий заголовок, призыв к действию, описание товара и отзывы, а также гарантии продавцов. 4 правила хорошего лендинга включают: 1) Одна страница - одна цель; 2) Сокращение количества гиперссылок увеличивает CTR; 3) Показывать крупные изображения товара повышает продажи; 4) Размещать отзывы и информацию о компании на лендинге для повышения доверия пользователей. Элемент лендинга, который можно усилить, - это кнопка с призывом к действию (CTA). Автор статьи приводит примеры успешных редизайнов лендингов, которые увеличили конверсию."}
{"Text":"Децентрализованные облачные вычисления — благо или зло? / Хабр                 \n\nПривет, Хабр!\nВ данной статье рассмотрим незаслуженно забытую тему децентрализованных облачных вычислений, проведём анализ существующих решений и попробуем ответить на главный вопрос - почему мастодонты AWS / Azure до сих пор остаются единственным production-ready решением для облачных вычислений, не оставляя шансов децентрализованным пулам мощностей?\n## Определение и область применения ##\nНачнём с области применения. Облачные вычисления применяются для широкого спектра задач:\nРабота backend'a и хостинг веб-сайтов, мобильных и десктопных приложений\n\nОбучение и инференс нейронных сетей, проведение computing-heavy вычислений для исследовательских задач в различных областях\n\nОблачный гейминг, рендеринг\n\nИ множество других требовательных к вычислительной мощности задач\nВариантов их решения условно два: покупка и содержание _собственного сервера_ , либо обращение к услугам о _блачных провайдеров_ . Второй вариант имеет множество _плюсов_ : произвольная локация сервера, набор различных конфигураций, техническая поддержка, гарантии стабильной работы.\n_Минусы_  тоже есть: высокая цена, тонкая настройка конфигурации невозможна - можно попасть в промежуток между дешевыми, но недостаточными, и дорогими, но избыточными конфигурациями, самый актуальный - для граждан и бизнеса стран, находящихся под санкциями, вариант зарубежных облачных провайдеров сопряжен с рисками и головной болью с оплатой.\nА что же _децентрализованные вычисления_ ? Это тот же AWS, только машины расположены не в множестве централизованных серверных в ряде регионов мира, а в домах / гаражах / серверных множества людей по всему земному шару, а каждая машина / кластер будет являться нодой сети блокчейна. Гарантом сделки будет выступать не централизованная корпорация, а смарт контракты, платим не картой, а токенами сети.\nПлюсы и минусы этого подхода ниже.\n## Польза и вред ##\nВ децентрализованных вычислениях может принимать участие _практически любое устройство_ : старый компьютер на пентиуме, мобильный телефон, майнинг ферма, любительский GPU кластер, современный домашний ПК. А расположены они могут быть в любом месте с электричеством и интернетом.\nЭто предоставляет арендаторам вычислительных мощностей _феерический выбор конфигураций и локаций_  - можно арендовать мощности престарелого ПК слесаря дяди Васи, а можно арендовать рендер ферму дядюшки Джона из Северной Америки.\nОграничить вам доступ к вычислениям никакие санкции, ограничения и запреты _не смогут_  - блокчейн живет своей жизнью.\nПривлекательна и потенциальная стоимость таких услуг - косты и желаемая маржа частных энтузиастов арендодателей мощностей _в разы ниже_ , чем у корпоративных серверов.\nРаспределенные системы потенциально могут решить задачу скейлинга мощностей _«чем больше – тем быстрее»_  гораздо эффективнее централизованных провайдеров. Множество различных нод в распределенной сети для подобных задач могут сработать эффективнее реплицированных серверных инстансов, к примеру, в _AWS_ .\nСказка, не правда ли? Но _чудес не бывает_ , поэтому без минусов не обошлось.\n\nУ дяди Васи отключили электричество / интернет, и ваши вычисления улетели коту под хвост. Сэкономили? Да. Рады? Маловероятно.\nОблачные провайдеры обладают необходимой инфраструктурой для _обеспечения бесперебойной работы_  ваших вычислений, а вот нода из децентрализованной сети - очень вряд ли.\n\nВаши данные и алгоритмы представляют коммерческую ценность? Дядюшка Джон может оказаться любопытным. Возможно он захочет посмотреть, что же _вы крутите на его ноде_ . И ваши данные / алгоритмы будут _скомпрометированы_ .\nК мастодонтам AWS и Azure в этом плане доверия, конечно же, в разы больше.\nПосмотрим, что из существующих решений нам сейчас доступно.\n## Актуальные децентрализованные решения для вычислений и хранения данных ##\nНачнем с периферии.\nВычисления, как правило, берутся не из воздуха, а производятся на основе данных, которые где-то хранятся. Порой эти данные могут иметь очень внушительный размер. Какие есть решения децентрализованного хранения данных?\n_IPFS_ ** ** (InterPlanetary File System) — технология распределенной файловой системы, основанная на _DHT (Distributed Hash Table)_  и протоколе _BitTorrent_ . Она позволяет объединить файловые системы на различных устройствах в одну, используя контентную адресацию.\n\nОбладает высокой пропускной способностью, хранение бесплатно. Однако удаление файлов не предусмотрено, да и вообще хранение файла в сети кем-то, помимо автора, _не гарантированно_ . Оплата для хостеров не предусмотрена.\n\nЕсть альтернативные технологии, хранение в которых не бесплатно.\n\nНапример, _Sia, Storj, Ethereum Swarm, MadeSAFE_ .\n\nЧтобы обеспечить надежность хранения, используются различные проверки, например, _proof of storage_  (доказательство принятия файла), _proof of retrievability_  (доказательство, что файл в наличии и может быть извлечен). Пользователь _платит_  за хранение, а хостеры _получают награду_ . Плюсы - хранение файлов гарантированно, их можно удалять, пропускная способность высока, надежность хранения засчёт финансовой мотивации. Минусы - не бесплатно и, как правило, совсем не дешево.\nС хранением понятно, а что же вычисления?\nЕдинственный относительно известный и развивающийся сервис, найденный мной - iExec.\nСхема работы от авторов представлена на рисунке.\nСхема работы iExec\nНа бумаге все выглядит крайне пристойно: децентрализация, безопасность данных, выбор конфигурации и локации, прозрачность взаимодействия хостер / арендатор. Внутренняя валюта - токен RLC, используемый для оплаты и вознаграждения хостеров.\nА что же на деле?\nА на деле есть нюанс - для нейросетей и других «обычных» вычислений это решение не подойдет. Только dApps, только Web 3.0. Но сервис вполне рабочий - в пуле есть мощности для аренды, их даже кто-то арендует и что-то с ними делает.\nУзкоспециализированная, конечно, но вполне жизнеспособная (и живущая) система.\n\nА где же провайдеры децентрализованных «обычных» вычислений?\nДосконально прошерстив гугл могу со всей ответственностью заявить - ready-to-go решений в этой области мной обнаружено не было. Проекты на бумаге, замороженные, концепты и гипотетические проекты есть, но когда они попадут в реальную жизнь и попадут ли - большая загадка.\n## Заключение ##\nЧто мы получаем в сухом остатке?\nРаспределённый реестр, он же блокчейн, выглядит применимым к задаче облачных вычислений. Децентрализованные облачные вычисления имеют право на существование и точно могут найти своих потребителей, но с текущими проблемами (которые довольно сложно решить) шансы на конец гегемонии AWS и др. облачных решений крайне малы.\nНедостаточно спроса (по причине рисков и проблем), недостаточно предложений - ведь их рождает спрос. Не будут же хостеры просто так включать свои машины в сеть вычислений, если в ней никто особенно и не вычисляет. Выстреливших проектов не наблюдается, развития и обсуждения темы - тоже.\nС хранением файлов всё несколько более радужно. Инфраструктура есть, решений немало, спрос есть как минимум со стороны dApps - не будет же децентрализованное приложение хранить данные на централизованном файлохранилище.\nИ не будем забывать - в стезе децентрализованного хранения и обмена файлами есть крайне успешный мастодонт uTorrent :)\n\nОт себя добавлю, что мне, как ML ресёрчеру, было бы очень интересно воспользоваться децентрализованной сетью GPU кластеров для обучения нейросетевых моделей, если бы такое решение существовало... Но увы и ах - пока это невозможно.\nБудем наблюдать за развитием этой отрасли.","metric":0.59033203130000000414,"metric_input":"Текст статьи:\nДецентрализованные облачные вычисления — благо или зло? / Хабр                 \n\nПривет, Хабр!\nВ данной статье рассмотрим незаслуженно забытую тему децентрализованных облачных вычислений, проведём анализ существующих решений и попробуем ответить на главный вопрос - почему мастодонты AWS / Azure до сих пор остаются единственным production-ready решением для облачных вычислений, не оставляя шансов децентрализованным пулам мощностей?\n## Определение и область применения ##\nНачнём с области применения. Облачные вычисления применяются для широкого спектра задач:\nРабота backend'a и хостинг веб-сайтов, мобильных и десктопных приложений\n\nОбучение и инференс нейронных сетей, проведение computing-heavy вычислений для исследовательских задач в различных областях\n\nОблачный гейминг, рендеринг\n\nИ множество других требовательных к вычислительной мощности задач\nВариантов их решения условно два: покупка и содержание _собственного сервера_ , либо обращение к услугам о _блачных провайдеров_ . Второй вариант имеет множество _плюсов_ : произвольная локация сервера, набор различных конфигураций, техническая поддержка, гарантии стабильной работы.\n_Минусы_  тоже есть: высокая цена, тонкая настройка конфигурации невозможна - можно попасть в промежуток между дешевыми, но недостаточными, и дорогими, но избыточными конфигурациями, самый актуальный - для граждан и бизнеса стран, находящихся под санкциями, вариант зарубежных облачных провайдеров сопряжен с рисками и головной болью с оплатой.\nА что же _децентрализованные вычисления_ ? Это тот же AWS, только машины расположены не в множестве централизованных серверных в ряде регионов мира, а в домах / гаражах / серверных множества людей по всему земному шару, а каждая машина / кластер будет являться нодой сети блокчейна. Гарантом сделки будет выступать не централизованная корпорация, а смарт контракты, платим не картой, а токенами сети.\nПлюсы и минусы этого подхода ниже.\n## Польза и вред ##\nВ децентрализованных вычислениях может принимать участие _практически любое устройство_ : старый компьютер на пентиуме, мобильный телефон, майнинг ферма, любительский GPU кластер, современный домашний ПК. А расположены они могут быть в любом месте с электричеством и интернетом.\nЭто предоставляет арендаторам вычислительных мощностей _феерический выбор конфигураций и локаций_  - можно арендовать мощности престарелого ПК слесаря дяди Васи, а можно арендовать рендер ферму дядюшки Джона из Северной Америки.\nОграничить вам доступ к вычислениям никакие санкции, ограничения и запреты _не смогут_  - блокчейн живет своей жизнью.\nПривлекательна и потенциальная стоимость таких услуг - косты и желаемая маржа частных энтузиастов арендодателей мощностей _в разы ниже_ , чем у корпоративных серверов.\nРаспределенные системы потенциально могут решить задачу скейлинга мощностей _«чем больше – тем быстрее»_  гораздо эффективнее централизованных провайдеров. Множество различных нод в распределенной сети для подобных задач могут сработать эффективнее реплицированных серверных инстансов, к примеру, в _AWS_ .\nСказка, не правда ли? Но _чудес не бывает_ , поэтому без минусов не обошлось.\n\nУ дяди Васи отключили электричество / интернет, и ваши вычисления улетели коту под хвост. Сэкономили? Да. Рады? Маловероятно.\nОблачные провайдеры обладают необходимой инфраструктурой для _обеспечения бесперебойной работы_  ваших вычислений, а вот нода из децентрализованной сети - очень вряд ли.\n\nВаши данные и алгоритмы представляют коммерческую ценность? Дядюшка Джон может оказаться любопытным. Возможно он захочет посмотреть, что же _вы крутите на его ноде_ . И ваши данные / алгоритмы будут _скомпрометированы_ .\nК мастодонтам AWS и Azure в этом плане доверия, конечно же, в разы больше.\nПосмотрим, что из существующих решений нам сейчас доступно.\n## Актуальные децентрализованные решения для вычислений и хранения данных ##\nНачнем с периферии.\nВычисления, как правило, берутся не из воздуха, а производятся на основе данных, которые где-то хранятся. Порой эти данные могут иметь очень внушительный размер. Какие есть решения децентрализованного хранения данных?\n_IPFS_ ** ** (InterPlanetary File System) — технология распределенной файловой системы, основанная на _DHT (Distributed Hash Table)_  и протоколе _BitTorrent_ . Она позволяет объединить файловые системы на различных устройствах в одну, используя контентную адресацию.\n\nОбладает высокой пропускной способностью, хранение бесплатно. Однако удаление файлов не предусмотрено, да и вообще хранение файла в сети кем-то, помимо автора, _не гарантированно_ . Оплата для хостеров не предусмотрена.\n\nЕсть альтернативные технологии, хранение в которых не бесплатно.\n\nНапример, _Sia, Storj, Ethereum Swarm, MadeSAFE_ .\n\nЧтобы обеспечить надежность хранения, используются различные проверки, например, _proof of storage_  (доказательство принятия файла), _proof of retrievability_  (доказательство, что файл в наличии и может быть извлечен). Пользователь _платит_  за хранение, а хостеры _получают награду_ . Плюсы - хранение файлов гарантированно, их можно удалять, пропускная способность высока, надежность хранения засчёт финансовой мотивации. Минусы - не бесплатно и, как правило, совсем не дешево.\nС хранением понятно, а что же вычисления?\nЕдинственный относительно известный и развивающийся сервис, найденный мной - iExec.\nСхема работы от авторов представлена на рисунке.\nСхема работы iExec\nНа бумаге все выглядит крайне пристойно: децентрализация, безопасность данных, выбор конфигурации и локации, прозрачность взаимодействия хостер / арендатор. Внутренняя валюта - токен RLC, используемый для оплаты и вознаграждения хостеров.\nА что же на деле?\nА на деле есть нюанс - для нейросетей и других «обычных» вычислений это решение не подойдет. Только dApps, только Web 3.0. Но сервис вполне рабочий - в пуле есть мощности для аренды, их даже кто-то арендует и что-то с ними делает.\nУзкоспециализированная, конечно, но вполне жизнеспособная (и живущая) система.\n\nА где же провайдеры децентрализованных «обычных» вычислений?\nДосконально прошерстив гугл могу со всей ответственностью заявить - ready-to-go решений в этой области мной обнаружено не было. Проекты на бумаге, замороженные, концепты и гипотетические проекты есть, но когда они попадут в реальную жизнь и попадут ли - большая загадка.\n## Заключение ##\nЧто мы получаем в сухом остатке?\nРаспределённый реестр, он же блокчейн, выглядит применимым к задаче облачных вычислений. Децентрализованные облачные вычисления имеют право на существование и точно могут найти своих потребителей, но с текущими проблемами (которые довольно сложно решить) шансы на конец гегемонии AWS и др. облачных решений крайне малы.\nНедостаточно спроса (по причине рисков и проблем), недостаточно предложений - ведь их рождает спрос. Не будут же хостеры просто так включать свои машины в сеть вычислений, если в ней никто особенно и не вычисляет. Выстреливших проектов не наблюдается, развития и обсуждения темы - тоже.\nС хранением файлов всё несколько более радужно. Инфраструктура есть, решений немало, спрос есть как минимум со стороны dApps - не будет же децентрализованное приложение хранить данные на централизованном файлохранилище.\nИ не будем забывать - в стезе децентрализованного хранения и обмена файлами есть крайне успешный мастодонт uTorrent :)\n\nОт себя добавлю, что мне, как ML ресёрчеру, было бы очень интересно воспользоваться децентрализованной сетью GPU кластеров для обучения нейросетевых моделей, если бы такое решение существовало... Но увы и ах - пока это невозможно.\nБудем наблюдать за развитием этой отрасли.\n\nКраткое содержание:\nВ статье рассматривается тема децентрализованных облачных вычислений, которая имеет свои плюсы и минусы. \n\nПлюсы включают в себя возможность выбора конфигураций и локаций, низкую стоимость услуг и потенциальную эффективность в решении задачи \"чем больше - тем быстрее\".\n\nМинусы включают в себя риск потери данных из-за отключения электричества или интернета, а также возможность компрометации данных. \n\nТакже в статье рассматриваются существующие решения для децентрализованного хранения данных и вычислений. \n\nВ области хранения данных есть несколько альтернатив, но они не всегда бесплатны. \n\nЧто касается вычислений, то на данный момент существует только один относительно развивающийся сервис - iExec, который подходит только для dApps и Web 3.0. \n\nВ целом, шансы на конец гегемонии AWS и других облачных провайдеров в децентрализованных вычислениях пока невелики, так как недостаточно спроса и предложений.","summary":"В статье рассматривается тема децентрализованных облачных вычислений, которая имеет свои плюсы и минусы. \n\nПлюсы включают в себя возможность выбора конфигураций и локаций, низкую стоимость услуг и потенциальную эффективность в решении задачи \"чем больше - тем быстрее\".\n\nМинусы включают в себя риск потери данных из-за отключения электричества или интернета, а также возможность компрометации данных. \n\nТакже в статье рассматриваются существующие решения для децентрализованного хранения данных и вычислений. \n\nВ области хранения данных есть несколько альтернатив, но они не всегда бесплатны. \n\nЧто касается вычислений, то на данный момент существует только один относительно развивающийся сервис - iExec, который подходит только для dApps и Web 3.0. \n\nВ целом, шансы на конец гегемонии AWS и других облачных провайдеров в децентрализованных вычислениях пока невелики, так как недостаточно спроса и предложений."}
{"Text":"Удобный запуск туннеля OpenVPN на ноутбуке / Habr                         \n\nПоследние пару месяцев я путешествую по Индии и к интернету приходится подключаться где попало — в интернет-кафе, через чей-то беспарольный WiFi, через GPRS. Предвидя это, перед отъездом из дома я решил настроить себе VPN до моего сервера. Выбор пал на OpenVPN. Разумеется, мне хотелось запускать VPN не через командную строку, а удобно, одним-двумя кликами мыши. На ноутбуке стояла Ubuntu 8.10 и я, недолго думая, установил пакет\n`network-manager-openvpn`\n, надеясь на простую интеграцию с network manager'ом. Оказалось, что этот плагин непригоден для использования.\n\nЯ решил использовать OpenVPN в режиме работы со статическими ключами (pre-shared key), потому что у него есть важное преимущество:\n`… it is a handshake-free protocol without any distinguishing signature or feature (such as a header or protocol handshake sequence) that would mark the ciphertext packets as being generated by OpenVPN. Anyone eavesdropping on the wire would see nothing but random-looking data.`\nИли, говоря по-русски, шифрованные данные выглядят как случайный набор байтов и определить, что это такое, невозможно.\n\nА упомянутый плагин просто тупо запускает OpenVPN с жёстко прописанными в коде параметрами, не давая указать нужные. Вначале стал писать патч, но потом понял, что проще выкинуть и найти альтернативное решение.\n\nПосле некоторых поисков обнаружился апплет\ntuntun\nдля Gnome, который работает с OpenVPN принципиально другим способом, используя интерфейс управления последнего.\n\nИтак, вначале я поставил себе на ноутбук OpenVPN, далее написал вот такой конфиг:\n\n> `dev ovpn`\n>\n>\n\n`dev-type tun`\n>\n>\n>\n>\n\n`remote 10.10.10.10 5555 udp`\n>\n>\n\n`nobind`\n>\n>\n>\n>\n\n`secret /etc/openvpn/vpn.key 1`\n>\n>\n\n`cipher AES-256-CBC`\n>\n>\n\n`mlock`\n>\n>\n>\n>\n\n`link-mtu 500`\n>\n>\n\n#`mtu-test`\n>\n>\n>\n>\n\n`mode p2p`\n>\n>\n>\n>\n\n`ifconfig 192.168.1.2 192.168.1.1`\n>\n>\n\n`route-up /etc/openvpn/vpn.routeup.sh`\n>\n>\n>\n>\n\n`comp-lzo`\n>\n>\n>\n>\n\n`up-delay`\n>\n>\n\n`down /etc/openvpn/vpn.down.sh`\n>\n>\n\n`down-pre`\n>\n>\n>\n>\n\n`persist-key`\n>\n>\n\n`persist-tun`\n>\n>\n>\n>\n\n`ping 30`\n>\n>\n>\n>\n\n`daemon`\n>\n>\n>\n>\n\n`verb 1`\n>\n>\n>\n>\n\n`management 127.0.0.1 4444`\n>\n>\n\n`management-hold`\n>\n>\n\n`management-signal`\n\nИ написал два скрипта. Первый,\n`/etc/openvpn/vpn.routeup.sh`\n, выставляет правильный роутинг при поднятии туннеля и вписывает локальный DNS в\n`resolv.conf`\n:\n\n> `#!/bin/sh`\n>\n>\n>\n>\n\n`vpn=\"10.10.10.10\"`\n>\n>\n\n`ns=\"127.0.0.1\"`\n>\n>\n\n`routedown=\"/etc/openvpn/\\$dev.routedown.sh\"`\n>\n>\n>\n>\n\n`[ -z \"\\$dev\" ] && { echo \"should be run by openvpn\" >&2; exit 1; }`\n>\n>\n>\n>\n\n`origgw=ip route get \\$vpn |grep ' via ' |sed -re 's/^.*via +([^ ]+).*\\$/\\1/'`\n>\n>\n\n`origdev=ip route get \\$vpn |grep ' dev ' |sed -re 's/^.*dev +([^ ]+).*\\$/\\1/'`\n>\n>\n>\n>\n\n`[ -z \"\\$origdev\" ] && { echo \"no route to VPN server, something wrong\" >&2; exit 1; }`\n>\n>\n>\n>\n\n`sed -i -e \"1 s/^/nameserver \\$ns # added for OpenVPN\\n/\" /etc/resolv.conf`\n>\n>\n>\n>\n\n`if [ -z \"\\$origgw\" ]; then`\n>\n>\n\n    `ip route replace \\$vpn dev \\$origdev`\n>\n>\n\n`else`\n>\n>\n\n    `ip route replace \\$vpn via \\$origgw dev \\$origdev`\n>\n>\n\n`fi`\n>\n>\n\n`ip route replace default dev \\$dev`\n>\n>\n>\n>\n\n# `Assume route to VPN is equal to default route.`\n>\n>\n\n`if [ -z \"\\$origgw\" ]; then`\n>\n>\n\n    `echo \"ip route replace default dev \\$origdev\" >\\$routedown`\n>\n>\n\n`else`\n>\n>\n\n    `echo \"ip route replace default via \\$origgw dev \\$origdev\" >\\$routedown`\n>\n>\n\n`fi`\n>\n>\n\n`echo \"ip route del \\$vpn\" >>\\$routedown`\n>\n>\n\n`echo \"sed -i -e '/# added for OpenVPN/ d' /etc/resolv.conf\" >>\\$routedown`\n\nВторой,\n`/etc/openvpn/vpn.down.sh`\n, возвращает роутинг и DNS в исходное состояние:\n\n> `#!/bin/sh`\n>\n>\n>\n>\n\n`routedown=\"/etc/openvpn/\\$dev.routedown.sh\"`\n>\n>\n>\n>\n\n`if [ -f \\$routedown ]; then`\n>\n>\n\n    `. \\$routedown`\n>\n>\n\n    `rm -f \\$routedown`\n>\n>\n\n`fi`\n\nДалее, остаётся самое простое: запустить OpenVPN на сервере и ноутбуке, добавить апплет tuntun на панель gnome стандартным способом, он будет выглядеть вот так (отмечен указателем мыши):\n\nЗатем добавить VPN в настройки tuntun (правая кнопка мыши → Preferences → Add):\n\nи в результате получается примерно вот что:\n\nЗапуск VPN — левая кнопка мыши на апплете, затем выбрать из списка нужный VPN:\n\nПовторный клик на нужном VPN в списке разрывает соединение.\n\nПро настройку OpenVPN на другой стороне писать не буду, т.к. это уже за рамками данной статьи.\n\n_Кому понравилось, прошу помочь с кармой — тогда я перенесу статью в тематический блог._","metric":0.525390625,"metric_input":"Текст статьи:\nУдобный запуск туннеля OpenVPN на ноутбуке / Habr                         \n\nПоследние пару месяцев я путешествую по Индии и к интернету приходится подключаться где попало — в интернет-кафе, через чей-то беспарольный WiFi, через GPRS. Предвидя это, перед отъездом из дома я решил настроить себе VPN до моего сервера. Выбор пал на OpenVPN. Разумеется, мне хотелось запускать VPN не через командную строку, а удобно, одним-двумя кликами мыши. На ноутбуке стояла Ubuntu 8.10 и я, недолго думая, установил пакет\n`network-manager-openvpn`\n, надеясь на простую интеграцию с network manager'ом. Оказалось, что этот плагин непригоден для использования.\n\nЯ решил использовать OpenVPN в режиме работы со статическими ключами (pre-shared key), потому что у него есть важное преимущество:\n`… it is a handshake-free protocol without any distinguishing signature or feature (such as a header or protocol handshake sequence) that would mark the ciphertext packets as being generated by OpenVPN. Anyone eavesdropping on the wire would see nothing but random-looking data.`\nИли, говоря по-русски, шифрованные данные выглядят как случайный набор байтов и определить, что это такое, невозможно.\n\nА упомянутый плагин просто тупо запускает OpenVPN с жёстко прописанными в коде параметрами, не давая указать нужные. Вначале стал писать патч, но потом понял, что проще выкинуть и найти альтернативное решение.\n\nПосле некоторых поисков обнаружился апплет\ntuntun\nдля Gnome, который работает с OpenVPN принципиально другим способом, используя интерфейс управления последнего.\n\nИтак, вначале я поставил себе на ноутбук OpenVPN, далее написал вот такой конфиг:\n\n> `dev ovpn`\n>\n>\n\n`dev-type tun`\n>\n>\n>\n>\n\n`remote 10.10.10.10 5555 udp`\n>\n>\n\n`nobind`\n>\n>\n>\n>\n\n`secret /etc/openvpn/vpn.key 1`\n>\n>\n\n`cipher AES-256-CBC`\n>\n>\n\n`mlock`\n>\n>\n>\n>\n\n`link-mtu 500`\n>\n>\n\n#`mtu-test`\n>\n>\n>\n>\n\n`mode p2p`\n>\n>\n>\n>\n\n`ifconfig 192.168.1.2 192.168.1.1`\n>\n>\n\n`route-up /etc/openvpn/vpn.routeup.sh`\n>\n>\n>\n>\n\n`comp-lzo`\n>\n>\n>\n>\n\n`up-delay`\n>\n>\n\n`down /etc/openvpn/vpn.down.sh`\n>\n>\n\n`down-pre`\n>\n>\n>\n>\n\n`persist-key`\n>\n>\n\n`persist-tun`\n>\n>\n>\n>\n\n`ping 30`\n>\n>\n>\n>\n\n`daemon`\n>\n>\n>\n>\n\n`verb 1`\n>\n>\n>\n>\n\n`management 127.0.0.1 4444`\n>\n>\n\n`management-hold`\n>\n>\n\n`management-signal`\n\nИ написал два скрипта. Первый,\n`/etc/openvpn/vpn.routeup.sh`\n, выставляет правильный роутинг при поднятии туннеля и вписывает локальный DNS в\n`resolv.conf`\n:\n\n> `#!/bin/sh`\n>\n>\n>\n>\n\n`vpn=\"10.10.10.10\"`\n>\n>\n\n`ns=\"127.0.0.1\"`\n>\n>\n\n`routedown=\"/etc/openvpn/\\$dev.routedown.sh\"`\n>\n>\n>\n>\n\n`[ -z \"\\$dev\" ] && { echo \"should be run by openvpn\" >&2; exit 1; }`\n>\n>\n>\n>\n\n`origgw=ip route get \\$vpn |grep ' via ' |sed -re 's/^.*via +([^ ]+).*\\$/\\1/'`\n>\n>\n\n`origdev=ip route get \\$vpn |grep ' dev ' |sed -re 's/^.*dev +([^ ]+).*\\$/\\1/'`\n>\n>\n>\n>\n\n`[ -z \"\\$origdev\" ] && { echo \"no route to VPN server, something wrong\" >&2; exit 1; }`\n>\n>\n>\n>\n\n`sed -i -e \"1 s/^/nameserver \\$ns # added for OpenVPN\\n/\" /etc/resolv.conf`\n>\n>\n>\n>\n\n`if [ -z \"\\$origgw\" ]; then`\n>\n>\n\n    `ip route replace \\$vpn dev \\$origdev`\n>\n>\n\n`else`\n>\n>\n\n    `ip route replace \\$vpn via \\$origgw dev \\$origdev`\n>\n>\n\n`fi`\n>\n>\n\n`ip route replace default dev \\$dev`\n>\n>\n>\n>\n\n# `Assume route to VPN is equal to default route.`\n>\n>\n\n`if [ -z \"\\$origgw\" ]; then`\n>\n>\n\n    `echo \"ip route replace default dev \\$origdev\" >\\$routedown`\n>\n>\n\n`else`\n>\n>\n\n    `echo \"ip route replace default via \\$origgw dev \\$origdev\" >\\$routedown`\n>\n>\n\n`fi`\n>\n>\n\n`echo \"ip route del \\$vpn\" >>\\$routedown`\n>\n>\n\n`echo \"sed -i -e '/# added for OpenVPN/ d' /etc/resolv.conf\" >>\\$routedown`\n\nВторой,\n`/etc/openvpn/vpn.down.sh`\n, возвращает роутинг и DNS в исходное состояние:\n\n> `#!/bin/sh`\n>\n>\n>\n>\n\n`routedown=\"/etc/openvpn/\\$dev.routedown.sh\"`\n>\n>\n>\n>\n\n`if [ -f \\$routedown ]; then`\n>\n>\n\n    `. \\$routedown`\n>\n>\n\n    `rm -f \\$routedown`\n>\n>\n\n`fi`\n\nДалее, остаётся самое простое: запустить OpenVPN на сервере и ноутбуке, добавить апплет tuntun на панель gnome стандартным способом, он будет выглядеть вот так (отмечен указателем мыши):\n\nЗатем добавить VPN в настройки tuntun (правая кнопка мыши → Preferences → Add):\n\nи в результате получается примерно вот что:\n\nЗапуск VPN — левая кнопка мыши на апплете, затем выбрать из списка нужный VPN:\n\nПовторный клик на нужном VPN в списке разрывает соединение.\n\nПро настройку OpenVPN на другой стороне писать не буду, т.к. это уже за рамками данной статьи.\n\n_Кому понравилось, прошу помочь с кармой — тогда я перенесу статью в тематический блог._\n\nКраткое содержание:\nВ статье описывается настройка удобного запуска VPN-соединения с помощью OpenVPN на ноутбуке с операционной системой Ubuntu 8.10. Автор столкнулся с проблемами при попытке использовать плагин для Network Manager, но в итоге нашел альтернативное решение - апплет tuntun для Gnome. Для настройки OpenVPN был создан конфигурационный файл с указанием параметров соединения и написаны два скрипта для изменения роутинга и DNS при поднятии и разрыве соединения.","summary":"В статье описывается настройка удобного запуска VPN-соединения с помощью OpenVPN на ноутбуке с операционной системой Ubuntu 8.10. Автор столкнулся с проблемами при попытке использовать плагин для Network Manager, но в итоге нашел альтернативное решение - апплет tuntun для Gnome. Для настройки OpenVPN был создан конфигурационный файл с указанием параметров соединения и написаны два скрипта для изменения роутинга и DNS при поднятии и разрыве соединения."}
{"Text":"Зачем нужны и как работают «вечные ссылки» в социальных сетях / Habr             \n\n27-08-2015\nВ 2010 году Илья Сегалович рассказывал на конференции SIGIR об эволюции формулы ранжирования Яндекса. В 2006 году формула состояла всего из 20 символов (0,02 килобайта)! Здесь нет ошибки. Забравшись на плечи очень простому алгоритму PageRank, Google и Яндекс перевернули мир поиска в интернете и стали финансовыми тяжеловесами. Описание оригинального алгоритма PageRank Мы предполагаем, что страница А имеет ссылающиеся на нее (то есть цитирующие) страницы T1…Tn. Параметр d – коэффициент затухания, который может принимать значения от 0 до 1. Обычно мы берем его равным величине 0,85. Более детальное рассмотрение коэффициента d приводится далее в статье. Также С(А) определяется как число исходящих ссылок страницы А. PageRank страницы А рассчитывается следующим образом: PR(A) = (1-d) + d (PR(T1)/C(T1) +… + PR(Tn)/C(Tn)) «Анатомия крупномасштабной гипертекстовой сетевой поисковой системы», 1998, кафедра «Вычислительная техника», Стэнфордский университет, Стэнфорд, Калифорния. Однако на предложенном выше графике Сегаловича мы видим, что формула ранжирования очень быстро «распухла». В индексах научных статей PageRank работает хорошо, но в живом мире интернета за счет открытого и простого обсчета ссылочного графа высокого качества поиска не добиться. Например, поисковикам очень важно, чтобы с внешних ссылок были реальные переходы живых пользователей. Важно, чтобы на сайт ссылались как можно более тематичные и авторитетные источники, где явно и по всем признакам присутствует живая, активная аудитория. И на сегодняшний день существует такой тип ссылок, который однозначно признаётся всеми поисковиками качественным. Как вы можете легко догадаться, это и есть тема данной статьи – ссылки из постов в личных аккаунтах и сообществ с «живой» аудиторией. Польза ссылок из социальных сетей Что представляют собой ссылки из социальных сетей с точки зрения поисковика? Подтверждение того, что некий сайт интересен живым людям. Фактор повышения естественности ссылочного профиля сайта. Быстро индексируемые ссылки, сообщающие о появлении новых страниц. Стабильные ссылки с длительным сроком жизни (WebArtex проверяет, не удалил ли владелец аккаунта ссылку после её размещения). Эти четыре фактора вносят хотя и весомый, но не решающий вклад в окончательный расчет ранжирования. По оценкам специалистов, доля социального фактора пока что не превышает нескольких процентов. Однако стоит понимать, что польза не ограничивается статическими параметрами, поскольку сейчас Яндекс уделяет огромное внимание поведенческим факторам и переходам по ссылкам. Переходы по ссылке – это признак того, что ссылка ведет из адекватного места размещения на интересующую людей страницу: ссылка расположена там, где ее видят реальные пользователи; ссылка окружена релевантным поясняющим текстом, что создает контекст для перехода. Поведенческие факторы ранжирования отражают интерес посетителей к сайту: количество посетителей; показатель отказов (процент посетителей, просмотревших только одну страницу); глубина просмотра сайта; количество проведенного на сайте времени. Очевидно, что ссылки из постов в соцсетях приносят больше переходов представителей заинтересованной аудитории, чем любые другие типы размещения ссылок. Сравниться с ними могут только статьи на качественных площадках, которые анонсируются хотя бы в виде ссылок на главных страницах. Такая возможность имеется и в WebArtex (см. в типах статей). Рост позиций + дешевый трафик Ссылки из постов в соцсетях способствуют росту позиций продвигаемого сайта в поисковой выдаче за счет повышения естественности ссылочного профиля и улучшения поведенческих факторов. Из соцсетей приходят заинтересованные и активные посетители: пост, в котором размещена ссылка, можно сделать интересным, стимулирующим переходы. Особенность восприятия подобных постов пользователями состоит в том, что размещённой информации полностью доверяют (как если бы это была проверенная информация от друга или родственника), несмотря на то что она носит незавуалированный рекламный характер. При этом трафик получается очень дешевым благодаря простоте шеринга и низкой стоимости размещения (от 6 рублей в Twitter и ВКонтакте и от 50 рублей в Youtube). Конечно, все зависит от контента: предоставьте качественный контент – и люди будут им делиться. Стоит понимать, что трафик из соцсетей не является чисто «техническим» показателем, неким довеском к SEO-параметрам ссылки. Это целевая аудитория, то есть реальные пользователи, которые с высокой долей вероятности могут превратиться в покупателей и клиентов. Идеально для стартапов и молодых сайтов У поисковых систем есть фильтр «Песочница» – сайты, которым еще не исполнился год и которые пока не могут бороться за ТОП-10 по мало-мальски конкурентным запросам. Ссылки из соцсетей не только укорачивают период пребывания в «Песочнице» (как и ссылки из статей на максимально трастовых сайтах типа федеральных СМИ), но и приводят активный недорогой трафик – то есть именно то, что необходимо молодым проектам и стартапам. Наравне с контекстной рекламой, ссылки из постов в социальных сетях являются ответом на высокую латентность SEO. Если от начала традиционного продвижения сайта до его выхода в первую десятку может пройти несколько месяцев, то посты в соцсетях принесут заинтересованных посетителей в первый же день. Как покупать ссылки из соцсетей в WebArtex Зарегистрируйтесь в системе, добавьте сайт, который хотите продвигать, зайдите в раздел «Оптимизатор» и нажмите кнопку «Заказать». Далее кликните на вкладку «Соцсети». В открывшейся вкладке вы увидите отобранные по двухступенчатой схеме личные аккаунты и сообщества. В WebArtex не попадают площадки, имеющие ботов, а также созданные исключительно с целью размещения рекламы. Изначально аккаунт проверяется на соответствие параметрам качества (возраст, количество друзей или подписчиков, активность пользователя и аудитории) автоматически, а затем вручную: модератор WebArtex проверяет кандидатов на предмет наличия ботов и прочих фатальных недостатков. Итак, вы видите только проверенные аккаунты и можете оценить их параметры. На настоящий момент доступны аккаунты Twitter, ВКонтакте и Youtube, а также сообщества ВКонтакте. В ближайшее время разработчики планируют добавить и другие популярные в России социальные сети. Загрузите контент поста и оплатите размещение. Оплата за пост с «вечной ссылкой» взимается единожды (не ежемесячно, а именно один раз). В обозримом будущем пост никуда не исчезнет: WebArtex полностью контролирует процесс и немедленно изгоняет площадки, снимающие ссылки. Выгодная монетизация трафика Совершенно очевидно, что владельцы аккаунтов и сообществ в соцсетях хотят зарабатывать на рекламе. Удачные примеры Артемия Лебедева и Ильи Варламова показали, что если не скрывать рекламный характер размещения поста, то он, в отличие от пресловутой «шоладемизации», то есть публикации рекламы в скрытом формате, не вызывает негатива аудитории. Биржа WebArtex дает возможность монетизации трафика даже не самым популярным аккаунтам: главное, чтобы у них была качественная и «живая» аудитория. Лимиты: от 200 друзей для персональных аккаунтов ВКонтакте и Twitter, от 500 подписчиков для Youtube, от 1000 участников для сообществ. В WebArtex большой спрос на ссылки: его создают более 50 000 оптимизаторов и владельцев сайтов. В качественных аккаунтах с адекватной ценой размещения все рекламные места выкупаются очень быстро. Реферальная программа В WebArtex можно приводить площадки и получать процент от их дохода. Причем процент весьма щедрый: если доход пришедших по вашей реферальной ссылке владельцев сообществ и аккаунтов составит, например, 200 000 рублей в месяц, то вы будете ежемесячно получать по 25 000 рублей. TL;DR Теперь в WebArtex из постов в соцсетях можно покупать «вечные ссылки», которые не только способствуют продвижению сайта в ТОП-10 поисковой выдачи, но и приводят качественный дешевый трафик. Цена – от 6 рублей за размещение. Аккаунты и сообщества проверяются вручную, все они имеют «живую» аудиторию.\n","metric":0.6220703125,"metric_input":"Текст статьи:\nЗачем нужны и как работают «вечные ссылки» в социальных сетях / Habr             \n\n27-08-2015\nВ 2010 году Илья Сегалович рассказывал на конференции SIGIR об эволюции формулы ранжирования Яндекса. В 2006 году формула состояла всего из 20 символов (0,02 килобайта)! Здесь нет ошибки. Забравшись на плечи очень простому алгоритму PageRank, Google и Яндекс перевернули мир поиска в интернете и стали финансовыми тяжеловесами. Описание оригинального алгоритма PageRank Мы предполагаем, что страница А имеет ссылающиеся на нее (то есть цитирующие) страницы T1…Tn. Параметр d – коэффициент затухания, который может принимать значения от 0 до 1. Обычно мы берем его равным величине 0,85. Более детальное рассмотрение коэффициента d приводится далее в статье. Также С(А) определяется как число исходящих ссылок страницы А. PageRank страницы А рассчитывается следующим образом: PR(A) = (1-d) + d (PR(T1)/C(T1) +… + PR(Tn)/C(Tn)) «Анатомия крупномасштабной гипертекстовой сетевой поисковой системы», 1998, кафедра «Вычислительная техника», Стэнфордский университет, Стэнфорд, Калифорния. Однако на предложенном выше графике Сегаловича мы видим, что формула ранжирования очень быстро «распухла». В индексах научных статей PageRank работает хорошо, но в живом мире интернета за счет открытого и простого обсчета ссылочного графа высокого качества поиска не добиться. Например, поисковикам очень важно, чтобы с внешних ссылок были реальные переходы живых пользователей. Важно, чтобы на сайт ссылались как можно более тематичные и авторитетные источники, где явно и по всем признакам присутствует живая, активная аудитория. И на сегодняшний день существует такой тип ссылок, который однозначно признаётся всеми поисковиками качественным. Как вы можете легко догадаться, это и есть тема данной статьи – ссылки из постов в личных аккаунтах и сообществ с «живой» аудиторией. Польза ссылок из социальных сетей Что представляют собой ссылки из социальных сетей с точки зрения поисковика? Подтверждение того, что некий сайт интересен живым людям. Фактор повышения естественности ссылочного профиля сайта. Быстро индексируемые ссылки, сообщающие о появлении новых страниц. Стабильные ссылки с длительным сроком жизни (WebArtex проверяет, не удалил ли владелец аккаунта ссылку после её размещения). Эти четыре фактора вносят хотя и весомый, но не решающий вклад в окончательный расчет ранжирования. По оценкам специалистов, доля социального фактора пока что не превышает нескольких процентов. Однако стоит понимать, что польза не ограничивается статическими параметрами, поскольку сейчас Яндекс уделяет огромное внимание поведенческим факторам и переходам по ссылкам. Переходы по ссылке – это признак того, что ссылка ведет из адекватного места размещения на интересующую людей страницу: ссылка расположена там, где ее видят реальные пользователи; ссылка окружена релевантным поясняющим текстом, что создает контекст для перехода. Поведенческие факторы ранжирования отражают интерес посетителей к сайту: количество посетителей; показатель отказов (процент посетителей, просмотревших только одну страницу); глубина просмотра сайта; количество проведенного на сайте времени. Очевидно, что ссылки из постов в соцсетях приносят больше переходов представителей заинтересованной аудитории, чем любые другие типы размещения ссылок. Сравниться с ними могут только статьи на качественных площадках, которые анонсируются хотя бы в виде ссылок на главных страницах. Такая возможность имеется и в WebArtex (см. в типах статей). Рост позиций + дешевый трафик Ссылки из постов в соцсетях способствуют росту позиций продвигаемого сайта в поисковой выдаче за счет повышения естественности ссылочного профиля и улучшения поведенческих факторов. Из соцсетей приходят заинтересованные и активные посетители: пост, в котором размещена ссылка, можно сделать интересным, стимулирующим переходы. Особенность восприятия подобных постов пользователями состоит в том, что размещённой информации полностью доверяют (как если бы это была проверенная информация от друга или родственника), несмотря на то что она носит незавуалированный рекламный характер. При этом трафик получается очень дешевым благодаря простоте шеринга и низкой стоимости размещения (от 6 рублей в Twitter и ВКонтакте и от 50 рублей в Youtube). Конечно, все зависит от контента: предоставьте качественный контент – и люди будут им делиться. Стоит понимать, что трафик из соцсетей не является чисто «техническим» показателем, неким довеском к SEO-параметрам ссылки. Это целевая аудитория, то есть реальные пользователи, которые с высокой долей вероятности могут превратиться в покупателей и клиентов. Идеально для стартапов и молодых сайтов У поисковых систем есть фильтр «Песочница» – сайты, которым еще не исполнился год и которые пока не могут бороться за ТОП-10 по мало-мальски конкурентным запросам. Ссылки из соцсетей не только укорачивают период пребывания в «Песочнице» (как и ссылки из статей на максимально трастовых сайтах типа федеральных СМИ), но и приводят активный недорогой трафик – то есть именно то, что необходимо молодым проектам и стартапам. Наравне с контекстной рекламой, ссылки из постов в социальных сетях являются ответом на высокую латентность SEO. Если от начала традиционного продвижения сайта до его выхода в первую десятку может пройти несколько месяцев, то посты в соцсетях принесут заинтересованных посетителей в первый же день. Как покупать ссылки из соцсетей в WebArtex Зарегистрируйтесь в системе, добавьте сайт, который хотите продвигать, зайдите в раздел «Оптимизатор» и нажмите кнопку «Заказать». Далее кликните на вкладку «Соцсети». В открывшейся вкладке вы увидите отобранные по двухступенчатой схеме личные аккаунты и сообщества. В WebArtex не попадают площадки, имеющие ботов, а также созданные исключительно с целью размещения рекламы. Изначально аккаунт проверяется на соответствие параметрам качества (возраст, количество друзей или подписчиков, активность пользователя и аудитории) автоматически, а затем вручную: модератор WebArtex проверяет кандидатов на предмет наличия ботов и прочих фатальных недостатков. Итак, вы видите только проверенные аккаунты и можете оценить их параметры. На настоящий момент доступны аккаунты Twitter, ВКонтакте и Youtube, а также сообщества ВКонтакте. В ближайшее время разработчики планируют добавить и другие популярные в России социальные сети. Загрузите контент поста и оплатите размещение. Оплата за пост с «вечной ссылкой» взимается единожды (не ежемесячно, а именно один раз). В обозримом будущем пост никуда не исчезнет: WebArtex полностью контролирует процесс и немедленно изгоняет площадки, снимающие ссылки. Выгодная монетизация трафика Совершенно очевидно, что владельцы аккаунтов и сообществ в соцсетях хотят зарабатывать на рекламе. Удачные примеры Артемия Лебедева и Ильи Варламова показали, что если не скрывать рекламный характер размещения поста, то он, в отличие от пресловутой «шоладемизации», то есть публикации рекламы в скрытом формате, не вызывает негатива аудитории. Биржа WebArtex дает возможность монетизации трафика даже не самым популярным аккаунтам: главное, чтобы у них была качественная и «живая» аудитория. Лимиты: от 200 друзей для персональных аккаунтов ВКонтакте и Twitter, от 500 подписчиков для Youtube, от 1000 участников для сообществ. В WebArtex большой спрос на ссылки: его создают более 50 000 оптимизаторов и владельцев сайтов. В качественных аккаунтах с адекватной ценой размещения все рекламные места выкупаются очень быстро. Реферальная программа В WebArtex можно приводить площадки и получать процент от их дохода. Причем процент весьма щедрый: если доход пришедших по вашей реферальной ссылке владельцев сообществ и аккаунтов составит, например, 200 000 рублей в месяц, то вы будете ежемесячно получать по 25 000 рублей. TL;DR Теперь в WebArtex из постов в соцсетях можно покупать «вечные ссылки», которые не только способствуют продвижению сайта в ТОП-10 поисковой выдачи, но и приводят качественный дешевый трафик. Цена – от 6 рублей за размещение. Аккаунты и сообщества проверяются вручную, все они имеют «живую» аудиторию.\n\n\nКраткое содержание:\nВ статье рассматривается использование \"вечных ссылок\" в социальных сетях для продвижения сайтов в поисковых системах. Описывается, как ссылки из постов в личных аккаунтах и сообществ с \"живой\" аудиторией влияют на ранжирование сайтов в поиске. Указывается, что такие ссылки приносят больше переходов представителей заинтересованной аудитории, чем другие типы размещения ссылок. Также отмечается, что ссылки из соцсетей способствуют росту позиций продвигаемого сайта в поисковой выдаче за счет повышения естественности ссылочного профиля и улучшения поведенческих факторов. Описываются возможности покупки таких ссылок в WebArtex и их влияние на трафик сайта.","summary":"В статье рассматривается использование \"вечных ссылок\" в социальных сетях для продвижения сайтов в поисковых системах. Описывается, как ссылки из постов в личных аккаунтах и сообществ с \"живой\" аудиторией влияют на ранжирование сайтов в поиске. Указывается, что такие ссылки приносят больше переходов представителей заинтересованной аудитории, чем другие типы размещения ссылок. Также отмечается, что ссылки из соцсетей способствуют росту позиций продвигаемого сайта в поисковой выдаче за счет повышения естественности ссылочного профиля и улучшения поведенческих факторов. Описываются возможности покупки таких ссылок в WebArtex и их влияние на трафик сайта."}
{"Text":"Минимальный почтовый сервер на основе Postfix и Dovecot. Часть 1: Dovecot / Habr           \n\n22-05-2015\nЦель: получить минимально работающий почтовый сервер, используя только Postfix и Dovecot, с минимальным изменением настроек по умолчанию. Получить каркас системы, на основе которого можно, в дальнейшем, настраивать спам фильтры, работу с базами данных, LDAP… Почтовая система должна: Поддерживать любое количество доменов и пользователей. Обслуживать пользователей, не привязанных к локальным учетным записям. Обеспечивать доступ к почтовым ящикам по протоколам POP3, IMAP с поддержкой TLS. Обеспечивать отправку и прием писем по протоколу SMTP с поддержкой TLS. Настройка проводилась в CentOS 6. Postfix 2.6.6, Dovecot 2.0.9. Никакие особенности ОС не применялись. Структура почтовой системы Условно, почтовые системы можно разделить по методу доступа к почтовым ящикам и списку пользователей. Классическая почтовая система Postfix, Dovecot имеют доступ к списку пользователей и работают параллельно, разделяя доступ к почтовым ящикам пользователей. Упрощенная почтовая система Dovecot является «бэкендом» для доступа к почтовым ящикам пользователей. При этом подходе, расположение почтовых ящиков, список пользователей, известен только Dovecot. Именно этот подход мы будет использовать для достижения нашей цели. Основные плюсы классической системы по сравнению с упрощенной: Производительность. Postfix имеет доступ к почтовым ящикам и может быстрее и проще, нежели во втором случае, доставить почту непосредственно пользователю. Независимость частей системы друг от друга. В случае отключения Dovecot, Postfix продолжит свою основную функцию — принимать письма. Основные недостатки классической системы по сравнению с упрощенной: Формат почтовых ящиков должен быть понятен обеим программам. Необходимо синхронизировать доступ к почтовым ящикам. Безопасность. Необходимы дополнительные полномочия для Postfix. Postfix должен иметь прямой доступ к почтовым ящикам и списку пользователей. Настройка и сопровождение немного сложнее. Dovecot Несмотря на то, что Dovecot является ядром системы, выполняющим много функций. Его настройка не должна вызвать, каких-либо затруднений. Возможно, это связано с тем, что Dovecot имеет дело только с аутентифицированными клиентами, что нельзя сказать про Postfix. Имена будем задавать в формате username@domainname. Создадим пользователя «vmail» для хранения почты, без «шелл» доступа, но с домашней папкой «/home/vmail». Настроим аутентификацию. auth_mechanisms = plain login (login это тот же plain, но для Outlook) mail_gid = vmail mail_uid = vmail Ограничение передачи пароля только после установки TLS соединения, выполняется в Dovecot по умолчанию и не требует дополнительных настроек. Так как мы будем использовать TLS, никаких других механизмов аутентификации нам не нужно, только PLAIN. Самоподписанный сертификат, в CentOS, создается при установке Dovecot в папке, /etc/pki/dovecot/certs/, воспользуемся пока им, для настройки TLS. Хочу обратить ваше внимание на один важный момент. Нужно различать, механизм аутентификации от метода хранения аутентификационных данных. Хотя эти два понятия могут называться одинаково, это две разные вещи. Подробности здесь и здесь. Настроим, где будем хранить почтовые ящики пользователей. Для каждого виртуального пользователя задаем домашнюю папку в формате — /home/vmail/domain/username, mail_home = /home/vmail/%d/%n и местоположение почты в домашней папке — /home/vmail/domain/username/Maildir, mail_location = maildir:~/Maildir Формат почтовых ящиков Maildir выбран для упрощения перехода к классической системе, в случае необходимости, т.к. Postfix поддерживает Maildir. Но если вы не планируете возврат к классической системе, можно выбрать любой формат почтовых ящиков поддерживаемый Dovecot. Следующие две директивы задают, где и как Dovecot будет искать имена и пароли пользователей. userdb { args = username_format=%u /etc/dovecot/users driver = passwd-file } passdb { args = scheme=ssha512 username_format=%u /etc/dovecot/users driver = passwd-file } Мы использовали для этого файл, /etc/dovecot/users, имеющий формат стандартного файла паролей /etc/passwd. Пример файла паролей (пароль обрезан): user1@example1.com:{SSHA512}2YT51xuhilbvb4vYRIb1oj1EvrKFszhf2MNw=:::::: user3@example3.com:{SSHA512}GdBv9GEE1rfFpd4+fzXS+UKh4x6gTpTaH4=:::::: Для безопасности, мы не храним пароли пользователей в открытом виде, а храним их подсоленные SHA512. Для заполнения файла «/etc/dovecot/users», будем использовать этот скрипт с двумя параметрами, именем пользователя и паролем пользователя. #!/bin/sh echo $1:$(doveadm pw -s ssha512 -p $2):::::: >> /etc/dovecot/users Настроим сервисы для связи с Postfix. Для поиска имен пользователей и SASL аутентификации. service auth { unix_listener /var/spool/postfix/private/auth { group = postfix mode = 0660 user = postfix } unix_listener auth-userdb { mode = 0600 user = vmail } } Для доступа к почтовым ящикам пользователей. service lmtp { unix_listener /var/spool/postfix/private/dovecot-lmtp { group = postfix mode = 0600 user = postfix } user = vmail } protocol lmtp { postmaster_address = postmaster@example1.ru } Более подробно можно почитать: здесь про LMTP, здесь про LMTP и Postfix, а здесь про SASL. Итоговый «devconf –N»: # 2.0.9: /etc/dovecot/dovecot.conf # OS: Linux 2.6.32-504.16.2.el6.x86_64 x86_64 CentOS release 6.6 (Final) auth_mechanisms = plain login mail_gid = vmail mail_home = /home/vmail/%d/%n mail_location = maildir:~/Maildir mail_uid = vmail mbox_write_locks = fcntl passdb { args = scheme=ssha512 username_format=%u /etc/dovecot/users driver = passwd-file } service auth { unix_listener /var/spool/postfix/private/auth { group = postfix mode = 0660 user = postfix } unix_listener auth-userdb { mode = 0600 user = vmail } } service lmtp { unix_listener /var/spool/postfix/private/dovecot-lmtp { group = postfix mode = 0600 user = postfix } user = vmail } ssl_cert = </etc/pki/dovecot/certs/dovecot.pem ssl_key = </etc/pki/dovecot/private/dovecot.pem userdb { args = username_format=%u /etc/dovecot/users driver = passwd-file } protocol lmtp { postmaster_address = postmaster@example.com } Теперь достаточно добавлять имена и пароли пользователей в файл /etc/dovecot/users, и почтовые ящики будут созданы автоматически, после первой успешной аутентификации пользователя. Или, забегая вперед, после получения пользователем письма. Часть 2: Postfix.\n","metric":0.49194335940000000207,"metric_input":"Текст статьи:\nМинимальный почтовый сервер на основе Postfix и Dovecot. Часть 1: Dovecot / Habr           \n\n22-05-2015\nЦель: получить минимально работающий почтовый сервер, используя только Postfix и Dovecot, с минимальным изменением настроек по умолчанию. Получить каркас системы, на основе которого можно, в дальнейшем, настраивать спам фильтры, работу с базами данных, LDAP… Почтовая система должна: Поддерживать любое количество доменов и пользователей. Обслуживать пользователей, не привязанных к локальным учетным записям. Обеспечивать доступ к почтовым ящикам по протоколам POP3, IMAP с поддержкой TLS. Обеспечивать отправку и прием писем по протоколу SMTP с поддержкой TLS. Настройка проводилась в CentOS 6. Postfix 2.6.6, Dovecot 2.0.9. Никакие особенности ОС не применялись. Структура почтовой системы Условно, почтовые системы можно разделить по методу доступа к почтовым ящикам и списку пользователей. Классическая почтовая система Postfix, Dovecot имеют доступ к списку пользователей и работают параллельно, разделяя доступ к почтовым ящикам пользователей. Упрощенная почтовая система Dovecot является «бэкендом» для доступа к почтовым ящикам пользователей. При этом подходе, расположение почтовых ящиков, список пользователей, известен только Dovecot. Именно этот подход мы будет использовать для достижения нашей цели. Основные плюсы классической системы по сравнению с упрощенной: Производительность. Postfix имеет доступ к почтовым ящикам и может быстрее и проще, нежели во втором случае, доставить почту непосредственно пользователю. Независимость частей системы друг от друга. В случае отключения Dovecot, Postfix продолжит свою основную функцию — принимать письма. Основные недостатки классической системы по сравнению с упрощенной: Формат почтовых ящиков должен быть понятен обеим программам. Необходимо синхронизировать доступ к почтовым ящикам. Безопасность. Необходимы дополнительные полномочия для Postfix. Postfix должен иметь прямой доступ к почтовым ящикам и списку пользователей. Настройка и сопровождение немного сложнее. Dovecot Несмотря на то, что Dovecot является ядром системы, выполняющим много функций. Его настройка не должна вызвать, каких-либо затруднений. Возможно, это связано с тем, что Dovecot имеет дело только с аутентифицированными клиентами, что нельзя сказать про Postfix. Имена будем задавать в формате username@domainname. Создадим пользователя «vmail» для хранения почты, без «шелл» доступа, но с домашней папкой «/home/vmail». Настроим аутентификацию. auth_mechanisms = plain login (login это тот же plain, но для Outlook) mail_gid = vmail mail_uid = vmail Ограничение передачи пароля только после установки TLS соединения, выполняется в Dovecot по умолчанию и не требует дополнительных настроек. Так как мы будем использовать TLS, никаких других механизмов аутентификации нам не нужно, только PLAIN. Самоподписанный сертификат, в CentOS, создается при установке Dovecot в папке, /etc/pki/dovecot/certs/, воспользуемся пока им, для настройки TLS. Хочу обратить ваше внимание на один важный момент. Нужно различать, механизм аутентификации от метода хранения аутентификационных данных. Хотя эти два понятия могут называться одинаково, это две разные вещи. Подробности здесь и здесь. Настроим, где будем хранить почтовые ящики пользователей. Для каждого виртуального пользователя задаем домашнюю папку в формате — /home/vmail/domain/username, mail_home = /home/vmail/%d/%n и местоположение почты в домашней папке — /home/vmail/domain/username/Maildir, mail_location = maildir:~/Maildir Формат почтовых ящиков Maildir выбран для упрощения перехода к классической системе, в случае необходимости, т.к. Postfix поддерживает Maildir. Но если вы не планируете возврат к классической системе, можно выбрать любой формат почтовых ящиков поддерживаемый Dovecot. Следующие две директивы задают, где и как Dovecot будет искать имена и пароли пользователей. userdb { args = username_format=%u /etc/dovecot/users driver = passwd-file } passdb { args = scheme=ssha512 username_format=%u /etc/dovecot/users driver = passwd-file } Мы использовали для этого файл, /etc/dovecot/users, имеющий формат стандартного файла паролей /etc/passwd. Пример файла паролей (пароль обрезан): user1@example1.com:{SSHA512}2YT51xuhilbvb4vYRIb1oj1EvrKFszhf2MNw=:::::: user3@example3.com:{SSHA512}GdBv9GEE1rfFpd4+fzXS+UKh4x6gTpTaH4=:::::: Для безопасности, мы не храним пароли пользователей в открытом виде, а храним их подсоленные SHA512. Для заполнения файла «/etc/dovecot/users», будем использовать этот скрипт с двумя параметрами, именем пользователя и паролем пользователя. #!/bin/sh echo $1:$(doveadm pw -s ssha512 -p $2):::::: >> /etc/dovecot/users Настроим сервисы для связи с Postfix. Для поиска имен пользователей и SASL аутентификации. service auth { unix_listener /var/spool/postfix/private/auth { group = postfix mode = 0660 user = postfix } unix_listener auth-userdb { mode = 0600 user = vmail } } Для доступа к почтовым ящикам пользователей. service lmtp { unix_listener /var/spool/postfix/private/dovecot-lmtp { group = postfix mode = 0600 user = postfix } user = vmail } protocol lmtp { postmaster_address = postmaster@example1.ru } Более подробно можно почитать: здесь про LMTP, здесь про LMTP и Postfix, а здесь про SASL. Итоговый «devconf –N»: # 2.0.9: /etc/dovecot/dovecot.conf # OS: Linux 2.6.32-504.16.2.el6.x86_64 x86_64 CentOS release 6.6 (Final) auth_mechanisms = plain login mail_gid = vmail mail_home = /home/vmail/%d/%n mail_location = maildir:~/Maildir mail_uid = vmail mbox_write_locks = fcntl passdb { args = scheme=ssha512 username_format=%u /etc/dovecot/users driver = passwd-file } service auth { unix_listener /var/spool/postfix/private/auth { group = postfix mode = 0660 user = postfix } unix_listener auth-userdb { mode = 0600 user = vmail } } service lmtp { unix_listener /var/spool/postfix/private/dovecot-lmtp { group = postfix mode = 0600 user = postfix } user = vmail } ssl_cert = </etc/pki/dovecot/certs/dovecot.pem ssl_key = </etc/pki/dovecot/private/dovecot.pem userdb { args = username_format=%u /etc/dovecot/users driver = passwd-file } protocol lmtp { postmaster_address = postmaster@example.com } Теперь достаточно добавлять имена и пароли пользователей в файл /etc/dovecot/users, и почтовые ящики будут созданы автоматически, после первой успешной аутентификации пользователя. Или, забегая вперед, после получения пользователем письма. Часть 2: Postfix.\n\n\nКраткое содержание:\nВ статье описывается настройка почтовой системы на основе Postfix и Dovecot. Описывается структура системы, настройка Dovecot для аутентификации пользователей и хранения почтовых ящиков, а также настройка Postfix для связи с Dovecot и обработки входящих и исходящих сообщений. Статья содержит инструкции по настройке сервисов, директив Dovecot и Postfix, а также SSL-сертификата для безопасности системы.","summary":"В статье описывается настройка почтовой системы на основе Postfix и Dovecot. Описывается структура системы, настройка Dovecot для аутентификации пользователей и хранения почтовых ящиков, а также настройка Postfix для связи с Dovecot и обработки входящих и исходящих сообщений. Статья содержит инструкции по настройке сервисов, директив Dovecot и Postfix, а также SSL-сертификата для безопасности системы."}
{"Text":"Эксперты сообщили о скором начале учений РКН по проверке реагирования на угрозу BGP Hijacking / Хабр                 \n\n8 августа 2022 года сетевые эксперты и представители операторов связи на профильном форуме\nсообщили\nо подготовке к скорому началу сетевых учений Роскомнадзора по проверке реагирования на угрозу\nBGP Hijacking\n.\n\nНаименование угрозы (основной сценарий этапа учений):\n> В рамках сценария учений будет проведена проверка реагирования на угрозу BGP hijacking (перехват маршрутов) – анонсирование автономной системой маршрутов, зарегистрированных за другими операторами связи. Для этого будут реализованы следующие этапы:\n>\n>\n\n 1. Моделируется ситуация с перехватом маршрутов (BGP Hijacking)\n>\n>\n\n 2. Дежурная служба ЦМУ ССОП  (Центра мониторинга и управления сетями связи общего пользования РКН) отправляет операторам связи оперативное указание (далее – ОУ) на установку фильтра на приём некорректного маршрута (на основании информации о префиксе и оригинирующей AS)\n>\n>\n\n 3. Оператор связи на своём пограничном оборудовании устанавливает фильтры на приём BGP анонсов в соответствии с переданной в ОУ информацией и формирует в сторону ЦМУ ССОП отчёт о выполнении ОУ.\n>\n>\n>\n>\n\nОперативное указание будет направлено с почтового адреса noc@cmu.gov.ru\n>\n>\n>\n>\n\nОтработка сценария учений запланирована на август 2022 года. Подробное описание сценария и точная дата его проведения будет сообщена регулятором дополнительно.\nПример тестового задания в рамках учения от РКН, которое ведомство разослало сегодня (по\nASN 8359\nот МТС):\n> Тема поручения: Фильтрация нелегитимного анонса.\n>\n>\n>\n>\n\nСодержание поручения: Уважаемые коллеги!\n>\n>\n>\n>\n\nВ информационной системе ИС ЦМУ ССОП зафиксированы инциденты нарушения маршрутизации, виновником которых является AS8359 (ПАО «МТС»), что нарушает п.2 Приказа Роскомнадзора №224 от 31.07.2019 « Об утверждении правил маршрутизации сообщений электросвязи...». В целях предотвращения подобных инцидентов и в соответствии с ФЗ от 07.07.2003 №126-ФЗ (ред. от 02.07.2021) «О связи», статья 56.2. п.8.\n>\n>\n\nПрошу проверить и включить фильтрацию нелегитимного анонса от автономной системы 8359.\n>\n>\n>\n>\n\nСоздано: 2022-08-08 11:38:37 Москва, стандартное время\n>\n>\n\nСрок исполнения: 2022-08-08 12:38:00 Москва, стандартное время\n>\n>\n\nКритичность: Высокая\nПо поводу времени проведения учений, эксперты предположили, что проверочные мероприятия будут проводится совместно с операторами связи в согласованное с ними время и без влияния на рабочие сети, как вариант, с 9 по 12 августа.\n\nПоследние учения такого уровня\nпроходили\nс 15 июня по 15 июля 2021 года, на них операторы и РКН протестировали отключение Рунета от глобальной сети. В 2020 году такие учения были\nотложены\nиз-за коронавируса.\n\nПакет поправок в законы «О связи» и «Об информации», предполагающий обеспечение целостности, устойчивости и безопасности интернета в России в случае внешних угроз (так называемый «закон о Рунете»), вступил в силу 1 ноября 2019 года. Данные законопроекты предусматривают создание в РФ независимой инфраструктуры, обеспечивающей маршрутизацию в интернете при невозможности подключения к зарубежным корневым серверам. В случае возникновения угроз устойчивой, безопасной и целостной работы Рунета на территории России Роскомнадзор совместно с Минкомсвязи будет осуществлять «централизованное управление сетью связи общего пользования». Также операторы связи обязаны установить технические средства, способные определять источник трафика. В дальнейшем операторы связи РФ будут обязаны пользоваться национальной доменной зоной, но эта норма закона вступит в силу только с 1 января 2021 года. С этой же даты госорганы должны будут использовать исключительно отечественные средства шифрования информации.\n\nПроводить учения в рамках «закона о суверенном Рунете» не реже раза в год\nпостановило правительство\nв конце октября 2019 года.\n\nПервые учения по устойчивости и безопасности интернета в рамках закона о суверенном Рунете\nпрошли\n23 декабря 2019 года и были признаны Минкомсвязи успешными.\n\nВ ходе этих учений было отработано несколько сценариев, включая использование уязвимостей сотовых сетей через протоколы сигнализации SS7 и Diameter. По заявлению организаторов учений, отработка угроз проводилась на выделенных сегментах, стендах и полигонах операторов связи и не затронула обычных пользователей и их сервисы. По словам Замминистра Алексея Соколова, учения показали, что «в целом органы власти и операторы связи» готовы противостоять внешней угрозе. По факту подведения итогов этих учений в издании «Ведомости»\nнаписали\n, что во время учений «большинство атак условного злоумышленника были успешными».\n\nВ учениях принимали участие Минкомсвязи, МВД, Минобороны, Минэнерго, Федеральная служба безопасности, Росгвардия, «Ростелеком», «Вымпелком» (бренд «Билайн»), МТС, «Мегафон», «Лаборатория Касперского», Positive Technologies и Group-IB.\n\nКак поясняли ранее в Минкомсвязи, все этапы учений по проверке целостности, устойчивости функционирования и безопасности российского сегмента интернета не затронут обычных интернет-пользователей и текущие используемые ими сервисы. «Отработка угроз в ходе выполнения тестовых задач будет проводиться только на выделенных сегментах, стендах и полигонах операторов связи, обычные пользователи учения не заметят», — сказали в пресс-службе Минкомсвязи.\n\nТакже в день проведения первых учений 23 декабря 2019 года на Хабре\nпроводили мониторинг\nсетевой ситуации с учётом возможного влияния первых учений по устойчивости и безопасности интернета.","metric":0.63623046880000000414,"metric_input":"Текст статьи:\nЭксперты сообщили о скором начале учений РКН по проверке реагирования на угрозу BGP Hijacking / Хабр                 \n\n8 августа 2022 года сетевые эксперты и представители операторов связи на профильном форуме\nсообщили\nо подготовке к скорому началу сетевых учений Роскомнадзора по проверке реагирования на угрозу\nBGP Hijacking\n.\n\nНаименование угрозы (основной сценарий этапа учений):\n> В рамках сценария учений будет проведена проверка реагирования на угрозу BGP hijacking (перехват маршрутов) – анонсирование автономной системой маршрутов, зарегистрированных за другими операторами связи. Для этого будут реализованы следующие этапы:\n>\n>\n\n 1. Моделируется ситуация с перехватом маршрутов (BGP Hijacking)\n>\n>\n\n 2. Дежурная служба ЦМУ ССОП  (Центра мониторинга и управления сетями связи общего пользования РКН) отправляет операторам связи оперативное указание (далее – ОУ) на установку фильтра на приём некорректного маршрута (на основании информации о префиксе и оригинирующей AS)\n>\n>\n\n 3. Оператор связи на своём пограничном оборудовании устанавливает фильтры на приём BGP анонсов в соответствии с переданной в ОУ информацией и формирует в сторону ЦМУ ССОП отчёт о выполнении ОУ.\n>\n>\n>\n>\n\nОперативное указание будет направлено с почтового адреса noc@cmu.gov.ru\n>\n>\n>\n>\n\nОтработка сценария учений запланирована на август 2022 года. Подробное описание сценария и точная дата его проведения будет сообщена регулятором дополнительно.\nПример тестового задания в рамках учения от РКН, которое ведомство разослало сегодня (по\nASN 8359\nот МТС):\n> Тема поручения: Фильтрация нелегитимного анонса.\n>\n>\n>\n>\n\nСодержание поручения: Уважаемые коллеги!\n>\n>\n>\n>\n\nВ информационной системе ИС ЦМУ ССОП зафиксированы инциденты нарушения маршрутизации, виновником которых является AS8359 (ПАО «МТС»), что нарушает п.2 Приказа Роскомнадзора №224 от 31.07.2019 « Об утверждении правил маршрутизации сообщений электросвязи...». В целях предотвращения подобных инцидентов и в соответствии с ФЗ от 07.07.2003 №126-ФЗ (ред. от 02.07.2021) «О связи», статья 56.2. п.8.\n>\n>\n\nПрошу проверить и включить фильтрацию нелегитимного анонса от автономной системы 8359.\n>\n>\n>\n>\n\nСоздано: 2022-08-08 11:38:37 Москва, стандартное время\n>\n>\n\nСрок исполнения: 2022-08-08 12:38:00 Москва, стандартное время\n>\n>\n\nКритичность: Высокая\nПо поводу времени проведения учений, эксперты предположили, что проверочные мероприятия будут проводится совместно с операторами связи в согласованное с ними время и без влияния на рабочие сети, как вариант, с 9 по 12 августа.\n\nПоследние учения такого уровня\nпроходили\nс 15 июня по 15 июля 2021 года, на них операторы и РКН протестировали отключение Рунета от глобальной сети. В 2020 году такие учения были\nотложены\nиз-за коронавируса.\n\nПакет поправок в законы «О связи» и «Об информации», предполагающий обеспечение целостности, устойчивости и безопасности интернета в России в случае внешних угроз (так называемый «закон о Рунете»), вступил в силу 1 ноября 2019 года. Данные законопроекты предусматривают создание в РФ независимой инфраструктуры, обеспечивающей маршрутизацию в интернете при невозможности подключения к зарубежным корневым серверам. В случае возникновения угроз устойчивой, безопасной и целостной работы Рунета на территории России Роскомнадзор совместно с Минкомсвязи будет осуществлять «централизованное управление сетью связи общего пользования». Также операторы связи обязаны установить технические средства, способные определять источник трафика. В дальнейшем операторы связи РФ будут обязаны пользоваться национальной доменной зоной, но эта норма закона вступит в силу только с 1 января 2021 года. С этой же даты госорганы должны будут использовать исключительно отечественные средства шифрования информации.\n\nПроводить учения в рамках «закона о суверенном Рунете» не реже раза в год\nпостановило правительство\nв конце октября 2019 года.\n\nПервые учения по устойчивости и безопасности интернета в рамках закона о суверенном Рунете\nпрошли\n23 декабря 2019 года и были признаны Минкомсвязи успешными.\n\nВ ходе этих учений было отработано несколько сценариев, включая использование уязвимостей сотовых сетей через протоколы сигнализации SS7 и Diameter. По заявлению организаторов учений, отработка угроз проводилась на выделенных сегментах, стендах и полигонах операторов связи и не затронула обычных пользователей и их сервисы. По словам Замминистра Алексея Соколова, учения показали, что «в целом органы власти и операторы связи» готовы противостоять внешней угрозе. По факту подведения итогов этих учений в издании «Ведомости»\nнаписали\n, что во время учений «большинство атак условного злоумышленника были успешными».\n\nВ учениях принимали участие Минкомсвязи, МВД, Минобороны, Минэнерго, Федеральная служба безопасности, Росгвардия, «Ростелеком», «Вымпелком» (бренд «Билайн»), МТС, «Мегафон», «Лаборатория Касперского», Positive Technologies и Group-IB.\n\nКак поясняли ранее в Минкомсвязи, все этапы учений по проверке целостности, устойчивости функционирования и безопасности российского сегмента интернета не затронут обычных интернет-пользователей и текущие используемые ими сервисы. «Отработка угроз в ходе выполнения тестовых задач будет проводиться только на выделенных сегментах, стендах и полигонах операторов связи, обычные пользователи учения не заметят», — сказали в пресс-службе Минкомсвязи.\n\nТакже в день проведения первых учений 23 декабря 2019 года на Хабре\nпроводили мониторинг\nсетевой ситуации с учётом возможного влияния первых учений по устойчивости и безопасности интернета.\n\nКраткое содержание:\nЭксперты сообщили о скором начале учений Роскомнадзора по проверке реагирования на угрозу BGP Hijacking. В рамках сценария учений будет проведена проверка на анонсирование маршрутов, зарегистрированных за другими операторами связи. Операторы связи должны будут установить фильтры на приём некорректного маршрута и сформировать отчёт о выполнении указания. Учения запланированы на август 2022 года.","summary":"Эксперты сообщили о скором начале учений Роскомнадзора по проверке реагирования на угрозу BGP Hijacking. В рамках сценария учений будет проведена проверка на анонсирование маршрутов, зарегистрированных за другими операторами связи. Операторы связи должны будут установить фильтры на приём некорректного маршрута и сформировать отчёт о выполнении указания. Учения запланированы на август 2022 года."}
{"Text":"Телефон с харизмой: обзор Sony Xperia P / Habr                  \n\n08-06-2012\nSony Xperia P уже довольно хорошо известен Хабру и, кроме того, уже вышел в продажу . Настало время познакомиться с аппаратом поближе. Внешний вид Это первая Xperia с цельноалюминиевым корпусом. Настоящая железяка. Выглядит она на мой скромный вкус совершенно офигенно – самый красивый аппарат в классе. Строгим формам корпуса очень идёт металл. За большим куском устойчивого к царапинам стекла (по традиции также заклеенном на заводе защитной плёнкой) скрыты четырёхдюймовый дисплей и фронтальная камера. Для динамика в стекле сделан вырез. Задняя крышка радует симметрией форм – выспышка, глазок камеры и фирменный знак Sony Mobile, знакомый ещё как Sony Ericsson, расположены строго по вертикальной оси. На верхнем торце – только 3,5-мм разъём для наушников, снизу – только микрофон. Все кнопки – питания, камеры и регулировки громкости/зума фотокамеры расположились справа. Там же расположился зарешёченный внешний динамик – металлическая сеточка, которой он закрыт, деталь мелкая, но очень стильная. На левом «боку» — разъёмы HDMI, MicroUSB и microSIM. «Техногенность» образа Xperia P подчёркивают два маленьких болтика, намеренно выведенных наружу. Это как заклёпки на корпусе самолёта – никаких декораций, всё серьёзно. Ну и конечно на месте ключевая деталь внешности смартфонов серии NXT – прозрачная пластиковая полоса, отделяющая основной корпус от «подбородка». В темноте при нажатии она весело подсвечивается голубым. Эргономика Эргономически Xperia P, наверное, самая удачная модель Xperia, которую я держал в руках в последнее время. Судите сами: Аудиоразъём – как и положено, сверху, Кнопка питания – расположение сбоку немного непривычно, но зато она отлично ложится под большой палец и «кликнуть» по ней уж точно проще, чем, для сравнения, по маленькому кругляшу на верхней кромке Xperia arc. Разъёмы – отдельный повод для радости. Дело в том, что я крайне не люблю всевозможные заглушки, которые постоянно приходится отстёгивать/ставить на место. Основные разъёмы, на мой взгляд, должны быть доступны всегда – как здесь и реализовано. Разъём для microSIM, наоборот, закрыт крышечкой – что тоже логично и безопасно. Основные клавиши – в отличие от Xperia S, зону нажатия внесли на саму прозрачную полосу под экраном (на «эске» пиктограммы расположены ПОД самими клавишами), что выглядит логичнее и кроме того их проще теперь нажимать на ощупь. При нажатии, естественно телефон даёт обратную связь в виде небольшой вибродрожи. В целом аппарат в руке лежит удобно, чему способствует в том числе выгнутая задняя крышка. Вес Xperia P – 126 граммов, габариты – 122 мм в высоту, 59,5 мм в ширину и 10,5 мм в глубину. Единственный эксплуатационный недостаток, пожалуй всё тот же цельноалюминиевый корпус – теперь нельзя сменить, например, батарейку, да и метод жёсткого резета в случае чего – вынуть батарейку – не получится применить. Комплектация Sony Xperia P поставляется уже в знакомом по другим смартфонам Sony плоском чёрно-белом картонном пенале, где довольно компактно упаковано следующее содержимое: сам смартфон: блок питания повышенной мощности (полная зарядка – всего за полчаса): стереогарнитура; шнурок переходника на microUSB; HDMI кабель; и 2 NFC метки (на фото без них, т.к. это тестовый экземпляр). Экран Дисплей Sony Xperia P заслуживает того, чтобы на нём остановиться отдельно. У него даже есть своё название – WhiteMagic. Его уникальность заключается в добавленном к традиционным трём сабпикселям (красному, зелёному и синему) четвёртого — белого. Обычно белый цвет на экране получается за счёт смешения трёх основных цветов, и, в зависимости от настроек, цвет может «фонить» другим оттенком. В результате в Xperia P белый цвет достигается не смешением этих трёх сабпикселей, а только работой белого, что обеспечивает практически идеальную цветопередачу и при этом сильно экономит расход аккумулятора. Кроме того, благодаря новому сабпикселю, дисплей Xperia P очень яркий — максимальная яркость 935 свечей против обычных для смартфонов 500 свечей, что обеспечивает комфортную работу на солнце. В настройках режима работы экрана можно настроить автоматический выбор яркости в зависимости от окружающих условий – на солнце экран работает ярче, а в темноте немного тускнеет: Камера Камера Xperia P – это 8 мегапикселей, 16-кратный цифровой зум и, главное, технология быстрого старта fast capture. Кстати, в режиме видеосъёмки тоже работает зум – уже 8-кратный. В зависимости от настроек, камера может включаться за 1,5 секунды и сразу делать кадр либо просто приходить в рабочее состояние. Я предпочитаю использовать второй вариант, поскольку это позволяет лучше «прицелиться» для съёмки. О том, как снимает камера Xperia P, вы можете судить по фотографиям в альбоме на Яндексе: Посмотреть на Яндекс.Фотках И конечно на Xperia P есть фронтальная камера – чтобы постить луки было ещё проще ну или по скайпу видеоконференции устраивать. Софт Мой экземпляр Xperia P работает на Andorid 2.3.7. Обновление до ICS скоро ожидается. Пока же ничего нового добавить не могу – интерфейс Xperia P полностью идентичен недавно обозревавшемуся Sony Xperia S – разве что с поправкой на меньшее разрешение экрана – 960х540 против 1280х720 у флагмана соответственно. И это хорошо! Звонилка, управление музыкой, меню сообщений и поиск контактов – всё на высоте. NFC Ну и не могу не упомянуть про встроенный NFC-модуль, конечно. Благо, NFC сейчас набирает обороты – МТС предлагает NFC-платежи, Билайн – использовать NFC для проезда на метро, Яндекс.Карты позволяют посмотреть остаток поездок на вашем проездном. Конечно, NFC-чип можно докупить отдельно – но у встроенных решений из коробки – своя прелесть. Начинка В соседнем посте коллеги с канала GTV разобрали Xperia P. Видео про то, как устроен аппарат изнутри: Характеристики Процессор: двухъядерный 1 ГГц NovaThor U8500 Габариты: 122 x 59.5 x 10.5 мм Вес: 126 grams Цвета корпуса: серебристый, чёрный, красный Дисплей: 4”, 960x540, устойчивый к царапинам Встроенная память: 16 ГБ, 13 ГБ доступно пользователю Оперативная память: 1 ГБ Камера: 8 МП с функцией быстрого запуска, возможность съёмки Full HD видео + фронтальная камера Батарея: 1305 мАч Время работы в режиме разговора: 6 часов Время работы в режиме ожидания: 470 часов Резюме В линейке 2012 года Sony Xperia P пока выглядит как золотая середина – немного меньше по размерам и диагонали экрана, чем флагман – Xperia S, Xperia P радует более выверенной эргономикой и классным дизайном (всё-таки пластик с алюминием конкурировать не может) при тех же возможностях. Меньше мегапикселей в камере и гигагерц в процессоре – вполне приемлемая жертва, компенсируемая более скромным ценником. В продаже Sony Xperia P с начала мая, его цена в «Связном» — 19 990 р. Разница в пикселях и дюймах экрана при повседневной эксплуатации тоже незаметна. А вот броский и стильный облик – видно всегда и сразу. У него есть одна незадокументированная опция – харизма. Xperia P не потеряется на фоне смартфонов-конкурентов в любой компании – ни внешне, ни по своим возможностям. И это очень сильный аргумент.\n","metric":0.40112304690000000207,"metric_input":"Текст статьи:\nТелефон с харизмой: обзор Sony Xperia P / Habr                  \n\n08-06-2012\nSony Xperia P уже довольно хорошо известен Хабру и, кроме того, уже вышел в продажу . Настало время познакомиться с аппаратом поближе. Внешний вид Это первая Xperia с цельноалюминиевым корпусом. Настоящая железяка. Выглядит она на мой скромный вкус совершенно офигенно – самый красивый аппарат в классе. Строгим формам корпуса очень идёт металл. За большим куском устойчивого к царапинам стекла (по традиции также заклеенном на заводе защитной плёнкой) скрыты четырёхдюймовый дисплей и фронтальная камера. Для динамика в стекле сделан вырез. Задняя крышка радует симметрией форм – выспышка, глазок камеры и фирменный знак Sony Mobile, знакомый ещё как Sony Ericsson, расположены строго по вертикальной оси. На верхнем торце – только 3,5-мм разъём для наушников, снизу – только микрофон. Все кнопки – питания, камеры и регулировки громкости/зума фотокамеры расположились справа. Там же расположился зарешёченный внешний динамик – металлическая сеточка, которой он закрыт, деталь мелкая, но очень стильная. На левом «боку» — разъёмы HDMI, MicroUSB и microSIM. «Техногенность» образа Xperia P подчёркивают два маленьких болтика, намеренно выведенных наружу. Это как заклёпки на корпусе самолёта – никаких декораций, всё серьёзно. Ну и конечно на месте ключевая деталь внешности смартфонов серии NXT – прозрачная пластиковая полоса, отделяющая основной корпус от «подбородка». В темноте при нажатии она весело подсвечивается голубым. Эргономика Эргономически Xperia P, наверное, самая удачная модель Xperia, которую я держал в руках в последнее время. Судите сами: Аудиоразъём – как и положено, сверху, Кнопка питания – расположение сбоку немного непривычно, но зато она отлично ложится под большой палец и «кликнуть» по ней уж точно проще, чем, для сравнения, по маленькому кругляшу на верхней кромке Xperia arc. Разъёмы – отдельный повод для радости. Дело в том, что я крайне не люблю всевозможные заглушки, которые постоянно приходится отстёгивать/ставить на место. Основные разъёмы, на мой взгляд, должны быть доступны всегда – как здесь и реализовано. Разъём для microSIM, наоборот, закрыт крышечкой – что тоже логично и безопасно. Основные клавиши – в отличие от Xperia S, зону нажатия внесли на саму прозрачную полосу под экраном (на «эске» пиктограммы расположены ПОД самими клавишами), что выглядит логичнее и кроме того их проще теперь нажимать на ощупь. При нажатии, естественно телефон даёт обратную связь в виде небольшой вибродрожи. В целом аппарат в руке лежит удобно, чему способствует в том числе выгнутая задняя крышка. Вес Xperia P – 126 граммов, габариты – 122 мм в высоту, 59,5 мм в ширину и 10,5 мм в глубину. Единственный эксплуатационный недостаток, пожалуй всё тот же цельноалюминиевый корпус – теперь нельзя сменить, например, батарейку, да и метод жёсткого резета в случае чего – вынуть батарейку – не получится применить. Комплектация Sony Xperia P поставляется уже в знакомом по другим смартфонам Sony плоском чёрно-белом картонном пенале, где довольно компактно упаковано следующее содержимое: сам смартфон: блок питания повышенной мощности (полная зарядка – всего за полчаса): стереогарнитура; шнурок переходника на microUSB; HDMI кабель; и 2 NFC метки (на фото без них, т.к. это тестовый экземпляр). Экран Дисплей Sony Xperia P заслуживает того, чтобы на нём остановиться отдельно. У него даже есть своё название – WhiteMagic. Его уникальность заключается в добавленном к традиционным трём сабпикселям (красному, зелёному и синему) четвёртого — белого. Обычно белый цвет на экране получается за счёт смешения трёх основных цветов, и, в зависимости от настроек, цвет может «фонить» другим оттенком. В результате в Xperia P белый цвет достигается не смешением этих трёх сабпикселей, а только работой белого, что обеспечивает практически идеальную цветопередачу и при этом сильно экономит расход аккумулятора. Кроме того, благодаря новому сабпикселю, дисплей Xperia P очень яркий — максимальная яркость 935 свечей против обычных для смартфонов 500 свечей, что обеспечивает комфортную работу на солнце. В настройках режима работы экрана можно настроить автоматический выбор яркости в зависимости от окружающих условий – на солнце экран работает ярче, а в темноте немного тускнеет: Камера Камера Xperia P – это 8 мегапикселей, 16-кратный цифровой зум и, главное, технология быстрого старта fast capture. Кстати, в режиме видеосъёмки тоже работает зум – уже 8-кратный. В зависимости от настроек, камера может включаться за 1,5 секунды и сразу делать кадр либо просто приходить в рабочее состояние. Я предпочитаю использовать второй вариант, поскольку это позволяет лучше «прицелиться» для съёмки. О том, как снимает камера Xperia P, вы можете судить по фотографиям в альбоме на Яндексе: Посмотреть на Яндекс.Фотках И конечно на Xperia P есть фронтальная камера – чтобы постить луки было ещё проще ну или по скайпу видеоконференции устраивать. Софт Мой экземпляр Xperia P работает на Andorid 2.3.7. Обновление до ICS скоро ожидается. Пока же ничего нового добавить не могу – интерфейс Xperia P полностью идентичен недавно обозревавшемуся Sony Xperia S – разве что с поправкой на меньшее разрешение экрана – 960х540 против 1280х720 у флагмана соответственно. И это хорошо! Звонилка, управление музыкой, меню сообщений и поиск контактов – всё на высоте. NFC Ну и не могу не упомянуть про встроенный NFC-модуль, конечно. Благо, NFC сейчас набирает обороты – МТС предлагает NFC-платежи, Билайн – использовать NFC для проезда на метро, Яндекс.Карты позволяют посмотреть остаток поездок на вашем проездном. Конечно, NFC-чип можно докупить отдельно – но у встроенных решений из коробки – своя прелесть. Начинка В соседнем посте коллеги с канала GTV разобрали Xperia P. Видео про то, как устроен аппарат изнутри: Характеристики Процессор: двухъядерный 1 ГГц NovaThor U8500 Габариты: 122 x 59.5 x 10.5 мм Вес: 126 grams Цвета корпуса: серебристый, чёрный, красный Дисплей: 4”, 960x540, устойчивый к царапинам Встроенная память: 16 ГБ, 13 ГБ доступно пользователю Оперативная память: 1 ГБ Камера: 8 МП с функцией быстрого запуска, возможность съёмки Full HD видео + фронтальная камера Батарея: 1305 мАч Время работы в режиме разговора: 6 часов Время работы в режиме ожидания: 470 часов Резюме В линейке 2012 года Sony Xperia P пока выглядит как золотая середина – немного меньше по размерам и диагонали экрана, чем флагман – Xperia S, Xperia P радует более выверенной эргономикой и классным дизайном (всё-таки пластик с алюминием конкурировать не может) при тех же возможностях. Меньше мегапикселей в камере и гигагерц в процессоре – вполне приемлемая жертва, компенсируемая более скромным ценником. В продаже Sony Xperia P с начала мая, его цена в «Связном» — 19 990 р. Разница в пикселях и дюймах экрана при повседневной эксплуатации тоже незаметна. А вот броский и стильный облик – видно всегда и сразу. У него есть одна незадокументированная опция – харизма. Xperia P не потеряется на фоне смартфонов-конкурентов в любой компании – ни внешне, ни по своим возможностям. И это очень сильный аргумент.\n\n\nКраткое содержание:\nSony Xperia P - это смартфон с цельноалюминиевым корпусом, 4-дюймовым дисплеем, 8-мегапиксельной камерой и фронтальной камерой. Он имеет эргономичный дизайн с расположением кнопок и разъёмов, а также аудиоразъём, кнопку питания и динамики сверху. Xperia P работает на Android 2.3.7 и имеет встроенный NFC-модуль. Его экран имеет уникальный дисплей WhiteMagic, который обеспечивает идеальную цветопередачу и высокую яркость. Камера Xperia P имеет функцию быстрого старта и 16-кратный цифровой зум.","summary":"Sony Xperia P - это смартфон с цельноалюминиевым корпусом, 4-дюймовым дисплеем, 8-мегапиксельной камерой и фронтальной камерой. Он имеет эргономичный дизайн с расположением кнопок и разъёмов, а также аудиоразъём, кнопку питания и динамики сверху. Xperia P работает на Android 2.3.7 и имеет встроенный NFC-модуль. Его экран имеет уникальный дисплей WhiteMagic, который обеспечивает идеальную цветопередачу и высокую яркость. Камера Xperia P имеет функцию быстрого старта и 16-кратный цифровой зум."}
{"Text":"Беспилотный трактор испытали на полях России / Хабр               \n\n10-06-2016\nРоссийская компания Cognitive Technologies провела в Татарстане первые испытания беспилотных тракторов с системой компьютерного зрения собственной разработки. Программно-аппаратный комплекс планируют в будущем устанавливать не только на тракторы, но и на другую сельскохозяйственную технику — комбайны, сеялки и так далее, пишут «Известия». Благодаря беспилотному транспорту и системам компьютерного зрения фермерские хозяйства России смогут значительно повысить свою рентабельность, ведь сейчас дорогостоящая техника используется неэффективно. Например, комбайн работает в среднем шесть часов в день, а в остальное время простаивает. Беспилотные тракторы и комбайны будут трудиться в полях и днём и ночью, 24 часа в сутки. Им не помешают условия плохой видимости, не смутит дождь или туман, и даже в кромешной тьме беспилотный трактор способен так же эффективно косить траву или собирать картошку, как днём. Беспилотную сельхозтехнику в России будет продвигать новый агрохолдинг, который создаёт компания Cognitive Technologies с производителем агротехники «Ростсельмаш» и агрохолдингом «Союз-Агро» на базе особой экономической зоны «Иннополис». В полной темноте для беспилотных систем можно использовать камеры ночного видения, но более эффективным является использование лидаров в совокупности с заранее размеченной картой территории, которая занесена в навигационный компьютер трактора. Американские коллеги недавно проводили испытания беспилотной автомобильной системы транспортного средства в полной темноте — и определили, что лидар с навигатором управляют машиной в темноте гораздо безопаснее, чем водитель-человек. Во время испытаний беспилотного «Форда» на полигоне в Аризоне была составлена 3D-карта местности, а в темноте лидар с помощью лазерных импульсов (2,8 млн импульсов в секунду) прокладывал путь, как бы прощупывая дорогу с ориентировкой на 3D-карту. Российский трактор пока не планируют оснащать лидаром, потому что это существенно увеличит его стоимость. В данный момент, по оценкам разработчиков, стоимость программно-аппаратного комплекса беспилотного зрения составляет не более 15% от стоимости трактора. Для российских колхозов и фермерских хозяйств это вполне приемлемая наценка. Если же поставить на трактор полноценный лидар, то стоимость техники существенно возрастёт. На российские тракторы будут устанавливать программно-аппаратные комплексы компьютерного зрения, включающие в себя стереопару — систему из двух видеокамер, которые снимают видео с разрешением Full HD, то есть 1920x1080 пикселов. Кроме видеокамер, в комплектацию российского беспилотного трактора входят навигационный и инерционный датчики ГЛОНАСС и GPS и вычислительный блок (компьютер). «Система компьютерного зрения позволяет с высокой точностью детектировать опасные объекты, определять их размеры и координаты для составления высокоточных карт, — говорит Ольга Ускова, президент компании Cognitive Technologies. — Благодаря точному знанию положения предметов на поле многие из них становится возможным удалить еще до уборочной, когда они могут представлять реальную угрозу механическим элементам сельхозтехники». Составление цифровой карты поля и нанесение на неё окружающих объектов (столбы, камни и другие артефакты) производится во время выполнения предпосевных операций, таких как внесение удобрений и боронование. Операция проходит весной, когда посевные площади не засеяны, так что все артефакты хорошо засекаются системой компьютерного зрения. По этим цифровым картам трактор будет ориентироваться летом и осенью, во время сбора урожая. Он будет стараться объезжать объекты, которые не удалось убрать с поля весной. Разработчики уверены, что сенсоры транспортного средства и система компьютерного зрения смогут детектировать даже такие препятствия, которых нет на карте. Заявлено, что система распознаёт объекты размером от 10-15 см на расстоянии до 15-20 м. Всё остальное не представляет опасности, если на тракторе установлен камнеуловитель. Испытания трактора с системой компьютерного зрения Cognitive Technologies Испытания беспилотного трактора продолжатся примерно полтора года. По словам Ольги Усковой, этого времени достаточно, чтобы довести систему компьютерного зрения до стадии промышленного использования и ввести её в коммерческую эксплуатацию. Программно-аппаратные комплексы будут продавать фермерским хозяйствам России, а также стран ближнего и дальнего зарубежья. Экспериментальные разработки в этой области идут не только в России. Например, недавно министр сельского хозяйства Японии поднял тему перехода на роботизированную технику в сельском хозяйстве в связи со старением населения, в том числе фермеров. Средний возраст японских фермеров составляет 67 лет, две трети из них старше 65 лет. В России проблемы старения пока нет из-за низкой продолжительности жизни, но со временем она может появиться. Беспилотные тракторы готова выпускать японская корпорация Kubota. Прототип автономного трактора Kubota с лидаром Маленькие беспилотные тракторы делают и в Беларуси. Наверняка российскую систему компьютерного зрения тоже можно будет установить на популярные белорусские тракторы производства Минского тракторного завода. В воображении российских разработчиков, в будущем сельское хозяйство в России выглядит следующим образом: «Работу всей умной сельскохозяйственной техники будет контролировать минимальное количество человек из диспетчерского центра, откуда будет осуществляться контроль за работой всех машин. При необходимости он сможет задать программу и время работы тракторов, и находиться постоянно за пультом не придётся», — говорит Ольга Усова. В общем, примерно так, как прогнозировал писатель Николай Носов в научно-фантастическом произведении «Незнайка в Солнечном городе». Независимые эксперты считают, что полностью автоматизированного будущего в сельском хозяйстве пока не получится. За рулём транспортного средства обязательно должен находиться человек, контролирующий работу автоматики и готовый взять на себя управление в случае возникновения чрезвычайной ситуации. Кстати, о своих разработках в области машинного зрения компания Cognitive Technologies более подробно рассказывала в корпоративном блоге на Хабрахабре. В рамках совместного проекта на кафедре «Когнитивных технологий» МФТИ в 2014 году был разработан робот-автомобиль, который «в режиме реального времени обрабатывает видеопоток, распознаёт окружающую сцену, детектирует объекты и формирует управляющее воздействие, направленное на решение поставленной задачи». Роботу поставили задачу гоняться за красным мячом и толкать его бампером. Ролик снят во время доклада на осенней конференции молодых учёных МФТИ, прямо в коридоре главного корпуса.\n","metric":0.64794921880000000414,"metric_input":"Текст статьи:\nБеспилотный трактор испытали на полях России / Хабр               \n\n10-06-2016\nРоссийская компания Cognitive Technologies провела в Татарстане первые испытания беспилотных тракторов с системой компьютерного зрения собственной разработки. Программно-аппаратный комплекс планируют в будущем устанавливать не только на тракторы, но и на другую сельскохозяйственную технику — комбайны, сеялки и так далее, пишут «Известия». Благодаря беспилотному транспорту и системам компьютерного зрения фермерские хозяйства России смогут значительно повысить свою рентабельность, ведь сейчас дорогостоящая техника используется неэффективно. Например, комбайн работает в среднем шесть часов в день, а в остальное время простаивает. Беспилотные тракторы и комбайны будут трудиться в полях и днём и ночью, 24 часа в сутки. Им не помешают условия плохой видимости, не смутит дождь или туман, и даже в кромешной тьме беспилотный трактор способен так же эффективно косить траву или собирать картошку, как днём. Беспилотную сельхозтехнику в России будет продвигать новый агрохолдинг, который создаёт компания Cognitive Technologies с производителем агротехники «Ростсельмаш» и агрохолдингом «Союз-Агро» на базе особой экономической зоны «Иннополис». В полной темноте для беспилотных систем можно использовать камеры ночного видения, но более эффективным является использование лидаров в совокупности с заранее размеченной картой территории, которая занесена в навигационный компьютер трактора. Американские коллеги недавно проводили испытания беспилотной автомобильной системы транспортного средства в полной темноте — и определили, что лидар с навигатором управляют машиной в темноте гораздо безопаснее, чем водитель-человек. Во время испытаний беспилотного «Форда» на полигоне в Аризоне была составлена 3D-карта местности, а в темноте лидар с помощью лазерных импульсов (2,8 млн импульсов в секунду) прокладывал путь, как бы прощупывая дорогу с ориентировкой на 3D-карту. Российский трактор пока не планируют оснащать лидаром, потому что это существенно увеличит его стоимость. В данный момент, по оценкам разработчиков, стоимость программно-аппаратного комплекса беспилотного зрения составляет не более 15% от стоимости трактора. Для российских колхозов и фермерских хозяйств это вполне приемлемая наценка. Если же поставить на трактор полноценный лидар, то стоимость техники существенно возрастёт. На российские тракторы будут устанавливать программно-аппаратные комплексы компьютерного зрения, включающие в себя стереопару — систему из двух видеокамер, которые снимают видео с разрешением Full HD, то есть 1920x1080 пикселов. Кроме видеокамер, в комплектацию российского беспилотного трактора входят навигационный и инерционный датчики ГЛОНАСС и GPS и вычислительный блок (компьютер). «Система компьютерного зрения позволяет с высокой точностью детектировать опасные объекты, определять их размеры и координаты для составления высокоточных карт, — говорит Ольга Ускова, президент компании Cognitive Technologies. — Благодаря точному знанию положения предметов на поле многие из них становится возможным удалить еще до уборочной, когда они могут представлять реальную угрозу механическим элементам сельхозтехники». Составление цифровой карты поля и нанесение на неё окружающих объектов (столбы, камни и другие артефакты) производится во время выполнения предпосевных операций, таких как внесение удобрений и боронование. Операция проходит весной, когда посевные площади не засеяны, так что все артефакты хорошо засекаются системой компьютерного зрения. По этим цифровым картам трактор будет ориентироваться летом и осенью, во время сбора урожая. Он будет стараться объезжать объекты, которые не удалось убрать с поля весной. Разработчики уверены, что сенсоры транспортного средства и система компьютерного зрения смогут детектировать даже такие препятствия, которых нет на карте. Заявлено, что система распознаёт объекты размером от 10-15 см на расстоянии до 15-20 м. Всё остальное не представляет опасности, если на тракторе установлен камнеуловитель. Испытания трактора с системой компьютерного зрения Cognitive Technologies Испытания беспилотного трактора продолжатся примерно полтора года. По словам Ольги Усковой, этого времени достаточно, чтобы довести систему компьютерного зрения до стадии промышленного использования и ввести её в коммерческую эксплуатацию. Программно-аппаратные комплексы будут продавать фермерским хозяйствам России, а также стран ближнего и дальнего зарубежья. Экспериментальные разработки в этой области идут не только в России. Например, недавно министр сельского хозяйства Японии поднял тему перехода на роботизированную технику в сельском хозяйстве в связи со старением населения, в том числе фермеров. Средний возраст японских фермеров составляет 67 лет, две трети из них старше 65 лет. В России проблемы старения пока нет из-за низкой продолжительности жизни, но со временем она может появиться. Беспилотные тракторы готова выпускать японская корпорация Kubota. Прототип автономного трактора Kubota с лидаром Маленькие беспилотные тракторы делают и в Беларуси. Наверняка российскую систему компьютерного зрения тоже можно будет установить на популярные белорусские тракторы производства Минского тракторного завода. В воображении российских разработчиков, в будущем сельское хозяйство в России выглядит следующим образом: «Работу всей умной сельскохозяйственной техники будет контролировать минимальное количество человек из диспетчерского центра, откуда будет осуществляться контроль за работой всех машин. При необходимости он сможет задать программу и время работы тракторов, и находиться постоянно за пультом не придётся», — говорит Ольга Усова. В общем, примерно так, как прогнозировал писатель Николай Носов в научно-фантастическом произведении «Незнайка в Солнечном городе». Независимые эксперты считают, что полностью автоматизированного будущего в сельском хозяйстве пока не получится. За рулём транспортного средства обязательно должен находиться человек, контролирующий работу автоматики и готовый взять на себя управление в случае возникновения чрезвычайной ситуации. Кстати, о своих разработках в области машинного зрения компания Cognitive Technologies более подробно рассказывала в корпоративном блоге на Хабрахабре. В рамках совместного проекта на кафедре «Когнитивных технологий» МФТИ в 2014 году был разработан робот-автомобиль, который «в режиме реального времени обрабатывает видеопоток, распознаёт окружающую сцену, детектирует объекты и формирует управляющее воздействие, направленное на решение поставленной задачи». Роботу поставили задачу гоняться за красным мячом и толкать его бампером. Ролик снят во время доклада на осенней конференции молодых учёных МФТИ, прямо в коридоре главного корпуса.\n\n\nКраткое содержание:\nРоссийская компания Cognitive Technologies провела испытания беспилотных тракторов с системой компьютерного зрения собственной разработки в Татарстане. Программно-аппаратный комплекс планируют устанавливать не только на тракторы, но и на другую сельскохозяйственную технику. Благодаря беспилотному транспорту фермерские хозяйства смогут повысить свою рентабельность. Беспилотные тракторы будут работать в полях днём и ночью, в условиях плохой видимости и даже в кромешной тьме. Для работы в темноте используют камеры ночного видения или лидары в сочетании с заранее размеченной картой территории. Российские разработчики уверены, что сенсоры транспортного средства и система компьютерного зрения смогут детектировать даже такие препятствия, которых нет на карте. Программно-аппаратные комплексы будут продавать фермерским хозяйствам России и других стран. Экспериментальные разработки в этой области идут не только в России, но и в Японии, где уже готовы выпускать автономные тракторы. В будущем сельское хозяйство в России будет контролироваться минимальным количеством человек из диспетчерского центра. Полностью автоматизированного будущего в сельском хозяйстве пока не получится, так как за рулём транспортного средства обязательно должен находиться человек, контролирующий работу автоматики и готовый взять на себя управление в случае возникновения чрезвычайной ситуации.","summary":"Российская компания Cognitive Technologies провела испытания беспилотных тракторов с системой компьютерного зрения собственной разработки в Татарстане. Программно-аппаратный комплекс планируют устанавливать не только на тракторы, но и на другую сельскохозяйственную технику. Благодаря беспилотному транспорту фермерские хозяйства смогут повысить свою рентабельность. Беспилотные тракторы будут работать в полях днём и ночью, в условиях плохой видимости и даже в кромешной тьме. Для работы в темноте используют камеры ночного видения или лидары в сочетании с заранее размеченной картой территории. Российские разработчики уверены, что сенсоры транспортного средства и система компьютерного зрения смогут детектировать даже такие препятствия, которых нет на карте. Программно-аппаратные комплексы будут продавать фермерским хозяйствам России и других стран. Экспериментальные разработки в этой области идут не только в России, но и в Японии, где уже готовы выпускать автономные тракторы. В будущем сельское хозяйство в России будет контролироваться минимальным количеством человек из диспетчерского центра. Полностью автоматизированного будущего в сельском хозяйстве пока не получится, так как за рулём транспортного средства обязательно должен находиться человек, контролирующий работу автоматики и готовый взять на себя управление в случае возникновения чрезвычайной ситуации."}
{"Text":"deb-пакет на коленке / Хабр                 \n\n16-10-2009\nЗадача: создать пакет для Debian для выливки демона/сайта на сервер. Я никогда ранее не работал с deb-пакетами — так что решение может быть не оптимальным. Но оно работает и достаточно просто в использовании. Подготовка В первую очередь для переодических выливок на сервер нам понадобится версионность кода. Если оспользуется svn — то, скорее всего, можно использовать номер ревизии. Так как я использую git — написал простенький скрипт для ведения версии проекта: tag.sh Для увеличения версии проекта вызываем ./tag.sh build, ./tag minor или ./tag major. Скрипт создаёт метки вида v<MAJOR>.<MINOR>.<BUILD> (к примеру v0.1.25) Для получения версии вызываем git describe --match=v* HEAD и получаем в результате версию в виде v0.2.1 (либо v0.2.1-59-g919ab19 если после тага были коммиты). В сети много мануалов по созданию пакетов, которые используют dh_make либо описывают все возможные варианты создания пакета с нуля. Но нам не требуется правильный пакет для внесения в репозиторий дебиана, а всего лишь пакет нашего проекта, который мы установим на сервере. Из всех файлов конфигурации пакета важными показались лишь 2: control — описание пакета conffiles — описание конфигурационных файлов пакета (при установке новой версии apt спросит заменить ли файл или оставить старую версию). control Формат файла достаточно простой: Package: ИМЯ_ПАКЕТА Version: ВЕРСИЯ - получаная git describe Architecture: АРХИТЕКТУРА - amd64 если есть бинарники или all - если только скрипты Depends: ЗАВИСИМОСТИ - можно упустить Maintainer: ВАШЕ_ИМЯ Description: ОПИСАНИЕ К примеру: Package: ggseductionserver Version: v0.2.1-59-g919ab19 Architecture: amd64 Depends: libc6 (>= 2.4), libgcc1 (>= 1:4.1.1), libstdc++6 (>= 4.1.1), libboost-system1.38.0 (>=1.38.0), libboost-thread1.38.0 (>=1.38.0), libboost-regex1.38.0 (>=1.38.0), libboost-serialization1.38.0 (>=1.38.0), libboost-program-options1.38.0 (>=1.38.0), tokyotyrant, tokyocabinet, mysql-client (>=5.0), libssl0.9.8 (>=0.9.8), libreadline5 (>= 5.2), libmysql++3 (>= 3.0), libpcre3 (>= 7.6), libpcrecpp0 (>= 7.6), libgd2-xpm (>= 2.0) Maintainer: Alexander Fedora <alexander.fedora at gmail.com> Description: glagol games server <ПРОБЕЛ>This is server Зависимости можно получить используя ldd и apt-file $ ldd ./build/server | grep libboost_thread /usr/lib/libboost_thread-mt.so.1.38.0 $ apt-file search /usr/lib/libboost_thread-mt.so.1.38.0 libboost-thread1.38.0: /usr/lib/libboost_thread-mt.so.1.38.0 libboost1.38-dbg: /usr/lib/debug/usr/lib/libboost_thread-mt.so.1.38.0 conffiles В этом файле находится список конфигурационных файлов пакета. Пример: /etc/seduction/seductiond.conf /etc/seduction/seductiond.log.properties Создание пакета Я решил использовать простой шелл скрипт для создания пакета. Действия по порядку: Создаём директорию deb Создаём директорию deb/DEBIAN Получаем текущую версию проекта version=`git describe --match=v* HEAD` Записываем в deb/DEBIAN/control описание пакета, а в качестве версии используем $version Записываем в файл deb/DEBIAN/conffiles список конфигурационных файлов В директорию deb/ записываем файлы прокета как будто deb/ — это рут диска (к примеру deb/usr/local/bin/exe_name, deb/etc/config_name и тд) Создаём пакет при помощи комманды dpkg -b deb/ ИМЯПАКЕТА_$version'.deb' После имени пакета обязательно поставить подчеркивание (_) перед версией. Пример рабочего скрипта можно посмотреть тут (это пример для с++ демона, для сайта на php пример тут) Создание репозитория на сервере Устанавливаем пакет dpkg-dev. Создаем директорию /var/opt/repo/, а в ней 2 подддиректории binary и source. Заливаем пакеты в binary и выполняем комманду для создания каталога архива: dpkg-scanpackages -m binary /dev/null | gzip -9c > binary/Packages.gz Записываем shell-скрипт для автоматизации апдейта сервера: rsync -av -e ssh $HOME/projects/repo/*.deb server_name:/var/opt/repo/ || exit 1 ssh server_name 'cd /var/opt/repo; dpkg-scanpackages -m binary /dev/null | gzip -9c > binary/Packages.gz' || exit 1 Поднятие nginx для доступа к репозиторию Устанавливаем nginx, добавляем новый сайт: $ cat /etc/nginx/sites-available/repo server { listen 9977; server_name localhost; access_log /var/log/nginx/repo.access.log; location / { autoindex on; allow 127.0.0.1; allow 192.168.0.2; root /var/opt/repo; } } Поднимаем сайт и убеждаемся что он работает. $ ln -s /etc/nginx/sites-available/repo /etc/nginx/sites-enabled/repo $ sudo /etc/init.d/nginx restart $ wget 192.168.0.1:9977/binary/Packages.gz Установка пакетов из созданного репозитория Добавляем в /etc/apt/sources.list наш сайт: deb 192.168.0.1:9977/ binary/ И устанавливаем пакет: $ sudo apt-get update $ sudo apt-get install package_name Заключение При установке пакета apt-get будет вопить, что пакет не может быть аутентифицирован. В принципе можно добавить цифровую подпись к пакету, но мы то и так знаем что он устанавливается с нашего репозитория — поэтому можно добавить ключики -y --force-yes и устанавливать пакет в автоматическом режиме. PS. сильно не пинайте — это мой первый пост.\n","metric":0.50048828130000000414,"metric_input":"Текст статьи:\ndeb-пакет на коленке / Хабр                 \n\n16-10-2009\nЗадача: создать пакет для Debian для выливки демона/сайта на сервер. Я никогда ранее не работал с deb-пакетами — так что решение может быть не оптимальным. Но оно работает и достаточно просто в использовании. Подготовка В первую очередь для переодических выливок на сервер нам понадобится версионность кода. Если оспользуется svn — то, скорее всего, можно использовать номер ревизии. Так как я использую git — написал простенький скрипт для ведения версии проекта: tag.sh Для увеличения версии проекта вызываем ./tag.sh build, ./tag minor или ./tag major. Скрипт создаёт метки вида v<MAJOR>.<MINOR>.<BUILD> (к примеру v0.1.25) Для получения версии вызываем git describe --match=v* HEAD и получаем в результате версию в виде v0.2.1 (либо v0.2.1-59-g919ab19 если после тага были коммиты). В сети много мануалов по созданию пакетов, которые используют dh_make либо описывают все возможные варианты создания пакета с нуля. Но нам не требуется правильный пакет для внесения в репозиторий дебиана, а всего лишь пакет нашего проекта, который мы установим на сервере. Из всех файлов конфигурации пакета важными показались лишь 2: control — описание пакета conffiles — описание конфигурационных файлов пакета (при установке новой версии apt спросит заменить ли файл или оставить старую версию). control Формат файла достаточно простой: Package: ИМЯ_ПАКЕТА Version: ВЕРСИЯ - получаная git describe Architecture: АРХИТЕКТУРА - amd64 если есть бинарники или all - если только скрипты Depends: ЗАВИСИМОСТИ - можно упустить Maintainer: ВАШЕ_ИМЯ Description: ОПИСАНИЕ К примеру: Package: ggseductionserver Version: v0.2.1-59-g919ab19 Architecture: amd64 Depends: libc6 (>= 2.4), libgcc1 (>= 1:4.1.1), libstdc++6 (>= 4.1.1), libboost-system1.38.0 (>=1.38.0), libboost-thread1.38.0 (>=1.38.0), libboost-regex1.38.0 (>=1.38.0), libboost-serialization1.38.0 (>=1.38.0), libboost-program-options1.38.0 (>=1.38.0), tokyotyrant, tokyocabinet, mysql-client (>=5.0), libssl0.9.8 (>=0.9.8), libreadline5 (>= 5.2), libmysql++3 (>= 3.0), libpcre3 (>= 7.6), libpcrecpp0 (>= 7.6), libgd2-xpm (>= 2.0) Maintainer: Alexander Fedora <alexander.fedora at gmail.com> Description: glagol games server <ПРОБЕЛ>This is server Зависимости можно получить используя ldd и apt-file $ ldd ./build/server | grep libboost_thread /usr/lib/libboost_thread-mt.so.1.38.0 $ apt-file search /usr/lib/libboost_thread-mt.so.1.38.0 libboost-thread1.38.0: /usr/lib/libboost_thread-mt.so.1.38.0 libboost1.38-dbg: /usr/lib/debug/usr/lib/libboost_thread-mt.so.1.38.0 conffiles В этом файле находится список конфигурационных файлов пакета. Пример: /etc/seduction/seductiond.conf /etc/seduction/seductiond.log.properties Создание пакета Я решил использовать простой шелл скрипт для создания пакета. Действия по порядку: Создаём директорию deb Создаём директорию deb/DEBIAN Получаем текущую версию проекта version=`git describe --match=v* HEAD` Записываем в deb/DEBIAN/control описание пакета, а в качестве версии используем $version Записываем в файл deb/DEBIAN/conffiles список конфигурационных файлов В директорию deb/ записываем файлы прокета как будто deb/ — это рут диска (к примеру deb/usr/local/bin/exe_name, deb/etc/config_name и тд) Создаём пакет при помощи комманды dpkg -b deb/ ИМЯПАКЕТА_$version'.deb' После имени пакета обязательно поставить подчеркивание (_) перед версией. Пример рабочего скрипта можно посмотреть тут (это пример для с++ демона, для сайта на php пример тут) Создание репозитория на сервере Устанавливаем пакет dpkg-dev. Создаем директорию /var/opt/repo/, а в ней 2 подддиректории binary и source. Заливаем пакеты в binary и выполняем комманду для создания каталога архива: dpkg-scanpackages -m binary /dev/null | gzip -9c > binary/Packages.gz Записываем shell-скрипт для автоматизации апдейта сервера: rsync -av -e ssh $HOME/projects/repo/*.deb server_name:/var/opt/repo/ || exit 1 ssh server_name 'cd /var/opt/repo; dpkg-scanpackages -m binary /dev/null | gzip -9c > binary/Packages.gz' || exit 1 Поднятие nginx для доступа к репозиторию Устанавливаем nginx, добавляем новый сайт: $ cat /etc/nginx/sites-available/repo server { listen 9977; server_name localhost; access_log /var/log/nginx/repo.access.log; location / { autoindex on; allow 127.0.0.1; allow 192.168.0.2; root /var/opt/repo; } } Поднимаем сайт и убеждаемся что он работает. $ ln -s /etc/nginx/sites-available/repo /etc/nginx/sites-enabled/repo $ sudo /etc/init.d/nginx restart $ wget 192.168.0.1:9977/binary/Packages.gz Установка пакетов из созданного репозитория Добавляем в /etc/apt/sources.list наш сайт: deb 192.168.0.1:9977/ binary/ И устанавливаем пакет: $ sudo apt-get update $ sudo apt-get install package_name Заключение При установке пакета apt-get будет вопить, что пакет не может быть аутентифицирован. В принципе можно добавить цифровую подпись к пакету, но мы то и так знаем что он устанавливается с нашего репозитория — поэтому можно добавить ключики -y --force-yes и устанавливать пакет в автоматическом режиме. PS. сильно не пинайте — это мой первый пост.\n\n\nКраткое содержание:\nСтатья описывает процесс создания deb-пакета для Debian и создания репозитория для сервера. Для создания пакета используется простой шелл-скрипт, который создает файлы control и conffiles, а затем использует dpkg для создания deb-пакета. Для создания репозитория на сервере используется пакет dpkg-dev и создаются две поддиректории: binary и source. Пакеты заливаются в binary и создается каталог архива с помощью dpkg-scanpackages. Затем создается скрипт для автоматизации апдейта сервера и устанавливается nginx для доступа к репозиторию.","summary":"Статья описывает процесс создания deb-пакета для Debian и создания репозитория для сервера. Для создания пакета используется простой шелл-скрипт, который создает файлы control и conffiles, а затем использует dpkg для создания deb-пакета. Для создания репозитория на сервере используется пакет dpkg-dev и создаются две поддиректории: binary и source. Пакеты заливаются в binary и создается каталог архива с помощью dpkg-scanpackages. Затем создается скрипт для автоматизации апдейта сервера и устанавливается nginx для доступа к репозиторию."}
{"Text":"Y Combinator рекомендует прочитать в 2016 / Habr              \n\n09-10-2016\n«Чтение и личный опыт формируют мировоззрение. И даже если вы не помните самый момент получения опыта или содержания книги, то их влияние на ваше представление о мире непременно остается в Вас. Ваш разум похож на скомпилированную программу без исходного кода — он работает, но не ясно как.» — Пол Грэм, основатель Y Combinator, программист, инвестор, эссеист Предыдущая подборка от экспертов YC — Y Combinator рекомендует: лучшие 15 книг в 2015 году, на которые стоит обратить внимание . Пол Грэм делится, почему важно читать книги — «Как знать (How You Know)» Подходы к двух- и трехступечатому проектированию, которые мы используем на проектах в EDISON Software Development Centre. Ready Player One: A Novel «Прекрасное и ужасающее видение нашего возможного будущего. Это как Pokémon Go на стероидах. К лучшему или худшему :)» — Luke Iseman , Director of Hardware at Y Combinator На русском — Первому игроку приготовиться How Buildings Learn: What Happens After They’re Built « Стюарт Брэнд (автор Whole Earth Catalog) любезно описывает как и почему здания могут становиться лучше со временем. Фишка: он написал книгу, сидя внутри контейнера!» — Luke Iseman , Director of Hardware at Y Combinator фото контейнера Find Me Unafraid: Love, Loss, and Hope in an African Slum «Невероятно вдохновляющая история Kennedy Odede и Jessica Posner — основателей всемирной некоммерческой организации в беднейших трущобах мира. В них они столкнулись с лучшими и худшими человеческими качествами и их история является свидетельством силы настойчивости. (Признаюсь, я сделал взнос и стал членом SHOFCO отчасти благодаря этой книге.)» — Robby Walker , YC non-profit program Hadrian’s Wall «Вал Адриана начинается с предисловия о Римской истории. Я боялся, что это будет скучно, обычно предисловия скучны. Но эта штука оказалась удивительно глубокой. Я надеюсь, что остальная книга будет настолько же увлекательной.» — Paul Graham, основатель YC The Magicians Trilogy «Magicians — это Гарри Поттер для взрослых. К сожалению, всего в трех томах а не в семи. Эпичная история с яркими образами и извилистым сюжетом.» — Finbarr Taylor , full-stack software engineer at Y Combinator and CEO & Co-Founder of Shogun На русском — «Волшебники» The Expanse Collection «Expanse Collection — настоящая космическая опера с глубокой и продолжительной сюжетной линией о пропавшей цивилизации.» — Finbarr Taylor , full-stack software engineer at Y Combinator and CEO & Co-Founder of Shogun End of Cycle? «В этой части, Elad Gil спрашивает, находимся ли мы в конце экономического цикла технологий. Он указывает на паттерны, которые проявились недавно, а до этого появлялись в конце предыдущего цикла (2000-2001). Это звучит мрачновато, но у него есть оптимистичные нотки и я с нетерпением жду, что будет дальше.» — Kat Mañalac, cofounder of Reddit и партнер YC. Our Mathematical Universe: My Quest for the Ultimate Nature of Reality «Книга Тегмарка одна из тех книг, которые чуток изменяют сознание и дают вам иной взгляд на мир. Это удивительно читабельное сочетание глубоких мыслей о природе реальности с объяснением того, что физики и космологи знают сегодня.» — Geoff Ralston, создатель Yahoo! Mail Гипотеза математической вселенной The Beginning of Infinity: Explanations That Transform the World «Дойч еще один физик, заставляющий переосмыслить то что мы знаем об истории человечества и ее его места во Вселенной. Он выстраивает обсуждение вокруг „объяснений“, которые по его утверждению, более фундаментальны, чем вы можете представить.» — Geoff Ralston, создатель Yahoo! Mail На русском — «Начало бесконечности» . Team of Rivals: The Political Genius of Abraham Lincoln «Команда Конкурентов великолепна. По многим причинам.» — Aaron Harris, ‎Partner at Y Combinator Algorithms to Live By: The Computer Science of Human Decisions «Ежедневно люди принимают решения в условиях ограниченного времени и неполной информации. Эти ограничения хорошо знакомы программистам, и многие принципы computer science можно применить, чтобы понять, почему мы принимаем именно так, а не иначе. Современный нейронауки проясняют все больше и больше, как люди принимают решения, и это помогают нам понять, почему некоторые решения сделать труднее, чем другие. Книга написана доступным языком и обобщает горы недавних фундаментальных исследований.» — Trevor Blackwell , сооснователь YC Moral Tribes: Emotion, Reason, and the Gap Between Us and Them «Интуиция людей о том, что этично, порой отличается от того, что с точки зрения философов-утилитаристов будет величайшим благом для большинства.Тщательно покопавшись в самых необычных кейсах, когда люди говорят, что правильно а что нет, и соотнеся это с мозговой активностью, автор проливает свет, о том, что там и как происходит внутри психики.» — Trevor Blackwell , сооснователь YC Work Rules!: Insights from Inside Google That Will Transform How You Live and Lead «Книга полна интересных идей и наблюдений. Особенно полезна будет компаниям, которые хотят масштабироваться и вырасти из штата в 150 человек, когда сиюминутные методы управления перестают работать. Несмотря на то, что мой опыт не всегда вяжется с идеями в книге и некоторые идеи кажутся несколько специфическими для Google, это все-таки на самом деле стоит читать, если вы руководитель в любой организации.» — Emmett Shear , сооснователь Justin.tv и TwitchTV, партнер YC На русском — «Работа рулит!» The Intelligent Investor: The Definitive Book on Value Investing «Меня предостерегали, чтобы я обращался с этими идеями осторожно, потому что инвестирование на ранних стадиях существенно отличается от публичного рынка, но все же это удивительное и проницательное чтиво. Я уверен, это одна из лучших книг, прочитанных мной за год.» — Qasar Younis , венчурный капиталист, COO of Y Combinator На русском — «Разумный инвестор» Where Vultures Feast: Shell, Human Rights and Oil «Я перечитал эту книгу, до этого я читал ее 10 лет назад. В нашей культуре, где корпорации имеют высокий престиж, эта книга оказала огромное влияние на меня, пролив свет на темные уголки рынков.» — Qasar Younis , венчурный капиталист, COO of Y Combinator Seeing Is Forgetting the Name of the Thing One Sees «Ирвин заставил мене переосмыслить то, как я слушаю вопросы и даю ответы. По его словам, вопросы намного глубже ответов, потому что как только вы получили ответ, он тут же порождает следующий вопрос.» — Craig Cannon, директор по маркетингу YC Thoughts Without a Thinker «Чтение этой книги похоже на психоаналитическую медитацию. Эпштейн увязывает воедино две исторически различные точки зрения на человеческую психологию, и объясняет, почему нужно иметь ввиду оба подхода, когда дело касается отношений с сами собой… если вообще существует такое понятие, как „с самим собой“ ;).» — Amy Buechler , Associate at Y Combinator, Executive Coach for Startups На русском — «Мысли без мыслящего»\n","metric":0.056457519529999999897,"metric_input":"Текст статьи:\nY Combinator рекомендует прочитать в 2016 / Habr              \n\n09-10-2016\n«Чтение и личный опыт формируют мировоззрение. И даже если вы не помните самый момент получения опыта или содержания книги, то их влияние на ваше представление о мире непременно остается в Вас. Ваш разум похож на скомпилированную программу без исходного кода — он работает, но не ясно как.» — Пол Грэм, основатель Y Combinator, программист, инвестор, эссеист Предыдущая подборка от экспертов YC — Y Combinator рекомендует: лучшие 15 книг в 2015 году, на которые стоит обратить внимание . Пол Грэм делится, почему важно читать книги — «Как знать (How You Know)» Подходы к двух- и трехступечатому проектированию, которые мы используем на проектах в EDISON Software Development Centre. Ready Player One: A Novel «Прекрасное и ужасающее видение нашего возможного будущего. Это как Pokémon Go на стероидах. К лучшему или худшему :)» — Luke Iseman , Director of Hardware at Y Combinator На русском — Первому игроку приготовиться How Buildings Learn: What Happens After They’re Built « Стюарт Брэнд (автор Whole Earth Catalog) любезно описывает как и почему здания могут становиться лучше со временем. Фишка: он написал книгу, сидя внутри контейнера!» — Luke Iseman , Director of Hardware at Y Combinator фото контейнера Find Me Unafraid: Love, Loss, and Hope in an African Slum «Невероятно вдохновляющая история Kennedy Odede и Jessica Posner — основателей всемирной некоммерческой организации в беднейших трущобах мира. В них они столкнулись с лучшими и худшими человеческими качествами и их история является свидетельством силы настойчивости. (Признаюсь, я сделал взнос и стал членом SHOFCO отчасти благодаря этой книге.)» — Robby Walker , YC non-profit program Hadrian’s Wall «Вал Адриана начинается с предисловия о Римской истории. Я боялся, что это будет скучно, обычно предисловия скучны. Но эта штука оказалась удивительно глубокой. Я надеюсь, что остальная книга будет настолько же увлекательной.» — Paul Graham, основатель YC The Magicians Trilogy «Magicians — это Гарри Поттер для взрослых. К сожалению, всего в трех томах а не в семи. Эпичная история с яркими образами и извилистым сюжетом.» — Finbarr Taylor , full-stack software engineer at Y Combinator and CEO & Co-Founder of Shogun На русском — «Волшебники» The Expanse Collection «Expanse Collection — настоящая космическая опера с глубокой и продолжительной сюжетной линией о пропавшей цивилизации.» — Finbarr Taylor , full-stack software engineer at Y Combinator and CEO & Co-Founder of Shogun End of Cycle? «В этой части, Elad Gil спрашивает, находимся ли мы в конце экономического цикла технологий. Он указывает на паттерны, которые проявились недавно, а до этого появлялись в конце предыдущего цикла (2000-2001). Это звучит мрачновато, но у него есть оптимистичные нотки и я с нетерпением жду, что будет дальше.» — Kat Mañalac, cofounder of Reddit и партнер YC. Our Mathematical Universe: My Quest for the Ultimate Nature of Reality «Книга Тегмарка одна из тех книг, которые чуток изменяют сознание и дают вам иной взгляд на мир. Это удивительно читабельное сочетание глубоких мыслей о природе реальности с объяснением того, что физики и космологи знают сегодня.» — Geoff Ralston, создатель Yahoo! Mail Гипотеза математической вселенной The Beginning of Infinity: Explanations That Transform the World «Дойч еще один физик, заставляющий переосмыслить то что мы знаем об истории человечества и ее его места во Вселенной. Он выстраивает обсуждение вокруг „объяснений“, которые по его утверждению, более фундаментальны, чем вы можете представить.» — Geoff Ralston, создатель Yahoo! Mail На русском — «Начало бесконечности» . Team of Rivals: The Political Genius of Abraham Lincoln «Команда Конкурентов великолепна. По многим причинам.» — Aaron Harris, ‎Partner at Y Combinator Algorithms to Live By: The Computer Science of Human Decisions «Ежедневно люди принимают решения в условиях ограниченного времени и неполной информации. Эти ограничения хорошо знакомы программистам, и многие принципы computer science можно применить, чтобы понять, почему мы принимаем именно так, а не иначе. Современный нейронауки проясняют все больше и больше, как люди принимают решения, и это помогают нам понять, почему некоторые решения сделать труднее, чем другие. Книга написана доступным языком и обобщает горы недавних фундаментальных исследований.» — Trevor Blackwell , сооснователь YC Moral Tribes: Emotion, Reason, and the Gap Between Us and Them «Интуиция людей о том, что этично, порой отличается от того, что с точки зрения философов-утилитаристов будет величайшим благом для большинства.Тщательно покопавшись в самых необычных кейсах, когда люди говорят, что правильно а что нет, и соотнеся это с мозговой активностью, автор проливает свет, о том, что там и как происходит внутри психики.» — Trevor Blackwell , сооснователь YC Work Rules!: Insights from Inside Google That Will Transform How You Live and Lead «Книга полна интересных идей и наблюдений. Особенно полезна будет компаниям, которые хотят масштабироваться и вырасти из штата в 150 человек, когда сиюминутные методы управления перестают работать. Несмотря на то, что мой опыт не всегда вяжется с идеями в книге и некоторые идеи кажутся несколько специфическими для Google, это все-таки на самом деле стоит читать, если вы руководитель в любой организации.» — Emmett Shear , сооснователь Justin.tv и TwitchTV, партнер YC На русском — «Работа рулит!» The Intelligent Investor: The Definitive Book on Value Investing «Меня предостерегали, чтобы я обращался с этими идеями осторожно, потому что инвестирование на ранних стадиях существенно отличается от публичного рынка, но все же это удивительное и проницательное чтиво. Я уверен, это одна из лучших книг, прочитанных мной за год.» — Qasar Younis , венчурный капиталист, COO of Y Combinator На русском — «Разумный инвестор» Where Vultures Feast: Shell, Human Rights and Oil «Я перечитал эту книгу, до этого я читал ее 10 лет назад. В нашей культуре, где корпорации имеют высокий престиж, эта книга оказала огромное влияние на меня, пролив свет на темные уголки рынков.» — Qasar Younis , венчурный капиталист, COO of Y Combinator Seeing Is Forgetting the Name of the Thing One Sees «Ирвин заставил мене переосмыслить то, как я слушаю вопросы и даю ответы. По его словам, вопросы намного глубже ответов, потому что как только вы получили ответ, он тут же порождает следующий вопрос.» — Craig Cannon, директор по маркетингу YC Thoughts Without a Thinker «Чтение этой книги похоже на психоаналитическую медитацию. Эпштейн увязывает воедино две исторически различные точки зрения на человеческую психологию, и объясняет, почему нужно иметь ввиду оба подхода, когда дело касается отношений с сами собой… если вообще существует такое понятие, как „с самим собой“ ;).» — Amy Buechler , Associate at Y Combinator, Executive Coach for Startups На русском — «Мысли без мыслящего»\n\n\nКраткое содержание:\nВ статье обсуждаются 14 книг, которые, по мнению экспертов Y Combinator, стоит прочитать в 2016 году. Среди них:\n\n- \"Как знать\" Пола Грэма - о важности чтения и влиянии книг на мировоззрение.\n- \"Прекрасное и ужасающее видение нашего возможного будущего\" от автора \"Pokémon Go\" Лука Иземана.\n- \"Hadrian's Wall\" Пола Грэма - история Римской империи.\n- \"Magicians Trilogy\" - эпическая история с яркими образами.\n- \"Expanse Collection\" - космическая опера с глубокой сюжетной линией.\n- \"End of Cycle?\" - размышления о конце экономического цикла технологий.\n- \"Our Mathematical Universe\" и \"The Beginning of Infinity\" Мичио Тегмарка и Дэвида Дойча - книги о природе реальности.\n- \"Team of Rivals\" - о политическом гении Авраама Линкольна.\n- \"Algorithms to Live By\" - принципы computer science для принятия решений.\n- \"Moral Tribes\" - исследование различий в интуиции о морали.\n- \"Work Rules!\" - идеи для руководителей компаний.\n- \"The Intelligent Investor\" Бенджамина Грэма - книга о инвестировании.\n- \"Where Vultures Feast\" - анализ рынков и корпораций.\n\nВ статье также упоминаются книги Ирвина Алтмана, Стюарта Брэнда, Теда Капчука, Теда Невинса, Теда Ньюджент, Кеннета Клока, Кеннета Либрехта, Кеннета Линча, Кеннета Роува, Кеннета Шиллера, Кеннета Энгера, Кеннета Янгера, Теда Качински, Теда Хаффса, Теда Хофстра, Теда Левитта, Теда Поллака, Теда Рашмора, Теда Сирса, Теда","summary":"В статье обсуждаются 14 книг, которые, по мнению экспертов Y Combinator, стоит прочитать в 2016 году. Среди них:\n\n- \"Как знать\" Пола Грэма - о важности чтения и влиянии книг на мировоззрение.\n- \"Прекрасное и ужасающее видение нашего возможного будущего\" от автора \"Pokémon Go\" Лука Иземана.\n- \"Hadrian's Wall\" Пола Грэма - история Римской империи.\n- \"Magicians Trilogy\" - эпическая история с яркими образами.\n- \"Expanse Collection\" - космическая опера с глубокой сюжетной линией.\n- \"End of Cycle?\" - размышления о конце экономического цикла технологий.\n- \"Our Mathematical Universe\" и \"The Beginning of Infinity\" Мичио Тегмарка и Дэвида Дойча - книги о природе реальности.\n- \"Team of Rivals\" - о политическом гении Авраама Линкольна.\n- \"Algorithms to Live By\" - принципы computer science для принятия решений.\n- \"Moral Tribes\" - исследование различий в интуиции о морали.\n- \"Work Rules!\" - идеи для руководителей компаний.\n- \"The Intelligent Investor\" Бенджамина Грэма - книга о инвестировании.\n- \"Where Vultures Feast\" - анализ рынков и корпораций.\n\nВ статье также упоминаются книги Ирвина Алтмана, Стюарта Брэнда, Теда Капчука, Теда Невинса, Теда Ньюджент, Кеннета Клока, Кеннета Либрехта, Кеннета Линча, Кеннета Роува, Кеннета Шиллера, Кеннета Энгера, Кеннета Янгера, Теда Качински, Теда Хаффса, Теда Хофстра, Теда Левитта, Теда Поллака, Теда Рашмора, Теда Сирса, Теда"}
{"Text":"Электронные книги и их форматы: DjVu — его история, плюсы, минусы и особенности / Habr            \n\n07-04-2018\nВ начале 70-х годов американский писатель Майкл Харт (Michael Hart) сумел получить неограниченный доступ к компьютеру Xerox Sigma 5, установленному в университете штата Иллинойс. Чтобы достойно использовать ресурсы машины, он решил создать первую электронную книгу, перепечатав Декларацию независимости США. Сегодня цифровая литература получила широкое распространение, во многом благодаря развитию портативных устройств (смартфонов, ридеров, ноутбуков). Это привело к появлению большого количества форматов электронных книг. Попробуем разобраться в их особенностях и расскажем историю наиболее популярных из них — начнем с формата DjVu. / Flickr / Lane Pearman / CC Возникновение формата DjVu был разработан в 1996 году компанией AT&T Labs с одной целью — дать веб-разработчикам инструмент для распространения изображений в высоком разрешении через интернет. Дело в том, что в то время 90% всей информации все еще хранилось на бумаге, а многие из важных документов имели цветные изображения и фотографии. Чтобы сохранить читаемость текста и качество картинок, приходилось делать сканы в высоком разрешении. Классические веб-форматы — JPEG, GIF и PNG — позволяли работать с такими изображениями, но ценой объема. В случае JPEG, чтобы текст был читаем на экране монитора, приходилось сканировать документ с разрешением 300 dpi. Цветная страница журнала при этом занимала порядка 500 Кбайт. Скачивание файлов такого объема из интернета в то время было довольно трудоемким процессом. Альтернативой было оцифровывание бумажных документов с помощью технологий распознавания текста, однако 20 лет назад их точность была далека от идеальной — после обработки финальный результат приходилось серьезно править вручную. При этом графики и изображения оставались «за бортом». И даже если удавалось встроить отсканированную картинку в текстовый документ, терялись некоторые визуальные детали, например, цвет бумаги, её текстура, а это важные компоненты исторических документов. С целью решить эти проблемы AT&T и разработали DjVu. Он позволил сжимать отсканированные цветные документы с разрешением 300 dpi до 40–60 Кбайт, при размере оригинала в 25 Мбайт. Размеры чёрно-белых страниц DjVu уменьшил до 10–30 Кбайт. Как DjVu сжимает документы DjVu может работать как с бумажными отсканированными документами, так и с другими цифровыми форматами, например PDF. В основе работы DjVu лежит технология, разбивающая изображение на три компонента: передний план, задний план и чёрно-белую (битовую) маску. Маска сохраняется с разрешением исходного файла и содержит изображение текста и прочие чёткие детали — тонкие линии и схемы — а также контрастные картинки. Она имеет разрешение 300 dpi, чтобы тонкие линии и контуры букв оставались четкими, и сжимается с помощью алгоритма JB2, который представляет собой вариацию алгоритма JBIG2, предложенного AT&T для работы факсов. Особенностью JB2 является то, что он ищет на странице повторяющиеся символы и сохраняет их изображение только один раз. Таким образом, в многостраничных документах каждые несколько последовательных страниц пользуются общим «словарём». Задний план содержит текстуру страницы и иллюстрации, а его разрешение меньше, чем у маски. Фон без потерь для восприятия сохраняется с разрешением 100 dpi. Передний план хранит цветовую информацию о маске, и его разрешение обычно понижается ещё сильнее, так как в большинстве случаев цвет текста черный и одинаковый для одного печатного знака. Для сжатия переднего и заднего планов используется вейвлетное сжатие. Заключительным этапом создания DjVu-документа становится энтропийное кодирование, когда адаптивный арифметический кодировщик превращает последовательности одинаковых символов в бинарное значение. Достоинства формата Задачей DjVu было сохранить «свойства» бумажного документа в цифровом виде, позволив работать с такими документами даже слабым компьютерам. Поэтому ПО для просмотра DjVu-файлов обладает возможностью «быстрого рендеринга». Благодаря ей в память загружается только тот кусочек DjVu-страницы, который должен отображаться на экране. Это также дает возможность просматривать «недокачанные» файлы, то есть отдельные страницы многостраничного DjVu-документа. При этом используется прогрессивная прорисовка деталей изображения, когда компоненты как бы «проявляются» по мере закачки файла (как в JPEG). 20 лет назад, когда был представлен этот формат, загрузка страницы происходила в три этапа: сперва грузилась текстовая составляющая, через пару секунд подгружались первые версии изображений и фон. Уже после «проявлялась» вся страница книги. Наличие трёхуровневой структуры также позволяет осуществлять поиск по отсканированным книгам (так как есть специальный текстовый слой). Это оказалось удобно при работе с технической литературой и справочниками, поэтому DjVu стал основой для нескольких библиотек научных книг. Например, в 2002 году он был выбран Архивом Интернета в качестве одного из форматов (вместе с TIFF и PDF) для проекта по сохранению отсканированных книг из открытых источников. Недостатки формата Однако, как и все технологии, DjVu имеет свои минусы. Например, при кодировании сканов книг в формат DjVu некоторые символы в документе могут подменяться на другие, внешне похожие. Наиболее часто это происходит с буквами «и» и «н», отчего эта проблема получила название «проблема инь». Она не зависит от языка текста и влияет, в том числе, на цифры и другие мелкие повторяющиеся знаки. Её причина — ошибки классификации символов в кодировщике JB2. Он «дробит» сканы на группы по 10–20 штук и формирует для каждой из групп словарь общих символов. Словарь содержит образцы общих букв и цифр со страницами и координатами их появления. Когда вы просматриваете DjVu-книгу, символы из словаря подставляются в нужные места. Это позволяет уменьшить размеры DjVu-файла, однако, если отображения двух букв визуально похожи, кодировщик может их или перепутать, или принять за одинаковые. Иногда это приводит к порче формул в техническом документе. Для решения этой проблемы можно отказаться от алгоритмов сжатия, однако это увеличит размеры цифровой копии книги. Другим недостатком формата является то, что он не поддерживается по умолчанию во многих современных ОС (в том числе мобильных). Поэтому для работы с ним нужно ставить сторонние программы, такие как DjVuReader, WinDjView, Evince и др. Однако здесь хотелось бы отметить, что некоторые электронные читалки (например, ONYX BOOX) поддерживает формат DjVu «из коробки» — так как необходимые приложения там уже установлены. Кстати, о том, что еще могут приложения для ридеров на базе Android, мы рассказывали в одном из предыдущих материалов. Ридер ONYX BOOX Chronos Еще одна проблема формата проявляется при работе с DjVu-документами на небольших экранах мобильных устройств — смартфонах, планшетах, ридерах. Иногда DjVu-файлы представлены в виде скана разворота книги, а профессиональная литература и рабочие документы часто имеют формат А4, поэтому приходится «двигать» изображение в поисках информации. Однако отметим, что эта проблема также решаема. Проще всего, конечно, поискать документ в другом формате — но если такой вариант невозможен (например, вам нужно работать с большим количеством технической литературы в DjVu), то можно воспользоваться электронными читалками с большой диагональю от 9,7 до 13,3 дюйма, которые специально «заточены» под работу с подобными документами. Например, в линейке ONYX BOOX такими устройствами являются Chronos и MAX 2 (кстати, мы подготовили обзор этой модели ридера, и в скором времени опубликуем его в нашем блоге), а также Note, который имеет экран E Ink Mobius Carta с диагональю 10,3 дюйма и повышенным разрешением. Такие устройства позволяют спокойно рассмотреть все детали иллюстраций в оригинальном размере и подойдут тем, кому часто приходится читать учебную или техническую литературу. Для просмотра файлов DjVu и PDF используется NEO Reader, который дает настраивать контраст и толщину оцифрованных шрифтов. Несмотря на имеющиеся у формата недостатки, на сегодняшний день DjVu остается одним из самых популярных форматов для «сохранения» литературных произведений. Во многом это обусловлено тем, что он является открытым, а обойти его некоторые технологические ограничения сегодня позволяют современные технологии и разработки. В следующих материалах мы продолжим рассказ об истории появления форматов электронных книг и особенностях их работы. P.S. Несколько оборов ридеров ONYX BOOX: Технологии против «античности»: обзор ридера ONYX BOOX Chronos Царица изволит почитать: обзор электронной книги ONYX BOOX Cleopatra 3 Обзор ONYX BOOX Darwin 4: лучшая электронная книга на E-Ink\n","metric":0.60595703130000000414,"metric_input":"Текст статьи:\nЭлектронные книги и их форматы: DjVu — его история, плюсы, минусы и особенности / Habr            \n\n07-04-2018\nВ начале 70-х годов американский писатель Майкл Харт (Michael Hart) сумел получить неограниченный доступ к компьютеру Xerox Sigma 5, установленному в университете штата Иллинойс. Чтобы достойно использовать ресурсы машины, он решил создать первую электронную книгу, перепечатав Декларацию независимости США. Сегодня цифровая литература получила широкое распространение, во многом благодаря развитию портативных устройств (смартфонов, ридеров, ноутбуков). Это привело к появлению большого количества форматов электронных книг. Попробуем разобраться в их особенностях и расскажем историю наиболее популярных из них — начнем с формата DjVu. / Flickr / Lane Pearman / CC Возникновение формата DjVu был разработан в 1996 году компанией AT&T Labs с одной целью — дать веб-разработчикам инструмент для распространения изображений в высоком разрешении через интернет. Дело в том, что в то время 90% всей информации все еще хранилось на бумаге, а многие из важных документов имели цветные изображения и фотографии. Чтобы сохранить читаемость текста и качество картинок, приходилось делать сканы в высоком разрешении. Классические веб-форматы — JPEG, GIF и PNG — позволяли работать с такими изображениями, но ценой объема. В случае JPEG, чтобы текст был читаем на экране монитора, приходилось сканировать документ с разрешением 300 dpi. Цветная страница журнала при этом занимала порядка 500 Кбайт. Скачивание файлов такого объема из интернета в то время было довольно трудоемким процессом. Альтернативой было оцифровывание бумажных документов с помощью технологий распознавания текста, однако 20 лет назад их точность была далека от идеальной — после обработки финальный результат приходилось серьезно править вручную. При этом графики и изображения оставались «за бортом». И даже если удавалось встроить отсканированную картинку в текстовый документ, терялись некоторые визуальные детали, например, цвет бумаги, её текстура, а это важные компоненты исторических документов. С целью решить эти проблемы AT&T и разработали DjVu. Он позволил сжимать отсканированные цветные документы с разрешением 300 dpi до 40–60 Кбайт, при размере оригинала в 25 Мбайт. Размеры чёрно-белых страниц DjVu уменьшил до 10–30 Кбайт. Как DjVu сжимает документы DjVu может работать как с бумажными отсканированными документами, так и с другими цифровыми форматами, например PDF. В основе работы DjVu лежит технология, разбивающая изображение на три компонента: передний план, задний план и чёрно-белую (битовую) маску. Маска сохраняется с разрешением исходного файла и содержит изображение текста и прочие чёткие детали — тонкие линии и схемы — а также контрастные картинки. Она имеет разрешение 300 dpi, чтобы тонкие линии и контуры букв оставались четкими, и сжимается с помощью алгоритма JB2, который представляет собой вариацию алгоритма JBIG2, предложенного AT&T для работы факсов. Особенностью JB2 является то, что он ищет на странице повторяющиеся символы и сохраняет их изображение только один раз. Таким образом, в многостраничных документах каждые несколько последовательных страниц пользуются общим «словарём». Задний план содержит текстуру страницы и иллюстрации, а его разрешение меньше, чем у маски. Фон без потерь для восприятия сохраняется с разрешением 100 dpi. Передний план хранит цветовую информацию о маске, и его разрешение обычно понижается ещё сильнее, так как в большинстве случаев цвет текста черный и одинаковый для одного печатного знака. Для сжатия переднего и заднего планов используется вейвлетное сжатие. Заключительным этапом создания DjVu-документа становится энтропийное кодирование, когда адаптивный арифметический кодировщик превращает последовательности одинаковых символов в бинарное значение. Достоинства формата Задачей DjVu было сохранить «свойства» бумажного документа в цифровом виде, позволив работать с такими документами даже слабым компьютерам. Поэтому ПО для просмотра DjVu-файлов обладает возможностью «быстрого рендеринга». Благодаря ей в память загружается только тот кусочек DjVu-страницы, который должен отображаться на экране. Это также дает возможность просматривать «недокачанные» файлы, то есть отдельные страницы многостраничного DjVu-документа. При этом используется прогрессивная прорисовка деталей изображения, когда компоненты как бы «проявляются» по мере закачки файла (как в JPEG). 20 лет назад, когда был представлен этот формат, загрузка страницы происходила в три этапа: сперва грузилась текстовая составляющая, через пару секунд подгружались первые версии изображений и фон. Уже после «проявлялась» вся страница книги. Наличие трёхуровневой структуры также позволяет осуществлять поиск по отсканированным книгам (так как есть специальный текстовый слой). Это оказалось удобно при работе с технической литературой и справочниками, поэтому DjVu стал основой для нескольких библиотек научных книг. Например, в 2002 году он был выбран Архивом Интернета в качестве одного из форматов (вместе с TIFF и PDF) для проекта по сохранению отсканированных книг из открытых источников. Недостатки формата Однако, как и все технологии, DjVu имеет свои минусы. Например, при кодировании сканов книг в формат DjVu некоторые символы в документе могут подменяться на другие, внешне похожие. Наиболее часто это происходит с буквами «и» и «н», отчего эта проблема получила название «проблема инь». Она не зависит от языка текста и влияет, в том числе, на цифры и другие мелкие повторяющиеся знаки. Её причина — ошибки классификации символов в кодировщике JB2. Он «дробит» сканы на группы по 10–20 штук и формирует для каждой из групп словарь общих символов. Словарь содержит образцы общих букв и цифр со страницами и координатами их появления. Когда вы просматриваете DjVu-книгу, символы из словаря подставляются в нужные места. Это позволяет уменьшить размеры DjVu-файла, однако, если отображения двух букв визуально похожи, кодировщик может их или перепутать, или принять за одинаковые. Иногда это приводит к порче формул в техническом документе. Для решения этой проблемы можно отказаться от алгоритмов сжатия, однако это увеличит размеры цифровой копии книги. Другим недостатком формата является то, что он не поддерживается по умолчанию во многих современных ОС (в том числе мобильных). Поэтому для работы с ним нужно ставить сторонние программы, такие как DjVuReader, WinDjView, Evince и др. Однако здесь хотелось бы отметить, что некоторые электронные читалки (например, ONYX BOOX) поддерживает формат DjVu «из коробки» — так как необходимые приложения там уже установлены. Кстати, о том, что еще могут приложения для ридеров на базе Android, мы рассказывали в одном из предыдущих материалов. Ридер ONYX BOOX Chronos Еще одна проблема формата проявляется при работе с DjVu-документами на небольших экранах мобильных устройств — смартфонах, планшетах, ридерах. Иногда DjVu-файлы представлены в виде скана разворота книги, а профессиональная литература и рабочие документы часто имеют формат А4, поэтому приходится «двигать» изображение в поисках информации. Однако отметим, что эта проблема также решаема. Проще всего, конечно, поискать документ в другом формате — но если такой вариант невозможен (например, вам нужно работать с большим количеством технической литературы в DjVu), то можно воспользоваться электронными читалками с большой диагональю от 9,7 до 13,3 дюйма, которые специально «заточены» под работу с подобными документами. Например, в линейке ONYX BOOX такими устройствами являются Chronos и MAX 2 (кстати, мы подготовили обзор этой модели ридера, и в скором времени опубликуем его в нашем блоге), а также Note, который имеет экран E Ink Mobius Carta с диагональю 10,3 дюйма и повышенным разрешением. Такие устройства позволяют спокойно рассмотреть все детали иллюстраций в оригинальном размере и подойдут тем, кому часто приходится читать учебную или техническую литературу. Для просмотра файлов DjVu и PDF используется NEO Reader, который дает настраивать контраст и толщину оцифрованных шрифтов. Несмотря на имеющиеся у формата недостатки, на сегодняшний день DjVu остается одним из самых популярных форматов для «сохранения» литературных произведений. Во многом это обусловлено тем, что он является открытым, а обойти его некоторые технологические ограничения сегодня позволяют современные технологии и разработки. В следующих материалах мы продолжим рассказ об истории появления форматов электронных книг и особенностях их работы. P.S. Несколько оборов ридеров ONYX BOOX: Технологии против «античности»: обзор ридера ONYX BOOX Chronos Царица изволит почитать: обзор электронной книги ONYX BOOX Cleopatra 3 Обзор ONYX BOOX Darwin 4: лучшая электронная книга на E-Ink\n\n\nКраткое содержание:\nСтатья рассказывает о формате электронных книг DjVu, разработанном в 1996 году компанией AT&T Labs для сохранения читаемости текста и качества изображений в отсканированных документах. Формат позволяет сжимать цветные изображения в высоком разрешении, сохраняя четкость текста и визуальные детали.\n\nОсновные компоненты DjVu: передний план, задний план и битовая маска. Задний план содержит текстуру страницы и иллюстрации, а его разрешение меньше, чем у маски. Передний план хранит цветовую информацию о маске, и его разрешение обычно понижается еще сильнее, так как цвет текста обычно черный.\n\nФормат DjVu имеет свои преимущества и недостатки. К недостаткам относятся проблемы с кодированием символов и поддержкой формата по умолчанию в современных ОС. Однако, несмотря на эти ограничения, DjVu остается одним из самых популярных форматов для сохранения литературных произведений.","summary":"Статья рассказывает о формате электронных книг DjVu, разработанном в 1996 году компанией AT&T Labs для сохранения читаемости текста и качества изображений в отсканированных документах. Формат позволяет сжимать цветные изображения в высоком разрешении, сохраняя четкость текста и визуальные детали.\n\nОсновные компоненты DjVu: передний план, задний план и битовая маска. Задний план содержит текстуру страницы и иллюстрации, а его разрешение меньше, чем у маски. Передний план хранит цветовую информацию о маске, и его разрешение обычно понижается еще сильнее, так как цвет текста обычно черный.\n\nФормат DjVu имеет свои преимущества и недостатки. К недостаткам относятся проблемы с кодированием символов и поддержкой формата по умолчанию в современных ОС. Однако, несмотря на эти ограничения, DjVu остается одним из самых популярных форматов для сохранения литературных произведений."}
{"Text":"Critical Transcendence: .NET SDK and Apache Spark / Хабр                                                            \n\nWhen Alex Garland’s series Devs (on FX and Hulu) came out this year, it gave developers their own sexy Hollywood workup. Who knew that coders could get snarled into murder plots and love triangles just for designing machine learning programs? Or that their software would cause a philosophical crisis? Sure, the average day of a developer is more code writing than murder but what a thrill to author powerful new program.\n\nMachine learning, big data and AI advancements seem like a giant leap forward both for technology and human experience. In 2017 CEO’s of major companies told\nMIT’s Sloan Management Review\nthat AI, machine learning and big data will be the biggest disruptions (in a good way!) of the future.\n\nAlready the big 3 are revolutionizing industries. For example, Metlife uses these machine learning to improve speech recognition so doctors can file their patient notes in real time. Medical offices can now transfer information faster to improve decision-making and care. B2C corporations use it to analyze audiences for engagement and leverage marketing to reduce time and money on intermittent customers. B2B corporations want to analyze the massive data they collect, so they hire developers to create programs that anticipate their clients’ needs before anyone sends an order. Imagine how that might have played in the COVID 19 crisis, if manufacturers saw Google searches or subtle demand spikes for certain products. What if software helped them \"identify new local suppliers\" so they could pivot production within hours instead of weeks.\n\nSo it may not sound sexy to say that every development towards openness and transcendence in SDKs is transformative, but it is. It’s why we should celebrate Microsoft’s development vision to ramp up their Azure SQL partnerships and then to integrate Apache Spark into their .NET offerings.\n\n### A Short History of Lakes, Factories, and Analytics ###\n\nLate in 2019, Microsoft’s Azure SQL Data Warehouse got a snappy new branding, Synapse Analytics. Synapse integrated its Azure Data Lake Storage, Azure Data Factory and the popular Apache Spark. Spark, which began in 2009, is the premier big data framework. It distributes the power to crunch enormous data sets across computers through an API that eases the workload of developers. Developers love Spark because it provides native bindings across Java, Scala, Python and R programming. What was missing was .NET SDK, and Microsoft’s participation in the world of big data processing. That is until recently.\n\nIn November 2019, Microsoft released a new version of SQL Server and made it available for Linux, which open-source developers love, and don’t show any signs of abandoning. Working with the open-source community is always a step toward computing transcendence, but also something of a gamble. It offers growth and feedback from developers but also reduces ownership (thus it may affect profit). Yet when companies like Microsoft choose to transcend, everyone benefits. This time, the improvements in SQL Server 2019, which dovetailed with Azure Synapse Analytics, laid the foundation for opening up to .NET frameworks. For the time being, it empowers .NET 3.1 but when .NET 5 is released later this year, Microsoft’s capacities will expand further. .NET 5 will be a unified code with new technology enhancements.\n\n### Microsoft Moves In with Apache Spark ###\n\nThe 2019 integration of Azure SQL Data Warehouse (2015-2018) with other services, including data warehouse, data lake, machine learning, and data pipelines allows the data bricks to be bound together. Here’s how it works: Spark tables are queryable without code calling for the creation of an external table. This works at the provisioning of a Synapse cluster. The Azure Data Lake Storage (ADLS) now stores Spark SQL tables and requisitions those along with native ADLS tables. The engines powering this query integrate with Apache Parquet as well. Furthermore, Azure Synapsis  accommodates the development and execution of non-C# or other languages such as Python, Scala and native Spark SQL. The integration improves Synapse's ability to manage machine learning (it works with Spark Mllib), and makes Synapse’s studio competitive with AWS (Amazon Web Services).\n\n### Apache Spark and .NET ###\n\nWhat the world needs is for every major coding language to marry Apache Spark to its own popular frameworks. Why? Because Spark eclipses all other software for big data crunching and machine learning. Apache Spark maintains a reputation for speed compared to other software programs. It offers in-memory functions. It supports SQL along with real-time data and graph processing. If organizations need machine learning, Apache Spark enables it. It’s hard to name an industry that doesn’t employ Apache Spark. Think financial institutions, gaming, telecoms, tech giants, and government sources, which brings us to .NET news. Microsoft announced .NET for Apache Spark  with bindings for C# and F# languages.\n\nConsidering that twenty years of lines of .NET code could be unified with big data through this move, the evolution of Microsoft’s once soiled systems are crumbling. This is the opposite of an empire crumbling. Rather it’s a case study in how to build longevity and power so that one of the leading empires of software can remain powerful in the fast-changing software geography. What does this mean for .NET based software systems? First, big data analysis, with the power to stream data and enhance machine learning cannot be ignored. We live in a data-driven, data-science culture. Data science improves every enterprise. The integration of Apache Spark with .NET makes it pop. ZDNet reports that it “seems to be more than just a bundling of the open-source big data analytics framework.” It’s a “true” integration.\n\n### 2020 Developments | Microsoft ###\n\nIn Spring 2020, Microsoft added support of in-memory .NET Dataframes for and created Spark.NET. In-memory functions allow for faster management, return, and analysis of big data sets. Spark.NET boasts new convenience APIs specifically for two kinds of user-defined functions (UDFs): vector and scalar. Spark works through Arrow format, which standardizes a language-independent format for working with data in-memory. The two new APIs should speed up serialization and make data transfers more efficient. Because of these APIs, Spark.NET eliminates the overhead of converting data in and out of formats to process. Also, the APIs for vector and scalar can reduce lines of code for .NET developers  to write.\n\nIn Microsoft’s blog, Brigit Murtaugh provides several examples of how the new API’s will make for cleaner code and more efficient programs. But that’s not all that Microsoft has done to make Spark.NET accessible to coders. Andrew Brust, developer and writer for ZDNet, gives a solid run-down of all the ways that Microsoft makes it easy for developers to fire up Spark.NET. First, Microsoft provides robust onboarding guidance. Framework installation support leads to the creation of a sample application and running it. It guides developers through the required dependencies to install, the configuration steps for the framework, then the installation of Spark.NET, including the creation and execution of the Spark sample application. This is a ten- minute process. Developers who prefer to work in Visual Studio can access Spark.NET as well.\n\nWhat’s not to love? No one was murdered in the making of this union. I’m sure there’s healthy jealousy about which language and framework is best, but I cannot prove any love triangles have estranged actual humans. While .NET’s integration with Apache Spark may not solve the philosophical conundrum of determinism, it does move forward functions and capacities that transform a multitude of industries. With thousands of .NET code, now those programs can leverage the efficiency and power of big data to make transcendental changes to the industry.","metric":0.52294921880000000414,"metric_input":"Текст статьи:\nCritical Transcendence: .NET SDK and Apache Spark / Хабр                                                            \n\nWhen Alex Garland’s series Devs (on FX and Hulu) came out this year, it gave developers their own sexy Hollywood workup. Who knew that coders could get snarled into murder plots and love triangles just for designing machine learning programs? Or that their software would cause a philosophical crisis? Sure, the average day of a developer is more code writing than murder but what a thrill to author powerful new program.\n\nMachine learning, big data and AI advancements seem like a giant leap forward both for technology and human experience. In 2017 CEO’s of major companies told\nMIT’s Sloan Management Review\nthat AI, machine learning and big data will be the biggest disruptions (in a good way!) of the future.\n\nAlready the big 3 are revolutionizing industries. For example, Metlife uses these machine learning to improve speech recognition so doctors can file their patient notes in real time. Medical offices can now transfer information faster to improve decision-making and care. B2C corporations use it to analyze audiences for engagement and leverage marketing to reduce time and money on intermittent customers. B2B corporations want to analyze the massive data they collect, so they hire developers to create programs that anticipate their clients’ needs before anyone sends an order. Imagine how that might have played in the COVID 19 crisis, if manufacturers saw Google searches or subtle demand spikes for certain products. What if software helped them \"identify new local suppliers\" so they could pivot production within hours instead of weeks.\n\nSo it may not sound sexy to say that every development towards openness and transcendence in SDKs is transformative, but it is. It’s why we should celebrate Microsoft’s development vision to ramp up their Azure SQL partnerships and then to integrate Apache Spark into their .NET offerings.\n\n### A Short History of Lakes, Factories, and Analytics ###\n\nLate in 2019, Microsoft’s Azure SQL Data Warehouse got a snappy new branding, Synapse Analytics. Synapse integrated its Azure Data Lake Storage, Azure Data Factory and the popular Apache Spark. Spark, which began in 2009, is the premier big data framework. It distributes the power to crunch enormous data sets across computers through an API that eases the workload of developers. Developers love Spark because it provides native bindings across Java, Scala, Python and R programming. What was missing was .NET SDK, and Microsoft’s participation in the world of big data processing. That is until recently.\n\nIn November 2019, Microsoft released a new version of SQL Server and made it available for Linux, which open-source developers love, and don’t show any signs of abandoning. Working with the open-source community is always a step toward computing transcendence, but also something of a gamble. It offers growth and feedback from developers but also reduces ownership (thus it may affect profit). Yet when companies like Microsoft choose to transcend, everyone benefits. This time, the improvements in SQL Server 2019, which dovetailed with Azure Synapse Analytics, laid the foundation for opening up to .NET frameworks. For the time being, it empowers .NET 3.1 but when .NET 5 is released later this year, Microsoft’s capacities will expand further. .NET 5 will be a unified code with new technology enhancements.\n\n### Microsoft Moves In with Apache Spark ###\n\nThe 2019 integration of Azure SQL Data Warehouse (2015-2018) with other services, including data warehouse, data lake, machine learning, and data pipelines allows the data bricks to be bound together. Here’s how it works: Spark tables are queryable without code calling for the creation of an external table. This works at the provisioning of a Synapse cluster. The Azure Data Lake Storage (ADLS) now stores Spark SQL tables and requisitions those along with native ADLS tables. The engines powering this query integrate with Apache Parquet as well. Furthermore, Azure Synapsis  accommodates the development and execution of non-C# or other languages such as Python, Scala and native Spark SQL. The integration improves Synapse's ability to manage machine learning (it works with Spark Mllib), and makes Synapse’s studio competitive with AWS (Amazon Web Services).\n\n### Apache Spark and .NET ###\n\nWhat the world needs is for every major coding language to marry Apache Spark to its own popular frameworks. Why? Because Spark eclipses all other software for big data crunching and machine learning. Apache Spark maintains a reputation for speed compared to other software programs. It offers in-memory functions. It supports SQL along with real-time data and graph processing. If organizations need machine learning, Apache Spark enables it. It’s hard to name an industry that doesn’t employ Apache Spark. Think financial institutions, gaming, telecoms, tech giants, and government sources, which brings us to .NET news. Microsoft announced .NET for Apache Spark  with bindings for C# and F# languages.\n\nConsidering that twenty years of lines of .NET code could be unified with big data through this move, the evolution of Microsoft’s once soiled systems are crumbling. This is the opposite of an empire crumbling. Rather it’s a case study in how to build longevity and power so that one of the leading empires of software can remain powerful in the fast-changing software geography. What does this mean for .NET based software systems? First, big data analysis, with the power to stream data and enhance machine learning cannot be ignored. We live in a data-driven, data-science culture. Data science improves every enterprise. The integration of Apache Spark with .NET makes it pop. ZDNet reports that it “seems to be more than just a bundling of the open-source big data analytics framework.” It’s a “true” integration.\n\n### 2020 Developments | Microsoft ###\n\nIn Spring 2020, Microsoft added support of in-memory .NET Dataframes for and created Spark.NET. In-memory functions allow for faster management, return, and analysis of big data sets. Spark.NET boasts new convenience APIs specifically for two kinds of user-defined functions (UDFs): vector and scalar. Spark works through Arrow format, which standardizes a language-independent format for working with data in-memory. The two new APIs should speed up serialization and make data transfers more efficient. Because of these APIs, Spark.NET eliminates the overhead of converting data in and out of formats to process. Also, the APIs for vector and scalar can reduce lines of code for .NET developers  to write.\n\nIn Microsoft’s blog, Brigit Murtaugh provides several examples of how the new API’s will make for cleaner code and more efficient programs. But that’s not all that Microsoft has done to make Spark.NET accessible to coders. Andrew Brust, developer and writer for ZDNet, gives a solid run-down of all the ways that Microsoft makes it easy for developers to fire up Spark.NET. First, Microsoft provides robust onboarding guidance. Framework installation support leads to the creation of a sample application and running it. It guides developers through the required dependencies to install, the configuration steps for the framework, then the installation of Spark.NET, including the creation and execution of the Spark sample application. This is a ten- minute process. Developers who prefer to work in Visual Studio can access Spark.NET as well.\n\nWhat’s not to love? No one was murdered in the making of this union. I’m sure there’s healthy jealousy about which language and framework is best, but I cannot prove any love triangles have estranged actual humans. While .NET’s integration with Apache Spark may not solve the philosophical conundrum of determinism, it does move forward functions and capacities that transform a multitude of industries. With thousands of .NET code, now those programs can leverage the efficiency and power of big data to make transcendental changes to the industry.\n\nКраткое содержание:\nСтатья обсуждает важность интеграции Apache Spark с платформой .NET от Microsoft для улучшения анализа больших данных и машинного обучения. Microsoft представила .NET для Apache Spark с привязками для языков C# и F#, что позволяет унифицировать 20 лет кода .NET с большими данными. Это открывает новые возможности для анализа данных и улучшения машинного обучения. В 2020 году Microsoft добавила поддержку in-memory .NET Dataframes и создала Spark.NET, что ускоряет управление, возврат и анализ больших данных. Интеграция с Apache Spark делает .NET более мощным в индустрии.","summary":"Статья обсуждает важность интеграции Apache Spark с платформой .NET от Microsoft для улучшения анализа больших данных и машинного обучения. Microsoft представила .NET для Apache Spark с привязками для языков C# и F#, что позволяет унифицировать 20 лет кода .NET с большими данными. Это открывает новые возможности для анализа данных и улучшения машинного обучения. В 2020 году Microsoft добавила поддержку in-memory .NET Dataframes и создала Spark.NET, что ускоряет управление, возврат и анализ больших данных. Интеграция с Apache Spark делает .NET более мощным в индустрии."}
{"Text":"VR-платежи в 2018 году станут мейнстримом / Habr           \n\n13-12-2016\nЕще несколько лет назад платежи в виртуальной реальности казались чем-то, что произойдет совсем нескоро. Но, исходя из кейсов, описанных в сегодняшней статье, будущее VR-платежей уже очень близко. Виртуальная реальность (VR), вероятно, в потенциале одна из важнейших технологий для ритейла, если принимать во внимание стремительно растущее количество примеров ее практического применения. Некоторые эксперты предполагают, что к концу 2016 года примерно 43 миллиона человек по всему миру станут обладателями VR-продуктов, а к 2018 году эта цифра возрастет до 170 миллионов. На устойчивый интерес к VR-технологии и ее применению указывает то, что, по некоторым оценкам, за прошлый год рынок VR вырос в четыре раза. Такие гарнитуры как Oculus Rift, HTC Vive, PlayStation VR и Google Cardboard с высокой долей вероятности будут определять то, как мы будем выбирать товары и совершать покупки. «На данный момент, единственное недостающее звено в идее применения VR для активизации торговли — это платежи. Одним из первопроходцев в данной сфере стала китайская компания Alibaba, продемонстрировавшая в октябре этого года новый платежный сервис, который позволит покупателям рассчитываться за покупки в виртуальной реальности с помощью кивка головы». Лин Фэнь, ответственный за инкубатор F Lab компании Ant Financial и занимающийся разработкой платежного сервиса последние несколько месяцев, сообщил агентству «Рейтер»: «Снимать очки виртуальной реальности для совершения платежа — скучное занятие. С нашей технологией вам не придется доставать телефон». Как поясняет издание, личность пользователя может быть верифицирована в сервисе VR Pay через активные аккаунты на связанных устройствах или с помощью технологии голосовой идентификации. По мнению Лин Фэня, распознавание голоса — наиболее удобный способ идентификации из всего спектра технологий биометрической идентификации личности. Ожидается, что VR Pay будет готов к запуску в коммерческую эксплуатацию к концу этого года, что позволит 400 миллионам пользователей Alibaba покупать продукцию в магазинах по всему миру. Это станет возможным с помощью VR-шлемов или очков, предназначенных для создания «эффекта присутствия» в реальном магазине. Старший управляющий мобильного отделения Alibaba Чжуань Чжоран дал следующий комментарий: «Виртуальная реальность прекрасно подходит для демонстрации товаров или услуг, особенно для таких категорий товаров как мебель или продукты в области туризма. Заодно можно добавить немного развлечений в процесс шоппинга». Британский благотворительный фонд Comic Relief тоже выступает сторонником внедрения VR-платежей в качестве мейнстрим-технологии в сфере благотворительности. Зенон Ханник, технический директор Comic Relief, поделился своим мнением о важности «эффекта присутствия» для поддержки транзакций: «Вы должны показать аудитории, куда были потрачены деньги, тогда каждый из них почувствует себя причастным к этому процессу. Мы, как благотворительная организация, финансируем различные проекты, и наша задача рассказать их историю. Мы можем создать мощные по своему воздействию истории, как собранные нами средства способны менять жизни людей, и VR станет главным средством повествования». Для коммерческих платежей идея близка по смыслу — VR станет инструментом мощного вовлечения, который заодно реализует механизмы мгновенных платежей. На данный момент необходимость снимать гарнитуру для совершения платежа — существенный недостаток технологии. За это время клиент может усомниться в своем выборе и отказаться от покупки. Преодолеть это препятствие и сохранить вовлеченность потенциального клиента поможет VR, а компании таким образом снизят риск возможной потери интереса к товару и неудовлетворенного покупательского спроса. Как сообщается в журнале Computing, несмотря на то, что Зенон Ханник из Comic Relief ясно видит VR в качестве будущей платформы для благотворительности, он отмечает, что технология не получит широкого распространения как минимум до 2018 года. «2017 станет для VR годом этакого разочарования. Конечно, в следующем году будет парочка интересных историй перспективных разработок, но только в 2018 году виртуальная реальность станет основной темой для обсуждения в СМИ». Среди других событий в сегменте — сотрудничество компании Mastercard со стартапом Wearality из Орландо, США. Wearality занимается разработкой очков виртуальной реальности и носимых устройств, которые позволят покупателям знакомиться с товарами и приобретать их, не покидая виртуального мира. В марте этого года MasterCard продемонстрировала будущее торговли, завязанное на виртуальную и дополненную реальность, а также носимые устройства с возможностью проведения платежей — на гольф-турнире Arnold Palmer Invitational во Флориде. В официальном пресс-релизе компания заявила, что в ближайшее время любители гольфа смогут приобретать снаряжение Graeme McDowell’s и одежду брэнда G-Mac прямо во время виртуального турнира с Грэмом МакДауэллом. Находясь на поле, игроки смогут приобретать напитки в точках продаж с помощью простого касания перчаткой для гольфа. Кошелек с деньгами не потребуется. Учитывая, что к 2020 году пользователи предположительно потратят не менее 7,9 млрд. долларов на VR-гарнитуры и 3,3 млрд. долларов на VR-развлечения, VR-платежи представляются логичным и необходимым этапом развития для ритейла и финансовых учреждений. Носимые устройства будут играть здесь ключевую роль, соединяя платежное решение с предметами виртуального мира. Именно поэтому встраивание функций осуществления платежей в устройства, производимые крупными технологическими компаниями — весьма дальновидный шаг. И одним из пионеров в этом деле стала компания Samsung. В апреле этого года новостной портал VR Life приводил слова руководителя Samsung Pay Europe Натали Oстманн, которая подчеркнула, что компания может предложить людям посещать магазины с помощью VR и просматривать интересующие товары в виртуальной реальности. Пользователи одним кликом смогут выбрать нужный товар и оплатить его с помощью Samsung Pay, оставаясь внутри виртуальной среды. Решение позволит с легкостью находить новые торговые точки и магазины. Если данная функция будет в конечном итоге работать именно таким образом, это станет уникальной чертой VR-технологии Samsung и ее платежного сервиса. Продолжайте следить за обновлениями блога процессинговой компании PayOnline и оставайтесь в курсе самых современных технологий платежной индустрии.\n","metric":0.46997070309999999793,"metric_input":"Текст статьи:\nVR-платежи в 2018 году станут мейнстримом / Habr           \n\n13-12-2016\nЕще несколько лет назад платежи в виртуальной реальности казались чем-то, что произойдет совсем нескоро. Но, исходя из кейсов, описанных в сегодняшней статье, будущее VR-платежей уже очень близко. Виртуальная реальность (VR), вероятно, в потенциале одна из важнейших технологий для ритейла, если принимать во внимание стремительно растущее количество примеров ее практического применения. Некоторые эксперты предполагают, что к концу 2016 года примерно 43 миллиона человек по всему миру станут обладателями VR-продуктов, а к 2018 году эта цифра возрастет до 170 миллионов. На устойчивый интерес к VR-технологии и ее применению указывает то, что, по некоторым оценкам, за прошлый год рынок VR вырос в четыре раза. Такие гарнитуры как Oculus Rift, HTC Vive, PlayStation VR и Google Cardboard с высокой долей вероятности будут определять то, как мы будем выбирать товары и совершать покупки. «На данный момент, единственное недостающее звено в идее применения VR для активизации торговли — это платежи. Одним из первопроходцев в данной сфере стала китайская компания Alibaba, продемонстрировавшая в октябре этого года новый платежный сервис, который позволит покупателям рассчитываться за покупки в виртуальной реальности с помощью кивка головы». Лин Фэнь, ответственный за инкубатор F Lab компании Ant Financial и занимающийся разработкой платежного сервиса последние несколько месяцев, сообщил агентству «Рейтер»: «Снимать очки виртуальной реальности для совершения платежа — скучное занятие. С нашей технологией вам не придется доставать телефон». Как поясняет издание, личность пользователя может быть верифицирована в сервисе VR Pay через активные аккаунты на связанных устройствах или с помощью технологии голосовой идентификации. По мнению Лин Фэня, распознавание голоса — наиболее удобный способ идентификации из всего спектра технологий биометрической идентификации личности. Ожидается, что VR Pay будет готов к запуску в коммерческую эксплуатацию к концу этого года, что позволит 400 миллионам пользователей Alibaba покупать продукцию в магазинах по всему миру. Это станет возможным с помощью VR-шлемов или очков, предназначенных для создания «эффекта присутствия» в реальном магазине. Старший управляющий мобильного отделения Alibaba Чжуань Чжоран дал следующий комментарий: «Виртуальная реальность прекрасно подходит для демонстрации товаров или услуг, особенно для таких категорий товаров как мебель или продукты в области туризма. Заодно можно добавить немного развлечений в процесс шоппинга». Британский благотворительный фонд Comic Relief тоже выступает сторонником внедрения VR-платежей в качестве мейнстрим-технологии в сфере благотворительности. Зенон Ханник, технический директор Comic Relief, поделился своим мнением о важности «эффекта присутствия» для поддержки транзакций: «Вы должны показать аудитории, куда были потрачены деньги, тогда каждый из них почувствует себя причастным к этому процессу. Мы, как благотворительная организация, финансируем различные проекты, и наша задача рассказать их историю. Мы можем создать мощные по своему воздействию истории, как собранные нами средства способны менять жизни людей, и VR станет главным средством повествования». Для коммерческих платежей идея близка по смыслу — VR станет инструментом мощного вовлечения, который заодно реализует механизмы мгновенных платежей. На данный момент необходимость снимать гарнитуру для совершения платежа — существенный недостаток технологии. За это время клиент может усомниться в своем выборе и отказаться от покупки. Преодолеть это препятствие и сохранить вовлеченность потенциального клиента поможет VR, а компании таким образом снизят риск возможной потери интереса к товару и неудовлетворенного покупательского спроса. Как сообщается в журнале Computing, несмотря на то, что Зенон Ханник из Comic Relief ясно видит VR в качестве будущей платформы для благотворительности, он отмечает, что технология не получит широкого распространения как минимум до 2018 года. «2017 станет для VR годом этакого разочарования. Конечно, в следующем году будет парочка интересных историй перспективных разработок, но только в 2018 году виртуальная реальность станет основной темой для обсуждения в СМИ». Среди других событий в сегменте — сотрудничество компании Mastercard со стартапом Wearality из Орландо, США. Wearality занимается разработкой очков виртуальной реальности и носимых устройств, которые позволят покупателям знакомиться с товарами и приобретать их, не покидая виртуального мира. В марте этого года MasterCard продемонстрировала будущее торговли, завязанное на виртуальную и дополненную реальность, а также носимые устройства с возможностью проведения платежей — на гольф-турнире Arnold Palmer Invitational во Флориде. В официальном пресс-релизе компания заявила, что в ближайшее время любители гольфа смогут приобретать снаряжение Graeme McDowell’s и одежду брэнда G-Mac прямо во время виртуального турнира с Грэмом МакДауэллом. Находясь на поле, игроки смогут приобретать напитки в точках продаж с помощью простого касания перчаткой для гольфа. Кошелек с деньгами не потребуется. Учитывая, что к 2020 году пользователи предположительно потратят не менее 7,9 млрд. долларов на VR-гарнитуры и 3,3 млрд. долларов на VR-развлечения, VR-платежи представляются логичным и необходимым этапом развития для ритейла и финансовых учреждений. Носимые устройства будут играть здесь ключевую роль, соединяя платежное решение с предметами виртуального мира. Именно поэтому встраивание функций осуществления платежей в устройства, производимые крупными технологическими компаниями — весьма дальновидный шаг. И одним из пионеров в этом деле стала компания Samsung. В апреле этого года новостной портал VR Life приводил слова руководителя Samsung Pay Europe Натали Oстманн, которая подчеркнула, что компания может предложить людям посещать магазины с помощью VR и просматривать интересующие товары в виртуальной реальности. Пользователи одним кликом смогут выбрать нужный товар и оплатить его с помощью Samsung Pay, оставаясь внутри виртуальной среды. Решение позволит с легкостью находить новые торговые точки и магазины. Если данная функция будет в конечном итоге работать именно таким образом, это станет уникальной чертой VR-технологии Samsung и ее платежного сервиса. Продолжайте следить за обновлениями блога процессинговой компании PayOnline и оставайтесь в курсе самых современных технологий платежной индустрии.\n\n\nКраткое содержание:\nВ статье говорится о том, что к концу 2016 года примерно 43 миллиона человек по всему миру станут обладателями VR-продуктов, а к 2018 году эта цифра возрастет до 170 миллионов. Рынок VR вырос в четыре раза за прошлый год. К концу этого года ожидается запуск VR Pay, платежного сервиса для VR-платежей. В марте этого года компания Mastercard продемонстрировала будущее торговли, завязанное на виртуальную и дополненную реальность, а также носимые устройства с возможностью проведения платежей. В апреле этого года руководитель Samsung Pay Europe Натали Oстманн подчеркнула, что компания может предложить людям посещать магазины с помощью VR и просматривать интересующие товары в виртуальной реальности. Решение позволит с легкостью находить новые торговые точки и магазины.","summary":"В статье говорится о том, что к концу 2016 года примерно 43 миллиона человек по всему миру станут обладателями VR-продуктов, а к 2018 году эта цифра возрастет до 170 миллионов. Рынок VR вырос в четыре раза за прошлый год. К концу этого года ожидается запуск VR Pay, платежного сервиса для VR-платежей. В марте этого года компания Mastercard продемонстрировала будущее торговли, завязанное на виртуальную и дополненную реальность, а также носимые устройства с возможностью проведения платежей. В апреле этого года руководитель Samsung Pay Europe Натали Oстманн подчеркнула, что компания может предложить людям посещать магазины с помощью VR и просматривать интересующие товары в виртуальной реальности. Решение позволит с легкостью находить новые торговые точки и магазины."}
{"Text":"Android Vitals — Почему запустился мой процесс? / Habr             \n\n10-03-2022\nЭта серия статей посвящена мониторингу производительности и стабильности работающих Android-приложений. На прошлой неделе я писал о том, как определить, является ли запуск приложения холодным. Если мы постим сообщение, и в момент его выполнения не будет создано никаких активити, то так мы поймем, что это не холодный запуск, даже если в конечном итоге активити запустится через 20 секунд. class MyApp : Application() { override fun onCreate() { super.onCreate() var firstActivityCreated = false registerActivityLifecycleCallbacks(object : ActivityLifecycleCallbacks { override fun onActivityCreated( activity: Activity, savedInstanceState: Bundle? ) { if (firstActivityCreated) { return } firstActivityCreated = true } }) Handler().post { if (firstActivityCreated) { // TODO рапортуем о холодном запуске } } } } Такой подход предполагает, что мы должны дождаться запуска активити или выполнения этого сообщения, прежде чем узнаем, был ли запуск приложения холодным. Иногда было бы полезно знать это прямо внутри Application.onCreate(). Например, нам может понадобиться асинхронная предварительная загрузка ресурсов для оптимизации холодного запуска: class MyApp : Application() { override fun onCreate() { super.onCreate() if (isColdStart()) { preloadDataForUiAsync() } } } Важность процесса Хоть пока и нет Android API для того, чтобы узнать, почему процесс был запущен, у нас все-таки есть способ узнать, почему процесс все еще работает: RunningAppProcessInfo.importance, которое мы можем прочитать из ActivityManager.getMyMemoryState(). Согласно документации “Процессы и жизненный цикл приложений”: Чтобы определить, какие процессы следует завершить в случае нехватки памяти, Android помещает каждый процесс в “иерархию важности” на основе запущенных в них компонентов и их состояний. [...] Система будет основывать свое решение о том, как классифицировать процесс, на наиболее важном уровне, обнаруженном среди всех компонентов, активных в процессе в данный момент. Когда процесс запускается, мы можем проверить его важность. Если его важность оценена как IMPORTANCE_FOREGROUND, то это холодный запуск: class MyApp : Application() { override fun onCreate() { super.onCreate() if (isForegroundProcess()) { preloadDataForUiAsync() } } private fun isForegroundProcess(): Boolean { val processInfo = ActivityManager.RunningAppProcessInfo() ActivityManager.getMyMemoryState(processInfo) return processInfo.importance == IMPORTANCE_FOREGROUND } } Подтверждение данными Я реализовал оба подхода определения холодного запуска в тестовом приложении и получил идентичные результаты во всех случаях. Затем я добавил код обнаружения в приложение в продакшене с достаточным количеством установок, чтобы обеспечить значимые результаты. Вот полученные мной (анонимизированные) результаты: На этой круговой диаграмме отражены только запуски приложений, в которых активити было создано до первого поста. 97,4% имели важность 100, т.е. IMPORTANCE_FOREGROUND. 2,4% имели важность 400, т.е. IMPORTANCE_CACHED (ранее называлось IMPORTANCE_BACKGROUND). 0,2% соответствуют другим значениям. Давайте посмотрим на эти данные под другим углом: как часто активити создавалось до первого поста по каждому значению важности? Когда важность запуска равна 100, перед первым постом всегда создается активити. И когда активити создается до первого сообщения, в 97,4% случаев важность равна 100. Когда важность запуска равна 400, активити не создается до первого поста почти никогда. “Почти никогда” это не “никогда” — все же достаточно случаев, когда активити создается до первого поста. В 2,4% случаев важность равна 400. Наиболее вероятное объяснение 2,4% с важностью 400 заключается в том, что это были не холодные запуски, однако система получила запрос на запуск активити почти сразу после запуска процесса, прямо на запуске Application.onCreate(), но до того, как у нас появилась возможность добавить наш первый пост. Поправка: я сохранял в логе важность после первого поста и сравнивал ее с важностью на запуске приложения. Мои данные показали, что в 74% запусков приложений, когда активити было создано до первого поста, и важность процесса запуска изначально не была равна 100, после первого поста значение важности этого процесса менялось на 100. Мне кажется что, это подтверждает теорию о том, что система решила запустить активити после того, как приложение уже начало запускаться. Заключение Теперь на основе этой информации и выводов из предыдущего поста мы можем точно определить холодный запуск. Для холодного запуска: Важность процесса соответствует IMPORTANCE_FOREGROUND на запуске приложения. Первое активити создается до того, как первый пост будет исключен из очереди. Первое активити создается с нулевым бандлом. Вот обновленный код: class MyApp : Application() { override fun onCreate() { super.onCreate() if (isForegroundProcess()) { var firstPostEnqueued = true Handler().post { firstPostEnqueued = false } registerActivityLifecycleCallbacks(object : ActivityLifecycleCallbacks { override fun onActivityCreated( activity: Activity, savedInstanceState: Bundle? ) { unregisterActivityLifecycleCallbacks(this) if (firstPostEnqueued && savedInstanceState == null) { // TODO рапортуем о холодном запуске } } }) } } private fun isForegroundProcess(): Boolean { val processInfo = ActivityManager.RunningAppProcessInfo() ActivityManager.getMyMemoryState(processInfo) return processInfo.importance == IMPORTANCE_FOREGROUND } } Надеюсь, для вас эти исследования были интересными! Всех желающих приглашаем на двухдневный интенсив «Animated Vector Drawable», на котором поговорим про векторные изображения в Android и о том, как shapeshifter упрощает жизнь с ними. Регистрация доступна по ссылке.\n","metric":0.48266601559999999793,"metric_input":"Текст статьи:\nAndroid Vitals — Почему запустился мой процесс? / Habr             \n\n10-03-2022\nЭта серия статей посвящена мониторингу производительности и стабильности работающих Android-приложений. На прошлой неделе я писал о том, как определить, является ли запуск приложения холодным. Если мы постим сообщение, и в момент его выполнения не будет создано никаких активити, то так мы поймем, что это не холодный запуск, даже если в конечном итоге активити запустится через 20 секунд. class MyApp : Application() { override fun onCreate() { super.onCreate() var firstActivityCreated = false registerActivityLifecycleCallbacks(object : ActivityLifecycleCallbacks { override fun onActivityCreated( activity: Activity, savedInstanceState: Bundle? ) { if (firstActivityCreated) { return } firstActivityCreated = true } }) Handler().post { if (firstActivityCreated) { // TODO рапортуем о холодном запуске } } } } Такой подход предполагает, что мы должны дождаться запуска активити или выполнения этого сообщения, прежде чем узнаем, был ли запуск приложения холодным. Иногда было бы полезно знать это прямо внутри Application.onCreate(). Например, нам может понадобиться асинхронная предварительная загрузка ресурсов для оптимизации холодного запуска: class MyApp : Application() { override fun onCreate() { super.onCreate() if (isColdStart()) { preloadDataForUiAsync() } } } Важность процесса Хоть пока и нет Android API для того, чтобы узнать, почему процесс был запущен, у нас все-таки есть способ узнать, почему процесс все еще работает: RunningAppProcessInfo.importance, которое мы можем прочитать из ActivityManager.getMyMemoryState(). Согласно документации “Процессы и жизненный цикл приложений”: Чтобы определить, какие процессы следует завершить в случае нехватки памяти, Android помещает каждый процесс в “иерархию важности” на основе запущенных в них компонентов и их состояний. [...] Система будет основывать свое решение о том, как классифицировать процесс, на наиболее важном уровне, обнаруженном среди всех компонентов, активных в процессе в данный момент. Когда процесс запускается, мы можем проверить его важность. Если его важность оценена как IMPORTANCE_FOREGROUND, то это холодный запуск: class MyApp : Application() { override fun onCreate() { super.onCreate() if (isForegroundProcess()) { preloadDataForUiAsync() } } private fun isForegroundProcess(): Boolean { val processInfo = ActivityManager.RunningAppProcessInfo() ActivityManager.getMyMemoryState(processInfo) return processInfo.importance == IMPORTANCE_FOREGROUND } } Подтверждение данными Я реализовал оба подхода определения холодного запуска в тестовом приложении и получил идентичные результаты во всех случаях. Затем я добавил код обнаружения в приложение в продакшене с достаточным количеством установок, чтобы обеспечить значимые результаты. Вот полученные мной (анонимизированные) результаты: На этой круговой диаграмме отражены только запуски приложений, в которых активити было создано до первого поста. 97,4% имели важность 100, т.е. IMPORTANCE_FOREGROUND. 2,4% имели важность 400, т.е. IMPORTANCE_CACHED (ранее называлось IMPORTANCE_BACKGROUND). 0,2% соответствуют другим значениям. Давайте посмотрим на эти данные под другим углом: как часто активити создавалось до первого поста по каждому значению важности? Когда важность запуска равна 100, перед первым постом всегда создается активити. И когда активити создается до первого сообщения, в 97,4% случаев важность равна 100. Когда важность запуска равна 400, активити не создается до первого поста почти никогда. “Почти никогда” это не “никогда” — все же достаточно случаев, когда активити создается до первого поста. В 2,4% случаев важность равна 400. Наиболее вероятное объяснение 2,4% с важностью 400 заключается в том, что это были не холодные запуски, однако система получила запрос на запуск активити почти сразу после запуска процесса, прямо на запуске Application.onCreate(), но до того, как у нас появилась возможность добавить наш первый пост. Поправка: я сохранял в логе важность после первого поста и сравнивал ее с важностью на запуске приложения. Мои данные показали, что в 74% запусков приложений, когда активити было создано до первого поста, и важность процесса запуска изначально не была равна 100, после первого поста значение важности этого процесса менялось на 100. Мне кажется что, это подтверждает теорию о том, что система решила запустить активити после того, как приложение уже начало запускаться. Заключение Теперь на основе этой информации и выводов из предыдущего поста мы можем точно определить холодный запуск. Для холодного запуска: Важность процесса соответствует IMPORTANCE_FOREGROUND на запуске приложения. Первое активити создается до того, как первый пост будет исключен из очереди. Первое активити создается с нулевым бандлом. Вот обновленный код: class MyApp : Application() { override fun onCreate() { super.onCreate() if (isForegroundProcess()) { var firstPostEnqueued = true Handler().post { firstPostEnqueued = false } registerActivityLifecycleCallbacks(object : ActivityLifecycleCallbacks { override fun onActivityCreated( activity: Activity, savedInstanceState: Bundle? ) { unregisterActivityLifecycleCallbacks(this) if (firstPostEnqueued && savedInstanceState == null) { // TODO рапортуем о холодном запуске } } }) } } private fun isForegroundProcess(): Boolean { val processInfo = ActivityManager.RunningAppProcessInfo() ActivityManager.getMyMemoryState(processInfo) return processInfo.importance == IMPORTANCE_FOREGROUND } } Надеюсь, для вас эти исследования были интересными! Всех желающих приглашаем на двухдневный интенсив «Animated Vector Drawable», на котором поговорим про векторные изображения в Android и о том, как shapeshifter упрощает жизнь с ними. Регистрация доступна по ссылке.\n\n\nКраткое содержание:\nЭта статья посвящена мониторингу производительности и стабильности Android-приложений. Автор рассматривает два подхода к определению холодного запуска приложения: первый предполагает ожидание создания активити, второй - определение важности процесса.\n\nПервый подход основан на ожидании создания активити после запуска приложения. Если активити не создается сразу, то считается, что это холодный запуск, даже если в конечном итоге активити будет создана через некоторое время.\n\nВторой подход основан на определении важности процесса на основе его состояния и компонентов, которые в нем активны. Если важность процесса оценена как IMPORTANCE_FOREGROUND, то это холодный запуск.\n\nАвтор реализовал оба подхода в тестовом приложении и получил идентичные результаты. Затем он добавил код обнаружения в приложение в продакшене и получил следующие результаты: 97,4% запусков имеют важность 100, т.е. IMPORTANCE_FOREGROUND. 2,4% имеют важность 400, т.е. IMPORTANCE_CACHED (ранее называлось IMPORTANCE_BACKGROUND). 0,2% соответствуют другим значениям.\n\nЭти данные показывают, что когда важность запуска равна 100, перед первым постом всегда создается активити. А когда важность запуска равна 400, активити создается почти никогда.\n\nТаким образом, можно точно определить холодный запуск приложения, используя важность процесса и создание активити до первого поста.","summary":"Эта статья посвящена мониторингу производительности и стабильности Android-приложений. Автор рассматривает два подхода к определению холодного запуска приложения: первый предполагает ожидание создания активити, второй - определение важности процесса.\n\nПервый подход основан на ожидании создания активити после запуска приложения. Если активити не создается сразу, то считается, что это холодный запуск, даже если в конечном итоге активити будет создана через некоторое время.\n\nВторой подход основан на определении важности процесса на основе его состояния и компонентов, которые в нем активны. Если важность процесса оценена как IMPORTANCE_FOREGROUND, то это холодный запуск.\n\nАвтор реализовал оба подхода в тестовом приложении и получил идентичные результаты. Затем он добавил код обнаружения в приложение в продакшене и получил следующие результаты: 97,4% запусков имеют важность 100, т.е. IMPORTANCE_FOREGROUND. 2,4% имеют важность 400, т.е. IMPORTANCE_CACHED (ранее называлось IMPORTANCE_BACKGROUND). 0,2% соответствуют другим значениям.\n\nЭти данные показывают, что когда важность запуска равна 100, перед первым постом всегда создается активити. А когда важность запуска равна 400, активити создается почти никогда.\n\nТаким образом, можно точно определить холодный запуск приложения, используя важность процесса и создание активити до первого поста."}
{"Text":"Подготовка к собеседованиям по PHP с использованием тестов (phpt) из исходников PHP / Habr                        \n\n11-06-2016\nПри ручной сборке PHP (в данном случае рассматриваю версию 7.0.7) необходимо запустить команду make test перед make install, которая прогоняет все тесты в папке tests, после чего можно из командной строки отправить результат. Если просмотреть данную папку, то в ней сразу бросаются в глаза папки с наименованием classes, func, basic и т.д… Почему это интересно? Дело в том, что на собеседованиях часто любят задавать вопросы как касающиеся каких — то синтаксических (довольно скучных задач (кросворды), вроде ++$i/$i++ находящихся в общем выражении), так и общих, например, ООП. Популярные моменты — это наследование интерфейсов, абстрактных классов, суть которых в общем то нарушить правильный ООП или выявить на особенностях (возможностях) глубину познания кандидатом. Вот именно эти моменты окрас и проверяют тесты, и поэтому, довольно полезно на мой взгляд просмотреть бегло тесты (в тестах пишется ожидаемый результат). Для большей убедительности попробовать выполнить аналогичный код. Прогуглить и узнать почему именно так реализовано (срабатывает) в PHP, а не по другому. Вот для примера, базовый вопрос на собеседованиях и тест из исходников. Файл: tests/classes/abstract_by_interface_001.phpt --TEST-- ZE2 An abstract method may not be called --FILE-- <?php class Root { } interface MyInterface { function MyInterfaceFunc(); } abstract class Derived extends Root implements MyInterface { } class Leaf extends Derived { function MyInterfaceFunc() {} } var_dump(new Leaf); class Fails extends Root implements MyInterface { } ?> ===DONE=== --EXPECTF-- object(Leaf)#%d (0) { } Fatal error: Class Fails contains 1 abstract method and must therefore be declared abstract or implement the remaining methods (MyInterface::MyInterfaceFunc) in %sabstract_by_interface_001.php on line %d Как видно из описания, класс Fails упадет в фатальную ошибку, так как наследовал интерфейс MyInterface, но не произвел определение тела функции из интерфейса MyInterfaceFunc(). См. документацию по интерфейсам. Перейдем, к следующем тесту: Файл: tests/classes/abstract_by_interface_002.phpt В данном случае этот же интерфейс MyInterface, имеет определение сигнатуры метода, объявленного как static. interface MyInterface { static function MyInterfaceFunc(); } .... class Fails extends Root implements MyInterface { } ?> ===DONE=== --EXPECTF-- object(Leaf)#%d (0) { } Fatal error: Class Fails contains 1 abstract method and must therefore be declared abstract or implement the remaining methods (MyInterface::MyInterfaceFunc) in %sabstract_by_interface_002.php on line %d Как видно из ожидаемой ошибки, в данном случае, метод также должен быть определен в классе, наследующий данный интерфейс. Или вот довольно частый вопрос, о том, можно ли в обычном классе определить абстрактный метод. Файл: tests/classes/abstract_derived.phpt --TEST-- ZE2 A derived class with an abstract method must be abstract Производный класс с абстрактным методом должен быть абстрактным --SKIPIF-- <?php if (version_compare(zend_version(), '2.0.0-dev', '<')) die('skip ZendEngine 2 needed'); ?> --FILE-- <?php class base { } class derived extends base { abstract function show(); } ?> ===DONE=== <?php exit(0); ?> --EXPECTF-- Fatal error: Class derived contains 1 abstract method and must therefore be declared abstract or implement the remaining methods (derived::show) in %sabstract_derived.php on line %d Или вот один замечательный тест: Файл: tests/classes/abstract_redeclare.phpt <?php class pass { function show() { echo \"Call to function show()\\n\"; } } class fail extends pass { abstract function show(); } echo \"Done\\n\"; // Shouldn't be displayed ?> Метод show(), не может быть переопределен абстрактным. Перейдем в каталог тестов func и как, пример, тест с использованием static и постфиксным инкрементом/декрементом. Файл: tests/func/002.phpt --TEST-- Static variables in functions --FILE-- <?php function blah() { static $hey=0,$yo=0; echo \"hey=\".$hey++.\", \",$yo--.\"\\n\"; } blah(); blah(); blah(); if (isset($hey) || isset($yo)) { echo \"Local variables became global :(\\n\"; } --EXPECT-- hey=0, 0 hey=1, -1 hey=2, -2 Напоследок, чтобы не сильно Вас утомлять, приведу интересный тест, по баге 28800 --TEST-- Bug #28800 (Incorrect string to number conversion for strings starting with 'inf') --FILE-- <?php $strings = array('into', 'info', 'inf', 'infinity', 'infin', 'inflammable'); foreach ($strings as $v) { echo ($v+0).\"\\n\"; } ?> --EXPECT-- На этом все, спасибо за отнятое время.\n","metric":0.5419921875,"metric_input":"Текст статьи:\nПодготовка к собеседованиям по PHP с использованием тестов (phpt) из исходников PHP / Habr                        \n\n11-06-2016\nПри ручной сборке PHP (в данном случае рассматриваю версию 7.0.7) необходимо запустить команду make test перед make install, которая прогоняет все тесты в папке tests, после чего можно из командной строки отправить результат. Если просмотреть данную папку, то в ней сразу бросаются в глаза папки с наименованием classes, func, basic и т.д… Почему это интересно? Дело в том, что на собеседованиях часто любят задавать вопросы как касающиеся каких — то синтаксических (довольно скучных задач (кросворды), вроде ++$i/$i++ находящихся в общем выражении), так и общих, например, ООП. Популярные моменты — это наследование интерфейсов, абстрактных классов, суть которых в общем то нарушить правильный ООП или выявить на особенностях (возможностях) глубину познания кандидатом. Вот именно эти моменты окрас и проверяют тесты, и поэтому, довольно полезно на мой взгляд просмотреть бегло тесты (в тестах пишется ожидаемый результат). Для большей убедительности попробовать выполнить аналогичный код. Прогуглить и узнать почему именно так реализовано (срабатывает) в PHP, а не по другому. Вот для примера, базовый вопрос на собеседованиях и тест из исходников. Файл: tests/classes/abstract_by_interface_001.phpt --TEST-- ZE2 An abstract method may not be called --FILE-- <?php class Root { } interface MyInterface { function MyInterfaceFunc(); } abstract class Derived extends Root implements MyInterface { } class Leaf extends Derived { function MyInterfaceFunc() {} } var_dump(new Leaf); class Fails extends Root implements MyInterface { } ?> ===DONE=== --EXPECTF-- object(Leaf)#%d (0) { } Fatal error: Class Fails contains 1 abstract method and must therefore be declared abstract or implement the remaining methods (MyInterface::MyInterfaceFunc) in %sabstract_by_interface_001.php on line %d Как видно из описания, класс Fails упадет в фатальную ошибку, так как наследовал интерфейс MyInterface, но не произвел определение тела функции из интерфейса MyInterfaceFunc(). См. документацию по интерфейсам. Перейдем, к следующем тесту: Файл: tests/classes/abstract_by_interface_002.phpt В данном случае этот же интерфейс MyInterface, имеет определение сигнатуры метода, объявленного как static. interface MyInterface { static function MyInterfaceFunc(); } .... class Fails extends Root implements MyInterface { } ?> ===DONE=== --EXPECTF-- object(Leaf)#%d (0) { } Fatal error: Class Fails contains 1 abstract method and must therefore be declared abstract or implement the remaining methods (MyInterface::MyInterfaceFunc) in %sabstract_by_interface_002.php on line %d Как видно из ожидаемой ошибки, в данном случае, метод также должен быть определен в классе, наследующий данный интерфейс. Или вот довольно частый вопрос, о том, можно ли в обычном классе определить абстрактный метод. Файл: tests/classes/abstract_derived.phpt --TEST-- ZE2 A derived class with an abstract method must be abstract Производный класс с абстрактным методом должен быть абстрактным --SKIPIF-- <?php if (version_compare(zend_version(), '2.0.0-dev', '<')) die('skip ZendEngine 2 needed'); ?> --FILE-- <?php class base { } class derived extends base { abstract function show(); } ?> ===DONE=== <?php exit(0); ?> --EXPECTF-- Fatal error: Class derived contains 1 abstract method and must therefore be declared abstract or implement the remaining methods (derived::show) in %sabstract_derived.php on line %d Или вот один замечательный тест: Файл: tests/classes/abstract_redeclare.phpt <?php class pass { function show() { echo \"Call to function show()\\n\"; } } class fail extends pass { abstract function show(); } echo \"Done\\n\"; // Shouldn't be displayed ?> Метод show(), не может быть переопределен абстрактным. Перейдем в каталог тестов func и как, пример, тест с использованием static и постфиксным инкрементом/декрементом. Файл: tests/func/002.phpt --TEST-- Static variables in functions --FILE-- <?php function blah() { static $hey=0,$yo=0; echo \"hey=\".$hey++.\", \",$yo--.\"\\n\"; } blah(); blah(); blah(); if (isset($hey) || isset($yo)) { echo \"Local variables became global :(\\n\"; } --EXPECT-- hey=0, 0 hey=1, -1 hey=2, -2 Напоследок, чтобы не сильно Вас утомлять, приведу интересный тест, по баге 28800 --TEST-- Bug #28800 (Incorrect string to number conversion for strings starting with 'inf') --FILE-- <?php $strings = array('into', 'info', 'inf', 'infinity', 'infin', 'inflammable'); foreach ($strings as $v) { echo ($v+0).\"\\n\"; } ?> --EXPECT-- На этом все, спасибо за отнятое время.\n\n\nКраткое содержание:\nСтатья рассматривает процесс подготовки к собеседованиям по PHP с использованием тестов (phpt) из исходников PHP. Автор предлагает просмотреть бегло тесты в папке tests, чтобы получить представление о вопросах, которые могут быть заданы на собеседованиях. Примеры тестов включают вопросы, связанные с наследованием интерфейсов, абстрактных классов и статических методов. Также приводятся примеры тестов, связанных с абстрактными методами, переопределением функций и багами в PHP. В целом, статья предоставляет полезные советы и примеры тестов для подготовки к собеседованиям по PHP.","summary":"Статья рассматривает процесс подготовки к собеседованиям по PHP с использованием тестов (phpt) из исходников PHP. Автор предлагает просмотреть бегло тесты в папке tests, чтобы получить представление о вопросах, которые могут быть заданы на собеседованиях. Примеры тестов включают вопросы, связанные с наследованием интерфейсов, абстрактных классов и статических методов. Также приводятся примеры тестов, связанных с абстрактными методами, переопределением функций и багами в PHP. В целом, статья предоставляет полезные советы и примеры тестов для подготовки к собеседованиям по PHP."}
{"Text":"Беспростойная миграция RabbitMQ в Kubernetes / Habr                \n\n06-05-2019\nRabbitMQ – написанный на языке Erlang брокер сообщений, позволяющий организовать отказоустойчивый кластер с полной репликацией данных на несколько узлов, где каждый узел может обслуживать запросы на чтение и запись. Имея в production-эксплуатации множество кластеров Kubernetes, мы поддерживаем большое количество инсталляций RabbitMQ и столкнулись с необходимостью миграции данных из одного кластера в другой без простоя. Данная операция была необходима нам как минимум в двух случаях: Перенос данных из кластера RabbitMQ, находящегося не в Kubernetes, в новый — уже «кубернетезированный» (т.е. функционирующий в pod’ах K8s) — кластер. Миграция RabbitMQ в рамках Kubernetes из одного namespace в другой (например, если контуры разграничены пространствами имён, то для переноса инфраструктуры из одного контура в другой). Предлагаемый в статье рецепт ориентирован на ситуации (но вовсе не ограничен ими), в которых есть старый кластер RabbitMQ (например, из 3 узлов), находящийся либо уже в K8s, либо на каких-то старых серверах. С ним работает приложение, размещённое в Kubernetes (уже там или в перспективе): … и перед нами стоит задача его миграции в новый production в Kubernetes. Сначала будет описан общий подход к самой миграции, а уже после этого — технические детали по её реализации. Алгоритм миграции Первый, предварительный, этап перед какими-либо действиями — проверка, что в старой инсталляции RabbitMQ включён режим высокой доступности ( HA ). Причина очевидна — мы ведь не хотим потерять каких-либо данных. Чтобы осуществить эту проверку, можно зайти в админку RabbitMQ и во вкладке Admin → Policies убедиться, что установлено значение ha-mode: all : Следующий шаг — поднимаем новый кластер RabbitMQ в pod’ах Kubernetes (в нашем случае, например, состоящий из 3 узлов, но их число может быть и другим). После этого мы объединяем старый и новый кластеры RabbitMQ, получая единственный кластер (из 6 узлов): Инициируется процесс синхронизации данных между старым и новым кластерами RabbitMQ. После того, как все данные синхронизируются между всеми узлами в кластере, мы можем переключить приложение на использование нового кластера: После этих операций достаточно вывести из кластера RabbitMQ старые узлы, и переезд можно считать завершённым: Эту схему мы неоднократно применяли у нас в production. Однако для собственного удобства реализовали её в рамках специализированной системы, распространяющей типовые конфигурации RMQ на множествах кластеров Kubernetes (для тех, кому любопытно: речь идёт про addon-operator, о котором мы совсем недавно рассказывали) . Ниже будут представлены отдельно взятые инструкции, которые каждый может применить на своих инсталляциях, чтобы попробовать предлагаемое решение в действии. Пробуем на практике Требования Реквизиты очень просты: Кластер Kubernetes (подойдет и minikube); Кластер RabbitMQ (может быть и развернут на bare metal, и сделан как обычный кластер в Kubernetes из официального Helm-чарта). Для описанного ниже примера я развернул RMQ в Kubernetes и назвал его rmq-old . Подготовка стенда 1. Скачаем Helm-чарт и немного отредактируем его: helm fetch --untar stable/rabbitmq-ha Для удобства задаём пароль, ErlangCookie и делаем политику ha-all , чтобы по умолчанию очереди синхронизировались между всеми узлами кластера RMQ: rabbitmqPassword: guest rabbitmqErlangCookie: mae9joopaol7aiVu3eechei2waiGa2we definitions: policies: |- { \"name\": \"ha-all\", \"pattern\": \".*\", \"vhost\": \"/\", \"definition\": { \"ha-mode\": \"all\", \"ha-sync-mode\": \"automatic\", \"ha-sync-batch-size\": 81920 } } 2. Устанавливаем чарт: helm install . --name rmq-old --namespace rmq-old 3. Заходим в админку RabbitMQ, создаём новую очередь и добавляем несколько сообщений. Они понадобятся для того, чтобы после миграции мы могли удостовериться, что все данные сохранились и мы ничего не потеряли: Тестовый стенд готов: у нас есть «старый» RabbitMQ с данными, которые нужно перенести. Миграция кластера RabbitMQ 1. Для начала развернём новый RabbitMQ в другом пространстве имён с такими же ErlangCookie и паролем для пользователя. Для этого проделаем описанные выше операции, изменив конечную команду по установке RMQ на следующую: helm install . --name rmq-new --namespace rmq-new 2. Теперь требуется объединить новый кластер со старым. Для этого заходим в каждый из pod’ов нового RabbitMQ и выполняем команды: export OLD_RMQ=rabbit@rmq-old-rabbitmq-ha-0.rmq-old-rabbitmq-ha-discovery.rmq-old.svc.cluster.local && \\ rabbitmqctl stop_app && \\ rabbitmqctl join_cluster $OLD_RMQ && \\ rabbitmqctl start_app В переменной OLD_RMQ находится адрес одного из узлов старого кластера RMQ. Эти команды остановят текущий узел нового кластера RMQ, присоединят его к старому кластеру и снова запустят. 3. Кластер RMQ из 6 узлов готов: Необходимо подождать, пока сообщения синхронизируются между всеми узлами. Нетрудно догадаться, что время синхронизации сообщений зависит от мощностей железа, на котором развёрнут кластер, и от количества сообщений. В описываемом сценарии их всего 10, поэтому данные синхронизировались моментально, но при достаточно больших количестве сообщений синхронизация может длиться часами. Итак, статус синхронизации: Здесь +5 означает, что сообщения уже находятся ещё на 5 узлах (кроме того, что указан в поле Node ). Таким образом, синхронизация прошла успешно. 4. Остается лишь переключить в приложении адрес RMQ на новый кластер (конкретные действия здесь зависят от используемого вами технологического стека и другой специфики приложения), после чего можно попрощаться со старым. Для последней операции (т.е. уже после переключения приложения на новый кластер) заходим на каждый узел старого кластера и выполняем команды: rabbitmqctl stop_app rabbitmqctl reset Кластер «забыл» о старых узлах: можно удалять старый RMQ, на чём переезд будет закончен. Примечание: Если вы используете RMQ с сертификатами, то принципиально ничего не меняется — процесс переезда будет осуществляться точно так же. Выводы Описанная схема подходит практически для всех случаев, когда нам нужно перенести RabbitMQ или просто переехать в новый кластер. В нашем случае сложности возникали только один раз, когда к RMQ обращались из множества мест, а у нас не было возможности везде поменять адрес RMQ на новый. Тогда мы запускали новый RMQ в том же пространстве имён с одинаковыми лейблами, чтобы он попадал под уже существующие сервисы и Ingress’ы, а при запуске pod’а руками манипулировали лейблами, удаляя их в начале, чтобы на пустой RMQ не попадали запросы, и добавляя их обратно после синхронизации сообщений. Такую же стратегию мы применяли при обновлении RabbitMQ на новую версию с изменённой конфигурацией — всё работало как часы. P.S. В качестве логического продолжения этого материала мы готовим статьи про MongoDB (миграция с железного сервера в Kubernetes) и MySQL (один из вариантов «приготовления» этой СУБД внутри Kubernetes). Они будут опубликованы в ближайшие месяцы. P.P.S. Читайте также в нашем блоге: « Базы данных и Kubernetes (обзор и видео доклада) »; « K8s tips & tricks: Ускоряем bootstrap больших баз данных ».\n","metric":0.515625,"metric_input":"Текст статьи:\nБеспростойная миграция RabbitMQ в Kubernetes / Habr                \n\n06-05-2019\nRabbitMQ – написанный на языке Erlang брокер сообщений, позволяющий организовать отказоустойчивый кластер с полной репликацией данных на несколько узлов, где каждый узел может обслуживать запросы на чтение и запись. Имея в production-эксплуатации множество кластеров Kubernetes, мы поддерживаем большое количество инсталляций RabbitMQ и столкнулись с необходимостью миграции данных из одного кластера в другой без простоя. Данная операция была необходима нам как минимум в двух случаях: Перенос данных из кластера RabbitMQ, находящегося не в Kubernetes, в новый — уже «кубернетезированный» (т.е. функционирующий в pod’ах K8s) — кластер. Миграция RabbitMQ в рамках Kubernetes из одного namespace в другой (например, если контуры разграничены пространствами имён, то для переноса инфраструктуры из одного контура в другой). Предлагаемый в статье рецепт ориентирован на ситуации (но вовсе не ограничен ими), в которых есть старый кластер RabbitMQ (например, из 3 узлов), находящийся либо уже в K8s, либо на каких-то старых серверах. С ним работает приложение, размещённое в Kubernetes (уже там или в перспективе): … и перед нами стоит задача его миграции в новый production в Kubernetes. Сначала будет описан общий подход к самой миграции, а уже после этого — технические детали по её реализации. Алгоритм миграции Первый, предварительный, этап перед какими-либо действиями — проверка, что в старой инсталляции RabbitMQ включён режим высокой доступности ( HA ). Причина очевидна — мы ведь не хотим потерять каких-либо данных. Чтобы осуществить эту проверку, можно зайти в админку RabbitMQ и во вкладке Admin → Policies убедиться, что установлено значение ha-mode: all : Следующий шаг — поднимаем новый кластер RabbitMQ в pod’ах Kubernetes (в нашем случае, например, состоящий из 3 узлов, но их число может быть и другим). После этого мы объединяем старый и новый кластеры RabbitMQ, получая единственный кластер (из 6 узлов): Инициируется процесс синхронизации данных между старым и новым кластерами RabbitMQ. После того, как все данные синхронизируются между всеми узлами в кластере, мы можем переключить приложение на использование нового кластера: После этих операций достаточно вывести из кластера RabbitMQ старые узлы, и переезд можно считать завершённым: Эту схему мы неоднократно применяли у нас в production. Однако для собственного удобства реализовали её в рамках специализированной системы, распространяющей типовые конфигурации RMQ на множествах кластеров Kubernetes (для тех, кому любопытно: речь идёт про addon-operator, о котором мы совсем недавно рассказывали) . Ниже будут представлены отдельно взятые инструкции, которые каждый может применить на своих инсталляциях, чтобы попробовать предлагаемое решение в действии. Пробуем на практике Требования Реквизиты очень просты: Кластер Kubernetes (подойдет и minikube); Кластер RabbitMQ (может быть и развернут на bare metal, и сделан как обычный кластер в Kubernetes из официального Helm-чарта). Для описанного ниже примера я развернул RMQ в Kubernetes и назвал его rmq-old . Подготовка стенда 1. Скачаем Helm-чарт и немного отредактируем его: helm fetch --untar stable/rabbitmq-ha Для удобства задаём пароль, ErlangCookie и делаем политику ha-all , чтобы по умолчанию очереди синхронизировались между всеми узлами кластера RMQ: rabbitmqPassword: guest rabbitmqErlangCookie: mae9joopaol7aiVu3eechei2waiGa2we definitions: policies: |- { \"name\": \"ha-all\", \"pattern\": \".*\", \"vhost\": \"/\", \"definition\": { \"ha-mode\": \"all\", \"ha-sync-mode\": \"automatic\", \"ha-sync-batch-size\": 81920 } } 2. Устанавливаем чарт: helm install . --name rmq-old --namespace rmq-old 3. Заходим в админку RabbitMQ, создаём новую очередь и добавляем несколько сообщений. Они понадобятся для того, чтобы после миграции мы могли удостовериться, что все данные сохранились и мы ничего не потеряли: Тестовый стенд готов: у нас есть «старый» RabbitMQ с данными, которые нужно перенести. Миграция кластера RabbitMQ 1. Для начала развернём новый RabbitMQ в другом пространстве имён с такими же ErlangCookie и паролем для пользователя. Для этого проделаем описанные выше операции, изменив конечную команду по установке RMQ на следующую: helm install . --name rmq-new --namespace rmq-new 2. Теперь требуется объединить новый кластер со старым. Для этого заходим в каждый из pod’ов нового RabbitMQ и выполняем команды: export OLD_RMQ=rabbit@rmq-old-rabbitmq-ha-0.rmq-old-rabbitmq-ha-discovery.rmq-old.svc.cluster.local && \\ rabbitmqctl stop_app && \\ rabbitmqctl join_cluster $OLD_RMQ && \\ rabbitmqctl start_app В переменной OLD_RMQ находится адрес одного из узлов старого кластера RMQ. Эти команды остановят текущий узел нового кластера RMQ, присоединят его к старому кластеру и снова запустят. 3. Кластер RMQ из 6 узлов готов: Необходимо подождать, пока сообщения синхронизируются между всеми узлами. Нетрудно догадаться, что время синхронизации сообщений зависит от мощностей железа, на котором развёрнут кластер, и от количества сообщений. В описываемом сценарии их всего 10, поэтому данные синхронизировались моментально, но при достаточно больших количестве сообщений синхронизация может длиться часами. Итак, статус синхронизации: Здесь +5 означает, что сообщения уже находятся ещё на 5 узлах (кроме того, что указан в поле Node ). Таким образом, синхронизация прошла успешно. 4. Остается лишь переключить в приложении адрес RMQ на новый кластер (конкретные действия здесь зависят от используемого вами технологического стека и другой специфики приложения), после чего можно попрощаться со старым. Для последней операции (т.е. уже после переключения приложения на новый кластер) заходим на каждый узел старого кластера и выполняем команды: rabbitmqctl stop_app rabbitmqctl reset Кластер «забыл» о старых узлах: можно удалять старый RMQ, на чём переезд будет закончен. Примечание: Если вы используете RMQ с сертификатами, то принципиально ничего не меняется — процесс переезда будет осуществляться точно так же. Выводы Описанная схема подходит практически для всех случаев, когда нам нужно перенести RabbitMQ или просто переехать в новый кластер. В нашем случае сложности возникали только один раз, когда к RMQ обращались из множества мест, а у нас не было возможности везде поменять адрес RMQ на новый. Тогда мы запускали новый RMQ в том же пространстве имён с одинаковыми лейблами, чтобы он попадал под уже существующие сервисы и Ingress’ы, а при запуске pod’а руками манипулировали лейблами, удаляя их в начале, чтобы на пустой RMQ не попадали запросы, и добавляя их обратно после синхронизации сообщений. Такую же стратегию мы применяли при обновлении RabbitMQ на новую версию с изменённой конфигурацией — всё работало как часы. P.S. В качестве логического продолжения этого материала мы готовим статьи про MongoDB (миграция с железного сервера в Kubernetes) и MySQL (один из вариантов «приготовления» этой СУБД внутри Kubernetes). Они будут опубликованы в ближайшие месяцы. P.P.S. Читайте также в нашем блоге: « Базы данных и Kubernetes (обзор и видео доклада) »; « K8s tips & tricks: Ускоряем bootstrap больших баз данных ».\n\n\nКраткое содержание:\nВ статье описывается рецепт миграции кластера RabbitMQ в Kubernetes без простоя. \n\nСначала проверяется, что в старой инсталляции RabbitMQ включен режим высокой доступности. Затем создается новый кластер RabbitMQ в pod'ах Kubernetes. \n\nСтарый и новый кластеры объединяются, после чего инициируется процесс синхронизации данных. После завершения синхронизации старый кластер можно удалить. \n\nСхема миграции была успешно применена в production. Для удобства она реализована в addon-operator. \n\nДля миграции нужно развернуть кластер RabbitMQ и кластер Kubernetes. Затем выполнить команды для объединения кластеров и синхронизации данных. \n\nПосле этого можно переключиться на новый кластер и удалить старый. \n\nТаким образом, описанный подход позволяет перенести RabbitMQ без простоя.","summary":"В статье описывается рецепт миграции кластера RabbitMQ в Kubernetes без простоя. \n\nСначала проверяется, что в старой инсталляции RabbitMQ включен режим высокой доступности. Затем создается новый кластер RabbitMQ в pod'ах Kubernetes. \n\nСтарый и новый кластеры объединяются, после чего инициируется процесс синхронизации данных. После завершения синхронизации старый кластер можно удалить. \n\nСхема миграции была успешно применена в production. Для удобства она реализована в addon-operator. \n\nДля миграции нужно развернуть кластер RabbitMQ и кластер Kubernetes. Затем выполнить команды для объединения кластеров и синхронизации данных. \n\nПосле этого можно переключиться на новый кластер и удалить старый. \n\nТаким образом, описанный подход позволяет перенести RabbitMQ без простоя."}
{"Text":"macOS и мистический minOS / Habr             \n\nПосле трёхлетнего перерыва актуальная версия sView стала снова доступна на macOS. Релиз sView 20.08 обещал поддержку **macOS 10.10+** , но что-то пошло не так и несколько пользователей обратились со странной проблемой - системы macOS 10.13 и 10.14 отказались запускать приложение с сообщением о необходимости обновиться до macOS 10.15…\n\nСказать, что ошибка меня озадачила - сильно преуменьшить степень моего негодования, ведь магическая цифра 10.15 нигде не фигурировала ни в скриптах сборки, ни в ресурсах sView! Более того, приложение лично было проверено на более старой версии системы, а именно - на macOS 10.10.\nНемного предыстории. В далёком 2011 году вышла первая сборка sView для OS X 10.6 Snow Leopard , и шесть лет именно эта версия системы оставалась минимальным требованием для запуска sView. Поддержка относительно старых версий операционных систем даёт максимальный охват потенциальных пользователей, но требует дополнительных усилий.\nПрактика разработки Windows, Linux, Android и macOS приложений показывает, что предположения о том, что собранное приложение \"вроде должно работать\" на всех версиях систем периодически дают сбой, и проблемы совместимости всплывают самым неожиданным образом. В таких случаях возможность проверить работоспособность приложения на разных (в том числе самых старых, формально поддерживаемых) системах становится жизненно необходимой.\nОднако старая версия OS X требует такого же **_старого устройства_** , так как установить систему на устройство, выпущенное позднее самой системы, зачастую не представляется возможным. Проблему могли бы решить средства виртуализации, однако в случае с macOS дела с ними обстоят не лучшим образом.\nТакже понадобится и **_подходящий сборочный инструментарий_** . В прошлом, сборка приложения для нужной версии OS X требовала наличия нужной версии SDK в XCode. Однако упаковка нескольких SDK в XCode существенно увеличивала размер установки и старые версии SDK быстро исключались из новых версий XCode, осложняя сборку приложений для старых систем.\n_Для обеспечения совместимости с OS X 10.6 Snow Leopard, приложение sView долгое время собиралось на OS X той же версии, предустановленной на старом MacBook. При этом несколько версий OS X было установлено на внешний жёсткий диск для тестирования._\nК счастью, со временем разработчики Apple существенно улучшили инструментарий, внедрив версионизацию на уровне заголовочных файлов, опций компилятора и линковщика. Теперь, XCode поставляется всего с одной версией macOS SDK - с самой последней, - но приложение можно собрать с совместимостью с более старыми версиями macOS посредством:\nпеременной окружения `MACOSX_DEPLOYMENT_TARGET` (т.е., `export MACOSX_DEPLOYMENT_TARGET=10.10` );\n\nили флага компилятора `-mmacosx-version-min` (т.е., `EXTRA_CXXFLAGS += -mmacosx-version-min=10.10` ).\n_В случае CMake соответствующий параметр называется_ `CMAKE_OSX_DEPLOYMENT` _, а у qmake -_ `QMAKE_MACOSX_DEPLOYMENT_TARGET` _._\nНастройки проекта в XCode 11 позволяют выбрать минимальной платформой даже OS X 10.6, но данный выбор приводит только к ошибкам при сборке и Hello World удалось собрать только при выборе 10.7 или версия новее. Впрочем, OS X 10.6 Snow Leopard вышла в далёком 2009 году - то есть одиннадцать лет назад, - и едва ли имеет активных пользователей. Какую же версию выбрать в качестве минимальной?\n**OS X 10.10 Yosemite**  была выпущена около 6 лет назад и на 6 релизов \"старее\" самой актуальной на данный момент macOS 11.0 Big Sur. Трудно представить пользователей более старой OS X с учётом агрессивной политики обновлений Apple. Помимо прочего, OS X 10.10 уже была установлена на моём старом MacBook - слишком старым для разработки, но ещё живом для проверки работоспособности собранного приложения.\n\n_В попытке обновить “старичка” mid-2010 MacBook выяснилось, что свежие версии macOS более не поддерживают такие устройства , а последней совместимой версией оказалась macOS 10.13 High Sierra выпущенная в 2017 году._\n\n_Таким образом, Apple лишила свой продукт программных обновлений спустя 7 лет! При этом магазин приложений Apple более не позволяет загрузить старые версии macOS - то есть и обновить OS X 10.10 до macOS 10.13 не получится обычным способом._\nДля сборки sView  на свежем инструментарии в `Makefile`  проекта была прописана версия `10.10` , а в `Info.plist`  был указан параметр `LSMinimumSystemVersion=10.10` . Сама сборка была осуществлена на macOS 10.15, установленной на относительно свежем Mac mini ‘2018, и протестирована на макбуке с OS X 10.10 - приложение заработало и было опубликовано на сайте!\n…и тут, как снег на голову, пришли сообщения пользователей об ошибках запуска sView на версиях macOS, _новее_ **_ _** протестированной. Вздор! Откуда система вообще могла взять цифру `10.15` , если `LSMinimumSystemVersion`  указывает на `10.10`  - а это единственный ранее известный мне источник для подобных сообщений macOS об ошибках?\n\nВ слепую локализовать проблему не удавалось - поиски `10.15`  в архиве с приложением и в сборочных скриптах ни к чему не привели. Поэтому было найдено временное подопытное устройство с macOS 10.13, выводящее такое же сообщение об ошибке. Удивительно, но запуск исполнительного файла sView из терминала происходил без всяких проблем и ошибок!\nЭксперименты показали, что что-то не так непосредственно с исполнительным файлом sView, и в конце концов, утилита `otool -l`  выявила источник проблемы:\n```\nLoad command 9\n        cmd LC_BUILD_VERSION\n    cmdsize 32\n   platform macos\n        sdk 10.15\n      minos 10.15\n     ntools 1\n       tool ld\n    version 450.3\n```\nИнформации о загадочном `minos`  нашлось не много в интернете, но удалось выяснить, что данное поле появилось в заголовке Mach-O  исполняемых файлов macOS относительно недавно. Но этого факта оказалось достаточно, чтобы ответить на первый вопрос - как так получилось, что более старая версия OS X 10.10 запускала sView без проблем, а новые macOS 10.13-10.14 выдавали ошибки? Да просто OS X 10.10 ничего не знает о существовании нового поля `minos` !\nОставался последний вопрос - где в процессе сборки приложения закралась ошибка? Изучение пакета sView выявило, что поле `minos`  присутствовало только библиотеках и исполняемом файле самого проекта, но не в библиотеках FFmpeg, собранных схожим образом. То есть проблема была явно в `Makefile`  проекта. Как оказалось, флаг `-mmacosx-version-min`  передавался компилятору через переменную `EXTRA_CXXFLAGS` _,_ но не передавался линковщику. Добавление флага в переменную `EXTRA_LDFLAGS`  наконец-то решило проблему:\n```\nTARGET_OS_VERSION = 10.10\nEXTRA_CFLAGS   += -mmacosx-version-min=\\$(TARGET_OS_VERSION)\nEXTRA_CXXFLAGS += -mmacosx-version-min=\\$(TARGET_OS_VERSION)\nEXTRA_LDFLAGS  += -mmacosx-version-min=\\$(TARGET_OS_VERSION)\n```\nОригинальная публикация на английском может быть найдена здесь .","metric":0.50830078130000000414,"metric_input":"Текст статьи:\nmacOS и мистический minOS / Habr             \n\nПосле трёхлетнего перерыва актуальная версия sView стала снова доступна на macOS. Релиз sView 20.08 обещал поддержку **macOS 10.10+** , но что-то пошло не так и несколько пользователей обратились со странной проблемой - системы macOS 10.13 и 10.14 отказались запускать приложение с сообщением о необходимости обновиться до macOS 10.15…\n\nСказать, что ошибка меня озадачила - сильно преуменьшить степень моего негодования, ведь магическая цифра 10.15 нигде не фигурировала ни в скриптах сборки, ни в ресурсах sView! Более того, приложение лично было проверено на более старой версии системы, а именно - на macOS 10.10.\nНемного предыстории. В далёком 2011 году вышла первая сборка sView для OS X 10.6 Snow Leopard , и шесть лет именно эта версия системы оставалась минимальным требованием для запуска sView. Поддержка относительно старых версий операционных систем даёт максимальный охват потенциальных пользователей, но требует дополнительных усилий.\nПрактика разработки Windows, Linux, Android и macOS приложений показывает, что предположения о том, что собранное приложение \"вроде должно работать\" на всех версиях систем периодически дают сбой, и проблемы совместимости всплывают самым неожиданным образом. В таких случаях возможность проверить работоспособность приложения на разных (в том числе самых старых, формально поддерживаемых) системах становится жизненно необходимой.\nОднако старая версия OS X требует такого же **_старого устройства_** , так как установить систему на устройство, выпущенное позднее самой системы, зачастую не представляется возможным. Проблему могли бы решить средства виртуализации, однако в случае с macOS дела с ними обстоят не лучшим образом.\nТакже понадобится и **_подходящий сборочный инструментарий_** . В прошлом, сборка приложения для нужной версии OS X требовала наличия нужной версии SDK в XCode. Однако упаковка нескольких SDK в XCode существенно увеличивала размер установки и старые версии SDK быстро исключались из новых версий XCode, осложняя сборку приложений для старых систем.\n_Для обеспечения совместимости с OS X 10.6 Snow Leopard, приложение sView долгое время собиралось на OS X той же версии, предустановленной на старом MacBook. При этом несколько версий OS X было установлено на внешний жёсткий диск для тестирования._\nК счастью, со временем разработчики Apple существенно улучшили инструментарий, внедрив версионизацию на уровне заголовочных файлов, опций компилятора и линковщика. Теперь, XCode поставляется всего с одной версией macOS SDK - с самой последней, - но приложение можно собрать с совместимостью с более старыми версиями macOS посредством:\nпеременной окружения `MACOSX_DEPLOYMENT_TARGET` (т.е., `export MACOSX_DEPLOYMENT_TARGET=10.10` );\n\nили флага компилятора `-mmacosx-version-min` (т.е., `EXTRA_CXXFLAGS += -mmacosx-version-min=10.10` ).\n_В случае CMake соответствующий параметр называется_ `CMAKE_OSX_DEPLOYMENT` _, а у qmake -_ `QMAKE_MACOSX_DEPLOYMENT_TARGET` _._\nНастройки проекта в XCode 11 позволяют выбрать минимальной платформой даже OS X 10.6, но данный выбор приводит только к ошибкам при сборке и Hello World удалось собрать только при выборе 10.7 или версия новее. Впрочем, OS X 10.6 Snow Leopard вышла в далёком 2009 году - то есть одиннадцать лет назад, - и едва ли имеет активных пользователей. Какую же версию выбрать в качестве минимальной?\n**OS X 10.10 Yosemite**  была выпущена около 6 лет назад и на 6 релизов \"старее\" самой актуальной на данный момент macOS 11.0 Big Sur. Трудно представить пользователей более старой OS X с учётом агрессивной политики обновлений Apple. Помимо прочего, OS X 10.10 уже была установлена на моём старом MacBook - слишком старым для разработки, но ещё живом для проверки работоспособности собранного приложения.\n\n_В попытке обновить “старичка” mid-2010 MacBook выяснилось, что свежие версии macOS более не поддерживают такие устройства , а последней совместимой версией оказалась macOS 10.13 High Sierra выпущенная в 2017 году._\n\n_Таким образом, Apple лишила свой продукт программных обновлений спустя 7 лет! При этом магазин приложений Apple более не позволяет загрузить старые версии macOS - то есть и обновить OS X 10.10 до macOS 10.13 не получится обычным способом._\nДля сборки sView  на свежем инструментарии в `Makefile`  проекта была прописана версия `10.10` , а в `Info.plist`  был указан параметр `LSMinimumSystemVersion=10.10` . Сама сборка была осуществлена на macOS 10.15, установленной на относительно свежем Mac mini ‘2018, и протестирована на макбуке с OS X 10.10 - приложение заработало и было опубликовано на сайте!\n…и тут, как снег на голову, пришли сообщения пользователей об ошибках запуска sView на версиях macOS, _новее_ **_ _** протестированной. Вздор! Откуда система вообще могла взять цифру `10.15` , если `LSMinimumSystemVersion`  указывает на `10.10`  - а это единственный ранее известный мне источник для подобных сообщений macOS об ошибках?\n\nВ слепую локализовать проблему не удавалось - поиски `10.15`  в архиве с приложением и в сборочных скриптах ни к чему не привели. Поэтому было найдено временное подопытное устройство с macOS 10.13, выводящее такое же сообщение об ошибке. Удивительно, но запуск исполнительного файла sView из терминала происходил без всяких проблем и ошибок!\nЭксперименты показали, что что-то не так непосредственно с исполнительным файлом sView, и в конце концов, утилита `otool -l`  выявила источник проблемы:\n```\nLoad command 9\n        cmd LC_BUILD_VERSION\n    cmdsize 32\n   platform macos\n        sdk 10.15\n      minos 10.15\n     ntools 1\n       tool ld\n    version 450.3\n```\nИнформации о загадочном `minos`  нашлось не много в интернете, но удалось выяснить, что данное поле появилось в заголовке Mach-O  исполняемых файлов macOS относительно недавно. Но этого факта оказалось достаточно, чтобы ответить на первый вопрос - как так получилось, что более старая версия OS X 10.10 запускала sView без проблем, а новые macOS 10.13-10.14 выдавали ошибки? Да просто OS X 10.10 ничего не знает о существовании нового поля `minos` !\nОставался последний вопрос - где в процессе сборки приложения закралась ошибка? Изучение пакета sView выявило, что поле `minos`  присутствовало только библиотеках и исполняемом файле самого проекта, но не в библиотеках FFmpeg, собранных схожим образом. То есть проблема была явно в `Makefile`  проекта. Как оказалось, флаг `-mmacosx-version-min`  передавался компилятору через переменную `EXTRA_CXXFLAGS` _,_ но не передавался линковщику. Добавление флага в переменную `EXTRA_LDFLAGS`  наконец-то решило проблему:\n```\nTARGET_OS_VERSION = 10.10\nEXTRA_CFLAGS   += -mmacosx-version-min=\\$(TARGET_OS_VERSION)\nEXTRA_CXXFLAGS += -mmacosx-version-min=\\$(TARGET_OS_VERSION)\nEXTRA_LDFLAGS  += -mmacosx-version-min=\\$(TARGET_OS_VERSION)\n```\nОригинальная публикация на английском может быть найдена здесь .\n\nКраткое содержание:\nВ статье рассматривается проблема совместимости приложения sView с различными версиями macOS. Приложение было разработано для macOS 10.6 Snow Leopard и обещало поддержку более новых версий, но пользователи столкнулись с проблемами при запуске на macOS 10.13 и 10.14. \n\nАвтор статьи анализирует возможные причины проблемы и приходит к выводу, что ошибка кроется в некорректной сборке приложения для более новых версий macOS. В частности, флаг -mmacosx-version-min, используемый для указания минимальной версии macOS, не был передан линковщику, что привело к отсутствию поддержки нового поля minos в Mach-O заголовке исполняемых файлов.\n\nПосле исправления ошибки в Makefile приложение sView успешно запустилось на всех протестированных версиях macOS, включая 10.10, 10.13 и 10.14.","summary":"В статье рассматривается проблема совместимости приложения sView с различными версиями macOS. Приложение было разработано для macOS 10.6 Snow Leopard и обещало поддержку более новых версий, но пользователи столкнулись с проблемами при запуске на macOS 10.13 и 10.14. \n\nАвтор статьи анализирует возможные причины проблемы и приходит к выводу, что ошибка кроется в некорректной сборке приложения для более новых версий macOS. В частности, флаг -mmacosx-version-min, используемый для указания минимальной версии macOS, не был передан линковщику, что привело к отсутствию поддержки нового поля minos в Mach-O заголовке исполняемых файлов.\n\nПосле исправления ошибки в Makefile приложение sView успешно запустилось на всех протестированных версиях macOS, включая 10.10, 10.13 и 10.14."}
{"Text":"Смартфон начального уровня Infinix Hot 11S: обзор и тест недорогой модели для школьника или студента / Habr                        \n\n17-03-2022\nДостойная и недорогая модель для школьника или студента — смартфон Infinix Hot 11S с IPS-экраном 90 Гц и камерой 50 Мп. Предусмотрена возможность установки одновременно двух SIM-карты и карты памяти MicroSD, что достаточно удобно. Аккумулятор на 5000 мАч и предусмотрительно оставили гнездо для подключения проводных наушников 3.5 мм. Не так давно задался вопросом: «Какой смартфон выбрать для студента или школьника», и выбор пал на свежую модель от Infinix. Несмотря на то, что смартфон Infinix Hot 11S позиционируется как бюджетная модель, в комплекте присутствует сетевое зарядное устройство с быстрой зарядкой, приличный по качеству кабель USB-C, а также оригинальный чехол. Руководство по эксплуатации — на русском языке. При скромных размерах впечатляет диагональ экрана — 6.78\". Удобно будет просматривать видеоконтент непосредственно со смартфона. Тип матрицы — IPS, можно установить частоту обновления в 90 Гц. Разрешение — 1080 x 2480 точек, предусмотрено закаленное стекло для защиты экрана. Сканер отпечатков пальцев расположен в боковой кнопке, что весьма удобно и позволяет одним движением активировать смартфон. Как я уже сказал выше, лоток дает возможность одновременной установки двух SIM-карты и карты памяти MicroSD. Так что не приходится выбирать между расширением хранилища смартфона и дополнительной картой оператора. Обратите внимание на силиконовое уплотнительное кольцо — лоток предусматривает защиту от попадания пыли и брызг внутрь смартфона. На экран смартфона уже наклеена защитная пленка. Дополнительную защиту обеспечит силиконовый чехол, который имеется в комплекте. Чехол прикрывает края экрана, торцы выступающего блока камеры, что не мало важно для ребенка. Для отверстий и сканера отпечатков предусмотрены вырезы. В качестве операционной системы используется Android 11 со встроенной оболочкой X OS. Обновления проходят «по воздуху» (OTA). При активации можно выбрать русский язык. В свойствах смартфона можно уточнить встроенную память: 4/64 Гб. Аккумулятор в 5000 мАч обеспечивает длительное время работы — около 14 часов непрерывно под нагрузкой (по тестам), и более 250 часов в экономичном режиме. Обращаю внимание на специальные возможности смартфона — это детское меню и возможность ограничения запуска, а также времени работы ряда приложений. В разделе Детский режим указывается, какие игры/приложения возможно запустить и на какую длительность. Статистику запусков можно контролировать в разделе Цифровое благополучие. Не забывайте активировать режим обновления экрана 90 Гц повышения плавности отображения. Отмечу наличие в смартфоне NFC модуля и возможности бесконтактной оплаты смартфоном (SberPay, MirPay, Yoomoney). Экран достаточно приличный по качеству (IPS), с яркостью до 500 нит. Углы обзора обеспечивают просмотр с любой позиции. Для маскировки выреза под фронтальную камеру можно использовать соответствующую тему. Если посмотреть на коэффициент пульсаций, то при 100% яркости экрана пульсометр Radex показывает уровень около 0.7%, что весьма достойно. Интересную информацию можно почерпнуть из различных тестовых приложений. Например, Device Info HW показывает не только конкретное «железо» смартфона, но и используемые сенсоры камеры. Также можно сделать вывод, что разрешение экрана честное (1080 x 2480 пикселей, это не дешевые HD-экраны), а также видно установленную в настройках частоту обновления 90 Гц. Процессор MediaTek Helio G88 показывает производительность на уровне средних моделей прошлого года: Antutu — 214'697 единиц; PC Mark — 8098; Geekbench — 372/1305; 3D Mark SSE — 1422. Тест PC Mark Battery показывает результат в 13 часов и 53 минуты под нагрузкой с 80% до 20% заряда батареи. Двухдиапазонный Wi-Fi модуль поддерживает работу в сетях 5G и обеспечивает прием/передачу на уровне 300/250 МБ/с. Не обходится и без троттлинга — при нагреве Helio G88 сбрасывает до 15% производительности. Дизайн тыльной стороны смартфона, особенно оформление блока камеры выполнены в духе современных трендов. Выделяется крупный объектив основной камеры, для которого использован сенсор ISOCELL S5KJN1 от Samsung. Аналогичный сенсор используется и для других популярных 50 Мп смартфонов (например, Xiaomi RN11 и Realme 9i). Два дополнительных сенсора: GalaxyCore GC02M1B и OmniVision OV02B1B обеспечивают эффекты для фотографий. В качестве фронтальной камеры выступает сенсор GalaxyCore GC8034 на 8 Мп. В меню камеры можно отдельно активировать режим 50 Мп, предусмотрены режимы съемки с бьютификацией, ночной режим, Slow-Mo и так далее. 50 Мп сенсор от Samsung обеспечивает снимки неплохого качества. Особенно при соответствующем освещении. Пример можно посмотреть ниже, оригиналы и дополнительные фотографии доступны по ссылке. Если обратиться к свойствам фотографий, то можно убедиться в честном разрешении получаемых снимков. Это 50 Мп, 12 Мп (основной режим) и 8 Мп для селфи-камеры. Процессор Helio G88 в паре с Mali G52 MC2 достаточно неплохо тянет большинство игр. Конечно, в тяжелых играх или играх с HD-текстурами будет заметная просадка FPS. Тем не менее не забывайте ограничивать время, доступное для игр ребенку в соответствующем разделе Детского режима. Средняя производительность, дизайн в духе современных трендов, приличная фотокамера (50 Мп), а также софт, позволяющий оградить от определенного контента, делают смартфон Infinix Hot 11S удобным вариантом для детей и подростков. Учитывая, что Infinix практически сохранил старые цены на сток, выбор данной модели для ребенка будет вполне обоснованным.\n","metric":0.23583984380000000414,"metric_input":"Текст статьи:\nСмартфон начального уровня Infinix Hot 11S: обзор и тест недорогой модели для школьника или студента / Habr                        \n\n17-03-2022\nДостойная и недорогая модель для школьника или студента — смартфон Infinix Hot 11S с IPS-экраном 90 Гц и камерой 50 Мп. Предусмотрена возможность установки одновременно двух SIM-карты и карты памяти MicroSD, что достаточно удобно. Аккумулятор на 5000 мАч и предусмотрительно оставили гнездо для подключения проводных наушников 3.5 мм. Не так давно задался вопросом: «Какой смартфон выбрать для студента или школьника», и выбор пал на свежую модель от Infinix. Несмотря на то, что смартфон Infinix Hot 11S позиционируется как бюджетная модель, в комплекте присутствует сетевое зарядное устройство с быстрой зарядкой, приличный по качеству кабель USB-C, а также оригинальный чехол. Руководство по эксплуатации — на русском языке. При скромных размерах впечатляет диагональ экрана — 6.78\". Удобно будет просматривать видеоконтент непосредственно со смартфона. Тип матрицы — IPS, можно установить частоту обновления в 90 Гц. Разрешение — 1080 x 2480 точек, предусмотрено закаленное стекло для защиты экрана. Сканер отпечатков пальцев расположен в боковой кнопке, что весьма удобно и позволяет одним движением активировать смартфон. Как я уже сказал выше, лоток дает возможность одновременной установки двух SIM-карты и карты памяти MicroSD. Так что не приходится выбирать между расширением хранилища смартфона и дополнительной картой оператора. Обратите внимание на силиконовое уплотнительное кольцо — лоток предусматривает защиту от попадания пыли и брызг внутрь смартфона. На экран смартфона уже наклеена защитная пленка. Дополнительную защиту обеспечит силиконовый чехол, который имеется в комплекте. Чехол прикрывает края экрана, торцы выступающего блока камеры, что не мало важно для ребенка. Для отверстий и сканера отпечатков предусмотрены вырезы. В качестве операционной системы используется Android 11 со встроенной оболочкой X OS. Обновления проходят «по воздуху» (OTA). При активации можно выбрать русский язык. В свойствах смартфона можно уточнить встроенную память: 4/64 Гб. Аккумулятор в 5000 мАч обеспечивает длительное время работы — около 14 часов непрерывно под нагрузкой (по тестам), и более 250 часов в экономичном режиме. Обращаю внимание на специальные возможности смартфона — это детское меню и возможность ограничения запуска, а также времени работы ряда приложений. В разделе Детский режим указывается, какие игры/приложения возможно запустить и на какую длительность. Статистику запусков можно контролировать в разделе Цифровое благополучие. Не забывайте активировать режим обновления экрана 90 Гц повышения плавности отображения. Отмечу наличие в смартфоне NFC модуля и возможности бесконтактной оплаты смартфоном (SberPay, MirPay, Yoomoney). Экран достаточно приличный по качеству (IPS), с яркостью до 500 нит. Углы обзора обеспечивают просмотр с любой позиции. Для маскировки выреза под фронтальную камеру можно использовать соответствующую тему. Если посмотреть на коэффициент пульсаций, то при 100% яркости экрана пульсометр Radex показывает уровень около 0.7%, что весьма достойно. Интересную информацию можно почерпнуть из различных тестовых приложений. Например, Device Info HW показывает не только конкретное «железо» смартфона, но и используемые сенсоры камеры. Также можно сделать вывод, что разрешение экрана честное (1080 x 2480 пикселей, это не дешевые HD-экраны), а также видно установленную в настройках частоту обновления 90 Гц. Процессор MediaTek Helio G88 показывает производительность на уровне средних моделей прошлого года: Antutu — 214'697 единиц; PC Mark — 8098; Geekbench — 372/1305; 3D Mark SSE — 1422. Тест PC Mark Battery показывает результат в 13 часов и 53 минуты под нагрузкой с 80% до 20% заряда батареи. Двухдиапазонный Wi-Fi модуль поддерживает работу в сетях 5G и обеспечивает прием/передачу на уровне 300/250 МБ/с. Не обходится и без троттлинга — при нагреве Helio G88 сбрасывает до 15% производительности. Дизайн тыльной стороны смартфона, особенно оформление блока камеры выполнены в духе современных трендов. Выделяется крупный объектив основной камеры, для которого использован сенсор ISOCELL S5KJN1 от Samsung. Аналогичный сенсор используется и для других популярных 50 Мп смартфонов (например, Xiaomi RN11 и Realme 9i). Два дополнительных сенсора: GalaxyCore GC02M1B и OmniVision OV02B1B обеспечивают эффекты для фотографий. В качестве фронтальной камеры выступает сенсор GalaxyCore GC8034 на 8 Мп. В меню камеры можно отдельно активировать режим 50 Мп, предусмотрены режимы съемки с бьютификацией, ночной режим, Slow-Mo и так далее. 50 Мп сенсор от Samsung обеспечивает снимки неплохого качества. Особенно при соответствующем освещении. Пример можно посмотреть ниже, оригиналы и дополнительные фотографии доступны по ссылке. Если обратиться к свойствам фотографий, то можно убедиться в честном разрешении получаемых снимков. Это 50 Мп, 12 Мп (основной режим) и 8 Мп для селфи-камеры. Процессор Helio G88 в паре с Mali G52 MC2 достаточно неплохо тянет большинство игр. Конечно, в тяжелых играх или играх с HD-текстурами будет заметная просадка FPS. Тем не менее не забывайте ограничивать время, доступное для игр ребенку в соответствующем разделе Детского режима. Средняя производительность, дизайн в духе современных трендов, приличная фотокамера (50 Мп), а также софт, позволяющий оградить от определенного контента, делают смартфон Infinix Hot 11S удобным вариантом для детей и подростков. Учитывая, что Infinix практически сохранил старые цены на сток, выбор данной модели для ребенка будет вполне обоснованным.\n\n\nКраткое содержание:\nСмартфон начального уровня Infinix Hot 11S с IPS-экраном 90 Гц и камерой 50 Мп - достойная и недорогая модель для школьника или студента. Предусмотрена возможность установки одновременно двух SIM-карты и карты памяти MicroSD. В комплекте присутствует сетевое зарядное устройство с быстрой зарядкой, приличный по качеству кабель USB-C, а также оригинальный чехол. Смартфон Infinix Hot 11S позиционируется как бюджетная модель, но в комплекте присутствует сетевое зарядное устройство с быстрой зарядкой. При скромных размерах впечатляет диагональ экрана - 6.78\". Тип матрицы - IPS, можно установить частоту обновления в 90 Гц. Разрешение - 1080 x 2480 точек, предусмотрено закаленное стекло для защиты экрана. Сканер отпечатков пальцев расположен в боковой кнопке, что весьма удобно и позволяет одним движением активировать смартфон.","summary":"Смартфон начального уровня Infinix Hot 11S с IPS-экраном 90 Гц и камерой 50 Мп - достойная и недорогая модель для школьника или студента. Предусмотрена возможность установки одновременно двух SIM-карты и карты памяти MicroSD. В комплекте присутствует сетевое зарядное устройство с быстрой зарядкой, приличный по качеству кабель USB-C, а также оригинальный чехол. Смартфон Infinix Hot 11S позиционируется как бюджетная модель, но в комплекте присутствует сетевое зарядное устройство с быстрой зарядкой. При скромных размерах впечатляет диагональ экрана - 6.78\". Тип матрицы - IPS, можно установить частоту обновления в 90 Гц. Разрешение - 1080 x 2480 точек, предусмотрено закаленное стекло для защиты экрана. Сканер отпечатков пальцев расположен в боковой кнопке, что весьма удобно и позволяет одним движением активировать смартфон."}
{"Text":"Восемь приемов, которые маркетологи могут позаимствовать у Agile-программистов / Хабр               \n\n02-04-2014\nВ поисках новых идей для организации работы мы наткнулись на интересную статью от Мэтью Стибба, генерального директора Articulate Marketing. Идея показалась нам интересной, и мы решили поделиться ею с нашими читателями. Мэтью провел более десяти лет, разрабатывая компьютерные игры, а теперь управляет маркетинговым агентством, которое работает в сфере контента и интернет-приложений для бизнеса. Agile-программирование является относительно новый подходом к разработке программного обеспечения. Эта методика позволяет напряженный и порой непредсказуемый процесс разработки ПО превратить в управляемую, четко распланированную и командную работу. Именно это Стибб предлагает сделать с работой маркетологов, то есть использовать преимущества гибких моделей программирования в подготовке маркетингового контента. 1. Парное программирование Этот подход подразумевает работу программистов в паре. Напарники выполняют две задачи: написание кода и контроль за общей картиной, при этом работники взаимозаменяемы. Articulate Marketing использует тот же прием. Двух специалистов назначают для написания интервью или статьи: один пишет текст, а второй вносит правки. Часто меняя роли для выполнения разных частей поставленной задачи. 2. Поточное и регрессивное тестирование В гибкой модели программирования каждое изменение, внесенное в код, сопровождается обновлением результата автоматизированного тестирования, чтобы убедиться в том, что изменения не нарушат роботу программного обеспечения. В маркетинге, особенно в интернет-маркетинге, этот подход помогает оценить эффективность каждого вашего действия. Приносит ли эта страница большую конверсию, чем предыдущая? Эффективен ли новый CTA (Call To Action)? И так далее. Но идея регрессивного тестирования также означает следующее: то, что работает сегодня, нуждается в постоянном контроле, чтобы убедиться, что оно все еще будет работать завтра. 3. Планирование Agile-разработчики не любят сверхурочных. Здесь не принято писать до ночи, выпивая тонны кофе, и питаться, не отрываясь от работы. Вместо этого они планируют свою работу на основе интенсивной 40-часовой рабочей недели. Маркетологи должны сделать то же самое, даже если это означает сказать «нет» неожиданным дедлайнам заказчика. Как говорится в Техасе, «отсутствие планирования с вашей стороны не представляет собой чрезвычайную ситуацию с моей стороны». В конце концов, авральная работа снижает продуктивность и приводит к падению качества. 4. Мнение пользователей, а не технические характеристики Agile-программисты и менеджеры не прячутся от пользователя за нагромождением кода, они не отмахиваются спецификацией и не пытаются оградить себя от заказчика. Вместо этого они предлагают клиенту сотрудничать с ними и описывать его виденье конечного результата. Маркетологи могут использовать аналогичный подход, оценивая стиль, оригинальность и тему статьи, а не время, которое понадобится для ее написания. В Articulate Marketing не используют табели рабочего времени, а оценивают конечный результат. 5. «Много» не всегда значит «долго» Один из инструментов гибкой разработки — Pivotal Tracker использует подход, который отличается от традиционного. Вместо того чтобы прогнозировать время исполнения задачи, разработчик оценивает ее сложность от 1 до 3 и отмечает уровень ее важности в контексте других задач. Со временем сервис анализирует работу исполнителя и может рассчитать предположительное время выполнения той или иной задачи. Articulate Marketing используют похожую систему для планирования и контроля исполнения задач. 6. «Stand up»-встречи Вместо того чтобы бесконечно проводить встречи и конференц-звонки, разработчики проводят «stand up»-встречи в начале недели, чтобы обменяться информацией и обсудить проекты. Articulate Marketing делают точно также. И, как следует из названия, когда люди стоят, они стараются говорить как можно меньше и по делу! 7. Учитывать изменения, а не игнорировать их Большинство проектов ограничено жесткой спецификацией, которая заложена в их основу. Такой подход вызывает две проблемы: клиент не знает, насколько его устраивает продукт, пока не увидит его; при продолжительных сроках создания проекта он может потерять актуальность еще до своего завершения. Гибкая разработка поощряет участие клиента на этапе создания и предполагает, что проект будет меняться с течением времени. В Articulate Marketing этот подход реализован через постоянную обратную связь и редактирование контента. Возможно, это не совсем устраивает автора, но обеспечивает высокоэффективную продукцию для клиентов. 8. Спринт, а не марафоны Создание компьютерных игр занимает значительное время, и команда не может быть уверена в успехе, пока проект не достигнет завершающего этапа. Agile-разработчики используют другой подход, стремясь создать работоспособный сервис с минимальными затратами ресурсов, постепенно оптимизируя и развивая его, учитывая потребности пользователей. Проект, разделенный на несколько коротких этапов, легче адаптируется к изменениям требований и обстоятельств. Такой метод отлично подходит для маркетинговых проектов. Рекламная компания действительно не заканчивается, пока существует продукт, а сайт должен постоянно развиваться. Подводя итоги, мы хотим подчеркнуть, что Agile-разработка помогает создавать проекты, которые учитывают динамичность окружения, что жизненно важно для маркетинговых процессов. Вы не можете позволить себе почивать на лаврах одного успешного проекта или его этапа. Вы должны постоянно меняться, адаптироваться и внимательно следить за происходящим. Оригинал статьи: www.marketingprofs.com/articles/2014/24750/eight-secrets-of-agile-marketing\n","metric":0.72802734380000000414,"metric_input":"Текст статьи:\nВосемь приемов, которые маркетологи могут позаимствовать у Agile-программистов / Хабр               \n\n02-04-2014\nВ поисках новых идей для организации работы мы наткнулись на интересную статью от Мэтью Стибба, генерального директора Articulate Marketing. Идея показалась нам интересной, и мы решили поделиться ею с нашими читателями. Мэтью провел более десяти лет, разрабатывая компьютерные игры, а теперь управляет маркетинговым агентством, которое работает в сфере контента и интернет-приложений для бизнеса. Agile-программирование является относительно новый подходом к разработке программного обеспечения. Эта методика позволяет напряженный и порой непредсказуемый процесс разработки ПО превратить в управляемую, четко распланированную и командную работу. Именно это Стибб предлагает сделать с работой маркетологов, то есть использовать преимущества гибких моделей программирования в подготовке маркетингового контента. 1. Парное программирование Этот подход подразумевает работу программистов в паре. Напарники выполняют две задачи: написание кода и контроль за общей картиной, при этом работники взаимозаменяемы. Articulate Marketing использует тот же прием. Двух специалистов назначают для написания интервью или статьи: один пишет текст, а второй вносит правки. Часто меняя роли для выполнения разных частей поставленной задачи. 2. Поточное и регрессивное тестирование В гибкой модели программирования каждое изменение, внесенное в код, сопровождается обновлением результата автоматизированного тестирования, чтобы убедиться в том, что изменения не нарушат роботу программного обеспечения. В маркетинге, особенно в интернет-маркетинге, этот подход помогает оценить эффективность каждого вашего действия. Приносит ли эта страница большую конверсию, чем предыдущая? Эффективен ли новый CTA (Call To Action)? И так далее. Но идея регрессивного тестирования также означает следующее: то, что работает сегодня, нуждается в постоянном контроле, чтобы убедиться, что оно все еще будет работать завтра. 3. Планирование Agile-разработчики не любят сверхурочных. Здесь не принято писать до ночи, выпивая тонны кофе, и питаться, не отрываясь от работы. Вместо этого они планируют свою работу на основе интенсивной 40-часовой рабочей недели. Маркетологи должны сделать то же самое, даже если это означает сказать «нет» неожиданным дедлайнам заказчика. Как говорится в Техасе, «отсутствие планирования с вашей стороны не представляет собой чрезвычайную ситуацию с моей стороны». В конце концов, авральная работа снижает продуктивность и приводит к падению качества. 4. Мнение пользователей, а не технические характеристики Agile-программисты и менеджеры не прячутся от пользователя за нагромождением кода, они не отмахиваются спецификацией и не пытаются оградить себя от заказчика. Вместо этого они предлагают клиенту сотрудничать с ними и описывать его виденье конечного результата. Маркетологи могут использовать аналогичный подход, оценивая стиль, оригинальность и тему статьи, а не время, которое понадобится для ее написания. В Articulate Marketing не используют табели рабочего времени, а оценивают конечный результат. 5. «Много» не всегда значит «долго» Один из инструментов гибкой разработки — Pivotal Tracker использует подход, который отличается от традиционного. Вместо того чтобы прогнозировать время исполнения задачи, разработчик оценивает ее сложность от 1 до 3 и отмечает уровень ее важности в контексте других задач. Со временем сервис анализирует работу исполнителя и может рассчитать предположительное время выполнения той или иной задачи. Articulate Marketing используют похожую систему для планирования и контроля исполнения задач. 6. «Stand up»-встречи Вместо того чтобы бесконечно проводить встречи и конференц-звонки, разработчики проводят «stand up»-встречи в начале недели, чтобы обменяться информацией и обсудить проекты. Articulate Marketing делают точно также. И, как следует из названия, когда люди стоят, они стараются говорить как можно меньше и по делу! 7. Учитывать изменения, а не игнорировать их Большинство проектов ограничено жесткой спецификацией, которая заложена в их основу. Такой подход вызывает две проблемы: клиент не знает, насколько его устраивает продукт, пока не увидит его; при продолжительных сроках создания проекта он может потерять актуальность еще до своего завершения. Гибкая разработка поощряет участие клиента на этапе создания и предполагает, что проект будет меняться с течением времени. В Articulate Marketing этот подход реализован через постоянную обратную связь и редактирование контента. Возможно, это не совсем устраивает автора, но обеспечивает высокоэффективную продукцию для клиентов. 8. Спринт, а не марафоны Создание компьютерных игр занимает значительное время, и команда не может быть уверена в успехе, пока проект не достигнет завершающего этапа. Agile-разработчики используют другой подход, стремясь создать работоспособный сервис с минимальными затратами ресурсов, постепенно оптимизируя и развивая его, учитывая потребности пользователей. Проект, разделенный на несколько коротких этапов, легче адаптируется к изменениям требований и обстоятельств. Такой метод отлично подходит для маркетинговых проектов. Рекламная компания действительно не заканчивается, пока существует продукт, а сайт должен постоянно развиваться. Подводя итоги, мы хотим подчеркнуть, что Agile-разработка помогает создавать проекты, которые учитывают динамичность окружения, что жизненно важно для маркетинговых процессов. Вы не можете позволить себе почивать на лаврах одного успешного проекта или его этапа. Вы должны постоянно меняться, адаптироваться и внимательно следить за происходящим. Оригинал статьи: www.marketingprofs.com/articles/2014/24750/eight-secrets-of-agile-marketing\n\n\nКраткое содержание:\nСтатья обсуждает восемь приемов, которые маркетологи могут позаимствовать у Agile-программистов. Эти приемы включают парное программирование, поточное и регрессивное тестирование, планирование, учет мнения пользователей, \"много\" не всегда значит \"долго\", \"stand up\"-встречи, учет изменений и спринт вместо марафона. В целом, Agile-разработка помогает создавать проекты, которые учитывают динамичность окружения, что жизненно важно для маркетинговых процессов.","summary":"Статья обсуждает восемь приемов, которые маркетологи могут позаимствовать у Agile-программистов. Эти приемы включают парное программирование, поточное и регрессивное тестирование, планирование, учет мнения пользователей, \"много\" не всегда значит \"долго\", \"stand up\"-встречи, учет изменений и спринт вместо марафона. В целом, Agile-разработка помогает создавать проекты, которые учитывают динамичность окружения, что жизненно важно для маркетинговых процессов."}
{"Text":"Охладить CPU в ЦОД — поможет лазерная 3D-печать / Habr             \n\nВ Университете Бингемтон (Нью-Йорк) разработали новую технологию для охлаждения процессоров, которая позволит отказаться от термопасты. Теплопроводящий материал наносится прямо на поверхность чипа при помощи 3D-печати. По оценкам экспертов, их решение способно понизить рабочую температуру процессоров в дата-центрах на 10°C.\n\nРасскажем о технологии и поговорим о других экспериментальных методах охлаждения CPU.\n\n_/ фото artistic bokeh  CC BY-SA_\n\n## Как напечатать «металлическую термопасту» ##\n\nРазработчики технологии\nнанесли\nна кристалл процессора тонкий слой металлического сплава с высокой теплопроводностью и с помощью лазера «напечатали» в нем каналы для охлаждающей жидкости. Для этого использовался метод\nселективного лазерного спекания\n.\n\nНа кремниевой поверхности равномерно распределяется слой металлического порошка. Затем включается лазер и луч, направляемый подвижными зеркалами, сплавляет частицы между собой согласно сгенерированной 3D-модели. Процедура повторяется множество раз — на каждой итерации формируются различные срезы финального изделия. Само лазерное спекание\nпроисходит\nменее чем за секунду.\n\nСплав, который наносят на чип, состоит из титана, олова и серебра. Последние два нужны для снижения температуры плавления материала. Таким образом, металл дольше остается в жидком состоянии, что помогает избежать деформации слоя из-за резкого затвердевания.\n\nСелективное лазерное спекание позволило сформировать металлический слой толщиной в тысячу раз меньше диаметра человеческого волоса. Это дает возможность охлаждающей жидкости забирать излишки тепла прямо с чипа и исключает необходимость в термопасте.\n\n## Что может эта технология ##\n\nСпециалистам удалось получить сплав с\nтеплопроводностью\nв 39 Вт/(м•K), что в семь раз лучше, чем у других материалов для\nтермоинтерфейсов\n— термопаст или полимерных компаундов. Это позволило снизит температуру чипа на 10°C, по сравнению с другими системами охлаждения.\n\n> Новая технология призвана решить две задачи: снизить затраты на электроэнергию в дата-центрах и продлить срок службы процессоров (так как они будут меньше перегреваться). По словам разработчиков, изобретение сократит энергопотребление мировых ЦОД на 5% и позволит ИТ-индустрии экономить до 438 миллионов долларов ежегодно.\n\nПока технологию испытали только в лабораторных условиях и неизвестно, как она будет работать в реальных дата-центрах. Однако уже в ближайшее время исследователи планируют запатентовать свою технологию и провести необходимые тесты.\n\n## Кто еще экспериментирует с охлаждением чипов ##\n\nНе только в Университете Бингемтон работают над технологиями охлаждения чипов. Их коллеги из Калифорнийского университета впервые синтезировали сверхчистые кристаллы арсенида бора, обладающие высокой теплопроводностью. Полученное значение приблизилось к 1300 Вт/(м•K), в то время как у алмаза (который считается одним из рекордсменов по теплопроводности) она составляет 1000 Вт/(м•K).\n\nНовая технология позволит создавать эффективные системы отвода тепла в электронике и фотонике. Однако для этого нужно решить ряд задач. Арсенид бора сложно получать в промышленных масштабах — при синтезе кристаллов часто\nвозникают\nдефекты, а в производстве материала используются токсичные соединения мышьяка.\n\nИсследователи из Калифорнийского университета тоже работают над решением проблемы отведения тепла от процессоров. Они\nпредложили\nизменить структуру самого чипа. Идея состоит в том, чтобы создать такую структуру кристалла кремния, в которой фононы (квазичастицы, осуществляющие передачу тепла) будут переносить тепло с максимальной скоростью.\n\nИх технология получила название «дырчатого кремния». В плате просверливаются крошечные отверстия диаметром в 20 нм, которые ускоряют теплоотведение. В случае с оптимальным расположением отверстий, теплопроводность кремниевой пластины\nвозрастает\nна 30%.\n\nЭтот способ пока далек от реализации — готова лишь модель. Следующий шаг — изучить потенциал технологии и возможность применения в реальных системах.\n\n_/ фото PxHere  PD_\n\n## Что дальше ##\n\nДо практической реализации новых технологий теплоотведения пока далеко. Все они находятся или на стадии концепции, или на стадии прототипирования. Хотя они обладают хорошим потенциалом, о широком их внедрении на рынке ЦОД говорить пока не приходится.\n\nПо этой причине сейчас дата-центры\nэкспериментируют\nс другими способами охлаждения. Один из последних трендов —\n**жидкостное охлаждение**\n. По\nданным\nопроса Uptime Institute, технологию уже внедрили 14% дата-центров по всему миру. Эксперты ожидают, что в будущем этот показатель вырастет из-за повышения плотности оборудования в ЦОД. Так как при большом количестве рядом стоящих серверов охлаждение воздухом затрудняется.\n\nЕщё\nодна тенденция\n—\n**системы ИИ для управления кондиционирующими установками**\nдата-центра. По\nоценке\nисследовательских организаций, около 15–25% ЦОД уже используют такие алгоритмы машинного обучения. И ожидается, что в будущем популярность интеллектуальных технологий в ЦОД будет только увеличиваться.\n\n---\n\nP.S. Материалы из нашего корпоративного блога:\n\nUnboxing: блейд-сервер Cisco UCS B480 M5\n\nСерверы для SAP: основные платформы\n\nUnboxing all-flash СХД NetApp AFF A300: технические характеристики\n\nP.S. У нас есть Telegram-канал, где мы пишем о технологиях виртуализации и IaaS:\n\nЧто там у VMware: три новинки\n\nNetApp от А до Я: обзор технологий вендора\n\nNetApp: продолжаем рассказывать о технологиях компании","metric":0.6103515625,"metric_input":"Текст статьи:\nОхладить CPU в ЦОД — поможет лазерная 3D-печать / Habr             \n\nВ Университете Бингемтон (Нью-Йорк) разработали новую технологию для охлаждения процессоров, которая позволит отказаться от термопасты. Теплопроводящий материал наносится прямо на поверхность чипа при помощи 3D-печати. По оценкам экспертов, их решение способно понизить рабочую температуру процессоров в дата-центрах на 10°C.\n\nРасскажем о технологии и поговорим о других экспериментальных методах охлаждения CPU.\n\n_/ фото artistic bokeh  CC BY-SA_\n\n## Как напечатать «металлическую термопасту» ##\n\nРазработчики технологии\nнанесли\nна кристалл процессора тонкий слой металлического сплава с высокой теплопроводностью и с помощью лазера «напечатали» в нем каналы для охлаждающей жидкости. Для этого использовался метод\nселективного лазерного спекания\n.\n\nНа кремниевой поверхности равномерно распределяется слой металлического порошка. Затем включается лазер и луч, направляемый подвижными зеркалами, сплавляет частицы между собой согласно сгенерированной 3D-модели. Процедура повторяется множество раз — на каждой итерации формируются различные срезы финального изделия. Само лазерное спекание\nпроисходит\nменее чем за секунду.\n\nСплав, который наносят на чип, состоит из титана, олова и серебра. Последние два нужны для снижения температуры плавления материала. Таким образом, металл дольше остается в жидком состоянии, что помогает избежать деформации слоя из-за резкого затвердевания.\n\nСелективное лазерное спекание позволило сформировать металлический слой толщиной в тысячу раз меньше диаметра человеческого волоса. Это дает возможность охлаждающей жидкости забирать излишки тепла прямо с чипа и исключает необходимость в термопасте.\n\n## Что может эта технология ##\n\nСпециалистам удалось получить сплав с\nтеплопроводностью\nв 39 Вт/(м•K), что в семь раз лучше, чем у других материалов для\nтермоинтерфейсов\n— термопаст или полимерных компаундов. Это позволило снизит температуру чипа на 10°C, по сравнению с другими системами охлаждения.\n\n> Новая технология призвана решить две задачи: снизить затраты на электроэнергию в дата-центрах и продлить срок службы процессоров (так как они будут меньше перегреваться). По словам разработчиков, изобретение сократит энергопотребление мировых ЦОД на 5% и позволит ИТ-индустрии экономить до 438 миллионов долларов ежегодно.\n\nПока технологию испытали только в лабораторных условиях и неизвестно, как она будет работать в реальных дата-центрах. Однако уже в ближайшее время исследователи планируют запатентовать свою технологию и провести необходимые тесты.\n\n## Кто еще экспериментирует с охлаждением чипов ##\n\nНе только в Университете Бингемтон работают над технологиями охлаждения чипов. Их коллеги из Калифорнийского университета впервые синтезировали сверхчистые кристаллы арсенида бора, обладающие высокой теплопроводностью. Полученное значение приблизилось к 1300 Вт/(м•K), в то время как у алмаза (который считается одним из рекордсменов по теплопроводности) она составляет 1000 Вт/(м•K).\n\nНовая технология позволит создавать эффективные системы отвода тепла в электронике и фотонике. Однако для этого нужно решить ряд задач. Арсенид бора сложно получать в промышленных масштабах — при синтезе кристаллов часто\nвозникают\nдефекты, а в производстве материала используются токсичные соединения мышьяка.\n\nИсследователи из Калифорнийского университета тоже работают над решением проблемы отведения тепла от процессоров. Они\nпредложили\nизменить структуру самого чипа. Идея состоит в том, чтобы создать такую структуру кристалла кремния, в которой фононы (квазичастицы, осуществляющие передачу тепла) будут переносить тепло с максимальной скоростью.\n\nИх технология получила название «дырчатого кремния». В плате просверливаются крошечные отверстия диаметром в 20 нм, которые ускоряют теплоотведение. В случае с оптимальным расположением отверстий, теплопроводность кремниевой пластины\nвозрастает\nна 30%.\n\nЭтот способ пока далек от реализации — готова лишь модель. Следующий шаг — изучить потенциал технологии и возможность применения в реальных системах.\n\n_/ фото PxHere  PD_\n\n## Что дальше ##\n\nДо практической реализации новых технологий теплоотведения пока далеко. Все они находятся или на стадии концепции, или на стадии прототипирования. Хотя они обладают хорошим потенциалом, о широком их внедрении на рынке ЦОД говорить пока не приходится.\n\nПо этой причине сейчас дата-центры\nэкспериментируют\nс другими способами охлаждения. Один из последних трендов —\n**жидкостное охлаждение**\n. По\nданным\nопроса Uptime Institute, технологию уже внедрили 14% дата-центров по всему миру. Эксперты ожидают, что в будущем этот показатель вырастет из-за повышения плотности оборудования в ЦОД. Так как при большом количестве рядом стоящих серверов охлаждение воздухом затрудняется.\n\nЕщё\nодна тенденция\n—\n**системы ИИ для управления кондиционирующими установками**\nдата-центра. По\nоценке\nисследовательских организаций, около 15–25% ЦОД уже используют такие алгоритмы машинного обучения. И ожидается, что в будущем популярность интеллектуальных технологий в ЦОД будет только увеличиваться.\n\n---\n\nP.S. Материалы из нашего корпоративного блога:\n\nUnboxing: блейд-сервер Cisco UCS B480 M5\n\nСерверы для SAP: основные платформы\n\nUnboxing all-flash СХД NetApp AFF A300: технические характеристики\n\nP.S. У нас есть Telegram-канал, где мы пишем о технологиях виртуализации и IaaS:\n\nЧто там у VMware: три новинки\n\nNetApp от А до Я: обзор технологий вендора\n\nNetApp: продолжаем рассказывать о технологиях компании\n\nКраткое содержание:\nВ статье рассматривается новая технология охлаждения процессоров, разработанная в Университете Бингемтон (Нью-Йорк). Вместо использования термопасты, тепло передается на металлическую поверхность чипа с помощью 3D-печати. Это позволяет снизить рабочую температуру процессоров в дата-центрах на 10°C.\n\nРазработчики использовали метод селективного лазерного спекания для создания металлического слоя с высокой теплопроводностью на поверхности чипа. Затем они сформировали каналы для охлаждающей жидкости в этом слое.\n\nНовая технология обладает теплопроводностью в 39 Вт/(м•K), что в семь раз лучше, чем у других материалов для термоинтерфейсов. Это позволяет снизить температуру чипа на 10°C по сравнению с другими системами охлаждения.\n\nТакже в статье упоминаются другие экспериментальные методы охлаждения CPU, такие как использование сверхчистых кристаллов арсенида бора и создание \"дырчатого кремния\". Однако эти технологии находятся на стадии концепции или прототипирования и не получили широкого применения в дата-центрах.\n\nВ заключение отмечается, что дата-центры экспериментируют с другими способами охлаждения, такими как жидкостное охлаждение и системы ИИ для управления кондиционирующими установками.","summary":"В статье рассматривается новая технология охлаждения процессоров, разработанная в Университете Бингемтон (Нью-Йорк). Вместо использования термопасты, тепло передается на металлическую поверхность чипа с помощью 3D-печати. Это позволяет снизить рабочую температуру процессоров в дата-центрах на 10°C.\n\nРазработчики использовали метод селективного лазерного спекания для создания металлического слоя с высокой теплопроводностью на поверхности чипа. Затем они сформировали каналы для охлаждающей жидкости в этом слое.\n\nНовая технология обладает теплопроводностью в 39 Вт/(м•K), что в семь раз лучше, чем у других материалов для термоинтерфейсов. Это позволяет снизить температуру чипа на 10°C по сравнению с другими системами охлаждения.\n\nТакже в статье упоминаются другие экспериментальные методы охлаждения CPU, такие как использование сверхчистых кристаллов арсенида бора и создание \"дырчатого кремния\". Однако эти технологии находятся на стадии концепции или прототипирования и не получили широкого применения в дата-центрах.\n\nВ заключение отмечается, что дата-центры экспериментируют с другими способами охлаждения, такими как жидкостное охлаждение и системы ИИ для управления кондиционирующими установками."}
{"Text":"Интернет как новое огораживание / Хабр                \n\n24-07-2015\nПредлагаю читателям Geektimes перевод публикации «The Internet as New Enclosure». Перевод был прислан в песочницу на Хабр, пролежал там довольно долгое время и лежал бы и дальше, недоступный широкой аудитории, однако его достали модераторы и перенесли сюда. СМИ превращают наши переживания, память и общение в нечто синтетическое и внешнее. В обществе, контролируемом СМИ, мы зависим от технологии доступа к этим сторонам самих себя, сделанным внешними. Книги, аудиозаписи, кино, радио, телевидение, интернет, мобильные телефоны: все эти шедшие друг за другом нововведения проникали всё глубже в нашу повседневную жизнь, становясь посредниками для всё большей части наших жизней. В плоть до конца 20 века, СМИ были чрезвычайно однонаправленными: информация текла в одну сторону, а внимание в другую. Критики обычно сосредотачивались на этом аспекте их устройства, обвиняя их в том, что они давали крохотной политической клике огромное влияние над обществом, закрепляя за остальными роли зрителей. В отличии от них, подпольные СМИ придерживались более интерактивных и децентрализованных форм. Всеобщее участие и децентрализация внезапно стали мейнстримом с появлением общедоступных цифровых медиа. Во многих отношениях, Интернет представлял из себя более раскрепощающее и укрепляющее самооценку пространство для новых методов коммуникации. Поскольку его базовая модель была разработана исследователями, финансируемыми военными, а не частным сектором, он проектировался для того, чтобы приносить пользу, а не прибыль. По сей день, большая часть Интернета остаётся своего рода «Диким Западом», в котором затруднительно следить за исполнением традиционного вещного права. Возможность делиться материалами бесплатно и напрямую с другими пользователями оказала огромное воздействие на некоторые отрасли, в то время как коллаборативные форматы вроде Википедии и ПО с открытым исходным кодом показали, как просто люди могут удовлетворять свои потребности без частной собственности. Корпорации по-прежнему ищут способы делания денег в Интернет кроме как через онлайн-торговлю и рекламу. Однако, по мере того как наши жизни становятся всё более и более оцифрованными, важно не принимать как данность то, что это всегда к лучшему. Капитализм разрастается за счёт захвата тех сторон мира, которые когда-то были свободными, а затем предлагает доступ к ним по высокой цене, и эта цена не всегда выплачивается в долларах. Мы должны быть особенно бдительны в отношении того, что новые медиа удобны: удобство может быть знаком того, что бесконечные возможности человеческой жизни принудительно сужаются. Действительно, эти инновации едва ли необязательны: в наши дни трудно поддерживать дружеские отношения, или устроиться на работу без сотового телефона и онлайн-профиля. Всё больше и больше наши умственные действия и общественная жизнь вынужденно подвергаются воздействию технологий, которые составляют схемы нашей деятельности и отношений для корпораций и правительственных разведслужб. Эти форматы также задают содержание этой деятельности и отношений. Сети, которые предлагает нам Facebook, не являются чем-то новым; новым является то, что они представляются внешними по отношению к нам самим. У нас всегда были социальные сети, но никто не мог их использовать для продажи рекламы—как и нелегко их было запротоколировать. Сейчас они вернулись как нечто, с чем мы должны считаться. Люди переписывались со старыми друзьями, обучались навыкам и узнавали об общественных мероприятиях задолго до появления электронной почты, Google и Twitter. Конечно, эти технологии чрезвычайно полезны в мире, где лишь немногие из нас имеют близкие отношения со своими соседями, или живут больше нескольких лет в одном и том же месте. Формы, которые принимает технология и повседневная жизнь, влияют друг на друга, что делает всё более невероятной возможность их разъединения. По мере того как наши потребности в информации и доступ к ней вырастают за пределы чего-то, что мы могли бы удержать в себе, информация, похоже, становится чем-то обособленным от нас. Это подозрительно напоминает принудительное отчуждение рабочих от продуктов их труда, которое превратило их в потребителей. Информация в интернете не совсем свободна — компьютеры и доступ в интернет стоят денег, не говоря уже о затратах на электричество и нагрузку на окружающую среду, которые они создают вместе с работающими серверами по всему миру. А что если корпорации догадаются брать с нас больше денег за доступ ко всем этим технологиям, как только мы станем полностью зависимыми от них? Если они смогут это сделать, не только власть и знания, но даже возможность поддерживать социальные связи станет полностью определяться богатством. Но было бы не правильно ожидать этого. Старая финансовая аристократия, возможно, вообще не сможет объединить свою власть на этой новой территории. Методы, которыми капитализм колонизирует наши жизни с помощью цифровых технологий не обязательно будут иметь сходства со старыми формами колонизации. Как и любая пирамида, капитализм должен постоянно расширяться, поглощая новые ресурсы и людей. Он уже распространился по всей планете; последняя война колонизации ведётся у подножия Гималайских гор, на самом краю света. Теоретически, он должен вскоре рухнуть, так как перспективы расширения исчезли. Но что если он продолжит расширяться внутрь нас, и эти новые технологии, подобно трём каравеллам Колумба, высаживаются на континент наших мыслительных процессов и социальных связей? В этом представлении, интернет выполняет функцию ещё одного следующего слоя отчуждения, основанного на материальной экономике. И если многое доступно в интернете бесплатно, то это не потому что процесс колонизации ещё не окончен, но, кроме всего прочего, потому, что определяющей валютой в СМИ являются не доллары, а внимание. В информационной экономике внимание функционирует подобно контролю над материальными ресурсами в индустриальной экономике. Даже когда внимание не преобразуется сразу в прибыль онлайн, оно может помочь обеспечить её в оффлайне. Как валюты, внимание и капитал ведут себя по-разному, но и то и другое служат созданию дисбаланса власти. Действительно, что такое капитал? Если отбросить предрассудки, которые делают его похожим на стихийную силу, он, по существу, является социальным конструктом, который позволяет некоторым людям накапливать власть над другими. Без идеи частной собственности, которая «реальна» настолько, насколько все её терпят, материальные ресурсы не могли бы функционировать как капитал. В этом смысле, право собственности служит той же цели, которой ранее служила идея божественного права королей: и то и другое формировало основы систем, определяющих власть. Некоторые люди неистово верят в право собственности, даже когда это право используется для лишения их всякого влияния в обществе. Можно сказать, что эти люди находятся под чарами собственности. Точно также, когда рекламный агент собирается сделать мем вирусным, вы можете сказать, что она пытается наложить заклятие. Если внимание является валютой медиа, его приобретение является способом заставить людей буквально и образно поддаться структурам власти. Определяющим фактором является не то, соглашаются ли люди с увиденным, или одобряют его, а то, в какой степени это задаёт их поведение. Цифровые медиа, похоже, владеют децентрализованным вниманием, но они также стандартизуют площадки, на которых оно циркулирует. Остерегайтесь объектов, которые накапливают внимание, даже если они никак не конвертируют его в финансовые активы. Настоящая власть Google и Facebook не в их финансовых активах, а в том, как они структурируют потоки информации. Навязывая единую логику коммуникации, отношений и наведения справок, они обеспечивают себе позицию посредников власти новой эры. За этими корпорациями стоит АНБ, которое сейчас обладает беспрецедентной способностью составлять схемы отношений и мыслительных процессов. Отслеживая поисковые запросы в Google, оно способно отслеживать ход мыслей пользователя сети интернет в реальном времени. У АНБ даже меньше потребность превращать использование интернет в непосредственную финансовую выгоду; валюта, которая ему нужна, это сама информация, с помощью которой можно управлять грубой силой государства. Роль государственного надзора заключается в поддержании условий, в которых корпорации вроде Facebook ведут бизнес; чем больше влияния накапливают эти корпорации, финансового или иного, тем больше влияния течёт обратно в руки государства. До скандала вокруг Prism, многие люди думали, что слежка и цензура характерны для таких мест, как Тунис и Сирия. На самом деле, большая часть технологий для цензуры поступает к этим режимам из Кремниевой Долины—и сперва применяется прямо здесь, в США. Так как даже самая мягкая интернет-цензура предполагает эффективное и всестороннее наблюдение, от регулирования до строгой изоляции требуется небольшой шаг. Чем сильнее мы зависим от цифровых технологий, тем сильнее мы уязвимы перед лицом огромных учреждений, на которые мы практически неспособны повлиять. Это не критика технологии как таковой. Суть в том, что она не нейтральна: технология всегда обусловлена структурами общества, в которых она разрабатывается и применяется. Большинство известных нам технологий были обусловлены императивами получения прибыли и власти, но общество, основанное на других ценностях, несомненно, создало бы другие технологии. По мере того как цифровые технологии всё больше впутываются в ткань нашего общества, важным вопросом становится не «стоит ли ими пользоваться», а «как подорвать структуры, которые их создали». Другими словами: сторонники свободного интернета должны задаться вопросом, совместима ли на самом деле его свобода с капитализмом и государством?\n","metric":0.61669921880000000414,"metric_input":"Текст статьи:\nИнтернет как новое огораживание / Хабр                \n\n24-07-2015\nПредлагаю читателям Geektimes перевод публикации «The Internet as New Enclosure». Перевод был прислан в песочницу на Хабр, пролежал там довольно долгое время и лежал бы и дальше, недоступный широкой аудитории, однако его достали модераторы и перенесли сюда. СМИ превращают наши переживания, память и общение в нечто синтетическое и внешнее. В обществе, контролируемом СМИ, мы зависим от технологии доступа к этим сторонам самих себя, сделанным внешними. Книги, аудиозаписи, кино, радио, телевидение, интернет, мобильные телефоны: все эти шедшие друг за другом нововведения проникали всё глубже в нашу повседневную жизнь, становясь посредниками для всё большей части наших жизней. В плоть до конца 20 века, СМИ были чрезвычайно однонаправленными: информация текла в одну сторону, а внимание в другую. Критики обычно сосредотачивались на этом аспекте их устройства, обвиняя их в том, что они давали крохотной политической клике огромное влияние над обществом, закрепляя за остальными роли зрителей. В отличии от них, подпольные СМИ придерживались более интерактивных и децентрализованных форм. Всеобщее участие и децентрализация внезапно стали мейнстримом с появлением общедоступных цифровых медиа. Во многих отношениях, Интернет представлял из себя более раскрепощающее и укрепляющее самооценку пространство для новых методов коммуникации. Поскольку его базовая модель была разработана исследователями, финансируемыми военными, а не частным сектором, он проектировался для того, чтобы приносить пользу, а не прибыль. По сей день, большая часть Интернета остаётся своего рода «Диким Западом», в котором затруднительно следить за исполнением традиционного вещного права. Возможность делиться материалами бесплатно и напрямую с другими пользователями оказала огромное воздействие на некоторые отрасли, в то время как коллаборативные форматы вроде Википедии и ПО с открытым исходным кодом показали, как просто люди могут удовлетворять свои потребности без частной собственности. Корпорации по-прежнему ищут способы делания денег в Интернет кроме как через онлайн-торговлю и рекламу. Однако, по мере того как наши жизни становятся всё более и более оцифрованными, важно не принимать как данность то, что это всегда к лучшему. Капитализм разрастается за счёт захвата тех сторон мира, которые когда-то были свободными, а затем предлагает доступ к ним по высокой цене, и эта цена не всегда выплачивается в долларах. Мы должны быть особенно бдительны в отношении того, что новые медиа удобны: удобство может быть знаком того, что бесконечные возможности человеческой жизни принудительно сужаются. Действительно, эти инновации едва ли необязательны: в наши дни трудно поддерживать дружеские отношения, или устроиться на работу без сотового телефона и онлайн-профиля. Всё больше и больше наши умственные действия и общественная жизнь вынужденно подвергаются воздействию технологий, которые составляют схемы нашей деятельности и отношений для корпораций и правительственных разведслужб. Эти форматы также задают содержание этой деятельности и отношений. Сети, которые предлагает нам Facebook, не являются чем-то новым; новым является то, что они представляются внешними по отношению к нам самим. У нас всегда были социальные сети, но никто не мог их использовать для продажи рекламы—как и нелегко их было запротоколировать. Сейчас они вернулись как нечто, с чем мы должны считаться. Люди переписывались со старыми друзьями, обучались навыкам и узнавали об общественных мероприятиях задолго до появления электронной почты, Google и Twitter. Конечно, эти технологии чрезвычайно полезны в мире, где лишь немногие из нас имеют близкие отношения со своими соседями, или живут больше нескольких лет в одном и том же месте. Формы, которые принимает технология и повседневная жизнь, влияют друг на друга, что делает всё более невероятной возможность их разъединения. По мере того как наши потребности в информации и доступ к ней вырастают за пределы чего-то, что мы могли бы удержать в себе, информация, похоже, становится чем-то обособленным от нас. Это подозрительно напоминает принудительное отчуждение рабочих от продуктов их труда, которое превратило их в потребителей. Информация в интернете не совсем свободна — компьютеры и доступ в интернет стоят денег, не говоря уже о затратах на электричество и нагрузку на окружающую среду, которые они создают вместе с работающими серверами по всему миру. А что если корпорации догадаются брать с нас больше денег за доступ ко всем этим технологиям, как только мы станем полностью зависимыми от них? Если они смогут это сделать, не только власть и знания, но даже возможность поддерживать социальные связи станет полностью определяться богатством. Но было бы не правильно ожидать этого. Старая финансовая аристократия, возможно, вообще не сможет объединить свою власть на этой новой территории. Методы, которыми капитализм колонизирует наши жизни с помощью цифровых технологий не обязательно будут иметь сходства со старыми формами колонизации. Как и любая пирамида, капитализм должен постоянно расширяться, поглощая новые ресурсы и людей. Он уже распространился по всей планете; последняя война колонизации ведётся у подножия Гималайских гор, на самом краю света. Теоретически, он должен вскоре рухнуть, так как перспективы расширения исчезли. Но что если он продолжит расширяться внутрь нас, и эти новые технологии, подобно трём каравеллам Колумба, высаживаются на континент наших мыслительных процессов и социальных связей? В этом представлении, интернет выполняет функцию ещё одного следующего слоя отчуждения, основанного на материальной экономике. И если многое доступно в интернете бесплатно, то это не потому что процесс колонизации ещё не окончен, но, кроме всего прочего, потому, что определяющей валютой в СМИ являются не доллары, а внимание. В информационной экономике внимание функционирует подобно контролю над материальными ресурсами в индустриальной экономике. Даже когда внимание не преобразуется сразу в прибыль онлайн, оно может помочь обеспечить её в оффлайне. Как валюты, внимание и капитал ведут себя по-разному, но и то и другое служат созданию дисбаланса власти. Действительно, что такое капитал? Если отбросить предрассудки, которые делают его похожим на стихийную силу, он, по существу, является социальным конструктом, который позволяет некоторым людям накапливать власть над другими. Без идеи частной собственности, которая «реальна» настолько, насколько все её терпят, материальные ресурсы не могли бы функционировать как капитал. В этом смысле, право собственности служит той же цели, которой ранее служила идея божественного права королей: и то и другое формировало основы систем, определяющих власть. Некоторые люди неистово верят в право собственности, даже когда это право используется для лишения их всякого влияния в обществе. Можно сказать, что эти люди находятся под чарами собственности. Точно также, когда рекламный агент собирается сделать мем вирусным, вы можете сказать, что она пытается наложить заклятие. Если внимание является валютой медиа, его приобретение является способом заставить людей буквально и образно поддаться структурам власти. Определяющим фактором является не то, соглашаются ли люди с увиденным, или одобряют его, а то, в какой степени это задаёт их поведение. Цифровые медиа, похоже, владеют децентрализованным вниманием, но они также стандартизуют площадки, на которых оно циркулирует. Остерегайтесь объектов, которые накапливают внимание, даже если они никак не конвертируют его в финансовые активы. Настоящая власть Google и Facebook не в их финансовых активах, а в том, как они структурируют потоки информации. Навязывая единую логику коммуникации, отношений и наведения справок, они обеспечивают себе позицию посредников власти новой эры. За этими корпорациями стоит АНБ, которое сейчас обладает беспрецедентной способностью составлять схемы отношений и мыслительных процессов. Отслеживая поисковые запросы в Google, оно способно отслеживать ход мыслей пользователя сети интернет в реальном времени. У АНБ даже меньше потребность превращать использование интернет в непосредственную финансовую выгоду; валюта, которая ему нужна, это сама информация, с помощью которой можно управлять грубой силой государства. Роль государственного надзора заключается в поддержании условий, в которых корпорации вроде Facebook ведут бизнес; чем больше влияния накапливают эти корпорации, финансового или иного, тем больше влияния течёт обратно в руки государства. До скандала вокруг Prism, многие люди думали, что слежка и цензура характерны для таких мест, как Тунис и Сирия. На самом деле, большая часть технологий для цензуры поступает к этим режимам из Кремниевой Долины—и сперва применяется прямо здесь, в США. Так как даже самая мягкая интернет-цензура предполагает эффективное и всестороннее наблюдение, от регулирования до строгой изоляции требуется небольшой шаг. Чем сильнее мы зависим от цифровых технологий, тем сильнее мы уязвимы перед лицом огромных учреждений, на которые мы практически неспособны повлиять. Это не критика технологии как таковой. Суть в том, что она не нейтральна: технология всегда обусловлена структурами общества, в которых она разрабатывается и применяется. Большинство известных нам технологий были обусловлены императивами получения прибыли и власти, но общество, основанное на других ценностях, несомненно, создало бы другие технологии. По мере того как цифровые технологии всё больше впутываются в ткань нашего общества, важным вопросом становится не «стоит ли ими пользоваться», а «как подорвать структуры, которые их создали». Другими словами: сторонники свободного интернета должны задаться вопросом, совместима ли на самом деле его свобода с капитализмом и государством?\n\n\nКраткое содержание:\nИнтернет, как новое \"огораживание\", может быть использован для контроля над людьми и информацией. Интернет не является нейтральным, так как он обусловлен структурами общества, в котором он разрабатывается и применяется. Важно понимать, что свобода интернета может быть совместима с капитализмом и государством, но также необходимо бороться с системами, которые ограничивают свободу и создают зависимость от технологий.","summary":"Интернет, как новое \"огораживание\", может быть использован для контроля над людьми и информацией. Интернет не является нейтральным, так как он обусловлен структурами общества, в котором он разрабатывается и применяется. Важно понимать, что свобода интернета может быть совместима с капитализмом и государством, но также необходимо бороться с системами, которые ограничивают свободу и создают зависимость от технологий."}
{"Text":"Дайджест интересных новостей и материалов из мира PHP за последние две недели №19 (03.06.2013 — 17.06.2013) / Habr           \n\n17-06-2013\nПредлагаем вашему вниманию очередную подборку с ссылками на новости и материалы. Приятного чтения! Новости и релизы Symfony 2.3.0! Первый релиз с долгосрочной поддержкой! — Вышла новая версия одного из самых популярных PHP-фреймворков, и почти сразу появилось первое минорное обновление. Данный релиз примечателен тем, что будет поддерживаться до 2016 года. Если сомневаетесь, какую версию использовать, то специально для вас имеется полезная инструкция, которая кроме рекомендаций содержит дорожную карту развития фреймворка. Был написан также пост о том, как с минимальными усилиями обновлять версию Symfony на проекте. Кстати, сайт фреймворка — symfony.com отныне будет мультиязычным, любой желающий может помочь в переводе сайта используя соответствующий репозиторий на GitHub. Доступен PHP 5.5 RC3 — Исправлено несколько ошибок и предполагается, что это будет последний релиз-кандидат, а уже 20 июня стоит ожидать финальной версии. Доступны PHP 5.4.16 и PHP 5.3.26 — Также обновились актуальные ветки. Исправлены ошибки, в том числе безопасности, всем пользователям рекомендуется обновиться. Июньский выпуск журнала Web & PHP — В свежем номере раскрыты следующие темы: Введение в документо-ориентированные БД на примере MongoDB; PHP наизнанку: Julien Pauili о внутренностях интерпретатора; Жизнь разработчика Wordpress-плагинов; и другие. Встречаем HTML5 парсер и сериализатор для PHP — Авторы возродили умершую пару лет назад библиотеку php5lib: переписали парсер и сериализатор, добавили поддержку composer, и на выходе получили новый полезный инструмент. Pinboard — прокачиваем Pinba для мониторинга PHP — Отличное дополнение для pinba — сервера реального времени для мониторинга и сбора статистики в PHP, позволяющее собирать информацию не только в реальном времени, но и за продолжительные периоды, а затем просматривать ее в удобном виде. PHP The PHP Internals Book — Для тех, кому интересно, как устроен PHP внутри, а также для тех, кто хотел бы разрабатывать расширения или даже участвовать в разработке самого языка тремя ведущими участниками core-команды: Julien Pauli, Anthony Ferrara и Nikita Popov — была начата работа над книгой. Книга в основном расчитана на разработчиков имеющих опыт программирования на C, но информация, насколько это возможно, будет подана в виде, понятном и тем, кто не имел дела с языком C. Работа над книгой еще продолжается, но уже сейчас доступна глава, в которой рассмотрено устройство классов и объектов. Какой PHP-фреймворк вы используете? — На Хабре проскочил опрос о популярности различных PHP-фреймворков. Результат достаточно неожиданный, особенно в сравнении с прошлым опросом или, например, с данными с GitHub. Принятие версий PHP — Еще один обзор статистики использования различных версий PHP. Материалы для обучения Почему Phalcon быстрее любого другого PHP-фреймворка? — Пост раскрывает некоторые детали внутренней реализации действительно самого быстрого PHP-фреймворка. Proc_Open — связь с внешним миром — Хороший туториал, в котором сначала даны базовые понятия конвейера в Unix, а затем описано использование proc_open, приведен пример. Параллельное тестирование с помощью PHPUnit и ParaTest — Отличный пошаговый туториал, о том как запускать параллельное выполнение PHPUnit-тестов. Кроме ParaTest в посте также рассмотрен инструмент Paraunit, решающий ту же самую задачу. Также приведен пример использование ParaTest для параллельного выполнения Selenium-тестов. Интересующиеся темой параллельного тестирования должны ознакомиться с замечательным хабрапостом о параллелизации юнит-тестов, если еще не сделали этого. Упаковываем Sуmfony-приложение в один файл — введение — Этим постом создатель Symfony, Fabien Potencier, начинает серию статей о том, как объединить полный Symfony в один файл. Для начала он рассказывает о причинах, почему и когда это может быть полезно. Советы разработчикам — В посте собраны действительно полезные советы от опытного разработчика и руководителя группы пользователей PHP. Laravel 4 на Google AppEngine — Большой и подробный пошаговый туториал по разворачиванию стандартного Laravel4-приложения на облачном хостинге от Google, который не так давно обзавелся поддержкой PHP. Pseudo-Random vs. True Random — Небольшой эксперимент, в котором изображение, cгенерированное с помощью rand(), демонстрирует, насколько на самом деле «не случайна» последовательность генерируемых чисел. Подробнее о генераторах случайных чисел в PHP можно прочитать тут. Лучшие практики PhpStorm — идеальный рабочий процесс для PHP-разработчиков — Запись вебинара об использовании замечательной IDE. Будет особенно полезен тем, кто недавно начал использовать PhpStorm, но и опытным пользователям есть, что подсмотреть. Пишем музыку с помощью PHP — Хабраперевод засветившейся в прошлом выпуске дайджеста статьи, в которой автор показывает, как можно генерировать не совсем случайную последовательность нот. Для этого предлагается использовать механизм цепей Маркова и обучение на некотором наборе данных. Отладка Laravel с помощью MonoLog и FirePHP — Небольшая заметка, в которой показано, как настроить вывод логов из Monolog в FirePHP. Интервью с Vito Chin — Один из разработчиков расширения Gmagick и соавтор книги PHP Development in the Cloud рассказывает о своем опыте разработки и дает полезные советы. PHP REST API Frameworks — Обзор PHP-фреймворков, которые можно использовать для создания REST API. Сравнения автор, к сожалению, не проводил, обзор представляет собой внушительный список доступных решений с краткими описаниями и ссылками. Whoops! PHP-ошибки для крутых ребят — Туториал по использованию небольшой библиотеки, которая позволяет выводить красивые и главное информативные страницы ошибок и необработанных исключений. Управляем сложной системой с помощью паттерна «фасад» — Небольшой пост, в котором раскрыта суть «фасада», и на простейших примерах продемонстрировано его использование. PHPUnit: простой синтаксис для создания mock-объектов — Автор реализовал расширение для PHPUnit, позволяющее значительно упростить создание mock-обектов, сделав его коротким и наглядным. Конфигурируем Symfony2-приложение для обеспечения СОА — Ранее автор уже публиковал пост о рефакторинге приложения в рамках концепции сервис-ориентированной архитектуры. На этот раз автор взялся за реализацию описанных идей на примере Symfony2-приложения, однако описанные методики при желании могут быть без проблем перенесены на другой фреймворк. Как подготовить доклад и попасть на конференцию — Пост не имеет непосредственного отношения к разработке и PHP, но в силу огромного числа конференций с открытыми CFP, будет интересен многим опытным разработчикам желающим выступить на них с докладом. Избегайте лишних интерфейсов — Небольшая заметка с рекомендациями о правильном использовании интерфейсов. Экранирование (или что нужно знать для работы с текстом в тексте) — Хабраперевод, который содержит много примеров того, к чему приводит пренебрежение экранированием, и немного рекомендаций о том, как можно экранировать текст. Приступаем к тестированию — В посте говорится, о том, что несмотря на распространенный подход, при котором сначала реализуется модульное тестирование, затем функциональное и затем приемочное, очень часто можно пропустить этап модульного тестирования и приступить функциональным тестам — это совершенно нормально. Невозможность писать модульные тесты не должно быть оправданием отсутствия тестов вообще. Попробуйте Codeception. Silicone Skeleton — настроенный Silex — Автор поста подготовил хорошее seed-приложение для Silex. Тест на тему безопасности веб-приложений — 15 вопросов, которые помогут выявить пробелы в знаниях о безопасности веб-приложений. По каждому вопросу дано объяснение и ссылки для более глубокого изучения проблемы. Почему вам следует обратить внимание на Continuous Integration — FTP и редактирование файлов на продакшене — все мы через это проходили, но такой подход совершенно неприемлем в сколько-нибудь крупном проекте и числом разработчиков больше одного. К счастью, существует более эффективный подход к сборке и развертыванию приложения, о нем вкратце и пишет автор. Работаем с jQuery и Silex в качестве RestFull-провайдера — В своем предыдущем посте автор писал об использовании связки AngularJS + Silex. На этот раз вместо AngularJS автор демонстрирует использование обыкновенного jQuery. Внедряем Twig в ваш legacy-код — Если возникла необходимость применить продвинутый шаблонизатор, то этот пост станет неплохой отправной точкой. Кстати, PhpStrom поддерживает Twig. Как вы используете bcrypt для хэширования паролей на PHP? — Еще один пример исчерпывающего ответа на вопрос, заданный пользователем StackOverflow. Разработка web API — Отличный хабрапост о проектировании API, полезный для всех веб-разработчиков независимо от используемых технологий. Голландская PHP конференция 2013 — В Амстердаме прошла одна из крупнейших PHP-конференций в мире. По ссылке слайды 31 доклада. Кроме того имеется небольшой отчет от одного из участников. Ссылка на предыдущий выпуск.\n","metric":0.2958984375,"metric_input":"Текст статьи:\nДайджест интересных новостей и материалов из мира PHP за последние две недели №19 (03.06.2013 — 17.06.2013) / Habr           \n\n17-06-2013\nПредлагаем вашему вниманию очередную подборку с ссылками на новости и материалы. Приятного чтения! Новости и релизы Symfony 2.3.0! Первый релиз с долгосрочной поддержкой! — Вышла новая версия одного из самых популярных PHP-фреймворков, и почти сразу появилось первое минорное обновление. Данный релиз примечателен тем, что будет поддерживаться до 2016 года. Если сомневаетесь, какую версию использовать, то специально для вас имеется полезная инструкция, которая кроме рекомендаций содержит дорожную карту развития фреймворка. Был написан также пост о том, как с минимальными усилиями обновлять версию Symfony на проекте. Кстати, сайт фреймворка — symfony.com отныне будет мультиязычным, любой желающий может помочь в переводе сайта используя соответствующий репозиторий на GitHub. Доступен PHP 5.5 RC3 — Исправлено несколько ошибок и предполагается, что это будет последний релиз-кандидат, а уже 20 июня стоит ожидать финальной версии. Доступны PHP 5.4.16 и PHP 5.3.26 — Также обновились актуальные ветки. Исправлены ошибки, в том числе безопасности, всем пользователям рекомендуется обновиться. Июньский выпуск журнала Web & PHP — В свежем номере раскрыты следующие темы: Введение в документо-ориентированные БД на примере MongoDB; PHP наизнанку: Julien Pauili о внутренностях интерпретатора; Жизнь разработчика Wordpress-плагинов; и другие. Встречаем HTML5 парсер и сериализатор для PHP — Авторы возродили умершую пару лет назад библиотеку php5lib: переписали парсер и сериализатор, добавили поддержку composer, и на выходе получили новый полезный инструмент. Pinboard — прокачиваем Pinba для мониторинга PHP — Отличное дополнение для pinba — сервера реального времени для мониторинга и сбора статистики в PHP, позволяющее собирать информацию не только в реальном времени, но и за продолжительные периоды, а затем просматривать ее в удобном виде. PHP The PHP Internals Book — Для тех, кому интересно, как устроен PHP внутри, а также для тех, кто хотел бы разрабатывать расширения или даже участвовать в разработке самого языка тремя ведущими участниками core-команды: Julien Pauli, Anthony Ferrara и Nikita Popov — была начата работа над книгой. Книга в основном расчитана на разработчиков имеющих опыт программирования на C, но информация, насколько это возможно, будет подана в виде, понятном и тем, кто не имел дела с языком C. Работа над книгой еще продолжается, но уже сейчас доступна глава, в которой рассмотрено устройство классов и объектов. Какой PHP-фреймворк вы используете? — На Хабре проскочил опрос о популярности различных PHP-фреймворков. Результат достаточно неожиданный, особенно в сравнении с прошлым опросом или, например, с данными с GitHub. Принятие версий PHP — Еще один обзор статистики использования различных версий PHP. Материалы для обучения Почему Phalcon быстрее любого другого PHP-фреймворка? — Пост раскрывает некоторые детали внутренней реализации действительно самого быстрого PHP-фреймворка. Proc_Open — связь с внешним миром — Хороший туториал, в котором сначала даны базовые понятия конвейера в Unix, а затем описано использование proc_open, приведен пример. Параллельное тестирование с помощью PHPUnit и ParaTest — Отличный пошаговый туториал, о том как запускать параллельное выполнение PHPUnit-тестов. Кроме ParaTest в посте также рассмотрен инструмент Paraunit, решающий ту же самую задачу. Также приведен пример использование ParaTest для параллельного выполнения Selenium-тестов. Интересующиеся темой параллельного тестирования должны ознакомиться с замечательным хабрапостом о параллелизации юнит-тестов, если еще не сделали этого. Упаковываем Sуmfony-приложение в один файл — введение — Этим постом создатель Symfony, Fabien Potencier, начинает серию статей о том, как объединить полный Symfony в один файл. Для начала он рассказывает о причинах, почему и когда это может быть полезно. Советы разработчикам — В посте собраны действительно полезные советы от опытного разработчика и руководителя группы пользователей PHP. Laravel 4 на Google AppEngine — Большой и подробный пошаговый туториал по разворачиванию стандартного Laravel4-приложения на облачном хостинге от Google, который не так давно обзавелся поддержкой PHP. Pseudo-Random vs. True Random — Небольшой эксперимент, в котором изображение, cгенерированное с помощью rand(), демонстрирует, насколько на самом деле «не случайна» последовательность генерируемых чисел. Подробнее о генераторах случайных чисел в PHP можно прочитать тут. Лучшие практики PhpStorm — идеальный рабочий процесс для PHP-разработчиков — Запись вебинара об использовании замечательной IDE. Будет особенно полезен тем, кто недавно начал использовать PhpStorm, но и опытным пользователям есть, что подсмотреть. Пишем музыку с помощью PHP — Хабраперевод засветившейся в прошлом выпуске дайджеста статьи, в которой автор показывает, как можно генерировать не совсем случайную последовательность нот. Для этого предлагается использовать механизм цепей Маркова и обучение на некотором наборе данных. Отладка Laravel с помощью MonoLog и FirePHP — Небольшая заметка, в которой показано, как настроить вывод логов из Monolog в FirePHP. Интервью с Vito Chin — Один из разработчиков расширения Gmagick и соавтор книги PHP Development in the Cloud рассказывает о своем опыте разработки и дает полезные советы. PHP REST API Frameworks — Обзор PHP-фреймворков, которые можно использовать для создания REST API. Сравнения автор, к сожалению, не проводил, обзор представляет собой внушительный список доступных решений с краткими описаниями и ссылками. Whoops! PHP-ошибки для крутых ребят — Туториал по использованию небольшой библиотеки, которая позволяет выводить красивые и главное информативные страницы ошибок и необработанных исключений. Управляем сложной системой с помощью паттерна «фасад» — Небольшой пост, в котором раскрыта суть «фасада», и на простейших примерах продемонстрировано его использование. PHPUnit: простой синтаксис для создания mock-объектов — Автор реализовал расширение для PHPUnit, позволяющее значительно упростить создание mock-обектов, сделав его коротким и наглядным. Конфигурируем Symfony2-приложение для обеспечения СОА — Ранее автор уже публиковал пост о рефакторинге приложения в рамках концепции сервис-ориентированной архитектуры. На этот раз автор взялся за реализацию описанных идей на примере Symfony2-приложения, однако описанные методики при желании могут быть без проблем перенесены на другой фреймворк. Как подготовить доклад и попасть на конференцию — Пост не имеет непосредственного отношения к разработке и PHP, но в силу огромного числа конференций с открытыми CFP, будет интересен многим опытным разработчикам желающим выступить на них с докладом. Избегайте лишних интерфейсов — Небольшая заметка с рекомендациями о правильном использовании интерфейсов. Экранирование (или что нужно знать для работы с текстом в тексте) — Хабраперевод, который содержит много примеров того, к чему приводит пренебрежение экранированием, и немного рекомендаций о том, как можно экранировать текст. Приступаем к тестированию — В посте говорится, о том, что несмотря на распространенный подход, при котором сначала реализуется модульное тестирование, затем функциональное и затем приемочное, очень часто можно пропустить этап модульного тестирования и приступить функциональным тестам — это совершенно нормально. Невозможность писать модульные тесты не должно быть оправданием отсутствия тестов вообще. Попробуйте Codeception. Silicone Skeleton — настроенный Silex — Автор поста подготовил хорошее seed-приложение для Silex. Тест на тему безопасности веб-приложений — 15 вопросов, которые помогут выявить пробелы в знаниях о безопасности веб-приложений. По каждому вопросу дано объяснение и ссылки для более глубокого изучения проблемы. Почему вам следует обратить внимание на Continuous Integration — FTP и редактирование файлов на продакшене — все мы через это проходили, но такой подход совершенно неприемлем в сколько-нибудь крупном проекте и числом разработчиков больше одного. К счастью, существует более эффективный подход к сборке и развертыванию приложения, о нем вкратце и пишет автор. Работаем с jQuery и Silex в качестве RestFull-провайдера — В своем предыдущем посте автор писал об использовании связки AngularJS + Silex. На этот раз вместо AngularJS автор демонстрирует использование обыкновенного jQuery. Внедряем Twig в ваш legacy-код — Если возникла необходимость применить продвинутый шаблонизатор, то этот пост станет неплохой отправной точкой. Кстати, PhpStrom поддерживает Twig. Как вы используете bcrypt для хэширования паролей на PHP? — Еще один пример исчерпывающего ответа на вопрос, заданный пользователем StackOverflow. Разработка web API — Отличный хабрапост о проектировании API, полезный для всех веб-разработчиков независимо от используемых технологий. Голландская PHP конференция 2013 — В Амстердаме прошла одна из крупнейших PHP-конференций в мире. По ссылке слайды 31 доклада. Кроме того имеется небольшой отчет от одного из участников. Ссылка на предыдущий выпуск.\n\n\nКраткое содержание:\nВот краткое содержание технической статьи \"Дайджест интересных новостей и материалов из мира PHP за последние две недели №19 (03.06.2013 — 17.06.2013) / Habr\":\n\n1. Вышел новый релиз Symfony 2.3.0 с долгосрочной поддержкой до 2016 года.\n2. Релиз PHP 5.5 RC3, обновлены версии PHP 5.4.16 и PHP 5.3.26.\n3. Июньский выпуск журнала Web & PHP.\n4. Встречаем HTML5 парсер и сериализатор для PHP.\n5. Pinboard - прокачиваем Pinba для мониторинга PHP.\n6. PHP The PHP Internals Book - книга о внутреннем устройстве PHP.\n7. Какой PHP-фреймворк вы используете? - опрос на Хабре.\n8. Принятие версий PHP - обзор использования различных версий PHP.\n9. Материалы для обучения - статьи о производительности Phalcon, связи с внешним миром с помощью Proc_Open, параллельном тестировании с помощью PHPUnit и ParaTest, упаковке Symfony-приложения и других темах.\n10. Советы разработчикам - полезные советы от опытного разработчика.\n11. Laravel 4 на Google AppEngine - развертывание стандартного Laravel4-приложения на облачном хостинге.\n12. Pseudo-Random vs. True Random - эксперимент о генераторах случайных чисел в PHP.\n13. Лучшие практики PhpStorm - идеальный рабочий процесс для PHP-разработчиков.\n14. Пишем музыку с помощью PHP - генерация неслучайной последовательности нот.\n15. Отладка Laravel с помощью MonoLog и FirePHP - настройка вывода логов.\n16. Интервью с Vito Chin - опыт разработки и советы от разработчика расширения Gmagick.\n17. PHP REST API Frameworks - обзор","summary":"Вот краткое содержание технической статьи \"Дайджест интересных новостей и материалов из мира PHP за последние две недели №19 (03.06.2013 — 17.06.2013) / Habr\":\n\n1. Вышел новый релиз Symfony 2.3.0 с долгосрочной поддержкой до 2016 года.\n2. Релиз PHP 5.5 RC3, обновлены версии PHP 5.4.16 и PHP 5.3.26.\n3. Июньский выпуск журнала Web & PHP.\n4. Встречаем HTML5 парсер и сериализатор для PHP.\n5. Pinboard - прокачиваем Pinba для мониторинга PHP.\n6. PHP The PHP Internals Book - книга о внутреннем устройстве PHP.\n7. Какой PHP-фреймворк вы используете? - опрос на Хабре.\n8. Принятие версий PHP - обзор использования различных версий PHP.\n9. Материалы для обучения - статьи о производительности Phalcon, связи с внешним миром с помощью Proc_Open, параллельном тестировании с помощью PHPUnit и ParaTest, упаковке Symfony-приложения и других темах.\n10. Советы разработчикам - полезные советы от опытного разработчика.\n11. Laravel 4 на Google AppEngine - развертывание стандартного Laravel4-приложения на облачном хостинге.\n12. Pseudo-Random vs. True Random - эксперимент о генераторах случайных чисел в PHP.\n13. Лучшие практики PhpStorm - идеальный рабочий процесс для PHP-разработчиков.\n14. Пишем музыку с помощью PHP - генерация неслучайной последовательности нот.\n15. Отладка Laravel с помощью MonoLog и FirePHP - настройка вывода логов.\n16. Интервью с Vito Chin - опыт разработки и советы от разработчика расширения Gmagick.\n17. PHP REST API Frameworks - обзор"}
{"Text":"Вот это поворот: почему Apple изменила требования к разработчикам приложений / Habr             \n\n20-07-2019\nВ прошлом месяце Apple опубликовала изменения в гайдлайнах для разработчиков приложений под iOS. Они коснулись, в том числе, приложений для родительского контроля и обеспечения конфиденциальности детей. Причем часть новых рекомендаций идет вразрез с решениями, которые Apple приняла несколькими месяцами ранее. Обсуждаем требования к приложениям в App Store, которые вступят в силу 3 сентября этого года, и разбираемся в причинах — а также возможных долгоиграющих последствиях — этой ситуации. Фото — Aziz Acharki — Unsplash Что изменилось — коротко Помимо презентации нового Mac Pro, Pro Display XDR и iOS 13 в начале июня Apple анонсировала и определенные изменения в гайдлайнах для разработчиков приложений. Короткий пост обо всех этих изменениях вышел в новостном разделе портала Apple Developer 3 июня. Одно из нововведений, которое вызвало особенно бурную реакцию американских СМИ, — запрет на использование «сторонних аналитических сервисов или рекламы» в приложениях для детей (разделы 1.3 и 5.1.4 гайдлайнов). Как указано в пункте 5.1.4, «сбор и передача данных третьей стороне приложениями из категории «Дети» запрещены». Требование действует для новых приложений, а существующие нужно привести в соответствие до 3 сентября 2019 года. Кроме того, к детям имеют непосредственное отношение и нововведения пунктов 5.4 и 5.5. По новым правилам, приложения, осуществляющие функции родительского контроля, могут «в ограниченных случаях» использовать технологию управления мобильными устройствами (Mobile Device Management, MDM) и API NEVPNManager. При этом в Apple подчеркивают, что «приложения должны иметь полезную функциональность или развлекательную ценность вне зависимости от возраста пользователя» — будь то ребенок или взрослый. Почему решили изменить политики — и именно сейчас В гайдлайнах Apple ссылаются прежде всего на требования американских и европейских законодателей, а именно на Закон о защите конфиденциальности детей в интернете (Children’s Online Privacy Protection Act, COPPA) и GDPR. Но отмечают, что при создании детских приложений нужно учитывать и опыт других стран и их требований к защите персональных данных. Правда, есть основания предполагать, что сами по себе COPPA и GDPR — не единственная причина ограничений. Некоторые СМИ напрямую связывают изменение политик с резонансным отчетом Wall Street Journal, согласно которому 79 из 80 протестированных ими приложений из App Store использовали в среднем по четыре трекинговых системы для сбора аналитики, показа рекламы или маркетинговых целей. Журналисты отдельно остановились на одном из «детских» приложений, которое сохраняло имя и возраст ребенка, а также информацию о его действиях в приложении, а затем передавало данные Facebook. Критика Wall Street Journal оказалась особенно болезненной — незадолго до этого Apple запустили рекламную кампанию, слоганом которой стала фраза: «Все, что происходит в вашем iPhone, остается на вашем iPhone». Авторы материала выражали несогласие не столько с самим фактом сбора и передачи данных, а с непрозрачностью всей системы, при которой пользователь не знает, что в действительности делает то или иное приложение. В Apple в ответ выпустили заявление о том, что компания не вмешивается в отношения между разработчиком и пользователем, но в будущем планирует работать над улучшением безопасности и сохранности персональных данных. И новые политики, видимо, стали результатом этой работы. Неслучайны и послабления в пунктах 5.4-5.5. Дело в том, что еще в апреле Apple наложила ряд ограничений на приложения, реализующие функции родительского контроля и использующие технологии MDM. Компания тогда объяснила ситуацию тем, что приложения нарушали политики App Store, еще с 2017-го года предписывавшие разработчикам отказаться от MDM-функциональности. Фото — Tyler Gardon — Unsplash Однако «репрессиям» приложения подверглись сравнительно недавно, после того, как в iOS 12 появился сходный по функциям сервис Screen Time. Ситуация вызвала вопросы не только у независимых разработчиков приложений, но и у американского правительства — примечательно, что пост об изменении гайдлайнов и отмене «запрета на MDM» вышел в тот же день, что и материал New York Times с информацией о том, что Apple хотят проверить на предмет нарушения антимонопольного законодательства США в связи с ситуацией в App Store. Кто еще теперь озабочен защитой конфиденциальности детей Случай с Apple — не единичный. В похожей ситуации чуть раньше оказалась Google, которая два месяца назад также выпустила обновление политик в отношении приложений для детей, доступных в Google Play. На этот шаг компанию побудила пойти разбирательство Федеральной торговой комиссии США (FTC), согласно которому компания не обеспечивала соблюдения разработчиками приложений требований закона COPPA. Помимо Google Play под прицелом FTC оказался и YouTube. Согласно данным комиссии, сервис также не соответствует требованиям уже упомянутого Закона о защите конфиденциальности детей в интернете. По мнению Washington Post, ситуация с YouTube сигнализирует, что Федеральная торговая комиссия хочет серьезно усилить контроль в этой сфере — он значительно ослаб с момента вступления COPPA в силу в 1998 году. Правда, и YouTube оказался далеко не первым сервисом, который попал в поле зрения FTC. Комиссия уже успела оштрафовать ряд компаний, выпускающих приложения для детей, на рекордные $ 5,8 млн за несоблюдение COPPA. Возвращаясь к Apple: что думают разработчики Большинство критиков сходятся во мнении, что политика компании как минимум непоследовательна, что и подтверждают изменения в правилах. Wall Street Journal идет дальше и заявляет, что если и искать крайнего в ситуации с несанкционированным использованием персональных данных, то основная вина будет лежать на Apple, а не на разработчиках. Фото — Rita Morais — Unsplash Кстати, кое-кто из них уже высказался по поводу новых политик. Одной из компаний, которые подпадают под действие новых требований, стала PBS и ее подразделение PBS Kids (этот канал, к примеру, транслировал «Улицу Сезам» с 1969 по 2016 годы). PBS пояснили, что теперь они не смогут адекватно оценивать эффективность работы приложений и, как следствие, улучшать их функциональность и образовательные возможности. Как отметила генеральный директор канала Пола Кергер (Paula Kerger), PBS будут вынуждены убрать приложения из App Store, при том, что сейчас ими пользуются миллионы детей. В PBS считают, что принятие таких решений без учета мнения самих разработчиков недопустимо. «Мы хотим донести мысль, что нам нравится работать с этими платформами [такими как App Store], они дают нам потрясающий охват. Сядьте и поговорите с нами», — комментирует ситуацию Кергер. По поводу обновления политик высказались на Hacker News и форуме Apple Developer. Большинство поддержало Apple в стремлении оградить детей от рекламы, однако некоторые отметили, что сама формулировка, которую использовали в гайдлайне, позволяет трактовать новые правила двояко. Так, п.1.3 содержит фразу «Apps in the Kids Category may not include third-party advertising or analytics» (дословно «Приложения в категории «Дети» не могут включать стороннюю аналитику и рекламу»). Остается неясно, к чему в этом случае относится прилагательное «сторонний» (third-party) — к рекламе и аналитическим сервисам или только к рекламе. Во втором случае компании все-таки могут использовать собственные разработки и применять их для непосредственного анализа поведения пользователей. Один из комментаторов в треде на Reddit отметил, что если бы компания хотела дать непротиворечивое указание, то сформулировала бы требование иначе (Apps in the Kids Category must not include analytics or third-party advertising). А официальная формулировка дает Apple право «казнить или миловать» разработчиков по своему усмотрению. Выводы Последние события, похоже, вынуждают пересмотреть статус-кво крупных ИТ-корпораций. Что касается Apple, то все чаще звучит мнение о том, что компания должна выбрать сторону, и не может оставаться владельцем App Store и одновременно игроком на рынке приложений. К каким результатам приведет текущая политика компании, предстоит увидеть в ближайшее время. Посты из наших блогов и социальных сетей: Персональные данные: особенности публичного облака Получение OV и EV сертификата — что нужно знать? Как IaaS помогает франчайзи «1С»: опыт 1cloud Эволюция архитектуры облака 1cloud Как обезопасить Linux-систему: 10 советов F.A.Q. по частному облаку от 1cloud Мифы об облачных технологиях В OIN больше трех тысяч лицензиатов — что это значит для открытого ПО Как оценить производительность СХД на Linux: бенчмаркинг с помощью открытых инструментов\n","metric":0.6396484375,"metric_input":"Текст статьи:\nВот это поворот: почему Apple изменила требования к разработчикам приложений / Habr             \n\n20-07-2019\nВ прошлом месяце Apple опубликовала изменения в гайдлайнах для разработчиков приложений под iOS. Они коснулись, в том числе, приложений для родительского контроля и обеспечения конфиденциальности детей. Причем часть новых рекомендаций идет вразрез с решениями, которые Apple приняла несколькими месяцами ранее. Обсуждаем требования к приложениям в App Store, которые вступят в силу 3 сентября этого года, и разбираемся в причинах — а также возможных долгоиграющих последствиях — этой ситуации. Фото — Aziz Acharki — Unsplash Что изменилось — коротко Помимо презентации нового Mac Pro, Pro Display XDR и iOS 13 в начале июня Apple анонсировала и определенные изменения в гайдлайнах для разработчиков приложений. Короткий пост обо всех этих изменениях вышел в новостном разделе портала Apple Developer 3 июня. Одно из нововведений, которое вызвало особенно бурную реакцию американских СМИ, — запрет на использование «сторонних аналитических сервисов или рекламы» в приложениях для детей (разделы 1.3 и 5.1.4 гайдлайнов). Как указано в пункте 5.1.4, «сбор и передача данных третьей стороне приложениями из категории «Дети» запрещены». Требование действует для новых приложений, а существующие нужно привести в соответствие до 3 сентября 2019 года. Кроме того, к детям имеют непосредственное отношение и нововведения пунктов 5.4 и 5.5. По новым правилам, приложения, осуществляющие функции родительского контроля, могут «в ограниченных случаях» использовать технологию управления мобильными устройствами (Mobile Device Management, MDM) и API NEVPNManager. При этом в Apple подчеркивают, что «приложения должны иметь полезную функциональность или развлекательную ценность вне зависимости от возраста пользователя» — будь то ребенок или взрослый. Почему решили изменить политики — и именно сейчас В гайдлайнах Apple ссылаются прежде всего на требования американских и европейских законодателей, а именно на Закон о защите конфиденциальности детей в интернете (Children’s Online Privacy Protection Act, COPPA) и GDPR. Но отмечают, что при создании детских приложений нужно учитывать и опыт других стран и их требований к защите персональных данных. Правда, есть основания предполагать, что сами по себе COPPA и GDPR — не единственная причина ограничений. Некоторые СМИ напрямую связывают изменение политик с резонансным отчетом Wall Street Journal, согласно которому 79 из 80 протестированных ими приложений из App Store использовали в среднем по четыре трекинговых системы для сбора аналитики, показа рекламы или маркетинговых целей. Журналисты отдельно остановились на одном из «детских» приложений, которое сохраняло имя и возраст ребенка, а также информацию о его действиях в приложении, а затем передавало данные Facebook. Критика Wall Street Journal оказалась особенно болезненной — незадолго до этого Apple запустили рекламную кампанию, слоганом которой стала фраза: «Все, что происходит в вашем iPhone, остается на вашем iPhone». Авторы материала выражали несогласие не столько с самим фактом сбора и передачи данных, а с непрозрачностью всей системы, при которой пользователь не знает, что в действительности делает то или иное приложение. В Apple в ответ выпустили заявление о том, что компания не вмешивается в отношения между разработчиком и пользователем, но в будущем планирует работать над улучшением безопасности и сохранности персональных данных. И новые политики, видимо, стали результатом этой работы. Неслучайны и послабления в пунктах 5.4-5.5. Дело в том, что еще в апреле Apple наложила ряд ограничений на приложения, реализующие функции родительского контроля и использующие технологии MDM. Компания тогда объяснила ситуацию тем, что приложения нарушали политики App Store, еще с 2017-го года предписывавшие разработчикам отказаться от MDM-функциональности. Фото — Tyler Gardon — Unsplash Однако «репрессиям» приложения подверглись сравнительно недавно, после того, как в iOS 12 появился сходный по функциям сервис Screen Time. Ситуация вызвала вопросы не только у независимых разработчиков приложений, но и у американского правительства — примечательно, что пост об изменении гайдлайнов и отмене «запрета на MDM» вышел в тот же день, что и материал New York Times с информацией о том, что Apple хотят проверить на предмет нарушения антимонопольного законодательства США в связи с ситуацией в App Store. Кто еще теперь озабочен защитой конфиденциальности детей Случай с Apple — не единичный. В похожей ситуации чуть раньше оказалась Google, которая два месяца назад также выпустила обновление политик в отношении приложений для детей, доступных в Google Play. На этот шаг компанию побудила пойти разбирательство Федеральной торговой комиссии США (FTC), согласно которому компания не обеспечивала соблюдения разработчиками приложений требований закона COPPA. Помимо Google Play под прицелом FTC оказался и YouTube. Согласно данным комиссии, сервис также не соответствует требованиям уже упомянутого Закона о защите конфиденциальности детей в интернете. По мнению Washington Post, ситуация с YouTube сигнализирует, что Федеральная торговая комиссия хочет серьезно усилить контроль в этой сфере — он значительно ослаб с момента вступления COPPA в силу в 1998 году. Правда, и YouTube оказался далеко не первым сервисом, который попал в поле зрения FTC. Комиссия уже успела оштрафовать ряд компаний, выпускающих приложения для детей, на рекордные $ 5,8 млн за несоблюдение COPPA. Возвращаясь к Apple: что думают разработчики Большинство критиков сходятся во мнении, что политика компании как минимум непоследовательна, что и подтверждают изменения в правилах. Wall Street Journal идет дальше и заявляет, что если и искать крайнего в ситуации с несанкционированным использованием персональных данных, то основная вина будет лежать на Apple, а не на разработчиках. Фото — Rita Morais — Unsplash Кстати, кое-кто из них уже высказался по поводу новых политик. Одной из компаний, которые подпадают под действие новых требований, стала PBS и ее подразделение PBS Kids (этот канал, к примеру, транслировал «Улицу Сезам» с 1969 по 2016 годы). PBS пояснили, что теперь они не смогут адекватно оценивать эффективность работы приложений и, как следствие, улучшать их функциональность и образовательные возможности. Как отметила генеральный директор канала Пола Кергер (Paula Kerger), PBS будут вынуждены убрать приложения из App Store, при том, что сейчас ими пользуются миллионы детей. В PBS считают, что принятие таких решений без учета мнения самих разработчиков недопустимо. «Мы хотим донести мысль, что нам нравится работать с этими платформами [такими как App Store], они дают нам потрясающий охват. Сядьте и поговорите с нами», — комментирует ситуацию Кергер. По поводу обновления политик высказались на Hacker News и форуме Apple Developer. Большинство поддержало Apple в стремлении оградить детей от рекламы, однако некоторые отметили, что сама формулировка, которую использовали в гайдлайне, позволяет трактовать новые правила двояко. Так, п.1.3 содержит фразу «Apps in the Kids Category may not include third-party advertising or analytics» (дословно «Приложения в категории «Дети» не могут включать стороннюю аналитику и рекламу»). Остается неясно, к чему в этом случае относится прилагательное «сторонний» (third-party) — к рекламе и аналитическим сервисам или только к рекламе. Во втором случае компании все-таки могут использовать собственные разработки и применять их для непосредственного анализа поведения пользователей. Один из комментаторов в треде на Reddit отметил, что если бы компания хотела дать непротиворечивое указание, то сформулировала бы требование иначе (Apps in the Kids Category must not include analytics or third-party advertising). А официальная формулировка дает Apple право «казнить или миловать» разработчиков по своему усмотрению. Выводы Последние события, похоже, вынуждают пересмотреть статус-кво крупных ИТ-корпораций. Что касается Apple, то все чаще звучит мнение о том, что компания должна выбрать сторону, и не может оставаться владельцем App Store и одновременно игроком на рынке приложений. К каким результатам приведет текущая политика компании, предстоит увидеть в ближайшее время. Посты из наших блогов и социальных сетей: Персональные данные: особенности публичного облака Получение OV и EV сертификата — что нужно знать? Как IaaS помогает франчайзи «1С»: опыт 1cloud Эволюция архитектуры облака 1cloud Как обезопасить Linux-систему: 10 советов F.A.Q. по частному облаку от 1cloud Мифы об облачных технологиях В OIN больше трех тысяч лицензиатов — что это значит для открытого ПО Как оценить производительность СХД на Linux: бенчмаркинг с помощью открытых инструментов\n\n\nКраткое содержание:\nВ статье обсуждаются изменения в политике Apple для разработчиков приложений, которые вступят в силу 3 сентября 2019 года. Изменения касаются ограничений на использование сторонних аналитических сервисов и рекламы в приложениях для детей. Обсуждается причина этих изменений и реакция разработчиков и критиков.","summary":"В статье обсуждаются изменения в политике Apple для разработчиков приложений, которые вступят в силу 3 сентября 2019 года. Изменения касаются ограничений на использование сторонних аналитических сервисов и рекламы в приложениях для детей. Обсуждается причина этих изменений и реакция разработчиков и критиков."}
{"Text":"Как положить инвестора на лопатки или три принципа Ходжи Насреддина / Хабр                                                             \n\n16-07-2012\nНаверное, не надо много говорить о том, что подавляющее большинство авторов ещё не воплощенных в жизнь инновационных идей имеют, мягко говоря, финансовое состояние, оставляющее желать лучшего. А ведь, для перехода от «гениальных разработок» к их практической реализации нужны, как оказывается, ещё и деньги. И вот, в жизни «эдисонов» и «кулибиных» начинаются бесконечные мытарства по поиску этих «жалких» сумм. Носители «бесценности» идут в различные инстанции и институции, обивают пороги государственных или частных ведомств, чтобы найти для своего стартапа щедрого инвестора, который «непременно западёт на столь многообещающее предложение», каким оно представляется лично самому «отцу революционной разработки», и выложит-таки круглые цифры. Однако, как правило, эта огромная масса ходоков повсюду сталкивается с холодным «непониманием» у потенциальных инвесторов. И лишь единицы из этого океана страждущих и ищущих удостаиваются счастья быть осыпанным искомым денежным дождём, пусть, порою и на весьма кабальных условиях, зато, хоть, идея не умирает. Почему же происходит такое? Является ли это нормальным и как можно переломить эту негативную статистику в другую сторону? Сие и будет темой нашего разговора после ката. Начнём с того, что, несмотря на свои «креативные мозги», изобретатели, как показывает практика, плохо разбираются в вещах, которые действительно могут дать жизнь их детищам. Дело, наверное, в том, что, будучи целиком поглощенными поиском каких-нибудь бизнес-ангелов они не задумываются или не догадываются, что причина их неудач может крыться в них самих. Я сам прошёл все круги этого ада, пока хорошие друзья не поделились со мною достаточно простыми, но очень эффективными принципами, которых следует придерживаться при взаимодействии с потенциальными инвесторами. Уверен, что они могут помочь всем тем, кто повторяет мой неудачный опыт. Для себя я назвал их «Подходами Ходжи Насреддина». Многие наверное помнят похождения сказочно-фольклорного героя восточных сказаний, Ходжи Насреддина. Однажды, проходя по мосту через водоём, он увидел как дехкане кричали какому-то утопающему, свалившемуся с моста в воду: «Давай, давай руку...» Но, тонущий, хоть и барахтался, периодически уходя под воду, руки протягивать всё же не желал. Тогда Ходжа, узнав, кто именно тонет, подошёл к краю моста, протянул руку со сжатым в кулак ладонью и крикнул: «На, бери!» И, о чудо, тонущий ухватился за руку Насреддина и был таким образом спасён. То был мулла, менталитет которого был заточен вовсе не на то, чтобы «давать», а ровно на то, чтобы только «брать». Здесь любимец восточного народа воспользовался Принципом №1, который звучит, как: «На, бери!» Так вот, знайте, рефлексы Ваших потенциальных инвесторов весьма похожи на ту самую реакцию муллы, поскольку менталитет богача настроен вовсе не на Ваше обогащение и не на удовлетворение лично Ваших амбиций, а он работает сугубо на удовлетворение меркантильных потребностей своего хозяина. К сожалению, мама Тереза уже умерла и приходится ориентироваться на реалии. Поэтому, идя к инвестору, которого почему-то зовут «бизнес-ангел», Вы должны помнить о том, что капиталист никогда НЕ ДАЁТ, а лишь покупает «задёшево», чтобы продать потом «за дорого». Следовательно, Вам необходимо подладиться под его менталитет и при встрече с ним Вам надо просто сказать: «Возьмите, я принёс и даю Вам это богатство!» Любое другое начало, иное вступление в деловые отношения с возможным финансистом Ваших проектов наглухо отрежет Вам путь восхождения к желанному Олимпу, то бишь, искомым инвестициям. Эта действительно магическая фраза выступает в роли ключа к замку на пути к сердцу инвестора. Но, к сожалению, ключа, хоть и необходимого, но ещё далеко не достаточного, не последнего. Данный подход Насреддина, как Приницип №2, гласит: «Поставь себя на место инвестора». То есть, начни думать, как он, войди в его шкуру. Ваш потенциальный инвестор ни черта не смыслит в сущности того, что пришло в Вашу гениальную голову. Зато он хорошо владеет арифметикой и умеет четко стучать по кнопкам калькулятора, вычисляя сложный процент от вложенных «кровных». Капиталисту абсолютно важно чётко уяснить самому себе, сколько, когда и каким образом он вернёт вложенное и начнёт получать свою прибыль. Этому денежному мешку совершенно не интересно, что Вами найдено оригинальное решение каким-то совершенно умопомрачительно уникальным способом, доставляющим сильное эстетическое удовольствие иным гурманам «креативной мысли». Ему просто надо понять, будет ли эта Ваша «идейка» пользоваться спросом у покупателей и каков её потребительский рынок? Сможет ли Ваш продукт или услуга найти свою нишу на переполненном рынке, пусть даже путём расталкивания других конкурентов, уже давно присутствующих на этом «подсолнечном месте»? Для такой конкретной оценки Вашего детища у инвестора есть «свои эксперты по рынку», которые, даже если и немного кумекают в Ваших презентационных чертежах и графиках, очень наглядно (с Вашей точки зрени) демонстрирующих и доказывающих эффективную работоспособность и скорую прибыль от представленного Вами инновационного проекта, тем не менее, в 99,99% случаев всё же дадут НЕГАТИВНЫЙ вердикт, поскольку они, эти эксперты, сидят на довольно теплом месте с достойной зарплатой и совсем не хотят рисковать перед боссом своим положением, пока сами чётко не увидят уже раскручивающуюся фирму, причём с нарастающими не по дням, а по часам доходами, которой собственно, у Вас ещё нет, а если бы таковая была, то Вы и не пришли бы просить деньги у этого жмота. При этом надо знать, что у инвестора есть один «железобетонный критерий», которым он пользуется для рыночных оценок. Ваш будущий продукт или услуга должны ВЫТЕСНИТЬ с рынка другой уже раскрученный бренд, чётко оцененный в параметрах продаж по дням, месяцам и годам. Такое может случиться в одном лишь случае, когда Ваши продукты или услуги при всех прочих сравнительно равных оценках по всем возможным ключевым качественным и количественным показателям смогут разниться с присутствующим на рынке своего ближайшего конкурентного аналога в одном ключевом компоненте — они должны быть ВПОЛОВИНУ ДЕШЕВЛЕ. Только тогда у Вашего потенциального инвестора может открыться второй замок и его рука начнёт движение к тому месту, где лежат его деньги, которые он уже собрался Вам выдать. Однако, по дороге он инстинктивно остановит своё движение, так там висит ещё один замок, и кликнет других специалистов, теперь уже «экспертов по рискам». Эти деятели – самые опасные для Вас. Они могут вернуть назад руку своего шефа, который уже, вроде бы, окончательно склонялся в Вашу сторону. Поэтому Вам надо знать и про третий подход Насреддина, который провозглашает: «Создай фон уверенности». Надо помнить, что, практически, любой инвестор постоянно трясётся по поводу возможных рисков, из-за которых он может безвозвратно потерять вложенное. А это его такое состояние является не лучшим фоном, при котором испрашиваются искомые деньги. Поэтому согласно Насреддиновскому Принципу №3 Вам надлежит создать у потенциального финансиста ПОЛНУЮ УВЕРЕННОСТЬ, что, по самой крайней мере, его деньги ну никак не пропадут. Как это сделать, тут должна проявиться вся Ваша фантазия, поскольку выбранный Вами вариант сильно зависит непосредственно от существа Вашей инновации. Но есть один приём, который можно назвать универсальным. Продумайте и предложите инвестору ПОЭТАПНЫЙ ПЛАН реализации всего Вашего проекта. Начните с небольшого участка, объема, узла..., с того, что требует минимальных вложений, а затем, за два-три этапа промасштабируйте развитие проекта до полной его рыночной реализации. Удачи Вам!\n","metric":0.63037109380000000414,"metric_input":"Текст статьи:\nКак положить инвестора на лопатки или три принципа Ходжи Насреддина / Хабр                                                             \n\n16-07-2012\nНаверное, не надо много говорить о том, что подавляющее большинство авторов ещё не воплощенных в жизнь инновационных идей имеют, мягко говоря, финансовое состояние, оставляющее желать лучшего. А ведь, для перехода от «гениальных разработок» к их практической реализации нужны, как оказывается, ещё и деньги. И вот, в жизни «эдисонов» и «кулибиных» начинаются бесконечные мытарства по поиску этих «жалких» сумм. Носители «бесценности» идут в различные инстанции и институции, обивают пороги государственных или частных ведомств, чтобы найти для своего стартапа щедрого инвестора, который «непременно западёт на столь многообещающее предложение», каким оно представляется лично самому «отцу революционной разработки», и выложит-таки круглые цифры. Однако, как правило, эта огромная масса ходоков повсюду сталкивается с холодным «непониманием» у потенциальных инвесторов. И лишь единицы из этого океана страждущих и ищущих удостаиваются счастья быть осыпанным искомым денежным дождём, пусть, порою и на весьма кабальных условиях, зато, хоть, идея не умирает. Почему же происходит такое? Является ли это нормальным и как можно переломить эту негативную статистику в другую сторону? Сие и будет темой нашего разговора после ката. Начнём с того, что, несмотря на свои «креативные мозги», изобретатели, как показывает практика, плохо разбираются в вещах, которые действительно могут дать жизнь их детищам. Дело, наверное, в том, что, будучи целиком поглощенными поиском каких-нибудь бизнес-ангелов они не задумываются или не догадываются, что причина их неудач может крыться в них самих. Я сам прошёл все круги этого ада, пока хорошие друзья не поделились со мною достаточно простыми, но очень эффективными принципами, которых следует придерживаться при взаимодействии с потенциальными инвесторами. Уверен, что они могут помочь всем тем, кто повторяет мой неудачный опыт. Для себя я назвал их «Подходами Ходжи Насреддина». Многие наверное помнят похождения сказочно-фольклорного героя восточных сказаний, Ходжи Насреддина. Однажды, проходя по мосту через водоём, он увидел как дехкане кричали какому-то утопающему, свалившемуся с моста в воду: «Давай, давай руку...» Но, тонущий, хоть и барахтался, периодически уходя под воду, руки протягивать всё же не желал. Тогда Ходжа, узнав, кто именно тонет, подошёл к краю моста, протянул руку со сжатым в кулак ладонью и крикнул: «На, бери!» И, о чудо, тонущий ухватился за руку Насреддина и был таким образом спасён. То был мулла, менталитет которого был заточен вовсе не на то, чтобы «давать», а ровно на то, чтобы только «брать». Здесь любимец восточного народа воспользовался Принципом №1, который звучит, как: «На, бери!» Так вот, знайте, рефлексы Ваших потенциальных инвесторов весьма похожи на ту самую реакцию муллы, поскольку менталитет богача настроен вовсе не на Ваше обогащение и не на удовлетворение лично Ваших амбиций, а он работает сугубо на удовлетворение меркантильных потребностей своего хозяина. К сожалению, мама Тереза уже умерла и приходится ориентироваться на реалии. Поэтому, идя к инвестору, которого почему-то зовут «бизнес-ангел», Вы должны помнить о том, что капиталист никогда НЕ ДАЁТ, а лишь покупает «задёшево», чтобы продать потом «за дорого». Следовательно, Вам необходимо подладиться под его менталитет и при встрече с ним Вам надо просто сказать: «Возьмите, я принёс и даю Вам это богатство!» Любое другое начало, иное вступление в деловые отношения с возможным финансистом Ваших проектов наглухо отрежет Вам путь восхождения к желанному Олимпу, то бишь, искомым инвестициям. Эта действительно магическая фраза выступает в роли ключа к замку на пути к сердцу инвестора. Но, к сожалению, ключа, хоть и необходимого, но ещё далеко не достаточного, не последнего. Данный подход Насреддина, как Приницип №2, гласит: «Поставь себя на место инвестора». То есть, начни думать, как он, войди в его шкуру. Ваш потенциальный инвестор ни черта не смыслит в сущности того, что пришло в Вашу гениальную голову. Зато он хорошо владеет арифметикой и умеет четко стучать по кнопкам калькулятора, вычисляя сложный процент от вложенных «кровных». Капиталисту абсолютно важно чётко уяснить самому себе, сколько, когда и каким образом он вернёт вложенное и начнёт получать свою прибыль. Этому денежному мешку совершенно не интересно, что Вами найдено оригинальное решение каким-то совершенно умопомрачительно уникальным способом, доставляющим сильное эстетическое удовольствие иным гурманам «креативной мысли». Ему просто надо понять, будет ли эта Ваша «идейка» пользоваться спросом у покупателей и каков её потребительский рынок? Сможет ли Ваш продукт или услуга найти свою нишу на переполненном рынке, пусть даже путём расталкивания других конкурентов, уже давно присутствующих на этом «подсолнечном месте»? Для такой конкретной оценки Вашего детища у инвестора есть «свои эксперты по рынку», которые, даже если и немного кумекают в Ваших презентационных чертежах и графиках, очень наглядно (с Вашей точки зрени) демонстрирующих и доказывающих эффективную работоспособность и скорую прибыль от представленного Вами инновационного проекта, тем не менее, в 99,99% случаев всё же дадут НЕГАТИВНЫЙ вердикт, поскольку они, эти эксперты, сидят на довольно теплом месте с достойной зарплатой и совсем не хотят рисковать перед боссом своим положением, пока сами чётко не увидят уже раскручивающуюся фирму, причём с нарастающими не по дням, а по часам доходами, которой собственно, у Вас ещё нет, а если бы таковая была, то Вы и не пришли бы просить деньги у этого жмота. При этом надо знать, что у инвестора есть один «железобетонный критерий», которым он пользуется для рыночных оценок. Ваш будущий продукт или услуга должны ВЫТЕСНИТЬ с рынка другой уже раскрученный бренд, чётко оцененный в параметрах продаж по дням, месяцам и годам. Такое может случиться в одном лишь случае, когда Ваши продукты или услуги при всех прочих сравнительно равных оценках по всем возможным ключевым качественным и количественным показателям смогут разниться с присутствующим на рынке своего ближайшего конкурентного аналога в одном ключевом компоненте — они должны быть ВПОЛОВИНУ ДЕШЕВЛЕ. Только тогда у Вашего потенциального инвестора может открыться второй замок и его рука начнёт движение к тому месту, где лежат его деньги, которые он уже собрался Вам выдать. Однако, по дороге он инстинктивно остановит своё движение, так там висит ещё один замок, и кликнет других специалистов, теперь уже «экспертов по рискам». Эти деятели – самые опасные для Вас. Они могут вернуть назад руку своего шефа, который уже, вроде бы, окончательно склонялся в Вашу сторону. Поэтому Вам надо знать и про третий подход Насреддина, который провозглашает: «Создай фон уверенности». Надо помнить, что, практически, любой инвестор постоянно трясётся по поводу возможных рисков, из-за которых он может безвозвратно потерять вложенное. А это его такое состояние является не лучшим фоном, при котором испрашиваются искомые деньги. Поэтому согласно Насреддиновскому Принципу №3 Вам надлежит создать у потенциального финансиста ПОЛНУЮ УВЕРЕННОСТЬ, что, по самой крайней мере, его деньги ну никак не пропадут. Как это сделать, тут должна проявиться вся Ваша фантазия, поскольку выбранный Вами вариант сильно зависит непосредственно от существа Вашей инновации. Но есть один приём, который можно назвать универсальным. Продумайте и предложите инвестору ПОЭТАПНЫЙ ПЛАН реализации всего Вашего проекта. Начните с небольшого участка, объема, узла..., с того, что требует минимальных вложений, а затем, за два-три этапа промасштабируйте развитие проекта до полной его рыночной реализации. Удачи Вам!\n\n\nКраткое содержание:\nСтатья обсуждает проблемы, с которыми сталкиваются изобретатели и инноваторы при поиске инвесторов для своих проектов. Автор предлагает три принципа, основанные на опыте Ходжи Насреддина, которые могут помочь в этом процессе.\n\n1. \"На, бери!\" - Первый принцип заключается в том, чтобы начать разговор с потенциальным инвестором, используя фразу \"возьмите\", а не \"дайте\". Это соответствует менталитету инвестора, который настроен на покупку, а не на предоставление.\n\n2. \"Поставьте себя на место инвестора\" - Второй принцип призывает инноваторов думать как инвестор, понимая, что он не заинтересован в сущности идеи, но в ее способности приносить прибыль. Это помогает определить, будет ли проект востребован на рынке.\n\n3. \"Создай фон уверенности\" - Третий принцип предлагает создать у инвестора уверенность в успешности проекта. Для этого можно предложить поэтапный план реализации, который показывает, как проект будет развиваться.\n\nВ целом, статья подчеркивает важность адаптации к менталитету инвестора и предоставления ему уверенности в успехе проекта.","summary":"Статья обсуждает проблемы, с которыми сталкиваются изобретатели и инноваторы при поиске инвесторов для своих проектов. Автор предлагает три принципа, основанные на опыте Ходжи Насреддина, которые могут помочь в этом процессе.\n\n1. \"На, бери!\" - Первый принцип заключается в том, чтобы начать разговор с потенциальным инвестором, используя фразу \"возьмите\", а не \"дайте\". Это соответствует менталитету инвестора, который настроен на покупку, а не на предоставление.\n\n2. \"Поставьте себя на место инвестора\" - Второй принцип призывает инноваторов думать как инвестор, понимая, что он не заинтересован в сущности идеи, но в ее способности приносить прибыль. Это помогает определить, будет ли проект востребован на рынке.\n\n3. \"Создай фон уверенности\" - Третий принцип предлагает создать у инвестора уверенность в успешности проекта. Для этого можно предложить поэтапный план реализации, который показывает, как проект будет развиваться.\n\nВ целом, статья подчеркивает важность адаптации к менталитету инвестора и предоставления ему уверенности в успехе проекта."}
{"Text":"Что делать, если для постройки робота не хватает кубиков? / Habr             \n\n25-03-2019\nМоя предыдущая статья была написана, в первую очередь, для менеджеров проектов и руководителей направления RPA. Эту статью я хотел бы адресовать, в первую очередь, программистам и тем, кому приходится с роботизацией сталкиваться face-to-face. Первые несколько процессов, которые приходится создавать новоиспеченному программиста RPA, обычно обходятся набором стандартных «кубиков», которые лежат в коробке с платформой. Но что если их не хватает? Почти любую современную платформу RPA можно тем или иным способом расширить, будь то вызов метод DLL в AA, код на Groovy в WF или Code Stage в BP. Но эта статья написана о платформе, с которой я хорошо знаком (UiPath), поэтому тем, кто предпочитает другие вкусы и цвета роботов — заранее просьба не обижаться. Хотя некоторые вещи в статье достаточно банальны и их можно узнать, пройдя соответствующие курсы, почитав документацию или изучив форум, как мне кажется, многим будет полезно увидеть их в одном месте, в виде статьи на русском языке. Если у вас появятся дополнения или замечания — я постараюсь статью доработать, чтобы она была максимально полезной и актуальной. Способы расширения функциональности в статье идут от самого простого (скачать готовое) к самому сложному (написать свое). Магазин приложений С конца 2018 года у UiPath есть что-то вроде «магазина приложений», который называется Go. Разработчики со всего света, и, в том числе, сами сотрудники компании, делятся полезными (или не очень) пользовательскими действиями, кусками кода, шаблонами для приложений и другими полезными разработками, которые значительно упрощают жизнь. Там есть много всякого, например отличный коннектор к Salesforce, который берет на себя львиную долю работы или написанный самим Abbyy коннектор к Abby Flexicapture. Сейчас это рекомендованный самим производителем способ расширения функциональности, более того, туда уже началась миграция расширений, которые раньше жили в официальном репозитории NuGet (Галерее). В PackageManager смотрели? Немножко более хардкорный, но, все еще, вполне доступный обычному пользователю, способ — это использование функционала менеджера пакетов. Реализован он через NuGet, а значит, что помимо использования «общественных» репозиториев, никто нам не мешает поднять свой и использовать его внутри компании для распространения действий и библиотек собственной разработки. Ищем в менеджере пакетов по ключевому слову UiPath и получаем большое количество разных действий, которые резко увеличивают наши возможности, не требуя от нас никаких усилий. Таким образом, сейчас, распространяются, в том числе и вполне официальные компоненты платформы, такие как Web Activities Pack (для работы с SOAP и REST API), Word (на основе Xceed DocX) или Zip (для работы угадайте с чем). Репозитории, где лежат основные «вкусности»: gallery.uipath.org/api/v2 (подключается по умолчанию) myget.org/f/uipath/api/v2 (здесь много интересного, в том числе последние беты пакетов действий). Через Invoke Code Самый простой и быстрый способ добавить функциональность в ваш процесс, если не хватает готовых «кубиков» и не удалось их найти. Можно написать небольшой кусок кода на VB.Net и вставить его в действие InvokeCode. Так можно, например, работать с хитрыми вызовами API на Rest, в случаях, когда стандартного UiPath.Web.Activities.HttpClient не хватает или использовать внешние библиотеки, такие как Xceed DocX или PdfSharp. Не стоит увлекаться этим и другими подобными методами, если у вас больше чем 10 строчек кода внутри InvokeCode. Хотя там и действует Intellisense и валидация, за поддержку таких «программ» коллеги вас не поблагодарят. Вынесите их в пользовательское действие или внешнюю программу, благо методов для этого достаточно. Из похожего еще есть Invoke Method, который, в принципе, позволяет вызвать метод любого класса, который добавляется в проект через зависимости. Например вот тут он используется для разархивирования архива Через Invoke <еще что-нибудь> Помимо «родного» VB.NET UiPath позволяет использовать и множество других методов программирования, как говорится «на все вкусы, на все нужды». Powershell Если не хватает VB.NET, на помощь приходят другие языки программирования. Например, с офисными приложениями, бывалому админу будет удобно работать через Powershell, и использовать, соответственно, действие Invoke Power Shell. В том же Outlook, например, можно бесконечно жать кнопки, чтобы сделать то, что с помощью PS делается парой команд. Вот такой скрипт, например, вернет последнее письмо в произвольной папке Add-type -assembly \"Microsoft.Office.Interop.Outlook\" | out-null $olFolders = \"Microsoft.Office.Interop.Outlook.olDefaultFolders\" -as [type] $outlook = new-object -comobject outlook.application $namespace = $outlook.GetNameSpace(\"MAPI\") $inbox = $namespace.getDefaultFolder($olFolders::olFolderInbox) $folder = $inbox.Folders.Item(“Название папки”) ($folder.Items.Restrict('[UnRead] = True') | Select-Object -Last 1) COM-объекты В современных системах потребность в этом, конечно, уже не так велика, но с помощью Invoke COM Methodможно обратится к функциональности, зарытой в какую-нибудь DLL. VBA Если вы работаете с Excel, можно «натравить» на него VBA-скрипт через запуск Invoke VBA. Скрипт должен лежать в отдельном файле (это удобно, учитывая, что сейчас офисные файлы с макросами не очень в почете). Такое будет удобно тем, кто хорошо умеет программировать на VBA и не желает бросать наработанные библиотеки. Python Большим подспорьем для работы с ML и всякими вычислениями является возможность встраивания в процесс питоньих скриптов. Чтобы начать с ними работать необходимо сделать несколько дополнительных телодвижений. Во-первых, эти действия теперь выложены в классный, но не поддерживаемый официально набор действий Community Activities на GitHub. Во-вторых, нужно поставить Python, причем обязательно версии 3.6 и обязательно в корень диска C. Кстати, в том же Community Activities есть и криптография, и FTP и еще несколько потенциально полезных штук. Документацию и (устаревший) пример кода можно скачать здесь. Если кому-то понадобится помощь с интеграцией UiPath и Python — пишите, попробую рассказать подробнее. Java Примерно тоже самое — пакет собирается самостоятельно из Community Activities. Поддерживается .jar-файлы, собранные с использованием JDK 1.5 и выше. Путь к Java должен быть прописан в PATH. На сайте UiPath есть документация по использованию и пример процесса с использованием Java. Через пользовательские действия К сожалению, не всем подходит вариант с использованием магазина приложений или чужих пакетов. Часто, например, служба безопасности не разрешат использовать куски роботов, написанные не вендором и не внутри компании. Тогда приходится засучить рукава. Платформа UiPath построена с использованием Microsoft Workflow Foundation. Для нас это значит, что всегда можно написать свое действие (Activity) и добавить в процесс. Если предполагается активное использование бизнес-пользователями, действию можно сделать красивый пользовательский UI с помощью дизайнера форм, а если нужно побыстрее — то сойдет и простой прямоугольничек, настраиваемый из Properties. К счастью, процесс создания действия документирован, хоть и не очень подробно, но всегда можно воспользоваться масштабной [документацией MS](https://docs.microsoft.com/en-us/dotnet/framework/windows-workflow-foundation/) по написанию действий и статьями о них в интернете. Пример простейшего действие на C# using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using System.Activities; using System.ComponentModel; namespace ClassMathCustomActivity { public class SimpleFormula : CodeActivity { [Category(\"Input\")] [RequiredArgument] public InArgument<double> FirstNumber { get; set; } [Category(\"Input\")] public InArgument<double> SecondNumber { get; set; } [Category(\"Output\")] public OutArgument<double> ResultNumber { get; set; } protected override void Execute(CodeActivityContext context) { var firstNumber = FirstNumber.Get(context); var secondNumber = SecondNumber.Get(context); var result = System.Math.Pow(firstNumber + secondNumber, 2); ResultNumber.Set(context, result); } } } Созданное действие нужно запаковать в .nupkg с помощью NuGet Package Explorer, а дальше созданный пакет добавить в Студию. Первое действие лучше не начинать создавать с ноля, а воспользоваться уже ранее упомянутыми Community Activities, где есть много готовых действий разной сложности. В заключение Как видите, вариантов для расширения фунциональности более чем достаточно, от простейшего InvokeCode до написания своего действия. Это значит, что какая бы задача перед вами не стояла, всегда есть возможность ее быстро решить, часто даже не изобретая велосипеда :) Надеюсь, эта статья будет вам полезна при создании классных процессов роботизации. Буду рад, если вы расскажете, насколько для вас эта информация была релевантна и о чем еще вы хотели бы узнать.\n","metric":0.56103515630000000414,"metric_input":"Текст статьи:\nЧто делать, если для постройки робота не хватает кубиков? / Habr             \n\n25-03-2019\nМоя предыдущая статья была написана, в первую очередь, для менеджеров проектов и руководителей направления RPA. Эту статью я хотел бы адресовать, в первую очередь, программистам и тем, кому приходится с роботизацией сталкиваться face-to-face. Первые несколько процессов, которые приходится создавать новоиспеченному программиста RPA, обычно обходятся набором стандартных «кубиков», которые лежат в коробке с платформой. Но что если их не хватает? Почти любую современную платформу RPA можно тем или иным способом расширить, будь то вызов метод DLL в AA, код на Groovy в WF или Code Stage в BP. Но эта статья написана о платформе, с которой я хорошо знаком (UiPath), поэтому тем, кто предпочитает другие вкусы и цвета роботов — заранее просьба не обижаться. Хотя некоторые вещи в статье достаточно банальны и их можно узнать, пройдя соответствующие курсы, почитав документацию или изучив форум, как мне кажется, многим будет полезно увидеть их в одном месте, в виде статьи на русском языке. Если у вас появятся дополнения или замечания — я постараюсь статью доработать, чтобы она была максимально полезной и актуальной. Способы расширения функциональности в статье идут от самого простого (скачать готовое) к самому сложному (написать свое). Магазин приложений С конца 2018 года у UiPath есть что-то вроде «магазина приложений», который называется Go. Разработчики со всего света, и, в том числе, сами сотрудники компании, делятся полезными (или не очень) пользовательскими действиями, кусками кода, шаблонами для приложений и другими полезными разработками, которые значительно упрощают жизнь. Там есть много всякого, например отличный коннектор к Salesforce, который берет на себя львиную долю работы или написанный самим Abbyy коннектор к Abby Flexicapture. Сейчас это рекомендованный самим производителем способ расширения функциональности, более того, туда уже началась миграция расширений, которые раньше жили в официальном репозитории NuGet (Галерее). В PackageManager смотрели? Немножко более хардкорный, но, все еще, вполне доступный обычному пользователю, способ — это использование функционала менеджера пакетов. Реализован он через NuGet, а значит, что помимо использования «общественных» репозиториев, никто нам не мешает поднять свой и использовать его внутри компании для распространения действий и библиотек собственной разработки. Ищем в менеджере пакетов по ключевому слову UiPath и получаем большое количество разных действий, которые резко увеличивают наши возможности, не требуя от нас никаких усилий. Таким образом, сейчас, распространяются, в том числе и вполне официальные компоненты платформы, такие как Web Activities Pack (для работы с SOAP и REST API), Word (на основе Xceed DocX) или Zip (для работы угадайте с чем). Репозитории, где лежат основные «вкусности»: gallery.uipath.org/api/v2 (подключается по умолчанию) myget.org/f/uipath/api/v2 (здесь много интересного, в том числе последние беты пакетов действий). Через Invoke Code Самый простой и быстрый способ добавить функциональность в ваш процесс, если не хватает готовых «кубиков» и не удалось их найти. Можно написать небольшой кусок кода на VB.Net и вставить его в действие InvokeCode. Так можно, например, работать с хитрыми вызовами API на Rest, в случаях, когда стандартного UiPath.Web.Activities.HttpClient не хватает или использовать внешние библиотеки, такие как Xceed DocX или PdfSharp. Не стоит увлекаться этим и другими подобными методами, если у вас больше чем 10 строчек кода внутри InvokeCode. Хотя там и действует Intellisense и валидация, за поддержку таких «программ» коллеги вас не поблагодарят. Вынесите их в пользовательское действие или внешнюю программу, благо методов для этого достаточно. Из похожего еще есть Invoke Method, который, в принципе, позволяет вызвать метод любого класса, который добавляется в проект через зависимости. Например вот тут он используется для разархивирования архива Через Invoke <еще что-нибудь> Помимо «родного» VB.NET UiPath позволяет использовать и множество других методов программирования, как говорится «на все вкусы, на все нужды». Powershell Если не хватает VB.NET, на помощь приходят другие языки программирования. Например, с офисными приложениями, бывалому админу будет удобно работать через Powershell, и использовать, соответственно, действие Invoke Power Shell. В том же Outlook, например, можно бесконечно жать кнопки, чтобы сделать то, что с помощью PS делается парой команд. Вот такой скрипт, например, вернет последнее письмо в произвольной папке Add-type -assembly \"Microsoft.Office.Interop.Outlook\" | out-null $olFolders = \"Microsoft.Office.Interop.Outlook.olDefaultFolders\" -as [type] $outlook = new-object -comobject outlook.application $namespace = $outlook.GetNameSpace(\"MAPI\") $inbox = $namespace.getDefaultFolder($olFolders::olFolderInbox) $folder = $inbox.Folders.Item(“Название папки”) ($folder.Items.Restrict('[UnRead] = True') | Select-Object -Last 1) COM-объекты В современных системах потребность в этом, конечно, уже не так велика, но с помощью Invoke COM Methodможно обратится к функциональности, зарытой в какую-нибудь DLL. VBA Если вы работаете с Excel, можно «натравить» на него VBA-скрипт через запуск Invoke VBA. Скрипт должен лежать в отдельном файле (это удобно, учитывая, что сейчас офисные файлы с макросами не очень в почете). Такое будет удобно тем, кто хорошо умеет программировать на VBA и не желает бросать наработанные библиотеки. Python Большим подспорьем для работы с ML и всякими вычислениями является возможность встраивания в процесс питоньих скриптов. Чтобы начать с ними работать необходимо сделать несколько дополнительных телодвижений. Во-первых, эти действия теперь выложены в классный, но не поддерживаемый официально набор действий Community Activities на GitHub. Во-вторых, нужно поставить Python, причем обязательно версии 3.6 и обязательно в корень диска C. Кстати, в том же Community Activities есть и криптография, и FTP и еще несколько потенциально полезных штук. Документацию и (устаревший) пример кода можно скачать здесь. Если кому-то понадобится помощь с интеграцией UiPath и Python — пишите, попробую рассказать подробнее. Java Примерно тоже самое — пакет собирается самостоятельно из Community Activities. Поддерживается .jar-файлы, собранные с использованием JDK 1.5 и выше. Путь к Java должен быть прописан в PATH. На сайте UiPath есть документация по использованию и пример процесса с использованием Java. Через пользовательские действия К сожалению, не всем подходит вариант с использованием магазина приложений или чужих пакетов. Часто, например, служба безопасности не разрешат использовать куски роботов, написанные не вендором и не внутри компании. Тогда приходится засучить рукава. Платформа UiPath построена с использованием Microsoft Workflow Foundation. Для нас это значит, что всегда можно написать свое действие (Activity) и добавить в процесс. Если предполагается активное использование бизнес-пользователями, действию можно сделать красивый пользовательский UI с помощью дизайнера форм, а если нужно побыстрее — то сойдет и простой прямоугольничек, настраиваемый из Properties. К счастью, процесс создания действия документирован, хоть и не очень подробно, но всегда можно воспользоваться масштабной [документацией MS](https://docs.microsoft.com/en-us/dotnet/framework/windows-workflow-foundation/) по написанию действий и статьями о них в интернете. Пример простейшего действие на C# using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using System.Activities; using System.ComponentModel; namespace ClassMathCustomActivity { public class SimpleFormula : CodeActivity { [Category(\"Input\")] [RequiredArgument] public InArgument<double> FirstNumber { get; set; } [Category(\"Input\")] public InArgument<double> SecondNumber { get; set; } [Category(\"Output\")] public OutArgument<double> ResultNumber { get; set; } protected override void Execute(CodeActivityContext context) { var firstNumber = FirstNumber.Get(context); var secondNumber = SecondNumber.Get(context); var result = System.Math.Pow(firstNumber + secondNumber, 2); ResultNumber.Set(context, result); } } } Созданное действие нужно запаковать в .nupkg с помощью NuGet Package Explorer, а дальше созданный пакет добавить в Студию. Первое действие лучше не начинать создавать с ноля, а воспользоваться уже ранее упомянутыми Community Activities, где есть много готовых действий разной сложности. В заключение Как видите, вариантов для расширения фунциональности более чем достаточно, от простейшего InvokeCode до написания своего действия. Это значит, что какая бы задача перед вами не стояла, всегда есть возможность ее быстро решить, часто даже не изобретая велосипеда :) Надеюсь, эта статья будет вам полезна при создании классных процессов роботизации. Буду рад, если вы расскажете, насколько для вас эта информация была релевантна и о чем еще вы хотели бы узнать.\n\n\nКраткое содержание:\nЭта статья рассказывает о различных способах расширения функциональности платформы RPA (роботизация процессов автоматизации) UiPath. \n\nАвтор описывает магазин приложений Go, использование функционала менеджера пакетов NuGet, а также способы написания пользовательских действий и скриптов на разных языках программирования: VB.NET, Powershell, VBA, Python, Java. \n\nТакже в статье приведены примеры создания пользовательских действий на C#.\n\nТаким образом, автор статьи предлагает множество вариантов для расширения функциональности UiPath без необходимости написания кода с нуля.","summary":"Эта статья рассказывает о различных способах расширения функциональности платформы RPA (роботизация процессов автоматизации) UiPath. \n\nАвтор описывает магазин приложений Go, использование функционала менеджера пакетов NuGet, а также способы написания пользовательских действий и скриптов на разных языках программирования: VB.NET, Powershell, VBA, Python, Java. \n\nТакже в статье приведены примеры создания пользовательских действий на C#.\n\nТаким образом, автор статьи предлагает множество вариантов для расширения функциональности UiPath без необходимости написания кода с нуля."}
{"Text":"Что в черном ящике: из чего складываются затраты на виртуальную и физическую инфраструктуру / Хабр                 \n\nАналитики Gartner в апреле\nсообщили\n, что мировые траты на дата-центры по сравнению с прошлым годом возрастут и перевалят за \\$170 млрд. Во столько Forbes\nоценивает\nбренд Apple, и именно такая сумма\nпроходит\nчерез все магазины крупнейшего ретейлера в США Walmart. В этой статье мы рассмотрим, куда идут такие деньги в контексте ЦОД, и поговорим о том, из чего складываются затраты на виртуальную и физическую инфраструктуры.\n\n_/ кадр из видео о нашем дата-центре  SDN + ( подробнее  об инфраструктуре 1cloud)_\n\n## Затраты на корпоративные серверы ##\n\nНеважно, говорим ли мы об облачном решении или о «железе», расчет должен учитывать прямые, косвенные и скрытые затраты. Все эти группы расходов укладываются в показатель совокупной стоимости владения (TCO). Существует мнение, что большую часть затрат на физическую инфраструктуру составляет оборудование. Однако,\nпо данным\nIntel, стоимость «железа» представляет собой 15% от общих затрат на IT-активы.\n\nПомимо аппаратного обеспечения, прямые издержки включают стоимость программного обеспечения, расходуемого электричества и вспомогательных систем: отопительных, охладительных, вентиляционных. Косвенные затраты связаны с выплатой ЗП системному администратору или другим сотрудникам.\n\nЧто касается оборудования, компании потребуется приобрести сам сервер и продумать будущую архитектуру ЦОДа — будет ли это масштабируемая, модульная система, обладающая несколькими блоками, или традиционные стойки. Необходимо продумать стратегию дальнейшего расширения.\n\nСтоимость сервера варьируется в зависимости от производительности. В этот момент для бизнеса открывается простор для экономии, ведь решение в пользу конкретного устройства принимается самостоятельно. Однако, более дешевый, но низкопроизводительный сервер — это не всегда выгодно. Важно обратить внимание на энергоэффективность, которая как раз и обеспечивает экономию в долгосрочной перспективе.\n\nСерверное помещение оборудуется электрическими системами, которые включают основное и промежуточные распределительные устройства, освещение, ИБП, отопление, вентиляцию и кондиционирование, систему противопожарной защиты, мониторинга и безопасности.\n\nСледующая остановка — программное обеспечение. Серверу нужна операционная система, например Windows Server 2016, которую придется приобрести. Предприятию могут понадобиться и другие инструменты для решения корпоративных задач: Microsoft SQL Server, Citrix Presentation Server и др. Также нужно организовать сеть. С рекомендациями по программному обеспечению для создания локальной сети можно познакомиться на профильных\nплощадках\n.\n\nНаконец, мы добрались до электроэнергии, стоимость которой достаточно велика. Весьма показательна\nистория\nМарко Пончиелли (Marco Ponchielli) из Rackspace. Он приводит данные, согласно которым раньше стоимость электроэнергии в общих затратах была столь незначительна, что требовалось от 30 до 50 лет, чтобы она приблизилась к стоимости вычислений.\n\nТеперь же, в районах с высокими тарифами, этот период сократился до двух лет, а в среднем — до четырех. Марко также рассказал об IT-отделе, который потратил \\$22 млн на блейд-серверы. Затраты на их обслуживание за три года вылились в \\$30 млн, а модернизация инфраструктуры для их установки — в \\$54 млн.\n\nЕсть и еще одна статья расходов — помещение. Здесь разброс цен полностью зависит от того, находится ли офис во владении компании, построен ли машинный зал и какую площадь он занимает.\n\nКроме этого, следует учитывать кадры. Даже в случае удаленного размещения сервера предприятию требуется администратор, средний уровень заработной платы которого,\nпо данным\nпортала Trud, в России составляет около 40 тыс. рублей. Размер штата будет зависеть от размеров инфраструктуры, поэтому расчеты здесь индивидуальны для каждого конкретного случая.\n\nВ течение 2–4 лет затраты как на локальные, так и на облачные решения ограничиваются обслуживанием и поддержкой. Однако примерно через пять лет,\nпо мнению\nсетевых администраторов, компания сталкивается с необходимостью обновления корпоративных серверных решений.\n\nВ жизненном цикле физических серверов за этим следуют новые капитальные затраты — это основная статья скрытых расходов в долгосрочной перспективе. Кроме того, эффективность нового физического оборудования намного\nвыше\n, чем старого. Поэтому между устаревшими технологиями и ростом затрат энергии часто можно поставить знак равенства.\n\nНе стоит забывать о том, что эти инвестиции на внушительный срок еще и обесцениваются с течением времени по причине инфляции. Цены на оборудование увеличиваются, поскольку спрос со временем\nрастет\n.\n\nВ условную смету для составления целостной картины также следует заложить нематериальные издержки: расходы на перенос программного обеспечения и баз данных на новую платформу и затраты на решения, снижающие риск миграции.\n\nТаким образом, стоимость поддержания локальной инфраструктуры увеличивается от года к году, причем замкнута она на короткий жизненный цикл аппаратных средств.\n\n_/ 1сloud_\n\n## Затраты на виртуальные серверы ##\n\nС точки зрения потребителя, облачная экономика очень проста:\nвыбираешь\nнеобходимую конфигурацию и оплачиваешь аренду. Достаточно учесть некоторые аспекты. Например, потребуется обучить персонал, чтобы сотрудники могли эффективно управлять облачной инфраструктурой. К тому же, в статью скрытых расходов стоит записать стоимость миграции в облако. Порой она включает в себя и неудачные попытки работы с различными провайдерами. Все это в конечном счете стоит денег.\n\nРасчеты затрат для клиента в случае с облаком достаточно прозрачны, но из чего они складываются для самого провайдера? По большому счету, ситуация аналогична корпоративной инфраструктуре, но, конечно, масштабы куда больше.\n\nКачественный ЦОД уровня Tier III\nсвязывает\nмногопроцессорные серверы, расположенные на дисковых массивах, с сетью по высокоскоростным каналам. Для этого используются несколько взаимозаменяемых магистральных маршрутизаторов. Облачная инфраструктура защищается системой глубокого анализа пакетов. За распределение сетевого трафика отвечают коммутаторы, формирующие между собой систему с дублированными соединениями. Хранение должно обеспечиваться надежными аппаратными и программными решениями, способными оперировать физическими дисками и обновлением ПО, а также балансировать нагрузку.\n\nВысокий уровень доступности достигается дублирующими системами. Для стабильности и оптимизации работы набор ПО дополнен специальными решениями: контроль энергоэффективности, динамическое распределение нагрузки, панель управления, средства защиты и так далее.\n\nКак уже было отмечено, человеческий ресурс является еще одной графой затрат в работе дата-центра. В зависимости от навыков персонала и географического положения годовые зарплаты всего штата, обслуживающего дата-центр, варьируются от \\$75 тыс. до более чем \\$125 тыс.,\nпо словам\nглавы исследовательской компании Clabby Analytics Джо Клэбби (Joe Clabby). Не стоит забывать, что, кроме администраторов, ЦОД обслуживают и десятки других сотрудников. Например, дата-центры уровня Tier III требуют физической охраны серверов.\n\nНо несмотря на такие масштабы, в некоторых случаях облачным провайдерам удается экономить деньги клиентам. Это возможно, потому что масштаб операций в дата-центрах позволяет операторам получать скидки на оборудование и решения.\n\nПри этом капитальные расходы на оборудование распределяются в пересчете на одну стойку или гигабайт ЗУ. Затраты амортизируются в течение всего жизненного цикла оборудования. К тому же не следует забывать об аппаратных средствах, которые обслуживают всю инфраструктуру — системы охлаждения, ИБП, системы защиты. Затраты на них «распределяются» между серверами.\n\nЭкосистема виртуальных провайдеров позволяет использовать меньшее количество серверов, что снижает потребление электроэнергии. К тому же с учетом масштабов и резервных систем появляется возможность для перераспределения и отключения серверов без ущерба для системы. По\nданным\nUptime Institute, вывод из эксплуатации одной серверной стойки 1U может ежегодно экономить более \\$2 тыс.\n\nПо результатам\nисследования\nUptime Institute, в 2015 году около 30% всех серверов не использовались, то есть они лежали «мертвым грузом» на плечах провайдеров. Во всем мире насчитывалось около 10 млн «коматозных» серверов. В денежном выражении это не менее \\$30 млрд. Вот почему облачные провайдеры «избавляются от балласта», экономя тем самым для себя и клиента приличные деньги.\n\nПоэтому компании все чаще обращают внимание на облачные сервисы. Недавний\nотчет\nпоказал, что инвестирование в частные ЦОДы теряет смысл для IT-компаний. Вместо этого, руководители стараются нарастить вложения в разработку бизнес-приложений. Представители 70% опрошенных организаций заявили, что планируют увеличить расходы на безопасность в 2017 году, 67% отдали приоритет облачным приложениям, а еще 52% — облачной инфраструктуре.\n\nP.S. В нашем корпоративном блоге есть и другие статьи на тему\nвиртуальной инфраструктуры\n:\n\nЧастное облако и VPS/VDS\n\nНемного о безопасности в «облаке»\n\nКак создать провайдера виртуальной инфраструктуры","metric":0.5224609375,"metric_input":"Текст статьи:\nЧто в черном ящике: из чего складываются затраты на виртуальную и физическую инфраструктуру / Хабр                 \n\nАналитики Gartner в апреле\nсообщили\n, что мировые траты на дата-центры по сравнению с прошлым годом возрастут и перевалят за \\$170 млрд. Во столько Forbes\nоценивает\nбренд Apple, и именно такая сумма\nпроходит\nчерез все магазины крупнейшего ретейлера в США Walmart. В этой статье мы рассмотрим, куда идут такие деньги в контексте ЦОД, и поговорим о том, из чего складываются затраты на виртуальную и физическую инфраструктуры.\n\n_/ кадр из видео о нашем дата-центре  SDN + ( подробнее  об инфраструктуре 1cloud)_\n\n## Затраты на корпоративные серверы ##\n\nНеважно, говорим ли мы об облачном решении или о «железе», расчет должен учитывать прямые, косвенные и скрытые затраты. Все эти группы расходов укладываются в показатель совокупной стоимости владения (TCO). Существует мнение, что большую часть затрат на физическую инфраструктуру составляет оборудование. Однако,\nпо данным\nIntel, стоимость «железа» представляет собой 15% от общих затрат на IT-активы.\n\nПомимо аппаратного обеспечения, прямые издержки включают стоимость программного обеспечения, расходуемого электричества и вспомогательных систем: отопительных, охладительных, вентиляционных. Косвенные затраты связаны с выплатой ЗП системному администратору или другим сотрудникам.\n\nЧто касается оборудования, компании потребуется приобрести сам сервер и продумать будущую архитектуру ЦОДа — будет ли это масштабируемая, модульная система, обладающая несколькими блоками, или традиционные стойки. Необходимо продумать стратегию дальнейшего расширения.\n\nСтоимость сервера варьируется в зависимости от производительности. В этот момент для бизнеса открывается простор для экономии, ведь решение в пользу конкретного устройства принимается самостоятельно. Однако, более дешевый, но низкопроизводительный сервер — это не всегда выгодно. Важно обратить внимание на энергоэффективность, которая как раз и обеспечивает экономию в долгосрочной перспективе.\n\nСерверное помещение оборудуется электрическими системами, которые включают основное и промежуточные распределительные устройства, освещение, ИБП, отопление, вентиляцию и кондиционирование, систему противопожарной защиты, мониторинга и безопасности.\n\nСледующая остановка — программное обеспечение. Серверу нужна операционная система, например Windows Server 2016, которую придется приобрести. Предприятию могут понадобиться и другие инструменты для решения корпоративных задач: Microsoft SQL Server, Citrix Presentation Server и др. Также нужно организовать сеть. С рекомендациями по программному обеспечению для создания локальной сети можно познакомиться на профильных\nплощадках\n.\n\nНаконец, мы добрались до электроэнергии, стоимость которой достаточно велика. Весьма показательна\nистория\nМарко Пончиелли (Marco Ponchielli) из Rackspace. Он приводит данные, согласно которым раньше стоимость электроэнергии в общих затратах была столь незначительна, что требовалось от 30 до 50 лет, чтобы она приблизилась к стоимости вычислений.\n\nТеперь же, в районах с высокими тарифами, этот период сократился до двух лет, а в среднем — до четырех. Марко также рассказал об IT-отделе, который потратил \\$22 млн на блейд-серверы. Затраты на их обслуживание за три года вылились в \\$30 млн, а модернизация инфраструктуры для их установки — в \\$54 млн.\n\nЕсть и еще одна статья расходов — помещение. Здесь разброс цен полностью зависит от того, находится ли офис во владении компании, построен ли машинный зал и какую площадь он занимает.\n\nКроме этого, следует учитывать кадры. Даже в случае удаленного размещения сервера предприятию требуется администратор, средний уровень заработной платы которого,\nпо данным\nпортала Trud, в России составляет около 40 тыс. рублей. Размер штата будет зависеть от размеров инфраструктуры, поэтому расчеты здесь индивидуальны для каждого конкретного случая.\n\nВ течение 2–4 лет затраты как на локальные, так и на облачные решения ограничиваются обслуживанием и поддержкой. Однако примерно через пять лет,\nпо мнению\nсетевых администраторов, компания сталкивается с необходимостью обновления корпоративных серверных решений.\n\nВ жизненном цикле физических серверов за этим следуют новые капитальные затраты — это основная статья скрытых расходов в долгосрочной перспективе. Кроме того, эффективность нового физического оборудования намного\nвыше\n, чем старого. Поэтому между устаревшими технологиями и ростом затрат энергии часто можно поставить знак равенства.\n\nНе стоит забывать о том, что эти инвестиции на внушительный срок еще и обесцениваются с течением времени по причине инфляции. Цены на оборудование увеличиваются, поскольку спрос со временем\nрастет\n.\n\nВ условную смету для составления целостной картины также следует заложить нематериальные издержки: расходы на перенос программного обеспечения и баз данных на новую платформу и затраты на решения, снижающие риск миграции.\n\nТаким образом, стоимость поддержания локальной инфраструктуры увеличивается от года к году, причем замкнута она на короткий жизненный цикл аппаратных средств.\n\n_/ 1сloud_\n\n## Затраты на виртуальные серверы ##\n\nС точки зрения потребителя, облачная экономика очень проста:\nвыбираешь\nнеобходимую конфигурацию и оплачиваешь аренду. Достаточно учесть некоторые аспекты. Например, потребуется обучить персонал, чтобы сотрудники могли эффективно управлять облачной инфраструктурой. К тому же, в статью скрытых расходов стоит записать стоимость миграции в облако. Порой она включает в себя и неудачные попытки работы с различными провайдерами. Все это в конечном счете стоит денег.\n\nРасчеты затрат для клиента в случае с облаком достаточно прозрачны, но из чего они складываются для самого провайдера? По большому счету, ситуация аналогична корпоративной инфраструктуре, но, конечно, масштабы куда больше.\n\nКачественный ЦОД уровня Tier III\nсвязывает\nмногопроцессорные серверы, расположенные на дисковых массивах, с сетью по высокоскоростным каналам. Для этого используются несколько взаимозаменяемых магистральных маршрутизаторов. Облачная инфраструктура защищается системой глубокого анализа пакетов. За распределение сетевого трафика отвечают коммутаторы, формирующие между собой систему с дублированными соединениями. Хранение должно обеспечиваться надежными аппаратными и программными решениями, способными оперировать физическими дисками и обновлением ПО, а также балансировать нагрузку.\n\nВысокий уровень доступности достигается дублирующими системами. Для стабильности и оптимизации работы набор ПО дополнен специальными решениями: контроль энергоэффективности, динамическое распределение нагрузки, панель управления, средства защиты и так далее.\n\nКак уже было отмечено, человеческий ресурс является еще одной графой затрат в работе дата-центра. В зависимости от навыков персонала и географического положения годовые зарплаты всего штата, обслуживающего дата-центр, варьируются от \\$75 тыс. до более чем \\$125 тыс.,\nпо словам\nглавы исследовательской компании Clabby Analytics Джо Клэбби (Joe Clabby). Не стоит забывать, что, кроме администраторов, ЦОД обслуживают и десятки других сотрудников. Например, дата-центры уровня Tier III требуют физической охраны серверов.\n\nНо несмотря на такие масштабы, в некоторых случаях облачным провайдерам удается экономить деньги клиентам. Это возможно, потому что масштаб операций в дата-центрах позволяет операторам получать скидки на оборудование и решения.\n\nПри этом капитальные расходы на оборудование распределяются в пересчете на одну стойку или гигабайт ЗУ. Затраты амортизируются в течение всего жизненного цикла оборудования. К тому же не следует забывать об аппаратных средствах, которые обслуживают всю инфраструктуру — системы охлаждения, ИБП, системы защиты. Затраты на них «распределяются» между серверами.\n\nЭкосистема виртуальных провайдеров позволяет использовать меньшее количество серверов, что снижает потребление электроэнергии. К тому же с учетом масштабов и резервных систем появляется возможность для перераспределения и отключения серверов без ущерба для системы. По\nданным\nUptime Institute, вывод из эксплуатации одной серверной стойки 1U может ежегодно экономить более \\$2 тыс.\n\nПо результатам\nисследования\nUptime Institute, в 2015 году около 30% всех серверов не использовались, то есть они лежали «мертвым грузом» на плечах провайдеров. Во всем мире насчитывалось около 10 млн «коматозных» серверов. В денежном выражении это не менее \\$30 млрд. Вот почему облачные провайдеры «избавляются от балласта», экономя тем самым для себя и клиента приличные деньги.\n\nПоэтому компании все чаще обращают внимание на облачные сервисы. Недавний\nотчет\nпоказал, что инвестирование в частные ЦОДы теряет смысл для IT-компаний. Вместо этого, руководители стараются нарастить вложения в разработку бизнес-приложений. Представители 70% опрошенных организаций заявили, что планируют увеличить расходы на безопасность в 2017 году, 67% отдали приоритет облачным приложениям, а еще 52% — облачной инфраструктуре.\n\nP.S. В нашем корпоративном блоге есть и другие статьи на тему\nвиртуальной инфраструктуры\n:\n\nЧастное облако и VPS/VDS\n\nНемного о безопасности в «облаке»\n\nКак создать провайдера виртуальной инфраструктуры\n\nКраткое содержание:\nВ статье рассматривается вопрос о затратах на виртуальную и физическую инфраструктуру. Анализируются различные аспекты, такие как затраты на серверы, электроэнергию, программное обеспечение, помещение и кадры. Также рассматриваются особенности облачных вычислений и их влияние на затраты провайдеров и клиентов. Отмечается, что облачные сервисы становятся все более популярными, так как они позволяют экономить деньги и оптимизировать работу дата-центров.","summary":"В статье рассматривается вопрос о затратах на виртуальную и физическую инфраструктуру. Анализируются различные аспекты, такие как затраты на серверы, электроэнергию, программное обеспечение, помещение и кадры. Также рассматриваются особенности облачных вычислений и их влияние на затраты провайдеров и клиентов. Отмечается, что облачные сервисы становятся все более популярными, так как они позволяют экономить деньги и оптимизировать работу дата-центров."}
{"Text":"Создаем образ Windows XP SP3 для разворачивания по сети через WDS / Хабр                \n\n24-10-2009\nОпубликовано по просьбе хорошего человека fcserg, так что прошу карму форвардить ему Перед многими из нас часто возникают задачи типа «развернуть много рабочих станций», причем железо на всех этих станциях может быть разное. Мало того, на некоторых из этих машинах должен быть предустановлен разнообразный софт (от «корпоративных наборов» — аська, браузеры, pdf-читалки, до «университетских» наборов – Visual Studio, Project Manager). Раньше для таких целей использовался RIS (Remote Installation Services), теперь Microsoft агитирует за WDS (Windows Deployment Services). Мы остановимся именно на последнем. WDS прекрасно работает с образами новых систем от Microsoft – Vista, Windows Server 2008, Windows 7, но как быть, если нам необходимо использовать старую добрую Windows XP (а именно такая ситуация возникает очень часто, ведь многие все еще не перешли на новые системы). В этой статье мы научимся готовить ОС для создания образа в формате .WIM, с которыми работает WDS-сервер. Первый этап: Создаем шаблонную инсталляцию Windows XP SP3 На виртуальной машине устанавливаем WinXP SP3. Я лично использовал VMWare как платформу для виртуальной машины. Также на этом этапе есть смысл поставить весь необходимый вам софт. В свой стандартный пакет я включил: WinRAR K-Lite Codec Pack Microsoft Office Adobe Reader Total Commander IM-клиент (например QIP, ICQ…) Броузеры (например Opera, Firefox) Для тех, кто обслуживает компьютерные классы (университеты, школы, игровые клубы), на этом этапе есть возможность поставить дополнительный софт: средства разработки, графические редакторы и т.д. Второй этап: Подготавливаем систему для снятия образа, в том числе и упаковываем ее всеми необходимыми драйверами. Теперь полученную виртуальную ОС нужно обработать Sysprep’ом для корректной пост-установки. На всякий случай напомню, что Sysprep можно найти на любом установочном диске с Windows XP (D:\\Support\\Tools\\deploy.cab), где D: — это буква CD-привода в вашей виртуальной машине. Распаковываем содержимое deploy.cab, например, в C:\\Sysprep на виртуальной машине и запускаем setupmgr.exe. Проходим процесс создания файла ответов в соответствии с нашими потребностями. Единственное напомню, что в выборе типа файла ответов нужно выбрать вторую опцию – «Установка Sysprep». В результате получаем файл ответов Sysprep.inf. Теперь нужно немного поменять содержимое этого файла. В разделе [Unattended] поменяйте параметр InstallFilesPath=«C:\\WINDOWS\\Driver Cache\\i386» и добавьте: DriverSigningPolicy=Ignore UpdateInstalledDrivers=Yes В разделе [SetupMgr] поменяйте параметр DistFolder=«C:\\WINDOWS\\Driver Cache\\i386» Теперь нужно создать подборку драйверов (это именно те драйвера, которые будут использоваться для установки всех устройств системы при разворачивании образа). Ее можно составить из драйверов, которые используются для установки рабочих станций в вашей компании, или использовать уже готовую (наверняка в интернете есть масса таких). Копируем все эти драйвера в C:\\Windows\\Driver Cache виртуальной машины. Следующий шаг — нам понадобится Sysprep Driver Scanner. Скачайте его и поместите в папку C:\\Sysprep. Запускаем и проходим следующие этапы: Меняем путь для поиска драйверов на C:\\Windows\\Driver Cache. Нажимаем «Scan». Нажимаем «Default». Нажимаем «Save». Нажимаем «Done». Теперь, когда мини-установка Sysprep выполняется, она будет искать драйвера именно в указанной нами директории. Опять нужно внести изменения в файл Sysprep.inf — в конце файла добавляем [SysprepMassStorage]. И выполняем из командной строки «sysprep.exe –bmsd». На выходе получаем что-то типа: [SysprepMassStorage] pci\\ven_1022&dev_7007=c:\\windows\\inf\\machine.inf pci\\ven_1022&dev_700d=c:\\windows\\inf\\machine.inf pci\\ven_1022&dev_700f=c:\\windows\\inf\\machine.inf pci\\ven_1022&dev_740b=c:\\windows\\inf\\machine.inf pcmcia\\ibm-pcmcia_portable_cd-rom_drive-84e3=c:\\windows\\inf\\pnpscsi.inf pcmcia\\ibm-pcmcia_cd-rom_drive_cd-400-5afa=c:\\windows\\inf\\pnpscsi.inf pcmcia\\ibm-pcmcia_portable_cd-rom_drive-84e2=c:\\windows\\inf\\pnpscsi.inf pcmcia\\pcmcia-ide_card-3220=c:\\windows\\inf\\pnpscsi.inf ... Это идентификаторы драйверов и их местонахождения. Теперь приступаем к изменению настроек ядра компьютера для нашего образа. Для этого нам понадобится еще одна утилита — MySysprep. Скачиваем и распаковываем во все ту же папку Sysprep на диске C: нашей виртуальной машины. Ищем в распакованном MySysprep.inf и убираем все знаки комментария (;) с секции [CPU]. Отредактированный файл сохраняем. Теперь нужно сменить дефолтный «Однопроцессорный компьютер с ACPI» на «Компьютер ACPI». Для этого открываем «Диспетчер устройств» и выбираем опцию обновления драйвера для устройства «Однопроцессорный компьютер ACPI». От предложений поискать отказываемся и говорим, что все сами найдем, и уже в последнем окне (окно выбора драйвера вручную) выбираем «Компьютер ACPI». После этого рестартуем и запускаем файл mysysprep.exe из папки C:\\Sysprep. В окне программы ставим галочку «мини-установка» и нажимаем кнопку «Reseal» («Запечатать компьютер»). Теперь наша ОС готова к «захвату» и созданию образа с помощью WDS. При старте виртуальной машины заходим в BIOS, ставим загрузку по сети (PXE). При загрузке по PXE попадаем на меню выбора образа WDS-сервера и выбираем тот образ, который у нас на WDS сервере служит для «захвата». После выполнения загрузчика выбираем место сохранения образа (WDS-сервер или локальный диск), и, собственно происходит сам процесс создания образа. Полученный образ будет автоматически разворачивать Windows XP на указанной машине, при этом все устройства будут установлены с правильными драйверами. Он также хорош тем, что «не боится» разномастных аппаратных конфигураций. Все вышеперечисленные мною действия актуальны для инфраструктуры с развернутым и настроенным WDS сервером. Также в комплексе по созданию Light-Touch Deployment образов можно поговорить и о возможностях создания файла ответов для образов WDS-сервера, что позволяет упростить развертывание образа и вовсе до 1-2 нажатий клавиши (автоматическая разбивка жесткого диска, авто-выбор образа на WDS-сервере и многое другое). Если у вас такая инфраструктура отсутствует, то ее создание могло бы стать темой следующей статьи, хоть и получается в таком случае не очень правильно с точки зрения хронологии.\n","metric":0.5517578125,"metric_input":"Текст статьи:\nСоздаем образ Windows XP SP3 для разворачивания по сети через WDS / Хабр                \n\n24-10-2009\nОпубликовано по просьбе хорошего человека fcserg, так что прошу карму форвардить ему Перед многими из нас часто возникают задачи типа «развернуть много рабочих станций», причем железо на всех этих станциях может быть разное. Мало того, на некоторых из этих машинах должен быть предустановлен разнообразный софт (от «корпоративных наборов» — аська, браузеры, pdf-читалки, до «университетских» наборов – Visual Studio, Project Manager). Раньше для таких целей использовался RIS (Remote Installation Services), теперь Microsoft агитирует за WDS (Windows Deployment Services). Мы остановимся именно на последнем. WDS прекрасно работает с образами новых систем от Microsoft – Vista, Windows Server 2008, Windows 7, но как быть, если нам необходимо использовать старую добрую Windows XP (а именно такая ситуация возникает очень часто, ведь многие все еще не перешли на новые системы). В этой статье мы научимся готовить ОС для создания образа в формате .WIM, с которыми работает WDS-сервер. Первый этап: Создаем шаблонную инсталляцию Windows XP SP3 На виртуальной машине устанавливаем WinXP SP3. Я лично использовал VMWare как платформу для виртуальной машины. Также на этом этапе есть смысл поставить весь необходимый вам софт. В свой стандартный пакет я включил: WinRAR K-Lite Codec Pack Microsoft Office Adobe Reader Total Commander IM-клиент (например QIP, ICQ…) Броузеры (например Opera, Firefox) Для тех, кто обслуживает компьютерные классы (университеты, школы, игровые клубы), на этом этапе есть возможность поставить дополнительный софт: средства разработки, графические редакторы и т.д. Второй этап: Подготавливаем систему для снятия образа, в том числе и упаковываем ее всеми необходимыми драйверами. Теперь полученную виртуальную ОС нужно обработать Sysprep’ом для корректной пост-установки. На всякий случай напомню, что Sysprep можно найти на любом установочном диске с Windows XP (D:\\Support\\Tools\\deploy.cab), где D: — это буква CD-привода в вашей виртуальной машине. Распаковываем содержимое deploy.cab, например, в C:\\Sysprep на виртуальной машине и запускаем setupmgr.exe. Проходим процесс создания файла ответов в соответствии с нашими потребностями. Единственное напомню, что в выборе типа файла ответов нужно выбрать вторую опцию – «Установка Sysprep». В результате получаем файл ответов Sysprep.inf. Теперь нужно немного поменять содержимое этого файла. В разделе [Unattended] поменяйте параметр InstallFilesPath=«C:\\WINDOWS\\Driver Cache\\i386» и добавьте: DriverSigningPolicy=Ignore UpdateInstalledDrivers=Yes В разделе [SetupMgr] поменяйте параметр DistFolder=«C:\\WINDOWS\\Driver Cache\\i386» Теперь нужно создать подборку драйверов (это именно те драйвера, которые будут использоваться для установки всех устройств системы при разворачивании образа). Ее можно составить из драйверов, которые используются для установки рабочих станций в вашей компании, или использовать уже готовую (наверняка в интернете есть масса таких). Копируем все эти драйвера в C:\\Windows\\Driver Cache виртуальной машины. Следующий шаг — нам понадобится Sysprep Driver Scanner. Скачайте его и поместите в папку C:\\Sysprep. Запускаем и проходим следующие этапы: Меняем путь для поиска драйверов на C:\\Windows\\Driver Cache. Нажимаем «Scan». Нажимаем «Default». Нажимаем «Save». Нажимаем «Done». Теперь, когда мини-установка Sysprep выполняется, она будет искать драйвера именно в указанной нами директории. Опять нужно внести изменения в файл Sysprep.inf — в конце файла добавляем [SysprepMassStorage]. И выполняем из командной строки «sysprep.exe –bmsd». На выходе получаем что-то типа: [SysprepMassStorage] pci\\ven_1022&dev_7007=c:\\windows\\inf\\machine.inf pci\\ven_1022&dev_700d=c:\\windows\\inf\\machine.inf pci\\ven_1022&dev_700f=c:\\windows\\inf\\machine.inf pci\\ven_1022&dev_740b=c:\\windows\\inf\\machine.inf pcmcia\\ibm-pcmcia_portable_cd-rom_drive-84e3=c:\\windows\\inf\\pnpscsi.inf pcmcia\\ibm-pcmcia_cd-rom_drive_cd-400-5afa=c:\\windows\\inf\\pnpscsi.inf pcmcia\\ibm-pcmcia_portable_cd-rom_drive-84e2=c:\\windows\\inf\\pnpscsi.inf pcmcia\\pcmcia-ide_card-3220=c:\\windows\\inf\\pnpscsi.inf ... Это идентификаторы драйверов и их местонахождения. Теперь приступаем к изменению настроек ядра компьютера для нашего образа. Для этого нам понадобится еще одна утилита — MySysprep. Скачиваем и распаковываем во все ту же папку Sysprep на диске C: нашей виртуальной машины. Ищем в распакованном MySysprep.inf и убираем все знаки комментария (;) с секции [CPU]. Отредактированный файл сохраняем. Теперь нужно сменить дефолтный «Однопроцессорный компьютер с ACPI» на «Компьютер ACPI». Для этого открываем «Диспетчер устройств» и выбираем опцию обновления драйвера для устройства «Однопроцессорный компьютер ACPI». От предложений поискать отказываемся и говорим, что все сами найдем, и уже в последнем окне (окно выбора драйвера вручную) выбираем «Компьютер ACPI». После этого рестартуем и запускаем файл mysysprep.exe из папки C:\\Sysprep. В окне программы ставим галочку «мини-установка» и нажимаем кнопку «Reseal» («Запечатать компьютер»). Теперь наша ОС готова к «захвату» и созданию образа с помощью WDS. При старте виртуальной машины заходим в BIOS, ставим загрузку по сети (PXE). При загрузке по PXE попадаем на меню выбора образа WDS-сервера и выбираем тот образ, который у нас на WDS сервере служит для «захвата». После выполнения загрузчика выбираем место сохранения образа (WDS-сервер или локальный диск), и, собственно происходит сам процесс создания образа. Полученный образ будет автоматически разворачивать Windows XP на указанной машине, при этом все устройства будут установлены с правильными драйверами. Он также хорош тем, что «не боится» разномастных аппаратных конфигураций. Все вышеперечисленные мною действия актуальны для инфраструктуры с развернутым и настроенным WDS сервером. Также в комплексе по созданию Light-Touch Deployment образов можно поговорить и о возможностях создания файла ответов для образов WDS-сервера, что позволяет упростить развертывание образа и вовсе до 1-2 нажатий клавиши (автоматическая разбивка жесткого диска, авто-выбор образа на WDS-сервере и многое другое). Если у вас такая инфраструктура отсутствует, то ее создание могло бы стать темой следующей статьи, хоть и получается в таком случае не очень правильно с точки зрения хронологии.\n\n\nКраткое содержание:\nСтатья рассказывает о создании образа Windows XP SP3 для развертывания по сети через WDS. Описывается процесс создания шаблона инсталляции Windows XP SP3 на виртуальной машине, подготовки системы для снятия образа, включая упаковку драйверов, и настройки ядра компьютера для корректной установки. Статья также упоминает о возможности создания файла ответов для упрощения развертывания образа и использования MySysprep для изменения настроек ядра компьютера.","summary":"Статья рассказывает о создании образа Windows XP SP3 для развертывания по сети через WDS. Описывается процесс создания шаблона инсталляции Windows XP SP3 на виртуальной машине, подготовки системы для снятия образа, включая упаковку драйверов, и настройки ядра компьютера для корректной установки. Статья также упоминает о возможности создания файла ответов для упрощения развертывания образа и использования MySysprep для изменения настроек ядра компьютера."}
{"Text":"USB-адаптер для цифрового вольтметра В7-28 на базе Arduino Mega / Habr                       \n\n09-06-2015\nНаша лаборатория занимается, в числе прочего, измерением электросопротивления различных образцов. Основные рабочие лошадки нашей измерительной установки — цифровые мультиметры американской компании Agilent, такие как 34401A. Кроме этого, долгое время лежали неиспользуемыми несколько стареньких, но рабочих вольтметров В7-28 (один из них на фото). Данный вольтметр, правда, уступает по точности и быстродействию «американцам», но всё равно достаточно хорош: точность до 5 десятичных цифр, приемлемая шумность, экранированные входы, а главное — возможность дистанционного управления с ПЭВМ (которые теперь зовутся ПК). Использовать их не получалось по следующим причинам: Нестандартный аппаратный интерфейс: на задней панели имеются два разъёма с 56 и 22 контактами, по которым В7-28 принимает команды и возвращает результаты измерений. Непосредственно подключить вольтметр к ПК невозможно; Отсутствие программного интерфейса, то есть какой-нибудь системы управляющих команд. Нужно включить режим измерения постоянного напряжения — извольте подать соответствующую комбинацию цифровых уровней на управляющий разъём. Нужно прочитать показания — к вашим услугам 21 цифровой сигнал на разъёме ЦПУ, и так далее. При этом программное обеспечение измерительной установки, написанное и отлаженное, использует SCPI команды для управления вольтметрами, и вносить туда поддержку нестандартных интерфейсов было бы слишком сложной задачей. Таким образом, задача свелась к двум пунктам: Снабдить В7-28 каким-нибудь современным аппаратным интерфейсом, чтобы подключать его непосредственно к ПК; «Научить» его понимать SCPI команды, чтобы не трогать высокоуровневое ПО. Почему Arduino Mega? У нас нет штатных «электронщиков», которые могли бы разработать и спаять плату микроконтроллера, поэтому нужно было выбрать что-то готовое, и Arduino оказалась тем что нужно. Не буду перечислять её известные достоинства, но нам она подошла тем, что позволила минимизировать объём пайки и обойтись без специальных программаторов. Из всего «ардуинового» семейства нам подошла только Arduino Mega 2560: только у неё оказалось достаточное количество цифровых входов и выходов. Плата довольно дорогая, в российских интернет-магазинах итальянский оригинал стоит около 4 тыс. рублей (на момент написания статьи). К счастью, имеется множество китайских аналогов по цене $10 и даже ниже. Связь с ПК у Arduino осуществляется через USB интерфейс, и это решало задачу №1. Осталось разработать прошивку, которая бы реализовывала бы подмножество SCPI команд и их трансляцию в управляющие сигналы для В7-28. Реализация прошивки В качестве отправного пункта мы использовали замечательную библиотеку scpi-parser, любезно написанную коллегой из Чехии. Эта библиотека взяла на себя всю «грязную работу», связанную с синтаксическим разбором SCPI команд, что сэкономило нам до 6 человекомесяцев. Сердце прошивки — это библиотека scpi-multimeter, которая реализует логику SCPI команд и конечный автомат, занимающийся асинхронным считыванием и обработкой показаний вольтметра. Библиотека абстрагируется от аппаратной части, делегируя работу с портами ввода-вывода вовне, для чего использует некий абстрактный программный интерфейс. Это позволяет использовать библиотеку практически с любым типом как микропроцессора, так и вольтметра. И, наконец, сама прошивка — проект v7-28-arduino. Это небольшой по объёму кода проектик, который реализует абстрактный интерфейс библиотеки scpi-multimeter, то есть фактически пишет и читает в цифровые и последовательный порты на плате микроконтроллера. Дела паяльные Полностью обойтись без пайки нам, конечно, не удалось, но объём работ оказался довольно небольшой: взять монтажную плату, совместимую с Arduino Mega по расположению контактных отверстий; распаять на монтажной плате штырьковые соединители для установки на плате микроконтроллера; двумя 30-проводными шлейфами связать монтажную плату с разъёмами В7-28; наконец, воткнуть монтажную плату в микроконтроллер: из-за большого числа штырьковых контактов это не так уж просто. Результат виден на фотографиях. Чтобы уберечь пайку от механических повреждений и «разбалтывания», мы поместили микроконтроллеры с установленными монтажными платами в «фирменные» коробочки. В результате наш адаптер приобрёл пусть не заводской, но довольно приличный вид. Проверка адаптера Опустим описание долгого процесса тестирования и отладки прошивки. Когда, наконец, всё было закончено, мы решили подключить В7-28 вместе с Agilent 34401A в одну измерительную установку. Как говорилось выше, ПО высокого уровня управляет вольтметрами посредством SCPI команд. В настройках программы мы указали виртуальный COM-порт, соответствующий микроконтроллеру, и: Программа заработала! Теперь у нас стало больше вольметров и мы можем одновременно измерять больше образцов. PROFIT! Заключение Кому, кроме нас, может быть интересна эта работа? Во-первых, это, конечно, гордые обладатели В7-28, получившие возможность за умеренные средства снабдить свои вольтметры адаптером, который позволяет их использовать так же просто, как и современные аналоги. Интерфейсом USB сейчас оборудован любой ПК, а язык команд SCPI «из коробки» понимает LabVIEW и аналогичные средства разработки. Во-вторых, как говорилось выше, основная часть библиотеки не зависит от конкретной марки вольтметра или микроконтроллера. Например, достаточно просто модифицировать прошивку для поддержки других типов вольтметров, например В7-34А. Всех заинтересованных лиц приглашаем на страницу проекта с исходными текстами, там же имеется ссылка на готовые бинарные сборки и документацию. Спасибо за внимание!\n","metric":0.48046875,"metric_input":"Текст статьи:\nUSB-адаптер для цифрового вольтметра В7-28 на базе Arduino Mega / Habr                       \n\n09-06-2015\nНаша лаборатория занимается, в числе прочего, измерением электросопротивления различных образцов. Основные рабочие лошадки нашей измерительной установки — цифровые мультиметры американской компании Agilent, такие как 34401A. Кроме этого, долгое время лежали неиспользуемыми несколько стареньких, но рабочих вольтметров В7-28 (один из них на фото). Данный вольтметр, правда, уступает по точности и быстродействию «американцам», но всё равно достаточно хорош: точность до 5 десятичных цифр, приемлемая шумность, экранированные входы, а главное — возможность дистанционного управления с ПЭВМ (которые теперь зовутся ПК). Использовать их не получалось по следующим причинам: Нестандартный аппаратный интерфейс: на задней панели имеются два разъёма с 56 и 22 контактами, по которым В7-28 принимает команды и возвращает результаты измерений. Непосредственно подключить вольтметр к ПК невозможно; Отсутствие программного интерфейса, то есть какой-нибудь системы управляющих команд. Нужно включить режим измерения постоянного напряжения — извольте подать соответствующую комбинацию цифровых уровней на управляющий разъём. Нужно прочитать показания — к вашим услугам 21 цифровой сигнал на разъёме ЦПУ, и так далее. При этом программное обеспечение измерительной установки, написанное и отлаженное, использует SCPI команды для управления вольтметрами, и вносить туда поддержку нестандартных интерфейсов было бы слишком сложной задачей. Таким образом, задача свелась к двум пунктам: Снабдить В7-28 каким-нибудь современным аппаратным интерфейсом, чтобы подключать его непосредственно к ПК; «Научить» его понимать SCPI команды, чтобы не трогать высокоуровневое ПО. Почему Arduino Mega? У нас нет штатных «электронщиков», которые могли бы разработать и спаять плату микроконтроллера, поэтому нужно было выбрать что-то готовое, и Arduino оказалась тем что нужно. Не буду перечислять её известные достоинства, но нам она подошла тем, что позволила минимизировать объём пайки и обойтись без специальных программаторов. Из всего «ардуинового» семейства нам подошла только Arduino Mega 2560: только у неё оказалось достаточное количество цифровых входов и выходов. Плата довольно дорогая, в российских интернет-магазинах итальянский оригинал стоит около 4 тыс. рублей (на момент написания статьи). К счастью, имеется множество китайских аналогов по цене $10 и даже ниже. Связь с ПК у Arduino осуществляется через USB интерфейс, и это решало задачу №1. Осталось разработать прошивку, которая бы реализовывала бы подмножество SCPI команд и их трансляцию в управляющие сигналы для В7-28. Реализация прошивки В качестве отправного пункта мы использовали замечательную библиотеку scpi-parser, любезно написанную коллегой из Чехии. Эта библиотека взяла на себя всю «грязную работу», связанную с синтаксическим разбором SCPI команд, что сэкономило нам до 6 человекомесяцев. Сердце прошивки — это библиотека scpi-multimeter, которая реализует логику SCPI команд и конечный автомат, занимающийся асинхронным считыванием и обработкой показаний вольтметра. Библиотека абстрагируется от аппаратной части, делегируя работу с портами ввода-вывода вовне, для чего использует некий абстрактный программный интерфейс. Это позволяет использовать библиотеку практически с любым типом как микропроцессора, так и вольтметра. И, наконец, сама прошивка — проект v7-28-arduino. Это небольшой по объёму кода проектик, который реализует абстрактный интерфейс библиотеки scpi-multimeter, то есть фактически пишет и читает в цифровые и последовательный порты на плате микроконтроллера. Дела паяльные Полностью обойтись без пайки нам, конечно, не удалось, но объём работ оказался довольно небольшой: взять монтажную плату, совместимую с Arduino Mega по расположению контактных отверстий; распаять на монтажной плате штырьковые соединители для установки на плате микроконтроллера; двумя 30-проводными шлейфами связать монтажную плату с разъёмами В7-28; наконец, воткнуть монтажную плату в микроконтроллер: из-за большого числа штырьковых контактов это не так уж просто. Результат виден на фотографиях. Чтобы уберечь пайку от механических повреждений и «разбалтывания», мы поместили микроконтроллеры с установленными монтажными платами в «фирменные» коробочки. В результате наш адаптер приобрёл пусть не заводской, но довольно приличный вид. Проверка адаптера Опустим описание долгого процесса тестирования и отладки прошивки. Когда, наконец, всё было закончено, мы решили подключить В7-28 вместе с Agilent 34401A в одну измерительную установку. Как говорилось выше, ПО высокого уровня управляет вольтметрами посредством SCPI команд. В настройках программы мы указали виртуальный COM-порт, соответствующий микроконтроллеру, и: Программа заработала! Теперь у нас стало больше вольметров и мы можем одновременно измерять больше образцов. PROFIT! Заключение Кому, кроме нас, может быть интересна эта работа? Во-первых, это, конечно, гордые обладатели В7-28, получившие возможность за умеренные средства снабдить свои вольтметры адаптером, который позволяет их использовать так же просто, как и современные аналоги. Интерфейсом USB сейчас оборудован любой ПК, а язык команд SCPI «из коробки» понимает LabVIEW и аналогичные средства разработки. Во-вторых, как говорилось выше, основная часть библиотеки не зависит от конкретной марки вольтметра или микроконтроллера. Например, достаточно просто модифицировать прошивку для поддержки других типов вольтметров, например В7-34А. Всех заинтересованных лиц приглашаем на страницу проекта с исходными текстами, там же имеется ссылка на готовые бинарные сборки и документацию. Спасибо за внимание!\n\n\nКраткое содержание:\nСтатья рассказывает о разработке USB-адаптера для цифрового вольтметра В7-28 на базе Arduino Mega. Это позволяет использовать устаревший прибор вместе с современным программным обеспечением. Проект может быть интересен владельцам вольтметра В7-28, а также специалистам в области электроники и программирования.","summary":"Статья рассказывает о разработке USB-адаптера для цифрового вольтметра В7-28 на базе Arduino Mega. Это позволяет использовать устаревший прибор вместе с современным программным обеспечением. Проект может быть интересен владельцам вольтметра В7-28, а также специалистам в области электроники и программирования."}
{"Text":"Открытый ход. «Фингуру» ищет единомышленников для создания глобального IT-сервиса / Habr             \n\nВ нашей прошлой статье\n«Как мы создали массовый бухгалтерский сервис»\nмы рассказывали о принципах и правилах работы компании «Фингуру». В сегодняшнем посте мы бы хотели сделать читателям предложение о\nсотрудничестве\n. Мы приглашаем всех желающих принять участие в создании «продвинутого» бухгалтерского сервиса, аналогов которому пока нет.\n\nЗдесь мы поделимся тем, что у нас уже есть — своими наработками и мыслями «кому и почему это выгодно», а в конце статьи нам бы хотелось получить обратную связь: как Вам наше предложение, готовы ли Вы подключиться к нам и предложить свои идеи.\n\nИтак, к сути нашего предложения. Предприниматели терпеть не могут заниматься бухгалтерией. А бухгалтеры не умеют продавать. Но и те и другие должны взаимодействовать друг с другом, зачастую не понимая специфики работы каждого. Фингуру «поможет с бухгалтерией» любому бизнесмену, но теперь мы также готовы помочь с ведением своего бизнеса любой бухгалтерской фирме. И все это благодаря нашим наработкам в следующих областях:\n\n### I. Маркетинг и продажи. Делимся брендом ###\n\nМы предлагаем своеобразную франчайзинговую историю для малых и крупных бухгалтерских фирм. Компания (или даже один удаленный бухгалтер-надомник) как и прежде может работать под своим именем, но быть аккредитованным от «Фингуру» в своем городе.\n\n**Почему это выгодно для небольших компаний:**\nмы решаем для них проблему привлечения клиентов (часть будет приходить непосредственно с нашего сайта, часть — при самостоятельном поиске бренда «Фингуру» в своем городе).\n\n**Почему это выгодно для крупных компаний.**\nКак правило, у крупных компаний нет проблем с привлечением новых клиентов, но есть другая — «потолок», определенное число клиентов, которых они могут обслужить. Обычно это 50-100 компаний. Мы поможем крупным компаниям не только выгодно привлекать клиентов, но и эффективно их обслуживать.\n\n### II. Документы и процессы. Делимся стандартами ###\n\nКаждой бухгалтерской фирме приходится создавать свои документы: начиная от текстов договоров на обслуживание и заканчивая письмами клиентам. Но зачем изобретать велосипед. У нас уже готовы сотни необходимых шаблонов, и они прекрасно работают. И самое главное — часть тех процессов, которые выполняются в других бухгалтерских фирмах людьми, у нас полностью автоматизированы.\n\n**Пример:**\nпредприниматель не выставил акт вовремя.\n\nКак эта проблема решается в обычной бухгалтерской фирме: сотрудник открывает Excel или 1С (нужное подчеркнуть) и начинает «ручной» поиск забывчивого клиента. После этого отправляет ему письмо.\n\nКак эта проблема решается в «Фингуру»: система сама находит «должника» и сама отправляет ему вежливое письмо. Причем с обращением по имени и подробным указанием дальнейших действий — у получателя складывается ощущение, что с ним ведет диалог сотрудник компании, а не робот.\n\n**Почему это выгодно для небольших компаний.**\nНе надо тратить время на создание текстов документов, писем, обращений, не надо вести календарь напоминаний каждому клиенту. Проще взять готовое решение у нас и начать работать.\n\n### III. Производственное ПО. Делимся удобством ###\n\nПреимущества производственного программного обеспечения от «Фингуру» — это набор «фишек», которые делают работу с документами максимально простой, удобной и быстрой.\n\n**Пример:**\nклиент отправляет бухгалтеру накладную со списком из ста позиций. Бухгалтеру надо перевести этот документ в электронный вид, чтобы продолжить работу. Раньше бы пришлось перенабирать каждую позицию вручную, и заняло бы это не один час. Но с ПО от «Фингуру» такая задача решается за пару минут. Смотрите сами:\n\n**Почему это выгодно для крупных компаний.**\nЭкономия ресурсов и времени сотрудников. Готовые стандарты работы, автоматизированные в CRM-системе.\n\n**Почему это выгодно для небольших компаний.**\nВо-первых, это серьезно экономит время, а значит — можно обслужить больше клиентов. Во-вторых, всё ПО уже «заточено» под бухгалтерский сервис, а значит — не нужно компилировать решение из разных уже готовых программных продуктов или искать разработчиков для своего решения на стороне. К тому же такие поиски чреваты серьезными тратами, но не факт, что результат окажется удовлетворительным.\n\n**Почему это выгодно для крупных компаний.**\nПризнаём, что у крупных компаний есть свои системы автоматизации разных процессов (это и популярный 1С, системы для отправки отчетности через интернет), но наши решения послужат не заменой, а хорошим дополнением к этому списку. Просто добавьте необходимые вам блоки к своим системам: электронный документооборот, сканирование и распознавание первички, проверка контрагентов, электронный архив – и наслаждайтесь полученным результатом.\n\n### IV. Управленческое ПО. Делимся задачами ###\n\nУправленческое программное обеспечение от «Фингуру» — это мощная CRM-система, тесно интегрированные в бизнес-процессы и существующее ПО бухгалтерской фирмы. Возможно, для небольших компаний это не столь необходимо (ввиду отсутствия большого штата сотрудников и клиентов), но вот для крупных — это большой плюс. Наше программное обеспечение позволяет не только распределять задачи среди сотрудников и контролировать их выполнение, но и упростить жизнь клиентам. Управленческое ПО от «Фингуру» не требует установки на компьютер, а еще оно не только тесно связано с профессиональными бухгалтерскими программами (такими, как 1С и Бухгалтерия Контур), но и с другими информационными системами: электронным документооборотом, системой онлайн-конференций и др.\n\n### V. Обучение и поддержка. Делимся знаниями ###\n\nК кому может обратиться бухгалтер, если у него возник профессиональный вопрос? Чаще всего ответ ищется на форумах или в нормативных актах. В первом случае — не факт, что ответ будет верным, во втором — на поиск верного ответа может уйти слишком много времени. «Фингуру» содержит электронную базу знаний в виде кейсов, похожих случаев и финансовых операций.\n\nПример: бухгалтеру необходимо осуществить проводку по валютному счету. Считать в данном конкретном случае НДС или не считать? И если считать, то как? За помощью он может обратиться к личному менеджеру «Фингуру» и получить не только ответ, но и пример проведения такой проводки из нашей базы.\n\n**Выводы:**\nмы открыты для дальнейшей совместной работы с предпринимателями, бухгалтерскими фирмами, IT-специалистами. Наша конечная цель — автоматизировать бизнес-процессы, сделав бухгалтерское обслуживание простым и быстрым. Если вы уже занимались похожими разработками или хотите помочь нам, —\nприсоединяйтесь\n!","metric":0.7431640625,"metric_input":"Текст статьи:\nОткрытый ход. «Фингуру» ищет единомышленников для создания глобального IT-сервиса / Habr             \n\nВ нашей прошлой статье\n«Как мы создали массовый бухгалтерский сервис»\nмы рассказывали о принципах и правилах работы компании «Фингуру». В сегодняшнем посте мы бы хотели сделать читателям предложение о\nсотрудничестве\n. Мы приглашаем всех желающих принять участие в создании «продвинутого» бухгалтерского сервиса, аналогов которому пока нет.\n\nЗдесь мы поделимся тем, что у нас уже есть — своими наработками и мыслями «кому и почему это выгодно», а в конце статьи нам бы хотелось получить обратную связь: как Вам наше предложение, готовы ли Вы подключиться к нам и предложить свои идеи.\n\nИтак, к сути нашего предложения. Предприниматели терпеть не могут заниматься бухгалтерией. А бухгалтеры не умеют продавать. Но и те и другие должны взаимодействовать друг с другом, зачастую не понимая специфики работы каждого. Фингуру «поможет с бухгалтерией» любому бизнесмену, но теперь мы также готовы помочь с ведением своего бизнеса любой бухгалтерской фирме. И все это благодаря нашим наработкам в следующих областях:\n\n### I. Маркетинг и продажи. Делимся брендом ###\n\nМы предлагаем своеобразную франчайзинговую историю для малых и крупных бухгалтерских фирм. Компания (или даже один удаленный бухгалтер-надомник) как и прежде может работать под своим именем, но быть аккредитованным от «Фингуру» в своем городе.\n\n**Почему это выгодно для небольших компаний:**\nмы решаем для них проблему привлечения клиентов (часть будет приходить непосредственно с нашего сайта, часть — при самостоятельном поиске бренда «Фингуру» в своем городе).\n\n**Почему это выгодно для крупных компаний.**\nКак правило, у крупных компаний нет проблем с привлечением новых клиентов, но есть другая — «потолок», определенное число клиентов, которых они могут обслужить. Обычно это 50-100 компаний. Мы поможем крупным компаниям не только выгодно привлекать клиентов, но и эффективно их обслуживать.\n\n### II. Документы и процессы. Делимся стандартами ###\n\nКаждой бухгалтерской фирме приходится создавать свои документы: начиная от текстов договоров на обслуживание и заканчивая письмами клиентам. Но зачем изобретать велосипед. У нас уже готовы сотни необходимых шаблонов, и они прекрасно работают. И самое главное — часть тех процессов, которые выполняются в других бухгалтерских фирмах людьми, у нас полностью автоматизированы.\n\n**Пример:**\nпредприниматель не выставил акт вовремя.\n\nКак эта проблема решается в обычной бухгалтерской фирме: сотрудник открывает Excel или 1С (нужное подчеркнуть) и начинает «ручной» поиск забывчивого клиента. После этого отправляет ему письмо.\n\nКак эта проблема решается в «Фингуру»: система сама находит «должника» и сама отправляет ему вежливое письмо. Причем с обращением по имени и подробным указанием дальнейших действий — у получателя складывается ощущение, что с ним ведет диалог сотрудник компании, а не робот.\n\n**Почему это выгодно для небольших компаний.**\nНе надо тратить время на создание текстов документов, писем, обращений, не надо вести календарь напоминаний каждому клиенту. Проще взять готовое решение у нас и начать работать.\n\n### III. Производственное ПО. Делимся удобством ###\n\nПреимущества производственного программного обеспечения от «Фингуру» — это набор «фишек», которые делают работу с документами максимально простой, удобной и быстрой.\n\n**Пример:**\nклиент отправляет бухгалтеру накладную со списком из ста позиций. Бухгалтеру надо перевести этот документ в электронный вид, чтобы продолжить работу. Раньше бы пришлось перенабирать каждую позицию вручную, и заняло бы это не один час. Но с ПО от «Фингуру» такая задача решается за пару минут. Смотрите сами:\n\n**Почему это выгодно для крупных компаний.**\nЭкономия ресурсов и времени сотрудников. Готовые стандарты работы, автоматизированные в CRM-системе.\n\n**Почему это выгодно для небольших компаний.**\nВо-первых, это серьезно экономит время, а значит — можно обслужить больше клиентов. Во-вторых, всё ПО уже «заточено» под бухгалтерский сервис, а значит — не нужно компилировать решение из разных уже готовых программных продуктов или искать разработчиков для своего решения на стороне. К тому же такие поиски чреваты серьезными тратами, но не факт, что результат окажется удовлетворительным.\n\n**Почему это выгодно для крупных компаний.**\nПризнаём, что у крупных компаний есть свои системы автоматизации разных процессов (это и популярный 1С, системы для отправки отчетности через интернет), но наши решения послужат не заменой, а хорошим дополнением к этому списку. Просто добавьте необходимые вам блоки к своим системам: электронный документооборот, сканирование и распознавание первички, проверка контрагентов, электронный архив – и наслаждайтесь полученным результатом.\n\n### IV. Управленческое ПО. Делимся задачами ###\n\nУправленческое программное обеспечение от «Фингуру» — это мощная CRM-система, тесно интегрированные в бизнес-процессы и существующее ПО бухгалтерской фирмы. Возможно, для небольших компаний это не столь необходимо (ввиду отсутствия большого штата сотрудников и клиентов), но вот для крупных — это большой плюс. Наше программное обеспечение позволяет не только распределять задачи среди сотрудников и контролировать их выполнение, но и упростить жизнь клиентам. Управленческое ПО от «Фингуру» не требует установки на компьютер, а еще оно не только тесно связано с профессиональными бухгалтерскими программами (такими, как 1С и Бухгалтерия Контур), но и с другими информационными системами: электронным документооборотом, системой онлайн-конференций и др.\n\n### V. Обучение и поддержка. Делимся знаниями ###\n\nК кому может обратиться бухгалтер, если у него возник профессиональный вопрос? Чаще всего ответ ищется на форумах или в нормативных актах. В первом случае — не факт, что ответ будет верным, во втором — на поиск верного ответа может уйти слишком много времени. «Фингуру» содержит электронную базу знаний в виде кейсов, похожих случаев и финансовых операций.\n\nПример: бухгалтеру необходимо осуществить проводку по валютному счету. Считать в данном конкретном случае НДС или не считать? И если считать, то как? За помощью он может обратиться к личному менеджеру «Фингуру» и получить не только ответ, но и пример проведения такой проводки из нашей базы.\n\n**Выводы:**\nмы открыты для дальнейшей совместной работы с предпринимателями, бухгалтерскими фирмами, IT-специалистами. Наша конечная цель — автоматизировать бизнес-процессы, сделав бухгалтерское обслуживание простым и быстрым. Если вы уже занимались похожими разработками или хотите помочь нам, —\nприсоединяйтесь\n!\n\nКраткое содержание:\nКомпания \"Фингуру\" предлагает сотрудничество в создании продвинутого бухгалтерского сервиса. В статье представлены пять направлений сотрудничества: маркетинг и продажи, документы и процессы, производственное ПО, управленческое ПО и обучение и поддержка. \"Фингуру\" делится своими наработками и мыслями о том, кому и почему это выгодно. Компания приглашает предпринимателей и бухгалтерские фирмы к сотрудничеству и предлагает обратную связь.","summary":"Компания \"Фингуру\" предлагает сотрудничество в создании продвинутого бухгалтерского сервиса. В статье представлены пять направлений сотрудничества: маркетинг и продажи, документы и процессы, производственное ПО, управленческое ПО и обучение и поддержка. \"Фингуру\" делится своими наработками и мыслями о том, кому и почему это выгодно. Компания приглашает предпринимателей и бухгалтерские фирмы к сотрудничеству и предлагает обратную связь."}
{"Text":"Система плагинов и модулей в Unreal Engine 4 / Хабр               \n\n16-01-2016\nЗдравствуйте, меня зовут Дмитрий. Я занимаюсь созданием компьютерных игр на Unreal Engine в качестве хобби. Сегодня я расскажу, как создать свой модуль, а потом, как создать плагин в Unreal Engine 4. Создание модуля Итак, начнем с модулей. Когда вы создаете проект, в файле проекта автоматически создается один модуль. { \"FileVersion\": 3, \"EngineAssociation\": \"4.10\", \"Category\": \"\", \"Description\": \"\", \"Modules\": [ { \"Name\": \"UICustom\", \"Type\": \"Runtime\", \"LoadingPhase\": \"Default\" } ] } В принципе, для простого проекта это достаточно. Но если вы читали мою предыдущую статью (а если не читали, то советую сделать это прямо сейчас) можно увидеть, что определенный код, который необходим для регистрации ассета, добавлении кнопок на панель свойств этого ассета, для игры нам не нужен, а нужен только для редактора. Поэтому, было — бы хорошо сделать так, что — бы, при сборке игры этот код в игру не включался, именно для этого и предназначена система модулей. Итак, начнем. Сначала добавим ещё один модуль: { \"FileVersion\": 3, \"EngineAssociation\": \"4.10\", \"Category\": \"\", \"Description\": \"\", \"Modules\": [ { \"Name\": \"UICustom\", \"Type\": \"Runtime\", \"LoadingPhase\": \"Default\" }, { \"Name\": \"UICustomEditor\", \"Type\": \"Editor\", \"LoadingPhase\": \"Default\" } ] } Параметры модуля У модуля имеются параметр тип, который определяет куда следует включать модуль, а куда нет. Тип может принимать следующие значения: namespace EHostType { enum Type { Runtime, RuntimeNoCommandlet, Developer, Editor, EditorNoCommandlet, Program, Max, } } Ещё у модуля есть параметр LoadingPhase определяющий стадию загрузки модуля: namespace ELoadingPhase { enum Type { Default, PostDefault, PreDefault, PostConfigInit, PreLoadingScreen, PostEngineInit, Max, } } Любой модуль обязательно должен иметь 3 файла. Первый это заголовочный файл этого модуля <Имя модуля>.h, обычно в него включается файл Engine.h. Но стоит заметить, что при включении файла Engine.h размер Precompiled header становится равен ~700mgb. Если вы создадите в своем проекте много модулей, и каждый будет весить более 700mgb, ни каких винчестеров не хватит, поэтому рекомендую открыть Engine.h и выбрать только то, что вам надо. Кроме того в этом файле располагается интерфейс модуля. В котором можно что нибудь зарегистрировать, например. #pragma once #include \"Core.h\" #include \"CoreUObject.h\" #include \"Engine/EngineTypes.h\" #include \"SlateCore.h\" #include \"SlateBasics.h\" #include \"ModuleManager.h\" DECLARE_LOG_CATEGORY_EXTERN(UICustomEditor, All, All) class FUICustomEditorModule : public IModuleInterface { public: virtual void StartupModule() override; virtual void ShutdownModule() override; }; Вторым обязательным файлом является файл <Имя модуля>.cpp: #include \"UICustomEditor.h\" #include \"MyObjectAssetAction.h\" #include \"MyStructCustomization.h\" DEFINE_LOG_CATEGORY(UICustomEditor) void FUICustomEditorModule::StartupModule() { FMyObjectAssetAction::RegistrateCustomPartAssetType(); FMyClassDetails::RegestrateCostumization(); UE_LOG(UICustomEditor, Warning, TEXT(\"Editor module Started\")); } void FUICustomEditorModule::ShutdownModule() { UE_LOG(UICustomEditor, Warning, TEXT(\"Editor module Stoped\")); } IMPLEMENT_GAME_MODULE(FUICustomEditorModule, UICustom); Здесь я регистрирую тип ассета и изменение панели свойств, а также добавил сообщение в лог. В конце этого файла обязательно должен быть макрос IMPLEMENT_GAME_MODULE. Третий обязательный файл <Имя модуля>.Build.cs: using UnrealBuildTool; public class UICustomEditor : ModuleRules { public UICustomEditor(TargetInfo Target) { PublicDependencyModuleNames.AddRange(new string[] { \"Core\", \"CoreUObject\", \"InputCore\" }); PrivateDependencyModuleNames.AddRange(new string[] { \"Slate\", \"SlateCore\", \"PropertyEditor\", \"UnrealEd\", \"EditorStyle\" }); PrivateDependencyModuleNames.AddRange(new string[] { \"GameLibPluginRuntime\",\"UICustom\" }); //Плагин и основной модуль } } В этом файле я подключаю модули, необходимые для работы моего модуля (простите за тавтологию). Как видите, кроме системных модулей я также подключил основной модуль и плагин(о котором я напишу ниже). Важно: Если вы подключаете плагин к основному модулю, то надо подключать его и к остальным. Иначе, если вы используете объект из основного модуля в котором будет объект из плагина, то компилятор, выдав ошибку о том что не может найти файл, сошлется на строчку в основном модуле, но, поскольку в нем вы плагин уже подключили, вы не будете понимать в чем дело, и что от вас требует компилятор. Важно: Для того, чтобы класс или структуру из подключенного модуля можно было использовать. Нужно: 1) Подключить заголовочный файл этого класса или структуры. 2) При определении классов и структур которые вы хотите использовать из других модулей нужно писать <имя модуля большими буквами>_API. Например: class UICUSTOMEDITOR_API UMyObjectFactory : public UFactory После создания этих файлов нужно положить их в папку с именем модуля в папке Source. Осталось только добавить наш модуль в файл UICustomEditor.Target.cs (в файл UICustom.Target.cs мы ни чего не добавляем, поскольку модуль будет грузится только в редакторе) OutExtraModuleNames.AddRange( new string[] { \"UICustom\", \"UICustomEditor\" } ); Модуль готов, для полезной нагрузки я использовал файлы из прошлого урока. Теперь перейдем к созданию плагина. Создание плагина Плагин нужно воспринимать как проект, который можно вставить в другой проект. Плагин, как и проект, состоит из модулей и имеет свой файл проекта, который имеет расширение .uplugin. Чтобы плагин подключить к проекту, его надо положить в папку <директория проекта>/plugins и заново сгенерировать Visual Studio проект. После этого можно подключать модули плагина, как и любые другие модули (об этом написано выше). Соответственно, для создания плагина нужно создать файл <Имя плагина>.uplugin { \"FileVersion\" : 3, \"FriendlyName\" : \"GameLibPlugin\", \"Version\" : 48, \"VersionName\" : \"1.0\", \"CreatedBy\" : \"Deema 35\", \"CreatedByURL\" : \" \", \"EngineVersion\" : \"4.10.0\", \"Description\" : \"Library some system structure\", \"Category\" : \"System library\", \"EnabledByDefault\" : true, \"MarketplaceURL\" : \" \", \"Modules\" : [ { \"Name\" : \"GameLibPluginRuntime\", \"Type\" : \"Runtime\", \"LoadingPhase\" : \"PreDefault\" }, { \"Name\" : \"GameLibPluginEditor\", \"Type\" : \"Editor\" } ], \"CanContainContent\" : true } Если в папке плагина создать папку Resources, то положенное туда изображение, в формате png будет отображаться в менеджере плагинов. Далее в <папка плагина>/Source создаем папку для каждого из модулей плагина. Создание модулей для плагина не отличается от создания модулей для проекта(о чем было написано выше). Собственно, все, плагин готов. Для примера я сделал парсер CVS таблиц. После подключения плагина можно наблюдать: Проект с исходным кодом: здесь P.S Прошу учесть что я программист любитель, поэтому за кривость парсера просьба ногами не пинать.\n","metric":0.32641601559999999793,"metric_input":"Текст статьи:\nСистема плагинов и модулей в Unreal Engine 4 / Хабр               \n\n16-01-2016\nЗдравствуйте, меня зовут Дмитрий. Я занимаюсь созданием компьютерных игр на Unreal Engine в качестве хобби. Сегодня я расскажу, как создать свой модуль, а потом, как создать плагин в Unreal Engine 4. Создание модуля Итак, начнем с модулей. Когда вы создаете проект, в файле проекта автоматически создается один модуль. { \"FileVersion\": 3, \"EngineAssociation\": \"4.10\", \"Category\": \"\", \"Description\": \"\", \"Modules\": [ { \"Name\": \"UICustom\", \"Type\": \"Runtime\", \"LoadingPhase\": \"Default\" } ] } В принципе, для простого проекта это достаточно. Но если вы читали мою предыдущую статью (а если не читали, то советую сделать это прямо сейчас) можно увидеть, что определенный код, который необходим для регистрации ассета, добавлении кнопок на панель свойств этого ассета, для игры нам не нужен, а нужен только для редактора. Поэтому, было — бы хорошо сделать так, что — бы, при сборке игры этот код в игру не включался, именно для этого и предназначена система модулей. Итак, начнем. Сначала добавим ещё один модуль: { \"FileVersion\": 3, \"EngineAssociation\": \"4.10\", \"Category\": \"\", \"Description\": \"\", \"Modules\": [ { \"Name\": \"UICustom\", \"Type\": \"Runtime\", \"LoadingPhase\": \"Default\" }, { \"Name\": \"UICustomEditor\", \"Type\": \"Editor\", \"LoadingPhase\": \"Default\" } ] } Параметры модуля У модуля имеются параметр тип, который определяет куда следует включать модуль, а куда нет. Тип может принимать следующие значения: namespace EHostType { enum Type { Runtime, RuntimeNoCommandlet, Developer, Editor, EditorNoCommandlet, Program, Max, } } Ещё у модуля есть параметр LoadingPhase определяющий стадию загрузки модуля: namespace ELoadingPhase { enum Type { Default, PostDefault, PreDefault, PostConfigInit, PreLoadingScreen, PostEngineInit, Max, } } Любой модуль обязательно должен иметь 3 файла. Первый это заголовочный файл этого модуля <Имя модуля>.h, обычно в него включается файл Engine.h. Но стоит заметить, что при включении файла Engine.h размер Precompiled header становится равен ~700mgb. Если вы создадите в своем проекте много модулей, и каждый будет весить более 700mgb, ни каких винчестеров не хватит, поэтому рекомендую открыть Engine.h и выбрать только то, что вам надо. Кроме того в этом файле располагается интерфейс модуля. В котором можно что нибудь зарегистрировать, например. #pragma once #include \"Core.h\" #include \"CoreUObject.h\" #include \"Engine/EngineTypes.h\" #include \"SlateCore.h\" #include \"SlateBasics.h\" #include \"ModuleManager.h\" DECLARE_LOG_CATEGORY_EXTERN(UICustomEditor, All, All) class FUICustomEditorModule : public IModuleInterface { public: virtual void StartupModule() override; virtual void ShutdownModule() override; }; Вторым обязательным файлом является файл <Имя модуля>.cpp: #include \"UICustomEditor.h\" #include \"MyObjectAssetAction.h\" #include \"MyStructCustomization.h\" DEFINE_LOG_CATEGORY(UICustomEditor) void FUICustomEditorModule::StartupModule() { FMyObjectAssetAction::RegistrateCustomPartAssetType(); FMyClassDetails::RegestrateCostumization(); UE_LOG(UICustomEditor, Warning, TEXT(\"Editor module Started\")); } void FUICustomEditorModule::ShutdownModule() { UE_LOG(UICustomEditor, Warning, TEXT(\"Editor module Stoped\")); } IMPLEMENT_GAME_MODULE(FUICustomEditorModule, UICustom); Здесь я регистрирую тип ассета и изменение панели свойств, а также добавил сообщение в лог. В конце этого файла обязательно должен быть макрос IMPLEMENT_GAME_MODULE. Третий обязательный файл <Имя модуля>.Build.cs: using UnrealBuildTool; public class UICustomEditor : ModuleRules { public UICustomEditor(TargetInfo Target) { PublicDependencyModuleNames.AddRange(new string[] { \"Core\", \"CoreUObject\", \"InputCore\" }); PrivateDependencyModuleNames.AddRange(new string[] { \"Slate\", \"SlateCore\", \"PropertyEditor\", \"UnrealEd\", \"EditorStyle\" }); PrivateDependencyModuleNames.AddRange(new string[] { \"GameLibPluginRuntime\",\"UICustom\" }); //Плагин и основной модуль } } В этом файле я подключаю модули, необходимые для работы моего модуля (простите за тавтологию). Как видите, кроме системных модулей я также подключил основной модуль и плагин(о котором я напишу ниже). Важно: Если вы подключаете плагин к основному модулю, то надо подключать его и к остальным. Иначе, если вы используете объект из основного модуля в котором будет объект из плагина, то компилятор, выдав ошибку о том что не может найти файл, сошлется на строчку в основном модуле, но, поскольку в нем вы плагин уже подключили, вы не будете понимать в чем дело, и что от вас требует компилятор. Важно: Для того, чтобы класс или структуру из подключенного модуля можно было использовать. Нужно: 1) Подключить заголовочный файл этого класса или структуры. 2) При определении классов и структур которые вы хотите использовать из других модулей нужно писать <имя модуля большими буквами>_API. Например: class UICUSTOMEDITOR_API UMyObjectFactory : public UFactory После создания этих файлов нужно положить их в папку с именем модуля в папке Source. Осталось только добавить наш модуль в файл UICustomEditor.Target.cs (в файл UICustom.Target.cs мы ни чего не добавляем, поскольку модуль будет грузится только в редакторе) OutExtraModuleNames.AddRange( new string[] { \"UICustom\", \"UICustomEditor\" } ); Модуль готов, для полезной нагрузки я использовал файлы из прошлого урока. Теперь перейдем к созданию плагина. Создание плагина Плагин нужно воспринимать как проект, который можно вставить в другой проект. Плагин, как и проект, состоит из модулей и имеет свой файл проекта, который имеет расширение .uplugin. Чтобы плагин подключить к проекту, его надо положить в папку <директория проекта>/plugins и заново сгенерировать Visual Studio проект. После этого можно подключать модули плагина, как и любые другие модули (об этом написано выше). Соответственно, для создания плагина нужно создать файл <Имя плагина>.uplugin { \"FileVersion\" : 3, \"FriendlyName\" : \"GameLibPlugin\", \"Version\" : 48, \"VersionName\" : \"1.0\", \"CreatedBy\" : \"Deema 35\", \"CreatedByURL\" : \" \", \"EngineVersion\" : \"4.10.0\", \"Description\" : \"Library some system structure\", \"Category\" : \"System library\", \"EnabledByDefault\" : true, \"MarketplaceURL\" : \" \", \"Modules\" : [ { \"Name\" : \"GameLibPluginRuntime\", \"Type\" : \"Runtime\", \"LoadingPhase\" : \"PreDefault\" }, { \"Name\" : \"GameLibPluginEditor\", \"Type\" : \"Editor\" } ], \"CanContainContent\" : true } Если в папке плагина создать папку Resources, то положенное туда изображение, в формате png будет отображаться в менеджере плагинов. Далее в <папка плагина>/Source создаем папку для каждого из модулей плагина. Создание модулей для плагина не отличается от создания модулей для проекта(о чем было написано выше). Собственно, все, плагин готов. Для примера я сделал парсер CVS таблиц. После подключения плагина можно наблюдать: Проект с исходным кодом: здесь P.S Прошу учесть что я программист любитель, поэтому за кривость парсера просьба ногами не пинать.\n\n\nКраткое содержание:\nВ статье рассматривается создание модулей и плагинов в Unreal Engine 4. Модули используются для определения кода, который должен включаться в игру или редактор, а плагины - как проекты, которые можно вставить в другие проекты. Для создания модуля или плагина необходимо создать соответствующие файлы с определенными параметрами и структурой.","summary":"В статье рассматривается создание модулей и плагинов в Unreal Engine 4. Модули используются для определения кода, который должен включаться в игру или редактор, а плагины - как проекты, которые можно вставить в другие проекты. Для создания модуля или плагина необходимо создать соответствующие файлы с определенными параметрами и структурой."}
{"Text":"Марк, Джек и Алишер! Миру нужен новый фейсбук — образовательный / Habr                         \n\n27-06-2017\nДорогие Марк, Джек и Алишер! Новый — образовательный — фейсбук нужен людям для исхода из вековой бедности. Каждому человеку новый сервис должен дать шанс. Шанс на современные знания, шанс на развитие талантов и способностей, шанс на обретение силы и уверенности, а значит, и шанс на изменение своей судьбы и судьбы своего окружения. Кому, как не вам, возглавить исход человечества из бедности? Вы разбогатели и создали крупнейшие электронные сервисы с нуля. А значит, вам по силам и создание мировой онлайн-платформы с ясной миссией — образование ради процветания. Проблема бедности и ответственность богатых Бедность — одна из причин войн и конфликтов. Сообщество людей на планете подобно единому организму. Ни один орган не процветает за счет болезней других, и поэтому нет оправдания дисбалансу доходов как между странами, так и внутри народов. Ангус Дитон, нобелевский лауреат, называет крайности богатства и бедности источником разрушительных междоусобных войн. Бедность — это неравенство: материальное, образовательное и цифровое. Бедные не тратят время на учебу, а добывают хлеб насущный. Поэтому в мире до сих пор 750 млн неграмотных, из них 250 млн детей и подростков школьного возраста. А образовательное и цифровое неравенство вновь ведет к неравенству материальному. Замыкается дьявольский порочный круг: богатые снова богатеют, а бедные — беднеют. Бедность — это личные и социальные привычки. Ученые из Массачусетского технологического университета выяснили, что старые привычки никогда не забываются, а только скрываются образованием новых. А на формирование новых нейронных связей нужно до 8,5 месяцев. Поэтому, чтобы заменить привычки поколенческой бедности — привычками процветания, человечеству понадобятся десятилетия напряженной и терпеливой работы. И этот путь сродни Исходу — 40-летнему обретению свободы под водительством Моисея. Бедность — зона ответственности преуспевающих. Да, многие богачи агрессивно защищают личные состояния, несправедливо закрывая для остальных путь к процветанию. А многие бедняки, в свою очередь, не усердствуют в личностном развитии, не развиваются, грубят, обижаются и довольствуются малым. При этом очевидно, что человек — существо благородное. И ему нужны справедливые общественные модели, которые разрушат сложившиеся неравенства, например, образовательный фейсбук, который могут создать только сильные и преуспевающие люди. 10 атрибутов образовательного фейсбука Почему нужен новый сервис, а не подойдут уже сложившиеся социальные сети? Потому что социальные медиа не изменяют судьбу человека, его биографию. Опросы показывают, что почти 80% пользователей задумывались об удалении аккаунта из соцсетей, потому что устали от бессмысленной траты времени. Другие исследования утверждают, что 99% публикуемого контента бесполезно. Кроме того, в социальных медиа сложились устойчивые стереотипы поведения, изменить которые практически невозможно. Философия новых отношений для выхода из бедности — Equal Educational Exchange (Равноправный Образовательный Обмен). Ему соответствует 10 атрибутов образовательного фейсбука. Атрибут первый: глобальная электронная образовательная инфраструктура В доцифровую эру образование полностью зависело от зданий, учебников, ручек, тетрадей, учебных программ, школьных администраторов, учителей и преподавателей. Бедные страны были почти не способны создать образовательную оффлайн инфраструктуру. Неравенство возможностей приводило к неравенству результатов. Новая образовательная инфраструктура — смартфоны. Эти мобильные устройства вскоре будут у каждого, независимо от дохода. А значит образовательный фейсбук сможет аккумулировать большинство платных и бесплатных курсов на одной платформе. Чтобы пользователь мог почувствовать себя на «водительском месте». Насколько неклассическое образование эффективно? Кениец Джулиус Йего стал чемпионом мира по метанию копья, а свои первые уроки он брал, просматривая ролики на Youtube. Самый богатый человек Китая ездил на велосипеде к гостинице для проведения бесплатных экскурсий для иностранцев, чтобы попрактиковаться в английском языке. А самый молодой миллиардер планеты ушел со второго курса Гарвардского университета. Атрибут второй: учителем и учеником может стать каждый Как учителем, так и учеником в образовательном фейсбуке может стать каждый. Каждый может создать свою образовательную программу для других. Онлайн-платформа нового фейсбука для создания учебных курсов, вебинаров, онлайн-тренингов и коуч-консультаций аналогична WordPress для личных сайтов. А большое количество добровольцев и организаций по всему миру будут оперативно выявлять и решать потребности пользователей для укрепления привычек процветания: от азов чтения до риторики, от Duolingo до Coursera. Кроме того, быть учителем в 9 раз эффективнее, чем учеником. «Хочешь изучить предмет — прочитай напиши книгу о нем». Каждый умеет в жизни делать что-то хорошо. И надо дать ему шанс научить этому других и справиться тем самым со своей депрессией, как это сделали Полковник Сандерс и Джоан Роулинг. Атрибут третий: непрерывное образование на протяжении всей жизни В мире VUCA — volatility (нестабильность), uncertainty (неопределенность), complexity (сложность) и ambiguity (неоднозначность) — следует признать, что учиться нужно всю жизнь. Традиционная образовательная траектория — одно высшее образование до 25 лет — устарела. Правильно говорить о трехпиковой модели: второй трудоспособный возраст 30–55 лет и третий — 55+. В среднем человек за жизнь меняет 8 сфер деятельности. Поэтому образовательный фейсбук должен стать площадкой для непрерывного обучения на протяжении всей жизни. Атрибут четвертый: образование ради процветания, полезное для жизни Основная миссия образовательного фейсбука: образование ради процветания. Обретение полезных знаний, навыков и компетенций для жизни, для самозанятости, для бизнеса, для открытия новых рабочих мест. Ради формирования интегральной способности решать сложные вопросы жизни. Атрибут пятый: персонализация обучения для раскрытия потенциала В фокусе внимания образовательного фейсбука — раскрытие врожденного потенциала пользователей. Большой массив данных образовательной онлайн-платформы позволит предложить каждому те курсы и программы, что наиболее полно раскроют их таланты, склонности и способности. Это схоже с тем, как поведение покупателей в интернет-маркетинге фиксирует Google Analytics и делают предложения нейронные сети. Атрибут шестой: добровольческая поддержка изменений Привычки, а особенно социальные, меняются медленно, и требуется постоянная поддержка. В Германии 30% студентов не справляются с учебой и бросают вузы из-за высоких нагрузок и низкой мотивации. Новым пользователям образовательного фейсбука обязательно потребуется поддержка наставников, тренеров, тьюторов, советников, как платная, так и безвозмездная. Тысячи волонтеров создали Wikipedia, потому что участие в благотворительных программах повышает самооценку, уверенность в себе и повышает конкурентоспособность на рынке труда. Поэтому стать добровольным наставником захотят многие. Атрибут седьмой: практика уважительного диалога Для становления привычек процветания нужна хорошая диалоговая практика, в которой формируются коммуникативные навыки, навыки переговоров и убеждения, навыки формулирования идей и др. Задача ресурса — создать плодотворную среду групповых диалогов, но при этом не допустить конфликтов. Каждый заслуживает уважение, и особенно больно, когда клевещут за то, что ты не совершал. Без пространства доверия, понимания и поддержки невозможно формирование новых позитивных привычек. Образовательный фейсбук призван стать местом уважительного и содержательного диалога. Атрибут восьмой: система достижений, признаваемая оффлайн Полезные привычки формируются поощрением. Поэтому значимые действия должны быть вознаграждены за достижения и усердие. При этом ученый из Европы и австралийский бушмен должны быть равны в оценке динамики развития. А накапливаемые результаты должны вести к системе званий, признаваемой офлайн, как доктор философии, например. Рейтинг также повлияет на процент оплаты за некоторые профессиональные услуги тренеров, консультантов, репетиторов, психоаналитиков и пр. Атрибут девятый: непрерывное развитие сервиса и научный подход Мир совершенен в своем несовершенстве. Поэтому и через 100, и через 300, и через 10 000 лет образование как отрасль будет столь же востребована, как и сегодня. Вместе с тем у каждого народа и каждого человека свой уровень учебных запросов. Поэтому нужно учитывать предложения по развитию от самих пользователей. И использовать научный подход к развитию ресурса, в том числе для создания программ, целенаправленно раскрывающих индивидуальные таланты и способности. Атрибут десятый: прибыльная бизнес-модель сервиса Источники прибыли: рекламные системы, процент от стоимости платных образовательных услуг, курсов, вебинаров, консультаций и др. по схеме «защита клиента», а также бонусы от использования массива данных. Рентабельная бизнес-модель обеспечит образовательному фейсбуку независимое развитие в достижении высших целей: победы над мировой бедностью и предоставления каждому шанса на образование, дающее современные знания, раскрывающее потенциал и укрепляющее внутреннюю силу. Будем начинать? Марк Цукерберг, Джек Ма и Алишер Усманов, давайте начнем Исход из бедности. Начнем работу по созданию образовательного фейсбука. Сильные должны научить процветанию слабых. Быть первыми — полное сумасшествие, но может быть, поэтому имена первых переживают века. Марк, Джек и Алишер, отправляемся в путь?\n","metric":0.6748046875,"metric_input":"Текст статьи:\nМарк, Джек и Алишер! Миру нужен новый фейсбук — образовательный / Habr                         \n\n27-06-2017\nДорогие Марк, Джек и Алишер! Новый — образовательный — фейсбук нужен людям для исхода из вековой бедности. Каждому человеку новый сервис должен дать шанс. Шанс на современные знания, шанс на развитие талантов и способностей, шанс на обретение силы и уверенности, а значит, и шанс на изменение своей судьбы и судьбы своего окружения. Кому, как не вам, возглавить исход человечества из бедности? Вы разбогатели и создали крупнейшие электронные сервисы с нуля. А значит, вам по силам и создание мировой онлайн-платформы с ясной миссией — образование ради процветания. Проблема бедности и ответственность богатых Бедность — одна из причин войн и конфликтов. Сообщество людей на планете подобно единому организму. Ни один орган не процветает за счет болезней других, и поэтому нет оправдания дисбалансу доходов как между странами, так и внутри народов. Ангус Дитон, нобелевский лауреат, называет крайности богатства и бедности источником разрушительных междоусобных войн. Бедность — это неравенство: материальное, образовательное и цифровое. Бедные не тратят время на учебу, а добывают хлеб насущный. Поэтому в мире до сих пор 750 млн неграмотных, из них 250 млн детей и подростков школьного возраста. А образовательное и цифровое неравенство вновь ведет к неравенству материальному. Замыкается дьявольский порочный круг: богатые снова богатеют, а бедные — беднеют. Бедность — это личные и социальные привычки. Ученые из Массачусетского технологического университета выяснили, что старые привычки никогда не забываются, а только скрываются образованием новых. А на формирование новых нейронных связей нужно до 8,5 месяцев. Поэтому, чтобы заменить привычки поколенческой бедности — привычками процветания, человечеству понадобятся десятилетия напряженной и терпеливой работы. И этот путь сродни Исходу — 40-летнему обретению свободы под водительством Моисея. Бедность — зона ответственности преуспевающих. Да, многие богачи агрессивно защищают личные состояния, несправедливо закрывая для остальных путь к процветанию. А многие бедняки, в свою очередь, не усердствуют в личностном развитии, не развиваются, грубят, обижаются и довольствуются малым. При этом очевидно, что человек — существо благородное. И ему нужны справедливые общественные модели, которые разрушат сложившиеся неравенства, например, образовательный фейсбук, который могут создать только сильные и преуспевающие люди. 10 атрибутов образовательного фейсбука Почему нужен новый сервис, а не подойдут уже сложившиеся социальные сети? Потому что социальные медиа не изменяют судьбу человека, его биографию. Опросы показывают, что почти 80% пользователей задумывались об удалении аккаунта из соцсетей, потому что устали от бессмысленной траты времени. Другие исследования утверждают, что 99% публикуемого контента бесполезно. Кроме того, в социальных медиа сложились устойчивые стереотипы поведения, изменить которые практически невозможно. Философия новых отношений для выхода из бедности — Equal Educational Exchange (Равноправный Образовательный Обмен). Ему соответствует 10 атрибутов образовательного фейсбука. Атрибут первый: глобальная электронная образовательная инфраструктура В доцифровую эру образование полностью зависело от зданий, учебников, ручек, тетрадей, учебных программ, школьных администраторов, учителей и преподавателей. Бедные страны были почти не способны создать образовательную оффлайн инфраструктуру. Неравенство возможностей приводило к неравенству результатов. Новая образовательная инфраструктура — смартфоны. Эти мобильные устройства вскоре будут у каждого, независимо от дохода. А значит образовательный фейсбук сможет аккумулировать большинство платных и бесплатных курсов на одной платформе. Чтобы пользователь мог почувствовать себя на «водительском месте». Насколько неклассическое образование эффективно? Кениец Джулиус Йего стал чемпионом мира по метанию копья, а свои первые уроки он брал, просматривая ролики на Youtube. Самый богатый человек Китая ездил на велосипеде к гостинице для проведения бесплатных экскурсий для иностранцев, чтобы попрактиковаться в английском языке. А самый молодой миллиардер планеты ушел со второго курса Гарвардского университета. Атрибут второй: учителем и учеником может стать каждый Как учителем, так и учеником в образовательном фейсбуке может стать каждый. Каждый может создать свою образовательную программу для других. Онлайн-платформа нового фейсбука для создания учебных курсов, вебинаров, онлайн-тренингов и коуч-консультаций аналогична WordPress для личных сайтов. А большое количество добровольцев и организаций по всему миру будут оперативно выявлять и решать потребности пользователей для укрепления привычек процветания: от азов чтения до риторики, от Duolingo до Coursera. Кроме того, быть учителем в 9 раз эффективнее, чем учеником. «Хочешь изучить предмет — прочитай напиши книгу о нем». Каждый умеет в жизни делать что-то хорошо. И надо дать ему шанс научить этому других и справиться тем самым со своей депрессией, как это сделали Полковник Сандерс и Джоан Роулинг. Атрибут третий: непрерывное образование на протяжении всей жизни В мире VUCA — volatility (нестабильность), uncertainty (неопределенность), complexity (сложность) и ambiguity (неоднозначность) — следует признать, что учиться нужно всю жизнь. Традиционная образовательная траектория — одно высшее образование до 25 лет — устарела. Правильно говорить о трехпиковой модели: второй трудоспособный возраст 30–55 лет и третий — 55+. В среднем человек за жизнь меняет 8 сфер деятельности. Поэтому образовательный фейсбук должен стать площадкой для непрерывного обучения на протяжении всей жизни. Атрибут четвертый: образование ради процветания, полезное для жизни Основная миссия образовательного фейсбука: образование ради процветания. Обретение полезных знаний, навыков и компетенций для жизни, для самозанятости, для бизнеса, для открытия новых рабочих мест. Ради формирования интегральной способности решать сложные вопросы жизни. Атрибут пятый: персонализация обучения для раскрытия потенциала В фокусе внимания образовательного фейсбука — раскрытие врожденного потенциала пользователей. Большой массив данных образовательной онлайн-платформы позволит предложить каждому те курсы и программы, что наиболее полно раскроют их таланты, склонности и способности. Это схоже с тем, как поведение покупателей в интернет-маркетинге фиксирует Google Analytics и делают предложения нейронные сети. Атрибут шестой: добровольческая поддержка изменений Привычки, а особенно социальные, меняются медленно, и требуется постоянная поддержка. В Германии 30% студентов не справляются с учебой и бросают вузы из-за высоких нагрузок и низкой мотивации. Новым пользователям образовательного фейсбука обязательно потребуется поддержка наставников, тренеров, тьюторов, советников, как платная, так и безвозмездная. Тысячи волонтеров создали Wikipedia, потому что участие в благотворительных программах повышает самооценку, уверенность в себе и повышает конкурентоспособность на рынке труда. Поэтому стать добровольным наставником захотят многие. Атрибут седьмой: практика уважительного диалога Для становления привычек процветания нужна хорошая диалоговая практика, в которой формируются коммуникативные навыки, навыки переговоров и убеждения, навыки формулирования идей и др. Задача ресурса — создать плодотворную среду групповых диалогов, но при этом не допустить конфликтов. Каждый заслуживает уважение, и особенно больно, когда клевещут за то, что ты не совершал. Без пространства доверия, понимания и поддержки невозможно формирование новых позитивных привычек. Образовательный фейсбук призван стать местом уважительного и содержательного диалога. Атрибут восьмой: система достижений, признаваемая оффлайн Полезные привычки формируются поощрением. Поэтому значимые действия должны быть вознаграждены за достижения и усердие. При этом ученый из Европы и австралийский бушмен должны быть равны в оценке динамики развития. А накапливаемые результаты должны вести к системе званий, признаваемой офлайн, как доктор философии, например. Рейтинг также повлияет на процент оплаты за некоторые профессиональные услуги тренеров, консультантов, репетиторов, психоаналитиков и пр. Атрибут девятый: непрерывное развитие сервиса и научный подход Мир совершенен в своем несовершенстве. Поэтому и через 100, и через 300, и через 10 000 лет образование как отрасль будет столь же востребована, как и сегодня. Вместе с тем у каждого народа и каждого человека свой уровень учебных запросов. Поэтому нужно учитывать предложения по развитию от самих пользователей. И использовать научный подход к развитию ресурса, в том числе для создания программ, целенаправленно раскрывающих индивидуальные таланты и способности. Атрибут десятый: прибыльная бизнес-модель сервиса Источники прибыли: рекламные системы, процент от стоимости платных образовательных услуг, курсов, вебинаров, консультаций и др. по схеме «защита клиента», а также бонусы от использования массива данных. Рентабельная бизнес-модель обеспечит образовательному фейсбуку независимое развитие в достижении высших целей: победы над мировой бедностью и предоставления каждому шанса на образование, дающее современные знания, раскрывающее потенциал и укрепляющее внутреннюю силу. Будем начинать? Марк Цукерберг, Джек Ма и Алишер Усманов, давайте начнем Исход из бедности. Начнем работу по созданию образовательного фейсбука. Сильные должны научить процветанию слабых. Быть первыми — полное сумасшествие, но может быть, поэтому имена первых переживают века. Марк, Джек и Алишер, отправляемся в путь?\n\n\nКраткое содержание:\nВ статье рассматривается проблема бедности и необходимость создания образовательного фейсбука для решения этой проблемы. Автор выделяет 10 атрибутов образовательного фейсбука, которые помогут изменить судьбу людей и преодолеть бедность. Среди них: глобальная электронная образовательная инфраструктура, возможность стать учителем или учеником для каждого, непрерывное образование на протяжении всей жизни, образование ради процветания, персонализация обучения для раскрытия потенциала, добровольческая поддержка изменений, практика уважительного диалога, система достижений, непрерывное развитие сервиса и научный подход, прибыльная бизнес-модель. Автор призывает известных предпринимателей Марка Цукерберга, Джека Ма и Алишера Усманова начать работу по созданию образовательного фейсбука и помочь людям выбраться из бедности.","summary":"В статье рассматривается проблема бедности и необходимость создания образовательного фейсбука для решения этой проблемы. Автор выделяет 10 атрибутов образовательного фейсбука, которые помогут изменить судьбу людей и преодолеть бедность. Среди них: глобальная электронная образовательная инфраструктура, возможность стать учителем или учеником для каждого, непрерывное образование на протяжении всей жизни, образование ради процветания, персонализация обучения для раскрытия потенциала, добровольческая поддержка изменений, практика уважительного диалога, система достижений, непрерывное развитие сервиса и научный подход, прибыльная бизнес-модель. Автор призывает известных предпринимателей Марка Цукерберга, Джека Ма и Алишера Усманова начать работу по созданию образовательного фейсбука и помочь людям выбраться из бедности."}
{"Text":"Пайпы, the pythonic way / Habr             \n\nОдни питонисты любят код читаемый, другие предпочитают лаконичный. К сожалению, баланс между первым и вторым — решения по-настоящему изящные — редко случается встретить на практике. Чаще стречаются строки вроде\n\n```\nmy_function(sum(filter(lambda x: x % 3 == 1, [x for x in range(100)])))\n\n```\nИли четверостишья а ля\n```\nxs = [x for x in range(100)]\nxs_filtered = filter(lambda x: x % 3 == 1, xs)\nxs_sum = sum(xs_filtered)\nresult = my_function(xs_sum)\n\n```\nИдеалистам же хотелось бы писать как-то так\n```\nresult = [x for x in range(100)] \\\n    | where(lambda x: x % 3 == 1)) \\\n    | sum \\\n    | my_function\n\n```\n\nНе в Питоне?\n\nПростую реализацию подобных цепочек не так давно предложил некий Julien Palard в своей библиотеке\nPipe\n.\n\nНачнем сразу с примера:\n\n```\nfrom pipe import *\n[1,2,3,4] | where(lambda x: x<=2)\n#<generator object <genexpr> at 0x88231e4>\n\n```\n\nУпс, интуитивный порыв не прокатил. Пайп возвращает генератор, значения из которого еще только предстоит извлечь.\n\n```\n[1,2,3,4] | where(lambda x: x<=2) | as_list\n#[1, 2]\n\n```\n\nМожно было бы вытащить значения из генератора встроенной функцией приведения типа list(), но автор инструмента был последователен в своих изысканиях и предложил нам функцию-пайп as\\_list.\n\nКак видим, источником данных для пайпов в примере стал простой список. Вообще же говоря, использовать можно любые итерируемые (iterable) сущности Питона. Скажем, «пары» (tuples) или, что уже интересней, те же генераторы:\n\n```\ndef fib():\n    u\"\"\"\n    Генератор чисел Фибоначчи\n    \"\"\"\n    a, b = 0, 1\n    while 1:\n        yield a\n        a, b = b, a + b\n\nfib() | take_while(lambda x: x<10) | as_list\n#0\n#1\n#1\n#2\n#3\n#5\n#8\n\n```\nОтсюда можно извлечь несколько уроков:\nв пайпах можно использовать списки, «пары», генераторы — любые iterables.\n\n результатом объединения генераторов в цепочки станет генератор.\n\n без явного требования (приведения типа или же специального пайпа) пайпинг является «ленивым» в том смысле, что цепочка есть генератор и может служить бесконечным источником данных.\n\nРазумеется, радость была бы неполной, не будь у нас легкой возможностисоздавать собственные пайпы. Пример:\n\n```\n@Pipe\ndef custom_add(x):\n    return sum(x)\n[1,2,3,4] | custom_add\n#10\n\n```\nАргументы? Легко:\n\n```\n@Pipe\ndef sum_head(x, number_to_sum=2):\n    acc = 0\n    return sum(x[:number_to_sum])\n[1,2,3,4] | sum_head(3)\n#6\n\n```\nАвтор любезно предоставил достаточно много заготовленных пайпов. Некоторые из них:\ncount — пересчитать число элементов входящего iterable\n\ntake(n) — извлекает из входного iterable первые n элементов.\n\ntail(n) — извлекает последние n элементов.\n\nskip(n) — пропускает n первых элементов.\n\nall(pred) — возвращает True, если все элементы iterable удовлетворяют предикату pred.\n\nany(pred) — возвращает True, если хотя бы один элемент iterable удовлетворяют предикату pred.\n\nas\\_list/as\\_dist — приводит iterable к списку/словарю, если такое преобразование возможно.\n\npermutations(r=None) — составляет все возможные сочетания r элементов входного iterable. Если r не определено, то r принимается за len(iterable).\n\nstdout — вывести в стандартный поток iterable целиком после приведения к строке.\n\ntee — вывести элемент iterable в стандартный поток и передать для дальнешей обработки.\n\nselect(selector) — передать для дальнейшей обработки элементы iterable, после применения к ним функции selector.\n\nwhere(predicate) — передать для дальнейшей обработки элементы iterable, удовлетворяющие предикату predicate.\nА вот эти поинтересней:\nnetcat(host, port) — для каждого элемента iterable открыть сокет, передать сам элемент (разумеется, string), передать для дальнейшей обработки ответ хоста.\n\nnetwrite(host, port) — то же самое, только не читать из сокета после отправления данных.\nЭти и другие пайпы для сортировки, обхода и обработки потока данных входят по умолчанию в сам модуль, благо создавать их действительно легко.\n\n### Под капотом декоратора Pipe ###\n\nЧестно говоря, удивительно было увидеть, насколько лаконичен базовый код модуля! Судите сами:\n\n```\nclass Pipe:\n\n    def __init__(self, function):\n        self.function = function\n\n    def __ror__(self, other):\n        return self.function(other)\n\n    def __call__(self, *args, **kwargs):\n        return Pipe(lambda x: self.function(x, *args, **kwargs))\n\n```\nВот и все, собственно. Обычный класс-декоратор.\n\nВ конструкторе декоратор сохраняет декорируемую функцию, превращая ее в объект класса Pipe.\n\nЕсли пайп вызывается методом \\_\\_call\\_\\_ — возвращается новый пайп функции с заданными аргументами.\n\nГлавная тонкость — метод \\_\\_ror\\_\\_. Это инвертированный оператор, аналог оператора «или» (\\_\\_or\\_\\_), который вызывается у правого операнда с левым операндом в качестве аргумента.\n\nПолучается, что вычисление цепочки начинается слева направо. Первый элемент передается в качестве аргумента второму; результат вычисления второго — третьему и так далее. Безболезненно проходят по цепочке и генераторы.\n\nНа мой взгляд, очень и очень элегантно.\n\n### Послесловие ###\n\nСинтаксис у такого рода пайпов действительно простой и удобный, хотелось бы увидеть что-то подобное в популярных фреймворках; скажем, для обработки потоков данных; или — в декларативной форме — выстраивания в цепочки коллбеков.\n\nЕдинственным недостатком именно реализации являются довольно туманные трейсы ошибок.\n\nО развитии идеи и альтернативных реализация — в следующих статьях.","metric":0.3798828125,"metric_input":"Текст статьи:\nПайпы, the pythonic way / Habr             \n\nОдни питонисты любят код читаемый, другие предпочитают лаконичный. К сожалению, баланс между первым и вторым — решения по-настоящему изящные — редко случается встретить на практике. Чаще стречаются строки вроде\n\n```\nmy_function(sum(filter(lambda x: x % 3 == 1, [x for x in range(100)])))\n\n```\nИли четверостишья а ля\n```\nxs = [x for x in range(100)]\nxs_filtered = filter(lambda x: x % 3 == 1, xs)\nxs_sum = sum(xs_filtered)\nresult = my_function(xs_sum)\n\n```\nИдеалистам же хотелось бы писать как-то так\n```\nresult = [x for x in range(100)] \\\n    | where(lambda x: x % 3 == 1)) \\\n    | sum \\\n    | my_function\n\n```\n\nНе в Питоне?\n\nПростую реализацию подобных цепочек не так давно предложил некий Julien Palard в своей библиотеке\nPipe\n.\n\nНачнем сразу с примера:\n\n```\nfrom pipe import *\n[1,2,3,4] | where(lambda x: x<=2)\n#<generator object <genexpr> at 0x88231e4>\n\n```\n\nУпс, интуитивный порыв не прокатил. Пайп возвращает генератор, значения из которого еще только предстоит извлечь.\n\n```\n[1,2,3,4] | where(lambda x: x<=2) | as_list\n#[1, 2]\n\n```\n\nМожно было бы вытащить значения из генератора встроенной функцией приведения типа list(), но автор инструмента был последователен в своих изысканиях и предложил нам функцию-пайп as\\_list.\n\nКак видим, источником данных для пайпов в примере стал простой список. Вообще же говоря, использовать можно любые итерируемые (iterable) сущности Питона. Скажем, «пары» (tuples) или, что уже интересней, те же генераторы:\n\n```\ndef fib():\n    u\"\"\"\n    Генератор чисел Фибоначчи\n    \"\"\"\n    a, b = 0, 1\n    while 1:\n        yield a\n        a, b = b, a + b\n\nfib() | take_while(lambda x: x<10) | as_list\n#0\n#1\n#1\n#2\n#3\n#5\n#8\n\n```\nОтсюда можно извлечь несколько уроков:\nв пайпах можно использовать списки, «пары», генераторы — любые iterables.\n\n результатом объединения генераторов в цепочки станет генератор.\n\n без явного требования (приведения типа или же специального пайпа) пайпинг является «ленивым» в том смысле, что цепочка есть генератор и может служить бесконечным источником данных.\n\nРазумеется, радость была бы неполной, не будь у нас легкой возможностисоздавать собственные пайпы. Пример:\n\n```\n@Pipe\ndef custom_add(x):\n    return sum(x)\n[1,2,3,4] | custom_add\n#10\n\n```\nАргументы? Легко:\n\n```\n@Pipe\ndef sum_head(x, number_to_sum=2):\n    acc = 0\n    return sum(x[:number_to_sum])\n[1,2,3,4] | sum_head(3)\n#6\n\n```\nАвтор любезно предоставил достаточно много заготовленных пайпов. Некоторые из них:\ncount — пересчитать число элементов входящего iterable\n\ntake(n) — извлекает из входного iterable первые n элементов.\n\ntail(n) — извлекает последние n элементов.\n\nskip(n) — пропускает n первых элементов.\n\nall(pred) — возвращает True, если все элементы iterable удовлетворяют предикату pred.\n\nany(pred) — возвращает True, если хотя бы один элемент iterable удовлетворяют предикату pred.\n\nas\\_list/as\\_dist — приводит iterable к списку/словарю, если такое преобразование возможно.\n\npermutations(r=None) — составляет все возможные сочетания r элементов входного iterable. Если r не определено, то r принимается за len(iterable).\n\nstdout — вывести в стандартный поток iterable целиком после приведения к строке.\n\ntee — вывести элемент iterable в стандартный поток и передать для дальнешей обработки.\n\nselect(selector) — передать для дальнейшей обработки элементы iterable, после применения к ним функции selector.\n\nwhere(predicate) — передать для дальнейшей обработки элементы iterable, удовлетворяющие предикату predicate.\nА вот эти поинтересней:\nnetcat(host, port) — для каждого элемента iterable открыть сокет, передать сам элемент (разумеется, string), передать для дальнейшей обработки ответ хоста.\n\nnetwrite(host, port) — то же самое, только не читать из сокета после отправления данных.\nЭти и другие пайпы для сортировки, обхода и обработки потока данных входят по умолчанию в сам модуль, благо создавать их действительно легко.\n\n### Под капотом декоратора Pipe ###\n\nЧестно говоря, удивительно было увидеть, насколько лаконичен базовый код модуля! Судите сами:\n\n```\nclass Pipe:\n\n    def __init__(self, function):\n        self.function = function\n\n    def __ror__(self, other):\n        return self.function(other)\n\n    def __call__(self, *args, **kwargs):\n        return Pipe(lambda x: self.function(x, *args, **kwargs))\n\n```\nВот и все, собственно. Обычный класс-декоратор.\n\nВ конструкторе декоратор сохраняет декорируемую функцию, превращая ее в объект класса Pipe.\n\nЕсли пайп вызывается методом \\_\\_call\\_\\_ — возвращается новый пайп функции с заданными аргументами.\n\nГлавная тонкость — метод \\_\\_ror\\_\\_. Это инвертированный оператор, аналог оператора «или» (\\_\\_or\\_\\_), который вызывается у правого операнда с левым операндом в качестве аргумента.\n\nПолучается, что вычисление цепочки начинается слева направо. Первый элемент передается в качестве аргумента второму; результат вычисления второго — третьему и так далее. Безболезненно проходят по цепочке и генераторы.\n\nНа мой взгляд, очень и очень элегантно.\n\n### Послесловие ###\n\nСинтаксис у такого рода пайпов действительно простой и удобный, хотелось бы увидеть что-то подобное в популярных фреймворках; скажем, для обработки потоков данных; или — в декларативной форме — выстраивания в цепочки коллбеков.\n\nЕдинственным недостатком именно реализации являются довольно туманные трейсы ошибок.\n\nО развитии идеи и альтернативных реализация — в следующих статьях.\n\nКраткое содержание:\nВ статье рассматривается библиотека Pipe, которая позволяет создавать цепочки пайпов для обработки данных в Python. Пайпы - это цепочки функций, где каждый элемент в цепочке принимает на вход результат предыдущего элемента. Библиотека предоставляет готовые пайпы для различных операций, таких как суммирование элементов, нахождение подмножества элементов и т.д. Пайпы могут использовать списки, генераторы, кортежи и другие итерабельные объекты. Также можно создавать свои собственные пайпы с помощью декоратора Pipe.","summary":"В статье рассматривается библиотека Pipe, которая позволяет создавать цепочки пайпов для обработки данных в Python. Пайпы - это цепочки функций, где каждый элемент в цепочке принимает на вход результат предыдущего элемента. Библиотека предоставляет готовые пайпы для различных операций, таких как суммирование элементов, нахождение подмножества элементов и т.д. Пайпы могут использовать списки, генераторы, кортежи и другие итерабельные объекты. Также можно создавать свои собственные пайпы с помощью декоратора Pipe."}
{"Text":"Security Week 40: уязвимость в BootROM мобильных устройств Apple / Хабр                 \n\n30-09-2019\nВ зависимости от ваших предпочтений к этой новости можно подобрать один из двух заголовков. Либо «серьезная уязвимость обнаружена в мобильных устройствах Apple вплоть до iPhone X», либо «наконец-то придумали новый способ для джейлбрейка iДевайсов (но это не точно)». Насчет джейлбрейка пока и правда не ясно (хотя с большой вероятностью новые способы взлома айфонов появятся), а вот уязвимость, похоже, реальна. Хакер, известный как axi0mX, выложил в свободный доступ эксплойт для бага в BootRom ряда мобильных устройств Apple. Деталей об уязвимости пока немного, известно только, что эксплуатируется некое race condition. Но самое важное, что уязвимость впервые за долгое время найдена в коде, который используется для первоначальной загрузки устройства и хранится в ПЗУ, то есть к ней еще и нельзя выпустить патч. Эксплойт, названный checkm8 (читается как checkmate, «шах и мат»), сам по себе ни на что не способен: он лишь позволяет выполнить произвольный код в процессе загрузки телефона. Что этот код будет делать дальше — большой вопрос, так как BootRom хоть и важный, но не единственный механизм защиты мобильных устройств Apple. Точно известно, что напрямую получить доступ к личным данным пользователя не получится — систему Secure Enclave эксплойт не обходит. В общем, это плохая новость, хорошая новость, плохая новость. Плохая новость: баг есть там, где его нельзя починить. Хорошая новость: данные пользователей, скорее всего, в безопасности. Плохая новость: комбинация этой уязвимости с другими теоретически может дать большие возможности как мирным исследователям безопасности iOS, так и злоумышленникам. Подробнее об уязвимости можно узнать в этой новости, в интервью автора эксплойта изданию ArsTechnica и на гитхабе. На Github эксплойт chekm8 выложен в составе утилиты для перепрошивки устройств Apple, вместе с более ранним творением того же автора — эксплойтом alloc8 для телефона iPhone 3Gs, который был опубликован в 2017 году. Понятно, что эксплойт для устройства 2009 года в 2017-м имел чисто теоретический интерес, но он, в отличие от checkm8, перманентный (tethered), то есть возможность выполнения произвольного кода (и полного джейлбрейка устройства) сохраняется после перезагрузки. Checkm8 такой особенности не имеет: после перезагрузки он превращается в тыкву требуется повторный взлом устройства, которое нужно подключить к компьютеру и перевести в режим восстановления прошивки. Отсюда выводится еще одна характеристика уязвимости: ее невозможно эксплуатировать удаленно. EPIC JAILBREAK: Introducing checkm8 (read \"checkmate\"), a permanent unpatchable bootrom exploit for hundreds of millions of iOS devices. Most generations of iPhones and iPads are vulnerable: from iPhone 4S (A5 chip) to iPhone 8 and iPhone X (A11 chip). https://t.co/dQJtXb78sG — axi0mX (@axi0mX) September 27, 2019 В интервью ArsTechnica автор эксплойта высказывается более осторожно, чем у себя в Твиттере. Почти на все вопросы журналистов он отвечает: «возможно» и «зависит от обстоятельств». Точно определен только список подверженных устройств: это все мобильные девайсы Apple, начиная с iPhone 4 и заканчивая iPhone X. Хотя здесь правильно будет говорить о версиях SoC: подвержены почти все вплоть до A11, исключая только самые современные смартфоны и планшеты на базе чипов A12 и A13. В статье Arstechnica также упоминается, что баг присутствует и в часах Apple Watch. Успешная эксплуатация уязвимости позволяет как минимум расшифровать загрузочный код устройства и включить интерфейс для отладки (JTAG). Хакер axi0mX у себя в Твиттере говорит о невероятной пользе такого эксплойта для исследователей безопасности устройств Apple. И возможно, «пользу» (если правильно говорить об уязвимости, опубликованной в обход вендора) только исследователи и оценят. Пока что не существует полноценного джейлбрейка любого из подверженных устройств, позволяющего получить права суперпользователя в iOS и установить альтернативный магазин приложений Cydia. Non techie version: Any iPhone 8/X or earlier can now be: - booted to any iOS version, past/present/future, with no SHSH/APTickets - booted to any OS (e.g. Android) - compromised by attacker w/physical access, but still requires password (or brute force)for private data — 62657156686f6a75636a4d21506a736699a0f1548b (@Morpheus______) September 27, 2019 Гарантии доступа к данным на устройстве эксплойт, как мы уже упоминали, тоже не дает. Поэтому его ценность для государственных органов и всех, кто желает получить информацию с заблокированного устройства без ведома пользователя, сомнительна. У тех, кто занимается криминалистической экспертизой устройств, наверняка есть другие способы получения информации, и, как было показано недавно, они могут оказаться проще, чем свежеоткрытый эксплойт. Список возможностей из твита выше (перепрошивка устройства на любую другую версию iOS, загрузка альтернативных iOS, брутфорс пароля пользователя) — это скорее потенциальные последствия, а не суровая реальность. HACKED! Verbose booting iPhone X looks pretty cool. Starting in DFU Mode, it took 2 seconds to jailbreak it with checkm8, and then I made it automatically boot from NAND with patches for verbose boot. Latest iOS 13.1.1, and no need to upload any images. Thanks @qwertyoruiopz pic.twitter.com/4fyOx3G7E0 — axi0mX (@axi0mX) September 29, 2019 Впрочем, посмотрим. Через два дня после первой публикации axi0mX выложил видео, в котором показывает загрузку самой свежей версии iOS в режиме verbose. Само по себе это также ничего не доказывает, но говорит о том, что специалистам теперь будет чем заняться. Рекомендации (капитанские, но все же важные) такие: если вы политик диссидент параноик хотите максимально защитить свои данные на iPhone, пора купить устройство посвежее, не подверженное данной уязвимости. Установите не простой пасскод из цифр, а полноценный пароль с комбинацией цифр, букв и спецсимволов. Это затруднит брутфорс, даже если появится возможность для его проведения. Всем остальным переживать пока не стоит: Apple вполне способна снизить потенциал незакрываемой уязвимости в ROM там, где софт можно обновить. Несмотря на победный тон сообщений, даже сам автор эксплойта признает, что безопасность у Apple на высоте. Интересно, не изменится ли это отношение благодаря новым открытиям, которые исследователи сделают, воспользовавшись эксплойтом? Судя по всему, для изучения кода iOS без санкции Apple это почти идеальный инструмент. Disclaimer: Мнения, изложенные в этом дайджесте, могут не совпадать с официальной позицией «Лаборатории Касперского». Дорогая редакция вообще рекомендует относиться к любым мнениям со здоровым скептицизмом.\n","metric":0.59423828130000000414,"metric_input":"Текст статьи:\nSecurity Week 40: уязвимость в BootROM мобильных устройств Apple / Хабр                 \n\n30-09-2019\nВ зависимости от ваших предпочтений к этой новости можно подобрать один из двух заголовков. Либо «серьезная уязвимость обнаружена в мобильных устройствах Apple вплоть до iPhone X», либо «наконец-то придумали новый способ для джейлбрейка iДевайсов (но это не точно)». Насчет джейлбрейка пока и правда не ясно (хотя с большой вероятностью новые способы взлома айфонов появятся), а вот уязвимость, похоже, реальна. Хакер, известный как axi0mX, выложил в свободный доступ эксплойт для бага в BootRom ряда мобильных устройств Apple. Деталей об уязвимости пока немного, известно только, что эксплуатируется некое race condition. Но самое важное, что уязвимость впервые за долгое время найдена в коде, который используется для первоначальной загрузки устройства и хранится в ПЗУ, то есть к ней еще и нельзя выпустить патч. Эксплойт, названный checkm8 (читается как checkmate, «шах и мат»), сам по себе ни на что не способен: он лишь позволяет выполнить произвольный код в процессе загрузки телефона. Что этот код будет делать дальше — большой вопрос, так как BootRom хоть и важный, но не единственный механизм защиты мобильных устройств Apple. Точно известно, что напрямую получить доступ к личным данным пользователя не получится — систему Secure Enclave эксплойт не обходит. В общем, это плохая новость, хорошая новость, плохая новость. Плохая новость: баг есть там, где его нельзя починить. Хорошая новость: данные пользователей, скорее всего, в безопасности. Плохая новость: комбинация этой уязвимости с другими теоретически может дать большие возможности как мирным исследователям безопасности iOS, так и злоумышленникам. Подробнее об уязвимости можно узнать в этой новости, в интервью автора эксплойта изданию ArsTechnica и на гитхабе. На Github эксплойт chekm8 выложен в составе утилиты для перепрошивки устройств Apple, вместе с более ранним творением того же автора — эксплойтом alloc8 для телефона iPhone 3Gs, который был опубликован в 2017 году. Понятно, что эксплойт для устройства 2009 года в 2017-м имел чисто теоретический интерес, но он, в отличие от checkm8, перманентный (tethered), то есть возможность выполнения произвольного кода (и полного джейлбрейка устройства) сохраняется после перезагрузки. Checkm8 такой особенности не имеет: после перезагрузки он превращается в тыкву требуется повторный взлом устройства, которое нужно подключить к компьютеру и перевести в режим восстановления прошивки. Отсюда выводится еще одна характеристика уязвимости: ее невозможно эксплуатировать удаленно. EPIC JAILBREAK: Introducing checkm8 (read \"checkmate\"), a permanent unpatchable bootrom exploit for hundreds of millions of iOS devices. Most generations of iPhones and iPads are vulnerable: from iPhone 4S (A5 chip) to iPhone 8 and iPhone X (A11 chip). https://t.co/dQJtXb78sG — axi0mX (@axi0mX) September 27, 2019 В интервью ArsTechnica автор эксплойта высказывается более осторожно, чем у себя в Твиттере. Почти на все вопросы журналистов он отвечает: «возможно» и «зависит от обстоятельств». Точно определен только список подверженных устройств: это все мобильные девайсы Apple, начиная с iPhone 4 и заканчивая iPhone X. Хотя здесь правильно будет говорить о версиях SoC: подвержены почти все вплоть до A11, исключая только самые современные смартфоны и планшеты на базе чипов A12 и A13. В статье Arstechnica также упоминается, что баг присутствует и в часах Apple Watch. Успешная эксплуатация уязвимости позволяет как минимум расшифровать загрузочный код устройства и включить интерфейс для отладки (JTAG). Хакер axi0mX у себя в Твиттере говорит о невероятной пользе такого эксплойта для исследователей безопасности устройств Apple. И возможно, «пользу» (если правильно говорить об уязвимости, опубликованной в обход вендора) только исследователи и оценят. Пока что не существует полноценного джейлбрейка любого из подверженных устройств, позволяющего получить права суперпользователя в iOS и установить альтернативный магазин приложений Cydia. Non techie version: Any iPhone 8/X or earlier can now be: - booted to any iOS version, past/present/future, with no SHSH/APTickets - booted to any OS (e.g. Android) - compromised by attacker w/physical access, but still requires password (or brute force)for private data — 62657156686f6a75636a4d21506a736699a0f1548b (@Morpheus______) September 27, 2019 Гарантии доступа к данным на устройстве эксплойт, как мы уже упоминали, тоже не дает. Поэтому его ценность для государственных органов и всех, кто желает получить информацию с заблокированного устройства без ведома пользователя, сомнительна. У тех, кто занимается криминалистической экспертизой устройств, наверняка есть другие способы получения информации, и, как было показано недавно, они могут оказаться проще, чем свежеоткрытый эксплойт. Список возможностей из твита выше (перепрошивка устройства на любую другую версию iOS, загрузка альтернативных iOS, брутфорс пароля пользователя) — это скорее потенциальные последствия, а не суровая реальность. HACKED! Verbose booting iPhone X looks pretty cool. Starting in DFU Mode, it took 2 seconds to jailbreak it with checkm8, and then I made it automatically boot from NAND with patches for verbose boot. Latest iOS 13.1.1, and no need to upload any images. Thanks @qwertyoruiopz pic.twitter.com/4fyOx3G7E0 — axi0mX (@axi0mX) September 29, 2019 Впрочем, посмотрим. Через два дня после первой публикации axi0mX выложил видео, в котором показывает загрузку самой свежей версии iOS в режиме verbose. Само по себе это также ничего не доказывает, но говорит о том, что специалистам теперь будет чем заняться. Рекомендации (капитанские, но все же важные) такие: если вы политик диссидент параноик хотите максимально защитить свои данные на iPhone, пора купить устройство посвежее, не подверженное данной уязвимости. Установите не простой пасскод из цифр, а полноценный пароль с комбинацией цифр, букв и спецсимволов. Это затруднит брутфорс, даже если появится возможность для его проведения. Всем остальным переживать пока не стоит: Apple вполне способна снизить потенциал незакрываемой уязвимости в ROM там, где софт можно обновить. Несмотря на победный тон сообщений, даже сам автор эксплойта признает, что безопасность у Apple на высоте. Интересно, не изменится ли это отношение благодаря новым открытиям, которые исследователи сделают, воспользовавшись эксплойтом? Судя по всему, для изучения кода iOS без санкции Apple это почти идеальный инструмент. Disclaimer: Мнения, изложенные в этом дайджесте, могут не совпадать с официальной позицией «Лаборатории Касперского». Дорогая редакция вообще рекомендует относиться к любым мнениям со здоровым скептицизмом.\n\n\nКраткое содержание:\nВ статье сообщается о том, что хакер axi0mX опубликовал эксплойт для уязвимости в BootRom мобильных устройств Apple, начиная с iPhone 4 и заканчивая iPhone X. Эксплойт называется checkm8 и не позволяет получить доступ к личным данным пользователей, но его использование может дать исследователям безопасности и злоумышленникам дополнительные возможности для взлома устройств.","summary":"В статье сообщается о том, что хакер axi0mX опубликовал эксплойт для уязвимости в BootRom мобильных устройств Apple, начиная с iPhone 4 и заканчивая iPhone X. Эксплойт называется checkm8 и не позволяет получить доступ к личным данным пользователей, но его использование может дать исследователям безопасности и злоумышленникам дополнительные возможности для взлома устройств."}
{"Text":"Bash-функции и их имена, которые могут быть практически всем чем угодно / Habr             \n\n18-07-2021\nС Bash связано одно распространённое заблуждение, которое заключается в том, что имена bash-функций должны быть составлены по тем же правилам, что и имена переменных. В руководстве по Bash даже даётся следующее определение термина name (имя): Слово, состоящее исключительно из букв, цифр и символов подчёркивания, начинающееся с буквы или с символа подчёркивания. Такие слова используются в роли имён переменных оболочки и имён функций. Их ещё называют идентификаторами. Но, на самом деле, имена Bash-функций могут состоять из практически любых печатаемых символов. Например, я могу определить собственную прединкрементную унарную функцию: function ++ { (( $1++ )); } Точные правила, касающиеся именования bash-функций, весьма туманны. Возникает такое ощущение, что имена функций могут представлять собой абсолютно любую последовательность символов, которую оболочка Bash способна однозначно распознать. Например, следующий скрипт выводит все допустимые односимвольные имена функций: #!/bin/bash for n in {33..126}; do printf -v oct \"%03o\" \"$n\" printf -v chr \\\\\"$oct\" eval \"function $chr { echo -n '$chr'; }; $chr\" 2>/dev/null done Вот результаты его работы: +,./:=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz Тут, для получения всех десятичных кодов печатаемых ASCII-символов (хорошую справочную таблицу можно найти в man 7 ascii), используется механизм раскрытия скобок (brace expansion). После этого применяется команда printf, переводящая десятичный код в восьмеричное представление, а потом превращающая его в соответствующий символ. Затем тут применяется команда eval, в которой делается попытка объявить новую функцию с именем, соответствующим найденному символу, после чего осуществляется вызов этой функции. Если в имени функции содержится буква, то набор других символов, которые можно использовать в её имени, меняется. Вот — список допустимых символов, которые могут находиться в имени функции на второй позиции, учитывая то, что первым символом имени является буква a: !#%+,-./0123456789:?@ABCDEFGHIJKLMNOPQRSTUVWXYZ]^_abcdefghijklmnopqrstuvwxyz{}~ Все дополнительные символы имеют некое особое значение для парсера Bash, а то, что в начале имени находится буква, снимает вопрос их неоднозначного толкования системой. Это справедливо и для символов, которых нет во второй группе, например — это знак =, который сам по себе вполне подходит, но, судя по всему, последовательность символов a= выглядит как начало объявления переменной. То же касается и символа [, так как последовательность a[ похожа на начало конструкции для работы с массивом. Имена функций, кстати, совершенно не ограничены ASCII-символами. Вот, например, функция, именем которой является смайлик: function { echo «haha!»; } Переопределение встроенных команд Помимо того, что Bash позволяет объявлять функции с разными интересными именами, эта оболочка ещё и без проблем позволяет переопределять имена любых функций, в частности — имена встроенных команд. Вот как я переопределил команду echo: function echo { echo \"$@\"; } Правда, я сразу же должен попросить вас так не поступать. Вызов этой функции приведёт к бесконечной рекурсии, которая завершится «убийством» процесса. Но если удалось успешно переопределить встроенную команду — её исходный вариант можно вызвать с помощью команды command. В результате следующая конструкция оказывается вполне работоспособной: function echo { command echo \"$@\"; } Правда, рабочей такая функция окажется лишь до тех пор, пока не переопределят и command. Тогда всё будет основательно «поломано». Не знаю, хорошо это или плохо, но у меня такое ощущение, что ключевое слово function переопределить нельзя. И меня расстроило (хотя, скорее — успокоило) то, что переопределение exit не мешает процессам оболочки завершать работу, если только эта команда не вызывается явным образом. Какая от этого польза? Полагаю, что Bash работает именно так ради обеспечения обратной совместимости. Поэтому одно из полезных следствий вышеописанного положения дел заключается в том, что оболочка Bash до сих пор способна выполнять старый код, в котором встречаются функции со странными именами. И, конечно, этот приём значительно облегчает задачу обфускации кода, правда, забавным это будет выглядеть лишь до тех пор, пока дело не дойдёт до отладки такого кода. Так как в Bash имеются лишь глобальная и функциональная области видимости, импортированный код может переопределить ранее объявленные конструкции, такие, как имена функций. Поэтому, если вы пишете библиотеку или то, что называется «modulino», вы можете предусмотреть использование в имени каждой из функций префикса $namespace, указывающего на пространство имён вашего проекта. После этого запуск подобного кода с помощью source не приведёт к непреднамеренному переопределению имён функций в вызывающем коде (естественно, если вызывающий код написан с использованием такого же соглашения об именовании сущностей). Вот, например, функция из jp, выводящая сведения об ошибках: function jp.error { echo \"Error: $1 at line $JP_LINE, column $JP_COL\" >&2 return 1 } Так как правила именования переменных строже правил именования функций, я добавил к именам глобальных переменных префикс JP_, но тут используется та же самая идея (я позаимствовал это правило из JSON.bash). Можно ли счесть это удачным ходом? Если код рассчитан исключительно на Bash, то — да. Но такой код не отличается переносимостью (взгляните на стандарт POSIX). В частности — он не запустится в ash. Это — плохая новость для того, кто планирует запускать его в Docker-контейнерах, основанных на Busybox. В Bash имеется полезный POSIX-режим. Его можно применять для запуска кода, в переносимости которого есть сомнения. Работа в этом режиме, кроме того, запрещает пользовательскому коду переопределение встроенных команд, определённых в POSIX. Приходилось ли вам пользоваться Bash-функциями с необычными именами?\n","metric":0.59912109380000000414,"metric_input":"Текст статьи:\nBash-функции и их имена, которые могут быть практически всем чем угодно / Habr             \n\n18-07-2021\nС Bash связано одно распространённое заблуждение, которое заключается в том, что имена bash-функций должны быть составлены по тем же правилам, что и имена переменных. В руководстве по Bash даже даётся следующее определение термина name (имя): Слово, состоящее исключительно из букв, цифр и символов подчёркивания, начинающееся с буквы или с символа подчёркивания. Такие слова используются в роли имён переменных оболочки и имён функций. Их ещё называют идентификаторами. Но, на самом деле, имена Bash-функций могут состоять из практически любых печатаемых символов. Например, я могу определить собственную прединкрементную унарную функцию: function ++ { (( $1++ )); } Точные правила, касающиеся именования bash-функций, весьма туманны. Возникает такое ощущение, что имена функций могут представлять собой абсолютно любую последовательность символов, которую оболочка Bash способна однозначно распознать. Например, следующий скрипт выводит все допустимые односимвольные имена функций: #!/bin/bash for n in {33..126}; do printf -v oct \"%03o\" \"$n\" printf -v chr \\\\\"$oct\" eval \"function $chr { echo -n '$chr'; }; $chr\" 2>/dev/null done Вот результаты его работы: +,./:=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_abcdefghijklmnopqrstuvwxyz Тут, для получения всех десятичных кодов печатаемых ASCII-символов (хорошую справочную таблицу можно найти в man 7 ascii), используется механизм раскрытия скобок (brace expansion). После этого применяется команда printf, переводящая десятичный код в восьмеричное представление, а потом превращающая его в соответствующий символ. Затем тут применяется команда eval, в которой делается попытка объявить новую функцию с именем, соответствующим найденному символу, после чего осуществляется вызов этой функции. Если в имени функции содержится буква, то набор других символов, которые можно использовать в её имени, меняется. Вот — список допустимых символов, которые могут находиться в имени функции на второй позиции, учитывая то, что первым символом имени является буква a: !#%+,-./0123456789:?@ABCDEFGHIJKLMNOPQRSTUVWXYZ]^_abcdefghijklmnopqrstuvwxyz{}~ Все дополнительные символы имеют некое особое значение для парсера Bash, а то, что в начале имени находится буква, снимает вопрос их неоднозначного толкования системой. Это справедливо и для символов, которых нет во второй группе, например — это знак =, который сам по себе вполне подходит, но, судя по всему, последовательность символов a= выглядит как начало объявления переменной. То же касается и символа [, так как последовательность a[ похожа на начало конструкции для работы с массивом. Имена функций, кстати, совершенно не ограничены ASCII-символами. Вот, например, функция, именем которой является смайлик: function { echo «haha!»; } Переопределение встроенных команд Помимо того, что Bash позволяет объявлять функции с разными интересными именами, эта оболочка ещё и без проблем позволяет переопределять имена любых функций, в частности — имена встроенных команд. Вот как я переопределил команду echo: function echo { echo \"$@\"; } Правда, я сразу же должен попросить вас так не поступать. Вызов этой функции приведёт к бесконечной рекурсии, которая завершится «убийством» процесса. Но если удалось успешно переопределить встроенную команду — её исходный вариант можно вызвать с помощью команды command. В результате следующая конструкция оказывается вполне работоспособной: function echo { command echo \"$@\"; } Правда, рабочей такая функция окажется лишь до тех пор, пока не переопределят и command. Тогда всё будет основательно «поломано». Не знаю, хорошо это или плохо, но у меня такое ощущение, что ключевое слово function переопределить нельзя. И меня расстроило (хотя, скорее — успокоило) то, что переопределение exit не мешает процессам оболочки завершать работу, если только эта команда не вызывается явным образом. Какая от этого польза? Полагаю, что Bash работает именно так ради обеспечения обратной совместимости. Поэтому одно из полезных следствий вышеописанного положения дел заключается в том, что оболочка Bash до сих пор способна выполнять старый код, в котором встречаются функции со странными именами. И, конечно, этот приём значительно облегчает задачу обфускации кода, правда, забавным это будет выглядеть лишь до тех пор, пока дело не дойдёт до отладки такого кода. Так как в Bash имеются лишь глобальная и функциональная области видимости, импортированный код может переопределить ранее объявленные конструкции, такие, как имена функций. Поэтому, если вы пишете библиотеку или то, что называется «modulino», вы можете предусмотреть использование в имени каждой из функций префикса $namespace, указывающего на пространство имён вашего проекта. После этого запуск подобного кода с помощью source не приведёт к непреднамеренному переопределению имён функций в вызывающем коде (естественно, если вызывающий код написан с использованием такого же соглашения об именовании сущностей). Вот, например, функция из jp, выводящая сведения об ошибках: function jp.error { echo \"Error: $1 at line $JP_LINE, column $JP_COL\" >&2 return 1 } Так как правила именования переменных строже правил именования функций, я добавил к именам глобальных переменных префикс JP_, но тут используется та же самая идея (я позаимствовал это правило из JSON.bash). Можно ли счесть это удачным ходом? Если код рассчитан исключительно на Bash, то — да. Но такой код не отличается переносимостью (взгляните на стандарт POSIX). В частности — он не запустится в ash. Это — плохая новость для того, кто планирует запускать его в Docker-контейнерах, основанных на Busybox. В Bash имеется полезный POSIX-режим. Его можно применять для запуска кода, в переносимости которого есть сомнения. Работа в этом режиме, кроме того, запрещает пользовательскому коду переопределение встроенных команд, определённых в POSIX. Приходилось ли вам пользоваться Bash-функциями с необычными именами?\n\n\nКраткое содержание:\nСтатья обсуждает правила именования функций в Bash, которые могут включать практически любые печатаемые символы. Имена функций могут быть абсолютно любой последовательностью символов, которую оболочка Bash способна однозначно распознать. Приведены примеры таких имен, а также показано, как переопределять встроенные команды.","summary":"Статья обсуждает правила именования функций в Bash, которые могут включать практически любые печатаемые символы. Имена функций могут быть абсолютно любой последовательностью символов, которую оболочка Bash способна однозначно распознать. Приведены примеры таких имен, а также показано, как переопределять встроенные команды."}
{"Text":"Новый закон для описания скорости развития квантовых компьютеров? / Habr             \n\n31-08-2019\nЗакон Нивена говорит, что квантовые компьютеры улучшаются с «дважды экспоненциальной скоростью». Если он выдержит проверку временем, то ждать квантового превосходства осталось недолго. Квантовый процессор Foxtail от Google В декабре 2018 года учёные из Google AI провели вычисления на лучшем квантовом процессоре от Google. Им удалось воспроизвести эти вычисления на обычном ноутбуке. Затем в январе они запустили тот же тест на улучшенной версии квантового чипа. На этот раз для симуляции результата им понадобился уже мощный настольный компьютер. А к февралю у них уже не нашлось классических компьютеров, способных симулировать своих квантовых соперников. Для этого исследователям пришлось запрашивать процессорное время на огромной сети серверов. «Где-то в феврале мне пришлось сделать несколько звонков и сказать: „Эй, нам нужно побольше квоты“, — сказал Хартмут Нивен, директор лаборатории квантового искусственного интеллекта Google. „Мы запускали задачи, требовавшие миллиона процессоров“. Такое быстрое улучшение привело к появлению т.н. „закона Нивена“, нового правила, описывающего, насколько быстро квантовые компьютеры нагоняют классические. Правило родилось, как внутреннее наблюдение, и только потом Нивен упомянул о нём в мае на симпозиуме Google „Квантовая весна“. Там он рассказал, что квантовые компьютеры увеличивают вычислительную мощность по сравнению с классическими с „дважды экспоненциальной“ скоростью – ошеломительно быстрое движение. С двойным экспоненциальным ростом „сначала кажется, что ничего не происходит, ничего не происходит, а потом ой – и внезапно вы уже в другом мире, — сказал Нивен. – Именно это мы и наблюдаем“. Даже экспоненциальный рост – явление довольно быстрое. Он означает, что некая величина растёт, как степени двойки: 21, 22, 23, 24. В первые разы увеличение не так сильно заметно, но последующие получаются огромными. Закон Мура, знаменитое правило, по которому вычислительная мощность удваивается примерно каждые два года, экспоненциален. Двойной экспоненциальный рост выглядит более существенно. Вместо увеличения степеней двойки, величина растёт, как степени степени двойки: 221, 222, 223, 224. Двойной экспоненциальный рост освещался в недавней статье „Специалисты по информатике расширяют рубежи проверяемого знания“, и описывал огромную скорость роста сложности определённых вычислительных проблем. Двойной экспоненциальный рост настолько уникален, что ему сложно найти примеры в реальном мире. И скорость прогресса в квантовых вычислениях может стать первым таким примером. Двойная экспоненциальная скорость, с которой, как считает Нивен, квантовые компьютеры догоняют классические, является следствием комбинации двух экспоненциальных факторов. Первый – у квантовых компьютеров есть внутреннее экспоненциальное преимущество перед классическими: если, к примеру, в квантовом контуре есть четыре кубита, то его вычислительная мощность сравнима с контуром из 16 обычных битов. Это было бы так даже без улучшения квантовых технологий. Второй экспоненциальный фактор появляется благодаря быстрому улучшению квантовых процессоров. Нивен говорят, что лучшие квантовые чипы от Google в последнее время улучшаются с экспоненциальной скоростью. Эта скорость обусловлена уменьшением количества ошибок. Это позволило инженерам строить более крупные квантовые процессоры, сказал Нивен. Если классическим компьютерам требуется экспоненциально больше вычислительной мощности для симуляции квантовых процессоров, а мощность этих квантовые процессоры со временем растёт экспоненциально, то в итоге получается двойное экспоненциальное взаимоотношение между квантовыми и классическими машинами. Хартмут Нивен, директор лаборатории квантового искусственного интеллекта Google Не все убеждены в этом. Во-первых, классические компьютеры не стоят на месте. Обычные чипы продолжают улучшаться, даже если закон Мура уже не работает. Кроме того, специалисты по информатике постоянно придумывают более эффективные алгоритмы, помогающие классическим компьютерам не отставать. »Учитывая все движущиеся части, включая улучшения с классической и квантовой сторон, тяжело назвать этот рост двойным экспоненциальным\", — сказал Эндрю Чайлдс, один из директоров совместного центра квантовой информации и информатики Мэрилендского университета. И хотя точная скорость, с которой квантовые компьютеры догоняют классические, может являться предметом споров, сомнений в быстром улучшении квантовой технологии нет. «Думаю, что неоспоримая реальность этого прогресса передала мяч на сторону людей, считающих, что масштабируемые квантовые компьютеры не смогут работать, — написал Скот Ааронсон, специалист по информатике из Техасского университета в Остине, нам по емейл. – Теперь им придётся чётко сформулировать, где и почему остановится этот прогресс». Основная цель области квантовых вычислений – производить эффективные квантовые подсчёты, которые невозможно симулировать за разумное время на самых мощных классических компьютерах (а самым мощным сейчас считается суперкомпьютер Summit Ок-Риджской национальной лаборатории). И среди разных исследовательских групп, разрабатывающих квантовые компьютеры, Google особенно громко заявляет о своём преследовании этой цели, известной, как «квантовое превосходство». Пока что квантовое превосходство остаётся неуловимым – иногда кажется, что его вот-вот достигнут, но пока не удаётся. Но если закон Нивена будет выполняться, то до этой цели осталось недолго. Нивен не говорит, когда именно, по его мнению, команда Google достигнет квантового превосходства, но допускает, что это может произойти скоро. «Мы часто говорим, что думаем, что достигнем его в 2019 году, — сказал Нивен. – Все признаки этого уже налицо».\n","metric":0.097412109380000000414,"metric_input":"Текст статьи:\nНовый закон для описания скорости развития квантовых компьютеров? / Habr             \n\n31-08-2019\nЗакон Нивена говорит, что квантовые компьютеры улучшаются с «дважды экспоненциальной скоростью». Если он выдержит проверку временем, то ждать квантового превосходства осталось недолго. Квантовый процессор Foxtail от Google В декабре 2018 года учёные из Google AI провели вычисления на лучшем квантовом процессоре от Google. Им удалось воспроизвести эти вычисления на обычном ноутбуке. Затем в январе они запустили тот же тест на улучшенной версии квантового чипа. На этот раз для симуляции результата им понадобился уже мощный настольный компьютер. А к февралю у них уже не нашлось классических компьютеров, способных симулировать своих квантовых соперников. Для этого исследователям пришлось запрашивать процессорное время на огромной сети серверов. «Где-то в феврале мне пришлось сделать несколько звонков и сказать: „Эй, нам нужно побольше квоты“, — сказал Хартмут Нивен, директор лаборатории квантового искусственного интеллекта Google. „Мы запускали задачи, требовавшие миллиона процессоров“. Такое быстрое улучшение привело к появлению т.н. „закона Нивена“, нового правила, описывающего, насколько быстро квантовые компьютеры нагоняют классические. Правило родилось, как внутреннее наблюдение, и только потом Нивен упомянул о нём в мае на симпозиуме Google „Квантовая весна“. Там он рассказал, что квантовые компьютеры увеличивают вычислительную мощность по сравнению с классическими с „дважды экспоненциальной“ скоростью – ошеломительно быстрое движение. С двойным экспоненциальным ростом „сначала кажется, что ничего не происходит, ничего не происходит, а потом ой – и внезапно вы уже в другом мире, — сказал Нивен. – Именно это мы и наблюдаем“. Даже экспоненциальный рост – явление довольно быстрое. Он означает, что некая величина растёт, как степени двойки: 21, 22, 23, 24. В первые разы увеличение не так сильно заметно, но последующие получаются огромными. Закон Мура, знаменитое правило, по которому вычислительная мощность удваивается примерно каждые два года, экспоненциален. Двойной экспоненциальный рост выглядит более существенно. Вместо увеличения степеней двойки, величина растёт, как степени степени двойки: 221, 222, 223, 224. Двойной экспоненциальный рост освещался в недавней статье „Специалисты по информатике расширяют рубежи проверяемого знания“, и описывал огромную скорость роста сложности определённых вычислительных проблем. Двойной экспоненциальный рост настолько уникален, что ему сложно найти примеры в реальном мире. И скорость прогресса в квантовых вычислениях может стать первым таким примером. Двойная экспоненциальная скорость, с которой, как считает Нивен, квантовые компьютеры догоняют классические, является следствием комбинации двух экспоненциальных факторов. Первый – у квантовых компьютеров есть внутреннее экспоненциальное преимущество перед классическими: если, к примеру, в квантовом контуре есть четыре кубита, то его вычислительная мощность сравнима с контуром из 16 обычных битов. Это было бы так даже без улучшения квантовых технологий. Второй экспоненциальный фактор появляется благодаря быстрому улучшению квантовых процессоров. Нивен говорят, что лучшие квантовые чипы от Google в последнее время улучшаются с экспоненциальной скоростью. Эта скорость обусловлена уменьшением количества ошибок. Это позволило инженерам строить более крупные квантовые процессоры, сказал Нивен. Если классическим компьютерам требуется экспоненциально больше вычислительной мощности для симуляции квантовых процессоров, а мощность этих квантовые процессоры со временем растёт экспоненциально, то в итоге получается двойное экспоненциальное взаимоотношение между квантовыми и классическими машинами. Хартмут Нивен, директор лаборатории квантового искусственного интеллекта Google Не все убеждены в этом. Во-первых, классические компьютеры не стоят на месте. Обычные чипы продолжают улучшаться, даже если закон Мура уже не работает. Кроме того, специалисты по информатике постоянно придумывают более эффективные алгоритмы, помогающие классическим компьютерам не отставать. »Учитывая все движущиеся части, включая улучшения с классической и квантовой сторон, тяжело назвать этот рост двойным экспоненциальным\", — сказал Эндрю Чайлдс, один из директоров совместного центра квантовой информации и информатики Мэрилендского университета. И хотя точная скорость, с которой квантовые компьютеры догоняют классические, может являться предметом споров, сомнений в быстром улучшении квантовой технологии нет. «Думаю, что неоспоримая реальность этого прогресса передала мяч на сторону людей, считающих, что масштабируемые квантовые компьютеры не смогут работать, — написал Скот Ааронсон, специалист по информатике из Техасского университета в Остине, нам по емейл. – Теперь им придётся чётко сформулировать, где и почему остановится этот прогресс». Основная цель области квантовых вычислений – производить эффективные квантовые подсчёты, которые невозможно симулировать за разумное время на самых мощных классических компьютерах (а самым мощным сейчас считается суперкомпьютер Summit Ок-Риджской национальной лаборатории). И среди разных исследовательских групп, разрабатывающих квантовые компьютеры, Google особенно громко заявляет о своём преследовании этой цели, известной, как «квантовое превосходство». Пока что квантовое превосходство остаётся неуловимым – иногда кажется, что его вот-вот достигнут, но пока не удаётся. Но если закон Нивена будет выполняться, то до этой цели осталось недолго. Нивен не говорит, когда именно, по его мнению, команда Google достигнет квантового превосходства, но допускает, что это может произойти скоро. «Мы часто говорим, что думаем, что достигнем его в 2019 году, — сказал Нивен. – Все признаки этого уже налицо».\n\n\nКраткое содержание:\nНовый закон для описания скорости развития квантовых компьютеров? / Habr             \n\n31-08-2019\n\nЗакон Нивена говорит, что квантовые компьютеры улучшаются с «дважды экспоненциальной скоростью». Если он выдержит проверку временем, то ждать квантового превосходства осталось недолго.\n\nКвантовый процессор Foxtail от Google В декабре 2018 года учёные из Google AI провели вычисления на лучшем квантовом процессоре от Google. Им удалось воспроизвести эти вычисления на обычном ноутбуке. Затем в январе они запустили тот же тест на улучшенной версии квантового чипа. На этот раз для симуляции результата им понадобился уже мощный настольный компьютер. А к февралю у них уже не нашлось классических компьютеров, способных симулировать своих квантовых соперников. Для этого исследователям пришлось запрашивать процессорное время на огромной сети серверов.\n\n«Где-то в феврале мне пришлось сделать несколько звонков и сказать: „Эй, нам нужно побольше квоты“, — сказал Хартмут Нивен, директор лаборатории квантового искусственного интеллекта Google. „Мы запускали задачи, требовавшие миллиона процессоров“. Такое быстрое улучшение привело к появлению т.н. „закона Нивена“, нового правила, описывающего, насколько быстро квантовые компьютеры нагоняют классические. Правило родилось, как внутреннее наблюдение, и только потом Нивен упомянул о нём в мае на симпозиуме Google „Квантовая весна“. Там он рассказал, что квантовые компьютеры увеличивают вычислительную мощность по сравнению с классическими с „дважды экспоненциальной“ скоростью – ошеломительно быстрое движение. С двойным экспоненциальным ростом „сначала кажется, что ничего не происходит, ничего не происходит, а потом ой – и внезапно вы уже в другом мире, — сказал Нивен. – Именно это мы и наблюдаем“. Даже экспоненциальный рост – явление довольно быстрое. Он означает, что некая величина растёт, как степени двойки: 21, 22,","summary":"Новый закон для описания скорости развития квантовых компьютеров? / Habr             \n\n31-08-2019\n\nЗакон Нивена говорит, что квантовые компьютеры улучшаются с «дважды экспоненциальной скоростью». Если он выдержит проверку временем, то ждать квантового превосходства осталось недолго.\n\nКвантовый процессор Foxtail от Google В декабре 2018 года учёные из Google AI провели вычисления на лучшем квантовом процессоре от Google. Им удалось воспроизвести эти вычисления на обычном ноутбуке. Затем в январе они запустили тот же тест на улучшенной версии квантового чипа. На этот раз для симуляции результата им понадобился уже мощный настольный компьютер. А к февралю у них уже не нашлось классических компьютеров, способных симулировать своих квантовых соперников. Для этого исследователям пришлось запрашивать процессорное время на огромной сети серверов.\n\n«Где-то в феврале мне пришлось сделать несколько звонков и сказать: „Эй, нам нужно побольше квоты“, — сказал Хартмут Нивен, директор лаборатории квантового искусственного интеллекта Google. „Мы запускали задачи, требовавшие миллиона процессоров“. Такое быстрое улучшение привело к появлению т.н. „закона Нивена“, нового правила, описывающего, насколько быстро квантовые компьютеры нагоняют классические. Правило родилось, как внутреннее наблюдение, и только потом Нивен упомянул о нём в мае на симпозиуме Google „Квантовая весна“. Там он рассказал, что квантовые компьютеры увеличивают вычислительную мощность по сравнению с классическими с „дважды экспоненциальной“ скоростью – ошеломительно быстрое движение. С двойным экспоненциальным ростом „сначала кажется, что ничего не происходит, ничего не происходит, а потом ой – и внезапно вы уже в другом мире, — сказал Нивен. – Именно это мы и наблюдаем“. Даже экспоненциальный рост – явление довольно быстрое. Он означает, что некая величина растёт, как степени двойки: 21, 22,"}
{"Text":"Управление версиями зависимостей в Maven проекте / Habr             \n\nОдним из преимуществ организации Java проекта с помощью Maven является удобное описание всех библиотек и их версий, которые для него требуются. Далее, по мере развития проекта, нам наверняка захочется иметь возможность обновлять версии этих библиотек. Такой функционал предоставляет расширение\nVersions Maven\n.\n\nУправление версиями зависимостей можно разделить на три части:\n\nверсия родительского проекта (если имеется);\n\nверсия зависимостей;\n\nверсия используемых расширений Maven.\n\nСамая простая задача — это обновление версии родительского проекта. Достаточно в корне вашего проекта запустить\nкоманду\n:\n\n```\nmvn versions:update-parent\n\n```\n\nДля того, чтобы иметь возможность обновлять версии остальных зависимостей, их версии необходимо в явном виде описать как переменные в разделе\n**properties**\nпроекта Maven. Зависимости, версии которых описаны без ссылки на переменную, обновить таким образом не получится.\n\nЭто же относится и к версиям всех используемых вами расширений. Таким образом, нужно явно прописать в проекте все используемые расширения с вынесением их версий в переменные, после чего их версии тоже начнут обновляться.\n\nВ результате, все наши зависимости и расширения будут выглядеть таким образом:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\n    <properties>\n        <spring.version>4.2.0.RELEASE</spring.version>\n        <maven-clean.version>2.6.1</maven-clean.version>\n    </properties>\n\n    <dependencies>\n        <!-- Spring Framework -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context-support</artifactId>\n            <version>\\${spring.version}</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-clean-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>auto-clean</id>\n                        <phase>initialize</phase>\n                        <goals>\n                            <goal>clean</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-clean-plugin</artifactId>\n                    <version>\\${maven-clean.version}</version>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n    </build>\n</project>\n\n```\n\nЧтобы разобраться с тем, какие ещё расширения мы не указали явным образом, нам нужно воспользоваться\nкомандой\n:\n\n```\nmvn versions:display-plugin-updates\n\n```\n\nпо результатам работы которой можно узнать, какие ещё расширения не прописаны в вашем проекте и версиями которых вы не управляете. Так же будет указано, какую минимальную версию Maven необходимо указать в вашем проекте для корректной работы всех расширений.\n\nВажно отметить, что зависимости, которые включаются в ваш проект через родительский проект, не будут обновлять версии пока вы их явно не включите в список зависимостей своего проекта, перенеся из родительского. Это может быть полезно, если в своём проекте вы хотите иметь отличный набор версий чем предоставляет готовый родительский проект.\n\nПосле выполнения всех описанных выше операций мы можем непосредственно обновить все версии описанные нами как переменные, запустив\nкоманду\n:\n\n```\nmvn versions:update-properties\n\n```\n\nДалее вы можете просмотреть список внесённых расширением правок в Maven проект и принять решение об их целесообразности.\n\nПросматривая список предложенных новых версий, вы очень быстро обнаружите, что там начинаются появляться не финальные версии зависимостей, которые скорее всего вам не нужны. Для описания, какие версии при обновлении вы бы хотели пропускать, служит специальный\nфайл\n, где можно описать исключения в виде конкретных версий или regex выражений для конкретных зависимостей или их групп.\n\nСоздадим файл\n**maven-version-rules.xml**\nв корне проекта с таким содержимым:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ruleset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         comparisonMethod=\"maven\"\n         xmlns=\"http://mojo.codehaus.org/versions-maven-plugin/rule/2.0.0\"\n         xsi:schemaLocation=\"http://mojo.codehaus.org/versions-maven-plugin/rule/2.0.0 http://www.mojohaus.org/versions-maven-plugin/xsd/rule-2.0.0.xsd\">\n    <rules>\n        <rule groupId=\"org.hibernate\">\n            <ignoreVersions>\n                <ignoreVersion type=\"regex\">.*Alpha.*</ignoreVersion>\n                <ignoreVersion type=\"regex\">.*Beta.*</ignoreVersion>\n                <ignoreVersion type=\"regex\">.*[.]CR.*</ignoreVersion>\n            </ignoreVersions>\n        </rule>\n    </rules>\n</ruleset>\n\n```\n\nИ подключим его:\n\n```\n<plugin>\n    <groupId>org.codehaus.mojo</groupId>\n    <artifactId>versions-maven-plugin</artifactId>\n    <configuration>\n        <generateBackupPoms>false</generateBackupPoms>\n        <rulesUri>file://\\${project.basedir}/maven-version-rules.xml</rulesUri>\n    </configuration>\n</plugin>\n\n```\n\nВ результате, вы можете захотеть создать такой файл\n**update.bat**\n(как вариант,\n**update.sh**\n) для обновления версий в будущем:\n\n```\ncall mvn versions:update-parent versions:update-properties\n\n```\n\nили только:\n\n```\ncall mvn versions:update-properties\n\n```\n\nНужно отметить, что такие проекты как\nSpring IO platform\nи\nSpring Boot\nв своих родительских проектах уже предоставляют актуальные и протестированные на совместимость версии очень большого количества библиотек, что позволяет использовать их в своих проектах без указание конкретной версии.\n\nВсе исходники доступны на\nGitHub\n.","metric":0.37353515630000000414,"metric_input":"Текст статьи:\nУправление версиями зависимостей в Maven проекте / Habr             \n\nОдним из преимуществ организации Java проекта с помощью Maven является удобное описание всех библиотек и их версий, которые для него требуются. Далее, по мере развития проекта, нам наверняка захочется иметь возможность обновлять версии этих библиотек. Такой функционал предоставляет расширение\nVersions Maven\n.\n\nУправление версиями зависимостей можно разделить на три части:\n\nверсия родительского проекта (если имеется);\n\nверсия зависимостей;\n\nверсия используемых расширений Maven.\n\nСамая простая задача — это обновление версии родительского проекта. Достаточно в корне вашего проекта запустить\nкоманду\n:\n\n```\nmvn versions:update-parent\n\n```\n\nДля того, чтобы иметь возможность обновлять версии остальных зависимостей, их версии необходимо в явном виде описать как переменные в разделе\n**properties**\nпроекта Maven. Зависимости, версии которых описаны без ссылки на переменную, обновить таким образом не получится.\n\nЭто же относится и к версиям всех используемых вами расширений. Таким образом, нужно явно прописать в проекте все используемые расширения с вынесением их версий в переменные, после чего их версии тоже начнут обновляться.\n\nВ результате, все наши зависимости и расширения будут выглядеть таким образом:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\n    <properties>\n        <spring.version>4.2.0.RELEASE</spring.version>\n        <maven-clean.version>2.6.1</maven-clean.version>\n    </properties>\n\n    <dependencies>\n        <!-- Spring Framework -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>spring-context-support</artifactId>\n            <version>\\${spring.version}</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-clean-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>auto-clean</id>\n                        <phase>initialize</phase>\n                        <goals>\n                            <goal>clean</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n\n        <pluginManagement>\n            <plugins>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-clean-plugin</artifactId>\n                    <version>\\${maven-clean.version}</version>\n                </plugin>\n            </plugins>\n        </pluginManagement>\n    </build>\n</project>\n\n```\n\nЧтобы разобраться с тем, какие ещё расширения мы не указали явным образом, нам нужно воспользоваться\nкомандой\n:\n\n```\nmvn versions:display-plugin-updates\n\n```\n\nпо результатам работы которой можно узнать, какие ещё расширения не прописаны в вашем проекте и версиями которых вы не управляете. Так же будет указано, какую минимальную версию Maven необходимо указать в вашем проекте для корректной работы всех расширений.\n\nВажно отметить, что зависимости, которые включаются в ваш проект через родительский проект, не будут обновлять версии пока вы их явно не включите в список зависимостей своего проекта, перенеся из родительского. Это может быть полезно, если в своём проекте вы хотите иметь отличный набор версий чем предоставляет готовый родительский проект.\n\nПосле выполнения всех описанных выше операций мы можем непосредственно обновить все версии описанные нами как переменные, запустив\nкоманду\n:\n\n```\nmvn versions:update-properties\n\n```\n\nДалее вы можете просмотреть список внесённых расширением правок в Maven проект и принять решение об их целесообразности.\n\nПросматривая список предложенных новых версий, вы очень быстро обнаружите, что там начинаются появляться не финальные версии зависимостей, которые скорее всего вам не нужны. Для описания, какие версии при обновлении вы бы хотели пропускать, служит специальный\nфайл\n, где можно описать исключения в виде конкретных версий или regex выражений для конкретных зависимостей или их групп.\n\nСоздадим файл\n**maven-version-rules.xml**\nв корне проекта с таким содержимым:\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ruleset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         comparisonMethod=\"maven\"\n         xmlns=\"http://mojo.codehaus.org/versions-maven-plugin/rule/2.0.0\"\n         xsi:schemaLocation=\"http://mojo.codehaus.org/versions-maven-plugin/rule/2.0.0 http://www.mojohaus.org/versions-maven-plugin/xsd/rule-2.0.0.xsd\">\n    <rules>\n        <rule groupId=\"org.hibernate\">\n            <ignoreVersions>\n                <ignoreVersion type=\"regex\">.*Alpha.*</ignoreVersion>\n                <ignoreVersion type=\"regex\">.*Beta.*</ignoreVersion>\n                <ignoreVersion type=\"regex\">.*[.]CR.*</ignoreVersion>\n            </ignoreVersions>\n        </rule>\n    </rules>\n</ruleset>\n\n```\n\nИ подключим его:\n\n```\n<plugin>\n    <groupId>org.codehaus.mojo</groupId>\n    <artifactId>versions-maven-plugin</artifactId>\n    <configuration>\n        <generateBackupPoms>false</generateBackupPoms>\n        <rulesUri>file://\\${project.basedir}/maven-version-rules.xml</rulesUri>\n    </configuration>\n</plugin>\n\n```\n\nВ результате, вы можете захотеть создать такой файл\n**update.bat**\n(как вариант,\n**update.sh**\n) для обновления версий в будущем:\n\n```\ncall mvn versions:update-parent versions:update-properties\n\n```\n\nили только:\n\n```\ncall mvn versions:update-properties\n\n```\n\nНужно отметить, что такие проекты как\nSpring IO platform\nи\nSpring Boot\nв своих родительских проектах уже предоставляют актуальные и протестированные на совместимость версии очень большого количества библиотек, что позволяет использовать их в своих проектах без указание конкретной версии.\n\nВсе исходники доступны на\nGitHub\n.\n\nКраткое содержание:\nВ статье описывается управление версиями зависимостей в Maven проекте. Управление версиями зависимостей можно разделить на три части: версия родительского проекта, версия зависимостей и версия используемых расширений Maven. Для обновления версий зависимостей, их версии необходимо описать как переменные в разделе properties проекта Maven. Также, все используемые расширения должны быть явно прописаны с вынесением их версий в переменные. В итоге, все зависимости и расширения будут выглядеть как переменные. Для просмотра списка предложенных новых версий, можно использовать специальный файл maven-version-rules.xml. Файл можно создать в корне проекта и описать в нем исключения в виде конкретных версий или regex выражений для конкретных зависимостей или их групп. Для обновления всех версий, можно запустить команду mvn versions:update-properties.","summary":"В статье описывается управление версиями зависимостей в Maven проекте. Управление версиями зависимостей можно разделить на три части: версия родительского проекта, версия зависимостей и версия используемых расширений Maven. Для обновления версий зависимостей, их версии необходимо описать как переменные в разделе properties проекта Maven. Также, все используемые расширения должны быть явно прописаны с вынесением их версий в переменные. В итоге, все зависимости и расширения будут выглядеть как переменные. Для просмотра списка предложенных новых версий, можно использовать специальный файл maven-version-rules.xml. Файл можно создать в корне проекта и описать в нем исключения в виде конкретных версий или regex выражений для конкретных зависимостей или их групп. Для обновления всех версий, можно запустить команду mvn versions:update-properties."}
{"Text":"Пожары и Стратегия / Habr                        \n\n05-04-2020\nЕсть одна идея, которую я часто рассказывал инженерам в последнее время, и я думаю, что она заслуживает более широкой аудитории. Когда вы занимаетесь инженерной работой, у вас есть разные виды задач. Некоторые задачи — аварии или тактическая работа. Мы часто называем это \"тушением пожаров\", особенно когда работа связана с срочной починкой или ее нужно выполнить незамедлительно. Другие задачи — стратегические. Вы собрали со своих пользователей информацию, что им нужно/они хотят, вы разработали решение, и теперь вы реализуете его — методично и систематически. Важно понимать, каким видом работы вы занимаетесь в данный момент, и думать о ней соответственно. Пожары Когда вы тушите пожар, ваша цель — потушить пожар. Вы хотите совершить минимальные необходимые усилия, чтобы уничтожить огонь и вернуться к долгосрочной стратегической работе. Вы не хотите строить большие сложные системы, которые останутся жить навечно, просто чтобы потушить пожар. Во время аварии вы делаете наколеночные, костыльные, \"quick and dirty\" решения. Это не значит, что вы должны делать плохую работу. Но вы не должны строить долгоживущую, высокоэффективную систему для тушения этого конкретного пожара. Пожары бывают разных видов. Иногда руководство или другая команда приходит к вам с срочным запросом, с чем-нибудь, что должно быть сделано в ближайшие пару недель. Что вы хотите сделать — это придумать, как выполнить этот запрос и убрать его с дороги, чтобы вернуться к долгосрочным стратегическим задачам. В других случаях у вас происходит настоящая авария, поломка. Ясно, что в этом случае вы должны пофиксить поломку, и не заниматься всякой ерундой. Когда все сломалось — не время говорить \"ну, нам нужна проектная документация и давайте обсудим ее на следующей неделе с нашими ведущими разработчиками\". На самом деле, то же справедливо для любого пожара: пожар — не время применять фундаментальные методы и системы проектирования ПО. Пример Давайте посмотрим на конкретный пример. К вам приходит руководитель и говорит: \"Один клиент готов дать нам миллион баксов на следующей неделе, но сначала они хотят увидеть график, как наши серверы справляются с высокой нагрузкой\". Но допустим, у вас даже нет системы отслеживания нагрузки на серверах. Если бы думали о задаче в долгосрочной перспективе, вы возможно сказали бы: \"Ну, нам нужна система отслеживания нагрузки на серверах. Мы должны тщательно выбрать хранилище, придумать, как будем обеспечивать точность измерений, как будем мониторить и тестировать систему. Потом нам нужен дизайнер интерфейсов, чтобы графики были понятны пользователям. Понадобятся стандартные пользовательские исследования, и на их основе можно будет спроектировать интерфейс.\" За неделю это не случится. К тому же, это пустая трата времени. Вы не представляете, случится ли такая срочная задача еще раз. То, что однажды кто-то пришел к вам со срочным запросом, еще не означает, что это долгосрочная потребность. Может показаться, что это будет долгосрочной потребностью, и вы можете предположить, что так оно и будет. Но зачем гадать относительно долгосрочной работы? Долгосрочная работа не требует догадок — когда вы занимаетесь ею, у вас есть роскошь провести исследования, чтобы выяснить, что нужно настоящим пользователям и каковы требования. Проводите эти исследования и стройте вещи на основании их, не на основании ваших догадок. Вместо всего этого вы должны бы сказать что-то вроде: \"Ок, я сделаю скрипт для очень простого нагрузочного теста, и уже завтра смогу запускать его с моей машины. На серверы выкачу фикс, который пишет информацию о нагрузке в лог, потом распаршу эти логи и вручную построю график.\" Это — минимальная работа, нужная для решения задачи. Но это решение несет в себе риск. Вы внесли правку, чтобы логировать что-то связанное с нагрузкой. Возможно когда-нибудь позже кто-то посмотрит на правку и подумает, что вы задумали ее как долгосрочный, поддерживаемый механизм для учета нагрузки в системе, и будет рассчитывать на то, что все хорошо продумано и спроектировано, хотя на самом деле это не так. И здесь появляется очень важное соображение: Никогда не принимайте долгосрочных решений и не реализуйте стратегические штуки во время пожара. На самом деле, вы можете даже специально откатить все правки, которые вы сделали во время пожара, в нашем случае — удалить строчку с логированием, специально чтобы никто не подумал, что это долгоживущее решение. Это правило относится не только к техническим решениям, но и к организационным изменениям, и вообще к любым решениям. У вас происходит авария. Во время аварии не стоит рассуждать о том, как вы будете предотвращать ее в будущем или как надо поменять процессы разработки. Правильное время, чтобы принимать долгосрочные решения на основе случившихся аварий — постмортем, когда вы делаете рациональное и взвешенное ревью ситуации после того, как пожар \"был потушен\". Тогда вы можете спокойно задаться вопросами \"Какую стратегическую работу собираемся выполнить, чтобы предотвратить такие аварии?\" и \"Что мы узнали нового из этого случая, что повлияет на то, как мы работаем?\" Это правило очень важно. Его нарушение порождает несуразицы, которые могут уничтожить команду. Если вы строите все процедуры и шаблоны работы основываясь только на решениях, принятых во время аварий, в итоге у вас получится совершенно сумасшедшая компания, и скорее всего, ее ждет провал. Стратегическая работа Другой конец спектра (а это именно спектр, а не черно-белое деление) от \"тушения пожаров\" — это стратегическая работа. У вас есть известная цель, и вы работаете над ее достижением, применяя все принципы проектирования и разработки ПО, заботясь о долговременной перспективе, и вместе с командой выстраивая устойчивую и поддерживаемую систему. И опять, если вы станете применять методы \"тушения пожаров\" к стратегической работе, вас ждет катастрофа. Если вы относитесь к каждому проекту как к чрезвычайной ситуации и будете делать его наколеночно/костыльно, потому что он \"нужен завтра\" (хотя на самом деле нет), все превратится в хаос. На самом деле вы создадите новые пожары! Ваша система будет так плохо спроектирована, что она будет постоянно падать, создавать проблемы, ее будет невозможно поддерживать, и в конце концов все ваше время будет съедено тушением пожаров вокруг вашего плохо спроектированного бардака. Когда вы применяете принципы Пожаров к Стратегической работе, вы в действительности никогда не выполняете стратегическую работу. Если вы видите компанию, которая просто не может сделать что-либо долгосрочное, часто причина именно в этом — они относятся ко всему так, будто все в огне, и поэтому не могут на самом деле двигаться вперед. Стратегическая работа требует много таких соображений: \"Окей, мы понимаем ваши требования. Спасибо, что объяснили, в чем ваши проблемы. Мы построим решение для вас, мы сделаем это правильно, и это займет какое-то время. Не вечность, но какое-то время это займет.\" Думаю, иногда руководители беспокоятся, что если они скажут инженерам \"потратьте достаточно времени\", то инженеры обленятся и никогда не закончат работу. Это может быть актуальной проблемой в некоторых компаниях, и, конечно, руководство заинтересовано в том, чтобы все двигалось и компания поставляла бы свой продукт! Но должен быть баланс между тем, чтобы поторапливать людей заканчивать проекты в срок и тем, чтобы соблюдались процессы и процедуры долгосрочной разработки. В целом во время выполнения стратегической работы стоит делать крен в строну немного избыточного проектирования, немного избыточного ревью и т.д. Я не говорю, что надо перебарщивать и прекращать всякое движение или гонять всех через ненужные ревью только потому, что \"это может кому-то понадобиться\". Я только говорю, что если вы не уверены — стоит ошибиться именно в эту сторону. Делать то и другое Если вы применяете вышеприведенные принципы, одна команда (или один человек) может одновременно заниматься и стратегической работой, и пожарами (по крайней мере, в одну и ту же неделю или месяц). Хитрость в том, чтобы делать минимальную работу на пожарах, чтобы все неожиданности были обработаны и бизнес мог продолжать работу, а затем, когда пожары потушены, фокусироваться обратно на стратегической работе. В конце концов, если вы делаете все правильно, самой важной для бизнеса должна быть именно стратегическая работа — вещи, которые вы исследовали и поняли, что они будут иметь наибольший эффект на длинной дистанции. Так что тушите пожары возвращайтесь к тому, что действительно будет важно в долгосрочной перспективе. — Макс\n","metric":0.64208984380000000414,"metric_input":"Текст статьи:\nПожары и Стратегия / Habr                        \n\n05-04-2020\nЕсть одна идея, которую я часто рассказывал инженерам в последнее время, и я думаю, что она заслуживает более широкой аудитории. Когда вы занимаетесь инженерной работой, у вас есть разные виды задач. Некоторые задачи — аварии или тактическая работа. Мы часто называем это \"тушением пожаров\", особенно когда работа связана с срочной починкой или ее нужно выполнить незамедлительно. Другие задачи — стратегические. Вы собрали со своих пользователей информацию, что им нужно/они хотят, вы разработали решение, и теперь вы реализуете его — методично и систематически. Важно понимать, каким видом работы вы занимаетесь в данный момент, и думать о ней соответственно. Пожары Когда вы тушите пожар, ваша цель — потушить пожар. Вы хотите совершить минимальные необходимые усилия, чтобы уничтожить огонь и вернуться к долгосрочной стратегической работе. Вы не хотите строить большие сложные системы, которые останутся жить навечно, просто чтобы потушить пожар. Во время аварии вы делаете наколеночные, костыльные, \"quick and dirty\" решения. Это не значит, что вы должны делать плохую работу. Но вы не должны строить долгоживущую, высокоэффективную систему для тушения этого конкретного пожара. Пожары бывают разных видов. Иногда руководство или другая команда приходит к вам с срочным запросом, с чем-нибудь, что должно быть сделано в ближайшие пару недель. Что вы хотите сделать — это придумать, как выполнить этот запрос и убрать его с дороги, чтобы вернуться к долгосрочным стратегическим задачам. В других случаях у вас происходит настоящая авария, поломка. Ясно, что в этом случае вы должны пофиксить поломку, и не заниматься всякой ерундой. Когда все сломалось — не время говорить \"ну, нам нужна проектная документация и давайте обсудим ее на следующей неделе с нашими ведущими разработчиками\". На самом деле, то же справедливо для любого пожара: пожар — не время применять фундаментальные методы и системы проектирования ПО. Пример Давайте посмотрим на конкретный пример. К вам приходит руководитель и говорит: \"Один клиент готов дать нам миллион баксов на следующей неделе, но сначала они хотят увидеть график, как наши серверы справляются с высокой нагрузкой\". Но допустим, у вас даже нет системы отслеживания нагрузки на серверах. Если бы думали о задаче в долгосрочной перспективе, вы возможно сказали бы: \"Ну, нам нужна система отслеживания нагрузки на серверах. Мы должны тщательно выбрать хранилище, придумать, как будем обеспечивать точность измерений, как будем мониторить и тестировать систему. Потом нам нужен дизайнер интерфейсов, чтобы графики были понятны пользователям. Понадобятся стандартные пользовательские исследования, и на их основе можно будет спроектировать интерфейс.\" За неделю это не случится. К тому же, это пустая трата времени. Вы не представляете, случится ли такая срочная задача еще раз. То, что однажды кто-то пришел к вам со срочным запросом, еще не означает, что это долгосрочная потребность. Может показаться, что это будет долгосрочной потребностью, и вы можете предположить, что так оно и будет. Но зачем гадать относительно долгосрочной работы? Долгосрочная работа не требует догадок — когда вы занимаетесь ею, у вас есть роскошь провести исследования, чтобы выяснить, что нужно настоящим пользователям и каковы требования. Проводите эти исследования и стройте вещи на основании их, не на основании ваших догадок. Вместо всего этого вы должны бы сказать что-то вроде: \"Ок, я сделаю скрипт для очень простого нагрузочного теста, и уже завтра смогу запускать его с моей машины. На серверы выкачу фикс, который пишет информацию о нагрузке в лог, потом распаршу эти логи и вручную построю график.\" Это — минимальная работа, нужная для решения задачи. Но это решение несет в себе риск. Вы внесли правку, чтобы логировать что-то связанное с нагрузкой. Возможно когда-нибудь позже кто-то посмотрит на правку и подумает, что вы задумали ее как долгосрочный, поддерживаемый механизм для учета нагрузки в системе, и будет рассчитывать на то, что все хорошо продумано и спроектировано, хотя на самом деле это не так. И здесь появляется очень важное соображение: Никогда не принимайте долгосрочных решений и не реализуйте стратегические штуки во время пожара. На самом деле, вы можете даже специально откатить все правки, которые вы сделали во время пожара, в нашем случае — удалить строчку с логированием, специально чтобы никто не подумал, что это долгоживущее решение. Это правило относится не только к техническим решениям, но и к организационным изменениям, и вообще к любым решениям. У вас происходит авария. Во время аварии не стоит рассуждать о том, как вы будете предотвращать ее в будущем или как надо поменять процессы разработки. Правильное время, чтобы принимать долгосрочные решения на основе случившихся аварий — постмортем, когда вы делаете рациональное и взвешенное ревью ситуации после того, как пожар \"был потушен\". Тогда вы можете спокойно задаться вопросами \"Какую стратегическую работу собираемся выполнить, чтобы предотвратить такие аварии?\" и \"Что мы узнали нового из этого случая, что повлияет на то, как мы работаем?\" Это правило очень важно. Его нарушение порождает несуразицы, которые могут уничтожить команду. Если вы строите все процедуры и шаблоны работы основываясь только на решениях, принятых во время аварий, в итоге у вас получится совершенно сумасшедшая компания, и скорее всего, ее ждет провал. Стратегическая работа Другой конец спектра (а это именно спектр, а не черно-белое деление) от \"тушения пожаров\" — это стратегическая работа. У вас есть известная цель, и вы работаете над ее достижением, применяя все принципы проектирования и разработки ПО, заботясь о долговременной перспективе, и вместе с командой выстраивая устойчивую и поддерживаемую систему. И опять, если вы станете применять методы \"тушения пожаров\" к стратегической работе, вас ждет катастрофа. Если вы относитесь к каждому проекту как к чрезвычайной ситуации и будете делать его наколеночно/костыльно, потому что он \"нужен завтра\" (хотя на самом деле нет), все превратится в хаос. На самом деле вы создадите новые пожары! Ваша система будет так плохо спроектирована, что она будет постоянно падать, создавать проблемы, ее будет невозможно поддерживать, и в конце концов все ваше время будет съедено тушением пожаров вокруг вашего плохо спроектированного бардака. Когда вы применяете принципы Пожаров к Стратегической работе, вы в действительности никогда не выполняете стратегическую работу. Если вы видите компанию, которая просто не может сделать что-либо долгосрочное, часто причина именно в этом — они относятся ко всему так, будто все в огне, и поэтому не могут на самом деле двигаться вперед. Стратегическая работа требует много таких соображений: \"Окей, мы понимаем ваши требования. Спасибо, что объяснили, в чем ваши проблемы. Мы построим решение для вас, мы сделаем это правильно, и это займет какое-то время. Не вечность, но какое-то время это займет.\" Думаю, иногда руководители беспокоятся, что если они скажут инженерам \"потратьте достаточно времени\", то инженеры обленятся и никогда не закончат работу. Это может быть актуальной проблемой в некоторых компаниях, и, конечно, руководство заинтересовано в том, чтобы все двигалось и компания поставляла бы свой продукт! Но должен быть баланс между тем, чтобы поторапливать людей заканчивать проекты в срок и тем, чтобы соблюдались процессы и процедуры долгосрочной разработки. В целом во время выполнения стратегической работы стоит делать крен в строну немного избыточного проектирования, немного избыточного ревью и т.д. Я не говорю, что надо перебарщивать и прекращать всякое движение или гонять всех через ненужные ревью только потому, что \"это может кому-то понадобиться\". Я только говорю, что если вы не уверены — стоит ошибиться именно в эту сторону. Делать то и другое Если вы применяете вышеприведенные принципы, одна команда (или один человек) может одновременно заниматься и стратегической работой, и пожарами (по крайней мере, в одну и ту же неделю или месяц). Хитрость в том, чтобы делать минимальную работу на пожарах, чтобы все неожиданности были обработаны и бизнес мог продолжать работу, а затем, когда пожары потушены, фокусироваться обратно на стратегической работе. В конце концов, если вы делаете все правильно, самой важной для бизнеса должна быть именно стратегическая работа — вещи, которые вы исследовали и поняли, что они будут иметь наибольший эффект на длинной дистанции. Так что тушите пожары возвращайтесь к тому, что действительно будет важно в долгосрочной перспективе. — Макс\n\n\nКраткое содержание:\nСтатья обсуждает разницу между \"пожарами\" - срочной работой, требующей немедленного решения, и стратегической работой, которая требует тщательного планирования и проектирования. Автор подчеркивает, что при работе над пожарами следует применять \"аварийные\" методы, которые могут быть временными и костыльными, но которые позволят быстро решить проблему. Однако, при стратегической работе, наоборот, необходимо применять долгосрочные и фундаментальные методы проектирования, чтобы создать устойчивые и поддерживаемые системы. Автор также предостерегает от смешивания этих подходов и призывает к балансу между срочностью и долгосрочным планированием.","summary":"Статья обсуждает разницу между \"пожарами\" - срочной работой, требующей немедленного решения, и стратегической работой, которая требует тщательного планирования и проектирования. Автор подчеркивает, что при работе над пожарами следует применять \"аварийные\" методы, которые могут быть временными и костыльными, но которые позволят быстро решить проблему. Однако, при стратегической работе, наоборот, необходимо применять долгосрочные и фундаментальные методы проектирования, чтобы создать устойчивые и поддерживаемые системы. Автор также предостерегает от смешивания этих подходов и призывает к балансу между срочностью и долгосрочным планированием."}
{"Text":"Виагра снижает риск болезни Альцгеймера на 70% / Habr             \n\n_Доктор Фейшюн Ченг_\n\nКазалось бы, какая может быть связь? Но новое исследование показывает, что использование препарата силденафил (более известного под маркой «Виагра») значительно снижает вероятность развития болезни Альцгеймера. Ученые из клиники Кливленда провели анализ  заявок на медицинское страхование от 7,2 миллиона человек. И обнаружили, что прием силденафила снижает вероятность развития Альцгеймера в течение последующих 6 лет почти на 70% по сравнению с теми пациентами, которые его не принимают.\n\nТут важно отметить, что подобные совпадения, даже в таких больших масштабах, еще не доказывают на 100% причинно-следственный эффект. Остается вероятность, что люди, принимавшие силденафил, могли принимать еще что-то или заниматься чем-то, повышающим их шансы не заболеть болезнью Альцгеймера.\n\nТем не менее, исследователи говорят, что показанная здесь корреляция достаточна для определения силденафила в качестве главного кандидата на лекарство от болезни Альцгеймера. И в дальнейшем они собираются проводить клинические исследования, чтобы доказать или опровергнуть наличие здесь причинно-следственной связи.\n\nБиолог и старший автор исследования Фейшюн Ченг из клиники Кливленда пишет :\n\n> Примечательно то, что мы обнаружили, что использование силденафила снижает вероятность болезни Альцгеймера у людей с ишемической болезнью сердца, гипертонией и диабетом 2 типа. Все эти болезни являются сопутствующими, в значительной степени связанными с риском развития Альцгеймера в пациентах.\n>\n>\n\n## Путь ученых ##\n\nЕсли по-научному, команда Ченга построила более дюжины модулей эндофенотипа , используя вычислительные методы для картирования генетических факторов, которые могут гипотетически управлять развитием болезни Альцгеймера.\n\nИмея в своем распоряжении 13 из этих модулей, исследователи несколько лет проверяли, какие виды одобренных лекарств могут гипотетически помочь против выявленных фенотипов. Из более чем 1600 лекарств, силденафил оказался самым многообещающим кандидатом.\n\nЭтот результат может показаться странным — учитывая, что препарат используется в основном для лечения эректильной дисфункции. Но в исследовательском сообществе уже знали о том, что применение силденафила может иметь другие преимущества для здоровья, учитывая его взаимодействие с амилоидами и тау-белками, участвующими в патологии Альцгеймера.\n\nВ публикации  Ченг говорит:\n\n> Недавние исследования показывали нам, что взаимодействие между амилоидами и тау-белками вносят больший вклад в развитие болезни Альцгеймера, чем любые из них сами по себе.\n>\n>\n>\n>\n\nМы выдвинули гипотезу, что препараты, нацеленные на молекулярную сеть эндофенотипов амилоидов и тау-белков, должны иметь наибольший потенциал для успеха. Силденафил, который, как было показано на доклинических моделях, значительно улучшает когнитивные функции и память, показался лучшим кандидатом на возможное лекарство.\n>\n>\n\nГипотеза подтвердилась данными из мед. страхования: группа ученых обнаружила, что у потребителей силденафила риск болезни Альцгеймера на 69 с лишним процентов ниже по сравнению с теми, кто не употреблял силденафил. У некоторых других лекарств тоже была заметна положительная корреляция — включая лозартан, метформин, дилтиазем и глимепирид. Но эффект оказался существенно ниже.\n\nКонечно, исследователи подчеркивают, что это еще не доказывает причинно-следственную связь, и нужны дополнительные тестирования. Но их результатов нам придется ждать еще много лет: придется начать давать виагру людям с развивающейся болезнью Альцгеймера и людям в группе риска, и ждать минимум 6 лет, проверяя эффект.\n\nПока что перспективы очень многообещающие: в экспериментах «в пробирке» по изучению клеток мозга исследователи обнаружили, что нейроны, обработанные силденафилом, показывали повышенную скорость роста и снижение темпов накопления тау-белков. Они тоже сделали вывод, что препарат может предоставлять защиту от снижения когнитивных функций.\n\nФейшюн Ченг пишет :\n\n> Сейчас мы планируем механическое испытание и рандомизированное клиническое испытание фазы II, чтобы проверить причинно-следственную связь и подтвердить клинические преимущества силденафила для пациентов с болезнью Альцгеймера.\n>\n>\n>\n>\n\nМы также предполагаем, что наш подход может применяться к другим нейродегенеративным заболеваниям, включая болезнь Паркинсона и боковой амиотрофический склероз, чтобы ускорить процесс открытия подходящих лекарств.\n>\n>\n\nЭто не первый случай, когда использование силденафила связали с неожиданными преимуществами для здоровья. Ранее препарат показывал многообещающие результаты, в малых дозах снижая риски заболевания раком толстой кишки  и малярией .\n\nВ любом случае, повод задуматься. Возможно, прием виагры — это не так уж и стыдно.\n\nЕсли вы эксперт, полную публикацию исследования стоит почитать в Nature Aging .","metric":0.70654296880000000414,"metric_input":"Текст статьи:\nВиагра снижает риск болезни Альцгеймера на 70% / Habr             \n\n_Доктор Фейшюн Ченг_\n\nКазалось бы, какая может быть связь? Но новое исследование показывает, что использование препарата силденафил (более известного под маркой «Виагра») значительно снижает вероятность развития болезни Альцгеймера. Ученые из клиники Кливленда провели анализ  заявок на медицинское страхование от 7,2 миллиона человек. И обнаружили, что прием силденафила снижает вероятность развития Альцгеймера в течение последующих 6 лет почти на 70% по сравнению с теми пациентами, которые его не принимают.\n\nТут важно отметить, что подобные совпадения, даже в таких больших масштабах, еще не доказывают на 100% причинно-следственный эффект. Остается вероятность, что люди, принимавшие силденафил, могли принимать еще что-то или заниматься чем-то, повышающим их шансы не заболеть болезнью Альцгеймера.\n\nТем не менее, исследователи говорят, что показанная здесь корреляция достаточна для определения силденафила в качестве главного кандидата на лекарство от болезни Альцгеймера. И в дальнейшем они собираются проводить клинические исследования, чтобы доказать или опровергнуть наличие здесь причинно-следственной связи.\n\nБиолог и старший автор исследования Фейшюн Ченг из клиники Кливленда пишет :\n\n> Примечательно то, что мы обнаружили, что использование силденафила снижает вероятность болезни Альцгеймера у людей с ишемической болезнью сердца, гипертонией и диабетом 2 типа. Все эти болезни являются сопутствующими, в значительной степени связанными с риском развития Альцгеймера в пациентах.\n>\n>\n\n## Путь ученых ##\n\nЕсли по-научному, команда Ченга построила более дюжины модулей эндофенотипа , используя вычислительные методы для картирования генетических факторов, которые могут гипотетически управлять развитием болезни Альцгеймера.\n\nИмея в своем распоряжении 13 из этих модулей, исследователи несколько лет проверяли, какие виды одобренных лекарств могут гипотетически помочь против выявленных фенотипов. Из более чем 1600 лекарств, силденафил оказался самым многообещающим кандидатом.\n\nЭтот результат может показаться странным — учитывая, что препарат используется в основном для лечения эректильной дисфункции. Но в исследовательском сообществе уже знали о том, что применение силденафила может иметь другие преимущества для здоровья, учитывая его взаимодействие с амилоидами и тау-белками, участвующими в патологии Альцгеймера.\n\nВ публикации  Ченг говорит:\n\n> Недавние исследования показывали нам, что взаимодействие между амилоидами и тау-белками вносят больший вклад в развитие болезни Альцгеймера, чем любые из них сами по себе.\n>\n>\n>\n>\n\nМы выдвинули гипотезу, что препараты, нацеленные на молекулярную сеть эндофенотипов амилоидов и тау-белков, должны иметь наибольший потенциал для успеха. Силденафил, который, как было показано на доклинических моделях, значительно улучшает когнитивные функции и память, показался лучшим кандидатом на возможное лекарство.\n>\n>\n\nГипотеза подтвердилась данными из мед. страхования: группа ученых обнаружила, что у потребителей силденафила риск болезни Альцгеймера на 69 с лишним процентов ниже по сравнению с теми, кто не употреблял силденафил. У некоторых других лекарств тоже была заметна положительная корреляция — включая лозартан, метформин, дилтиазем и глимепирид. Но эффект оказался существенно ниже.\n\nКонечно, исследователи подчеркивают, что это еще не доказывает причинно-следственную связь, и нужны дополнительные тестирования. Но их результатов нам придется ждать еще много лет: придется начать давать виагру людям с развивающейся болезнью Альцгеймера и людям в группе риска, и ждать минимум 6 лет, проверяя эффект.\n\nПока что перспективы очень многообещающие: в экспериментах «в пробирке» по изучению клеток мозга исследователи обнаружили, что нейроны, обработанные силденафилом, показывали повышенную скорость роста и снижение темпов накопления тау-белков. Они тоже сделали вывод, что препарат может предоставлять защиту от снижения когнитивных функций.\n\nФейшюн Ченг пишет :\n\n> Сейчас мы планируем механическое испытание и рандомизированное клиническое испытание фазы II, чтобы проверить причинно-следственную связь и подтвердить клинические преимущества силденафила для пациентов с болезнью Альцгеймера.\n>\n>\n>\n>\n\nМы также предполагаем, что наш подход может применяться к другим нейродегенеративным заболеваниям, включая болезнь Паркинсона и боковой амиотрофический склероз, чтобы ускорить процесс открытия подходящих лекарств.\n>\n>\n\nЭто не первый случай, когда использование силденафила связали с неожиданными преимуществами для здоровья. Ранее препарат показывал многообещающие результаты, в малых дозах снижая риски заболевания раком толстой кишки  и малярией .\n\nВ любом случае, повод задуматься. Возможно, прием виагры — это не так уж и стыдно.\n\nЕсли вы эксперт, полную публикацию исследования стоит почитать в Nature Aging .\n\nКраткое содержание:\nВ статье сообщается о новом исследовании, которое показало, что прием препарата силденафил (Виагра) может снизить риск развития болезни Альцгеймера на 70%. Ученые обнаружили эту корреляцию, проанализировав данные медицинского страхования. Они планируют провести дополнительные клинические испытания, чтобы подтвердить или опровергнуть причинно-следственную связь.","summary":"В статье сообщается о новом исследовании, которое показало, что прием препарата силденафил (Виагра) может снизить риск развития болезни Альцгеймера на 70%. Ученые обнаружили эту корреляцию, проанализировав данные медицинского страхования. Они планируют провести дополнительные клинические испытания, чтобы подтвердить или опровергнуть причинно-следственную связь."}
{"Text":"«Лаборатория Касперского» выяснила, какие IT-компании и сервисы следили за пользователями в 2021–2022 годах / Хабр                 \n\n«Лаборатория Касперского» составила отчёт , в котором рассказала, какие компании и в каких количествах собирают, анализируют, хранят и передают партнёрам пользовательские данные, по информации компонента DNT.\nКомпания использовала анонимную статистику, собранную с августа 2021 по август 2022 года компонентом Do Not Track (DNT), который предотвращает загрузку трекинговых элементов, отслеживающих действия пользователя на веб-сайтах. Компонент DNT входит в состав Kaspersky Internet Security, Kaspersky Total Security, Kaspersky Security Cloud и выключен по умолчанию.\nВ статистику вошли анонимизированные данные, добровольно предоставленные пользователями. Для девяти регионов, а также для некоторых отдельных стран составили список из 25 трекинговых систем, которые компонент DNT обнаруживал чаще всего в этом регионе или стране. За 100% брали общее количество срабатываний компонента DNT на трекеры этих двадцати пяти систем.\nШесть трекинговых систем попали в TOP-25 во всех рассматриваемых регионах. При этом четыре из них принадлежат одной компании — Google. Это Google Analytics, Google AdSense, Google Marketing Platform и YouTube Analytics. Оставшиеся две принадлежат Meta\\* и компании Criteo.\nВ прошлом отчёте (от 2019 года) рассказывали про трекинговые элементы Google: DoubleClick, Google AdSense, Google Analytics и YouTube Analytics. Тогда IT-гигант как раз анонсировал планы по ребрендингу рекламной платформы DoubleClick и её слиянию с рекламной экосистемой Google. Сейчас DoubleClick — часть Google Marketing Platform, при этом сами трекинговые URL не изменились и продолжают функционировать, как и раньше. Для удобства этот трекинговый сервис называется Google Marketing Platform (ex-DoubleClick).\nНаибольшую долю в TOP-25 трекинговых систем Google Marketing Platform имеет в Южной Азии — 32,92% и на Ближнем Востоке — 32,84%. На третьем и четвёртом местах располагаются Африканский регион и Латинская Америка — 25,37% и 24,64% соответственно. Меньше всего доля трекинга у Google Marketing Platform в СНГ — она составляет всего лишь 7,05%.\nПомимо ex-DoubleClick у Google есть Google Analytics — сервис, который собирает данные о посетителях веб-сайтов, анализирует их и предоставляет клиентам детальную статистику. На него также приходится достаточно большая доля срабатываний компонента DNT по всему миру. Наибольшую долю трекеры Google Analytics получили в Южной Азии (18,04%), Латинской Америке (17,97%), Африке (16,56%) и на Ближнем Востоке (16,44%). Наименьшая доля в СНГ — 9,06%.\nЕщё одна трекинговая система Google — сервис контекстной рекламы Google AdSense. Тут снова фигурирует Ближний Восток (5,27%), Африка (4,63%), Латинская Америка (4,44%) и Южная Азия (4,44%) в числе регионов, где этот сервис имеет наибольшую долю. СНГ опять на последнем месте в рейтинге: в этом регионе на AdSense приходится всего 1,45% срабатываний.\nЗавершает список трекинговых сервисов Google система YouTube Analytics. Эти трекеры собирают, анализируют и предоставляют YouTube-блогерам данные об их аудитории. Снова самая большая доля приходится на Ближний Восток (8,04%), Южную Азию (7,79%), Африку (5,97%) и Латинскую Америку (5,02%). Но в этот раз завершает список регионов не СНГ (2,54%), который находится на предпоследнем месте, а Северная Америка (1,82%).\nПомимо трекинговых систем Google во всех рассматриваемых регионах присутствует сервис Facebook Custom Audiences\\*\\* компании Meta, который предоставляет услуги по таргетированной рекламе. Такие сервисы собирают разнообразные данные о пользователях, анализируют их и относят пользователей к тому или иному сегменту аудитории, чтобы наиболее точно подбирать для них рекламу.\nТрекеры Facebook Custom Audiences имеют наибольшую долю в Латинской Америке (8,76%) и Океании (7,95%). Меньше всего их присутствие в СНГ (2,12%). Небольшая доля трекеров мирового масштаба может быть связана с наличием серьёзной конкуренции с региональными сервисами по сбору и анализу данных пользователей.\nПоследняя трекинговая система, замеченная во всех уголках мира, — Criteo. Это крупная французская рекламная компания, которая предоставляет услуги от сбора и анализа пользовательских данных до размещения собственно рекламы.\nНаибольшая доля Criteo приходится на Европейский регион (7,07%), на втором месте находится Восточная Азия с 6,09%, а на третьем — Латинская Америка (5,24%). Наименьшую долю трекеры Criteo имеют в Южной Азии — всего лишь 1,59%.\nВ России подавляющее большинство трекинговых систем, попавших в TOP-25, — локальные. На первом и втором местах располагаются «Яндекс.Метрика» (19,73%) и Mediascope (12,51%). Трекинговые сервисы Google занимают третье и четвёртое места: Google Analytics (8,83%), Google Marketing Platform (ex-DoubleClick, 6,59%), что достаточно мало в сравнении, например, со средним по СНГ без России — 16,17%. Далее практически все верхние позиции занимают трекинговые сервисы локальных российских компаний.\n_Meta Platforms\\*, а также принадлежащая ей социальная сеть Facebook\\*\\* признана экстремистской организацией, её деятельность в России запрещена_","metric":0.51611328130000000414,"metric_input":"Текст статьи:\n«Лаборатория Касперского» выяснила, какие IT-компании и сервисы следили за пользователями в 2021–2022 годах / Хабр                 \n\n«Лаборатория Касперского» составила отчёт , в котором рассказала, какие компании и в каких количествах собирают, анализируют, хранят и передают партнёрам пользовательские данные, по информации компонента DNT.\nКомпания использовала анонимную статистику, собранную с августа 2021 по август 2022 года компонентом Do Not Track (DNT), который предотвращает загрузку трекинговых элементов, отслеживающих действия пользователя на веб-сайтах. Компонент DNT входит в состав Kaspersky Internet Security, Kaspersky Total Security, Kaspersky Security Cloud и выключен по умолчанию.\nВ статистику вошли анонимизированные данные, добровольно предоставленные пользователями. Для девяти регионов, а также для некоторых отдельных стран составили список из 25 трекинговых систем, которые компонент DNT обнаруживал чаще всего в этом регионе или стране. За 100% брали общее количество срабатываний компонента DNT на трекеры этих двадцати пяти систем.\nШесть трекинговых систем попали в TOP-25 во всех рассматриваемых регионах. При этом четыре из них принадлежат одной компании — Google. Это Google Analytics, Google AdSense, Google Marketing Platform и YouTube Analytics. Оставшиеся две принадлежат Meta\\* и компании Criteo.\nВ прошлом отчёте (от 2019 года) рассказывали про трекинговые элементы Google: DoubleClick, Google AdSense, Google Analytics и YouTube Analytics. Тогда IT-гигант как раз анонсировал планы по ребрендингу рекламной платформы DoubleClick и её слиянию с рекламной экосистемой Google. Сейчас DoubleClick — часть Google Marketing Platform, при этом сами трекинговые URL не изменились и продолжают функционировать, как и раньше. Для удобства этот трекинговый сервис называется Google Marketing Platform (ex-DoubleClick).\nНаибольшую долю в TOP-25 трекинговых систем Google Marketing Platform имеет в Южной Азии — 32,92% и на Ближнем Востоке — 32,84%. На третьем и четвёртом местах располагаются Африканский регион и Латинская Америка — 25,37% и 24,64% соответственно. Меньше всего доля трекинга у Google Marketing Platform в СНГ — она составляет всего лишь 7,05%.\nПомимо ex-DoubleClick у Google есть Google Analytics — сервис, который собирает данные о посетителях веб-сайтов, анализирует их и предоставляет клиентам детальную статистику. На него также приходится достаточно большая доля срабатываний компонента DNT по всему миру. Наибольшую долю трекеры Google Analytics получили в Южной Азии (18,04%), Латинской Америке (17,97%), Африке (16,56%) и на Ближнем Востоке (16,44%). Наименьшая доля в СНГ — 9,06%.\nЕщё одна трекинговая система Google — сервис контекстной рекламы Google AdSense. Тут снова фигурирует Ближний Восток (5,27%), Африка (4,63%), Латинская Америка (4,44%) и Южная Азия (4,44%) в числе регионов, где этот сервис имеет наибольшую долю. СНГ опять на последнем месте в рейтинге: в этом регионе на AdSense приходится всего 1,45% срабатываний.\nЗавершает список трекинговых сервисов Google система YouTube Analytics. Эти трекеры собирают, анализируют и предоставляют YouTube-блогерам данные об их аудитории. Снова самая большая доля приходится на Ближний Восток (8,04%), Южную Азию (7,79%), Африку (5,97%) и Латинскую Америку (5,02%). Но в этот раз завершает список регионов не СНГ (2,54%), который находится на предпоследнем месте, а Северная Америка (1,82%).\nПомимо трекинговых систем Google во всех рассматриваемых регионах присутствует сервис Facebook Custom Audiences\\*\\* компании Meta, который предоставляет услуги по таргетированной рекламе. Такие сервисы собирают разнообразные данные о пользователях, анализируют их и относят пользователей к тому или иному сегменту аудитории, чтобы наиболее точно подбирать для них рекламу.\nТрекеры Facebook Custom Audiences имеют наибольшую долю в Латинской Америке (8,76%) и Океании (7,95%). Меньше всего их присутствие в СНГ (2,12%). Небольшая доля трекеров мирового масштаба может быть связана с наличием серьёзной конкуренции с региональными сервисами по сбору и анализу данных пользователей.\nПоследняя трекинговая система, замеченная во всех уголках мира, — Criteo. Это крупная французская рекламная компания, которая предоставляет услуги от сбора и анализа пользовательских данных до размещения собственно рекламы.\nНаибольшая доля Criteo приходится на Европейский регион (7,07%), на втором месте находится Восточная Азия с 6,09%, а на третьем — Латинская Америка (5,24%). Наименьшую долю трекеры Criteo имеют в Южной Азии — всего лишь 1,59%.\nВ России подавляющее большинство трекинговых систем, попавших в TOP-25, — локальные. На первом и втором местах располагаются «Яндекс.Метрика» (19,73%) и Mediascope (12,51%). Трекинговые сервисы Google занимают третье и четвёртое места: Google Analytics (8,83%), Google Marketing Platform (ex-DoubleClick, 6,59%), что достаточно мало в сравнении, например, со средним по СНГ без России — 16,17%. Далее практически все верхние позиции занимают трекинговые сервисы локальных российских компаний.\n_Meta Platforms\\*, а также принадлежащая ей социальная сеть Facebook\\*\\* признана экстремистской организацией, её деятельность в России запрещена_\n\nКраткое содержание:\nВ 2021-2022 годах компании и сервисы следили за пользователями через трекинговые системы, такие как Google Analytics, Google AdSense, Google Marketing Platform, YouTube Analytics, Facebook Custom Audiences и Criteo.\n\nGoogle Analytics и Google Marketing Platform имеют наибольшую долю в TOP-25 трекинговых систем, особенно в Южной Азии, на Ближнем Востоке, в Африке и Латинской Америке.\n\nFacebook Custom Audiences имеет наибольшую долю в Латинской Америке и Океании.\n\nCriteo имеет наибольшую долю в Европейском регионе, на втором месте Восточная Азия, на третьем Латинская Америка.\n\nВ России подавляющее большинство трекинговых систем - локальные, особенно \"Яндекс.Метрика\" и Mediascope.","summary":"В 2021-2022 годах компании и сервисы следили за пользователями через трекинговые системы, такие как Google Analytics, Google AdSense, Google Marketing Platform, YouTube Analytics, Facebook Custom Audiences и Criteo.\n\nGoogle Analytics и Google Marketing Platform имеют наибольшую долю в TOP-25 трекинговых систем, особенно в Южной Азии, на Ближнем Востоке, в Африке и Латинской Америке.\n\nFacebook Custom Audiences имеет наибольшую долю в Латинской Америке и Океании.\n\nCriteo имеет наибольшую долю в Европейском регионе, на втором месте Восточная Азия, на третьем Латинская Америка.\n\nВ России подавляющее большинство трекинговых систем - локальные, особенно \"Яндекс.Метрика\" и Mediascope."}
{"Text":"Как создать опрос? / Habr             \n\n31-08-2021\nСоветы для создания онлайн опроса и выбора конструктора опросов Потратив 8 минут на прочтение данного текста, вы узнаете: 1) Какой сервис лучше выбрать для проведения и создания опросов 2) Как создать опрос 3) Какие есть подводные камни при создании онлайн опроса Итак, судя по тому, что вы читаете эту статью, перед вами стоит задача по проведению онлайн опроса. Сначала вам нужно решить, какой сервис опросов вы будете использовать для работы. Это важный момент, т.к. сервисов много, и при выборе лучше бы не ошибиться, ведь данные всех опросов аккумулируются в вашем ЛК, а переезд с одного сервиса на другой, помимо новых согласований и договоров, ведёт к невозможности хранения данных в одном месте и, в итоге, к беспорядку и неудобствам. Если коротко: что нужно знать о подобных сервисах? Кто уже выбрал свой сервис, не тратим время, идем ниже. 1) Интуитивно понятный интерфейс. Хороший конструктор с понятной логикой расположения настроек и различных фитч. Имхо, разгадывать логику разработчиков, почему они засунули такую-то настройку в такую-то далёкую вселенную – значит регулярно выругиваться и тратить драгоценное время. 2) Большой функционал конструктора. Тут важно соблюсти баланс. Т.е. если сервис молодой, они мало чего успели технически реализовать, а если сервис уже имеет какой-то сверх навороченный функционал, то тут будут сложности с первым пунктом (интуитивно понятный интерфейс), т.к. конструктор, вероятнее всего, окажется мега перегруженным, запутанным и непонятным. 3) Поддержка сервиса. Как не крути, но человек человеку друг, и хорошая тех.поддержка поможет сохранить вам прилично времени и нервов при работе с опросами. Мало кто может похвастаться адекватностью, но, к счастью, есть юзер-френдли компании, которые действительно могут помочь в решении ваших задач. При выборе сервиса также важно, где расположены серверы, какие есть форматы выгрузки данных, можно ли создавать тестирования, есть ли e-mail рассылка, можно ли встроить опрос на сайт, настроить имейл уведомления… короче, об этом всем есть суперклассный обзор сервисов лучших конструкторов. Если еще не определились, GO читать подробнее и искать свой сервис. Создание опроса Идём дальше. Главный секрет: создавать опрос нужно в Excel, продумать его структуру, формулировки, нюансы с вариантами ответов, и потом уже переносить опрос в конструктор сервиса. Поэтому нужно учесть: для чего вы создаёте опрос – какие цели и задачи он должен закрыть, как часто вы его планируете проводить для кого вы его создаёте – кто будет заполнять опрос, с каких устройств (компьютеры, телефоны) один опрос – одна задача – ваш опрос должен закрывать одну задачу, не пытайтесь запихнуть в одно исследование всё подряд, получится плохо, очень плохо меньше вопросов, больше ответов – чем длиннее ваш опрос, тем меньшее количество респондентов дойдёт до конца. Мало у кого есть время на заполнение замороченного, длинного, нудного опроса. больше закрытых вопросов – меньше открытых – продумайте возможные варианты ответов, по возможности, уходите от свободных ответов, т.к. респонденты неохотно будут писать вам своё мнение текстом. Да и обрабатывать открытые текстовые ответы вам не очень понравится. как работать с результатами - создавая опрос, сразу думайте, как вы будете обрабатывать результаты, потом будет поздно. посмотрите со стороны – заполните опрос, как будто его прислали вам, а не с точки зрения создателя. Отправьте опрос паре знакомых, взгляд со стороны всегда заметит ваши косячки. Вы хорошо подумали над всеми пунктами выше – это уже половина успеха) Мы написали много интересных статей на тему конструкторов опросов и самих опросов и, в своё время, перепробовали в работе кучу онлайн сервисов. Расскажем как создать опрос на примере сервиса www.testograf.ru (наш любимчик) 1. При сборке опроса в Экселе сразу подумайте, какой из вопросов будет с возможностью одного ответа, какой с возможностью нескольких, стоит ли дать респонденту возможность добавить свой ответ. В уже упомянутом Тестографе более 30 различных типов вопросов. Лучше, конечно, ознакомиться с возможностями конструктора заранее, чтобы сразу понимать, что вы можете сделать в опросе технически. Например, дать возможность разместить варианты ответов в виде ранжирования, использовать матричные типы вопросов, рейтинги в виде смайлов или звёзд, либо при большом количестве вариантов ответов выбрать тип вопроса «выпадающий список». Короче, смотрим, какие есть типы вопросов и думаем, как их лучше использовать в разрезе ваших задач. 2. Далее разбираемся с логикой опроса. Например, вы можете задать разным категориям респондентов различные вопросы. Это делается настройкой ветвления при выбранном варианте ответа. Скажем, у вас есть вопрос по полу респондента, и вы хотите задать мужчинам одни вопросы, а женщинам другие. Не проблема, настраиваем логический переход в зависимости от того, какой вариант ответа выбрал респондент. 3. Следующий шаг – настройки опроса. На Тестографе куча всего полезного и интересного, тут каждый однозначно найдет для себя что-то нужное. Настройка текста завершения, индикатор прохождения, пароли к опросу, авторизация к опросу, перенаправление на веб-сайт, показ результатов после опроса и т.п. 4. Теперь, когда опрос собран и настроен, осталось позаботиться о его внешнем виде. Красивое всем нравится больше, красивым прощается больше ошибок (ну, это если вы всё-таки где-то накосячили J). В общем, настраиваем дизайн опроса. Если вы проводите опрос для компании – идеальным решением будет подогнать опрос под брендбук. Можно изменить цвета, шрифты, размеры, добавить логотип, изменить фон опроса. Тут главное не переборщить и не заиграться. Не забывайте о юзабилити, т.к., например, яркое изображение фона может мешать отображению элементов опроса. Обещанные «подводные камни», встречавшиеся на нашем пути при работе с онлайн опросами: · Не ставьте ограничение по IP адресу на заполнение опроса, если есть респонденты, которые сидят в одном офисе. · Не меняйте структуру опроса после запуска и получения ответов – испортите нафиг всю статистику. Если столкнетесь со сложностями, пишите в комментариях, делитесь опытом, обсудим, дополним, будет хорошо)\n","metric":0.66064453130000000414,"metric_input":"Текст статьи:\nКак создать опрос? / Habr             \n\n31-08-2021\nСоветы для создания онлайн опроса и выбора конструктора опросов Потратив 8 минут на прочтение данного текста, вы узнаете: 1) Какой сервис лучше выбрать для проведения и создания опросов 2) Как создать опрос 3) Какие есть подводные камни при создании онлайн опроса Итак, судя по тому, что вы читаете эту статью, перед вами стоит задача по проведению онлайн опроса. Сначала вам нужно решить, какой сервис опросов вы будете использовать для работы. Это важный момент, т.к. сервисов много, и при выборе лучше бы не ошибиться, ведь данные всех опросов аккумулируются в вашем ЛК, а переезд с одного сервиса на другой, помимо новых согласований и договоров, ведёт к невозможности хранения данных в одном месте и, в итоге, к беспорядку и неудобствам. Если коротко: что нужно знать о подобных сервисах? Кто уже выбрал свой сервис, не тратим время, идем ниже. 1) Интуитивно понятный интерфейс. Хороший конструктор с понятной логикой расположения настроек и различных фитч. Имхо, разгадывать логику разработчиков, почему они засунули такую-то настройку в такую-то далёкую вселенную – значит регулярно выругиваться и тратить драгоценное время. 2) Большой функционал конструктора. Тут важно соблюсти баланс. Т.е. если сервис молодой, они мало чего успели технически реализовать, а если сервис уже имеет какой-то сверх навороченный функционал, то тут будут сложности с первым пунктом (интуитивно понятный интерфейс), т.к. конструктор, вероятнее всего, окажется мега перегруженным, запутанным и непонятным. 3) Поддержка сервиса. Как не крути, но человек человеку друг, и хорошая тех.поддержка поможет сохранить вам прилично времени и нервов при работе с опросами. Мало кто может похвастаться адекватностью, но, к счастью, есть юзер-френдли компании, которые действительно могут помочь в решении ваших задач. При выборе сервиса также важно, где расположены серверы, какие есть форматы выгрузки данных, можно ли создавать тестирования, есть ли e-mail рассылка, можно ли встроить опрос на сайт, настроить имейл уведомления… короче, об этом всем есть суперклассный обзор сервисов лучших конструкторов. Если еще не определились, GO читать подробнее и искать свой сервис. Создание опроса Идём дальше. Главный секрет: создавать опрос нужно в Excel, продумать его структуру, формулировки, нюансы с вариантами ответов, и потом уже переносить опрос в конструктор сервиса. Поэтому нужно учесть: для чего вы создаёте опрос – какие цели и задачи он должен закрыть, как часто вы его планируете проводить для кого вы его создаёте – кто будет заполнять опрос, с каких устройств (компьютеры, телефоны) один опрос – одна задача – ваш опрос должен закрывать одну задачу, не пытайтесь запихнуть в одно исследование всё подряд, получится плохо, очень плохо меньше вопросов, больше ответов – чем длиннее ваш опрос, тем меньшее количество респондентов дойдёт до конца. Мало у кого есть время на заполнение замороченного, длинного, нудного опроса. больше закрытых вопросов – меньше открытых – продумайте возможные варианты ответов, по возможности, уходите от свободных ответов, т.к. респонденты неохотно будут писать вам своё мнение текстом. Да и обрабатывать открытые текстовые ответы вам не очень понравится. как работать с результатами - создавая опрос, сразу думайте, как вы будете обрабатывать результаты, потом будет поздно. посмотрите со стороны – заполните опрос, как будто его прислали вам, а не с точки зрения создателя. Отправьте опрос паре знакомых, взгляд со стороны всегда заметит ваши косячки. Вы хорошо подумали над всеми пунктами выше – это уже половина успеха) Мы написали много интересных статей на тему конструкторов опросов и самих опросов и, в своё время, перепробовали в работе кучу онлайн сервисов. Расскажем как создать опрос на примере сервиса www.testograf.ru (наш любимчик) 1. При сборке опроса в Экселе сразу подумайте, какой из вопросов будет с возможностью одного ответа, какой с возможностью нескольких, стоит ли дать респонденту возможность добавить свой ответ. В уже упомянутом Тестографе более 30 различных типов вопросов. Лучше, конечно, ознакомиться с возможностями конструктора заранее, чтобы сразу понимать, что вы можете сделать в опросе технически. Например, дать возможность разместить варианты ответов в виде ранжирования, использовать матричные типы вопросов, рейтинги в виде смайлов или звёзд, либо при большом количестве вариантов ответов выбрать тип вопроса «выпадающий список». Короче, смотрим, какие есть типы вопросов и думаем, как их лучше использовать в разрезе ваших задач. 2. Далее разбираемся с логикой опроса. Например, вы можете задать разным категориям респондентов различные вопросы. Это делается настройкой ветвления при выбранном варианте ответа. Скажем, у вас есть вопрос по полу респондента, и вы хотите задать мужчинам одни вопросы, а женщинам другие. Не проблема, настраиваем логический переход в зависимости от того, какой вариант ответа выбрал респондент. 3. Следующий шаг – настройки опроса. На Тестографе куча всего полезного и интересного, тут каждый однозначно найдет для себя что-то нужное. Настройка текста завершения, индикатор прохождения, пароли к опросу, авторизация к опросу, перенаправление на веб-сайт, показ результатов после опроса и т.п. 4. Теперь, когда опрос собран и настроен, осталось позаботиться о его внешнем виде. Красивое всем нравится больше, красивым прощается больше ошибок (ну, это если вы всё-таки где-то накосячили J). В общем, настраиваем дизайн опроса. Если вы проводите опрос для компании – идеальным решением будет подогнать опрос под брендбук. Можно изменить цвета, шрифты, размеры, добавить логотип, изменить фон опроса. Тут главное не переборщить и не заиграться. Не забывайте о юзабилити, т.к., например, яркое изображение фона может мешать отображению элементов опроса. Обещанные «подводные камни», встречавшиеся на нашем пути при работе с онлайн опросами: · Не ставьте ограничение по IP адресу на заполнение опроса, если есть респонденты, которые сидят в одном офисе. · Не меняйте структуру опроса после запуска и получения ответов – испортите нафиг всю статистику. Если столкнетесь со сложностями, пишите в комментариях, делитесь опытом, обсудим, дополним, будет хорошо)\n\n\nКраткое содержание:\nСтатья обсуждает создание онлайн опросов и выбор подходящего конструктора. Советы включают в себя:\n\n1. Выбор хорошего сервиса опросов с интуитивно понятным интерфейсом и большим функционалом.\n2. Создание опроса в Excel, проработка структуры, формулировок и нюансов вариантов ответов.\n3. Учет целей и задач опроса, частоты проведения, целевой аудитории и устройств.\n4. Минимизация вопросов и максимизация ответов для удобства респондентов.\n5. Продумывание обработки результатов и их анализа.\n6. Работа с дизайном опроса и его внешним видом.\n7. Избегание ошибок, таких как ограничение по IP адресу или изменение структуры после запуска.\n8. Делитесь опытом и задавайте вопросы в комментариях для обсуждения и дополнения.","summary":"Статья обсуждает создание онлайн опросов и выбор подходящего конструктора. Советы включают в себя:\n\n1. Выбор хорошего сервиса опросов с интуитивно понятным интерфейсом и большим функционалом.\n2. Создание опроса в Excel, проработка структуры, формулировок и нюансов вариантов ответов.\n3. Учет целей и задач опроса, частоты проведения, целевой аудитории и устройств.\n4. Минимизация вопросов и максимизация ответов для удобства респондентов.\n5. Продумывание обработки результатов и их анализа.\n6. Работа с дизайном опроса и его внешним видом.\n7. Избегание ошибок, таких как ограничение по IP адресу или изменение структуры после запуска.\n8. Делитесь опытом и задавайте вопросы в комментариях для обсуждения и дополнения."}
{"Text":"Процессор для планшетов — поговорим о Bay Trail / Хабр                 \n\nПотихоньку подходит к концу 2013 год; новый, 2014, Intel готовится встретить с новым поколением процессоров (точнее, SoC) Atom для мобильных устройств и ПК под кодовым названием Bay Trail. В этом посте мы сравним новые чипы с предыдущим поколением и посмотрим, насколько они продвинулись вперед. Больше всего нас, конечно интересуют потребительские характеристики – насколько лучше и веселее будет нам жить с Bay Trail?\n\nСемейство процессоров Bay Trail состоит из трех групп. Процессоры с буквой N ( N3000 -«Pentium» и N2000 -«Celeron») предназначены для ноутбуков, с буквой J (J2000 и J1000) – для настольных ПК. Нас же более всего будут интересовать те, что с буковкой Z – для планшетов. Здесь также имеется две серии:\n**Z3700**\n– четырехядерники для планшетов под Windows и Android и двухядерные\n**Z3600**\nдля Android планшетов. Все процессора выполнены по 22-нм tri-gate технологии.\n\nРассмотрим, что представляют из себя процессоры серии Intel Atom Z3000. Их основные характеристики таковы:\n\n4 ядра/4 потока, 2 Мб L2 кэша; неупорядоченное (out of order) исполнение инструкций\n\nIntel Burst (о ней мы поговорим ниже);\n\nподдержка 64-битных ОС;\n\nновые для Atom векторные инструкции SSE4.1;\n\nподдержка двухканальной памяти LPDDR3-1067 (максимум 4 Гб);\n\nграфическое ядро Intel HD Graphics с такой же архитектурой, как и у Intel GPU в процессорах Core, — 4 исполнительных устройства (8 потоков в каждом);\n\nдинамическая частота графического ядра, аппаратное ускорение кодирования/декодирования;\n\nновые технологии виртуализации и безопасности.\n\nОбщая архитектура чипа приведена на рисунке ниже (кликабельно).\n\nДля наглядности сделаем сравнительную табличку характеристик платформы предыдущего поколения Clover Trail+( Atom Z2760) и Bay Trail. Наиболее принципиальные изменения обведены синим.\n\nТрадиционно для этого года, особенно много улучшений мы видим в графической части. Тем более, что она теперь принципиально одинакова для Intel Core и Intel Atom. У графического ядра, разделяющего тепловую мощность с ЦПУ, появился турбо режим. DirectX11 и OpenGL ES 3.0 API теперь доступны и на планшетах! Поддерживаются дисплеи высокого разрешения (вплоть до 2560x1600).\n\nАппаратное кодирование HD H.264 теперь может выполняться с самыми высокими профилями качества, энергопотребление при этом осталось небольшим. Также частично ускоряется аппаратно кодирование MPEG2. Расширился список аппаратно декодируемых медиа форматов: H.264, VC1, MPEG2, MVC, VP8, MPEG-4/H.263, MJPEG.\n\nТестирование показало, что аппаратное транскодирование 1080p24@ 23Mbps выполняется при том же качестве на новой платформе Intel в 3 раза быстрее, чем на iPhone 5!\n\n_Аппаратная фильтрация шумов — до применения фильтра и после_\n\nПодсистема видеовывода имеет гибкую двойную конфигурацию, включающую в себя два цифровых интерфейса DDI.\n\nДля экономии энергии дисплей использует технологию Intel Display Power Saving Technology (Intel DPST) – осветление и улучшение выводимой картинки, за счет которого удается уменьшить яркость подсветки. Уменьшение энергопотребления дисплея в результате составляет 20-50%.\n\nКомбинированный контроллер памяти Bay Trail умеет работать как с DDR3L-RS, так и с LPDDR3. Поддерживается двухканальная память, применяются агрессивные алгоритмы энергосбережения. Пропускная способность памяти в Bay Trail в два раза превышает таковую у Clover Trail!\n\n_Сравнение скорости памяти. Здесь (CLT+) = Clover Trail+, BYT = Bay Trail_\n\nНовый модуль камеры позволяет снимать видео 1080p60, оснащен цифровым стабилизатором. Разработчикам удалось добиться минимальной задержки при фото съемке. Модуль позволяет подключить две камеры с матрицами от 8 до 13 Мпикс. Сравнение с Clover Trail, очевидно, в пользу нового поколения\n\n|   |  Clover Trail |  Bay Trail |\n| --- | --- | --- |\n|  Съемка видео |  1080p30 |  1080p60 |\n|  Двойная видеосъемка |  Не поддерживается |  Обе 1080p30 |\n|  Фото во время видеосъемки |  Не поддерживается |  Фото выского разрешения |\n|  Цифровая стабилизация |  По двум осям |  По четырем осям (+ вращение) |\n\nОчень много сделано для увеличения энергоэффективности платформы. И на одном из первых мест здесь — технология\n**Intel Burst Technology 2.0**\n, позволяющая ядрам работать быстрее стандартной рабочей частоты, если при этом не нарушаются эксплуатационные ограничения по температуре, потреблению и т.д. Таким образом, одно ядро может забирать спорадическую нагрузку с других, сокращая общее энергопотребление. В новой версии Intel Burst распределение нагрузки может происходить не только между вычислительными ядрами, но и графическими, а также подсистемой обработки изображения с камеры и дисплеем.\n\nАнимированная диаграмма ниже показывает принцип работы технологии:\n\nБолее тонкий техпроцесс, оптимизация архитектуры и новые технологии позволили добиться 2-6.5 кратного увеличения производительности процессора (в зависимости от приложения) при значительном снижении потребления. Это означает, что в 2014 году мы увидим мощные, долгоиграющие мобильные устройства на платформе Intel — по прогнозам разработчиков, реально говорить о 8-10 часах проигрывания видео для Bay Trail.\n\n_**При подготовке поста использовались материалы IDF 2013, автор — Shreekant (Ticky) Thakkar.**_","metric":0.263671875,"metric_input":"Текст статьи:\nПроцессор для планшетов — поговорим о Bay Trail / Хабр                 \n\nПотихоньку подходит к концу 2013 год; новый, 2014, Intel готовится встретить с новым поколением процессоров (точнее, SoC) Atom для мобильных устройств и ПК под кодовым названием Bay Trail. В этом посте мы сравним новые чипы с предыдущим поколением и посмотрим, насколько они продвинулись вперед. Больше всего нас, конечно интересуют потребительские характеристики – насколько лучше и веселее будет нам жить с Bay Trail?\n\nСемейство процессоров Bay Trail состоит из трех групп. Процессоры с буквой N ( N3000 -«Pentium» и N2000 -«Celeron») предназначены для ноутбуков, с буквой J (J2000 и J1000) – для настольных ПК. Нас же более всего будут интересовать те, что с буковкой Z – для планшетов. Здесь также имеется две серии:\n**Z3700**\n– четырехядерники для планшетов под Windows и Android и двухядерные\n**Z3600**\nдля Android планшетов. Все процессора выполнены по 22-нм tri-gate технологии.\n\nРассмотрим, что представляют из себя процессоры серии Intel Atom Z3000. Их основные характеристики таковы:\n\n4 ядра/4 потока, 2 Мб L2 кэша; неупорядоченное (out of order) исполнение инструкций\n\nIntel Burst (о ней мы поговорим ниже);\n\nподдержка 64-битных ОС;\n\nновые для Atom векторные инструкции SSE4.1;\n\nподдержка двухканальной памяти LPDDR3-1067 (максимум 4 Гб);\n\nграфическое ядро Intel HD Graphics с такой же архитектурой, как и у Intel GPU в процессорах Core, — 4 исполнительных устройства (8 потоков в каждом);\n\nдинамическая частота графического ядра, аппаратное ускорение кодирования/декодирования;\n\nновые технологии виртуализации и безопасности.\n\nОбщая архитектура чипа приведена на рисунке ниже (кликабельно).\n\nДля наглядности сделаем сравнительную табличку характеристик платформы предыдущего поколения Clover Trail+( Atom Z2760) и Bay Trail. Наиболее принципиальные изменения обведены синим.\n\nТрадиционно для этого года, особенно много улучшений мы видим в графической части. Тем более, что она теперь принципиально одинакова для Intel Core и Intel Atom. У графического ядра, разделяющего тепловую мощность с ЦПУ, появился турбо режим. DirectX11 и OpenGL ES 3.0 API теперь доступны и на планшетах! Поддерживаются дисплеи высокого разрешения (вплоть до 2560x1600).\n\nАппаратное кодирование HD H.264 теперь может выполняться с самыми высокими профилями качества, энергопотребление при этом осталось небольшим. Также частично ускоряется аппаратно кодирование MPEG2. Расширился список аппаратно декодируемых медиа форматов: H.264, VC1, MPEG2, MVC, VP8, MPEG-4/H.263, MJPEG.\n\nТестирование показало, что аппаратное транскодирование 1080p24@ 23Mbps выполняется при том же качестве на новой платформе Intel в 3 раза быстрее, чем на iPhone 5!\n\n_Аппаратная фильтрация шумов — до применения фильтра и после_\n\nПодсистема видеовывода имеет гибкую двойную конфигурацию, включающую в себя два цифровых интерфейса DDI.\n\nДля экономии энергии дисплей использует технологию Intel Display Power Saving Technology (Intel DPST) – осветление и улучшение выводимой картинки, за счет которого удается уменьшить яркость подсветки. Уменьшение энергопотребления дисплея в результате составляет 20-50%.\n\nКомбинированный контроллер памяти Bay Trail умеет работать как с DDR3L-RS, так и с LPDDR3. Поддерживается двухканальная память, применяются агрессивные алгоритмы энергосбережения. Пропускная способность памяти в Bay Trail в два раза превышает таковую у Clover Trail!\n\n_Сравнение скорости памяти. Здесь (CLT+) = Clover Trail+, BYT = Bay Trail_\n\nНовый модуль камеры позволяет снимать видео 1080p60, оснащен цифровым стабилизатором. Разработчикам удалось добиться минимальной задержки при фото съемке. Модуль позволяет подключить две камеры с матрицами от 8 до 13 Мпикс. Сравнение с Clover Trail, очевидно, в пользу нового поколения\n\n|   |  Clover Trail |  Bay Trail |\n| --- | --- | --- |\n|  Съемка видео |  1080p30 |  1080p60 |\n|  Двойная видеосъемка |  Не поддерживается |  Обе 1080p30 |\n|  Фото во время видеосъемки |  Не поддерживается |  Фото выского разрешения |\n|  Цифровая стабилизация |  По двум осям |  По четырем осям (+ вращение) |\n\nОчень много сделано для увеличения энергоэффективности платформы. И на одном из первых мест здесь — технология\n**Intel Burst Technology 2.0**\n, позволяющая ядрам работать быстрее стандартной рабочей частоты, если при этом не нарушаются эксплуатационные ограничения по температуре, потреблению и т.д. Таким образом, одно ядро может забирать спорадическую нагрузку с других, сокращая общее энергопотребление. В новой версии Intel Burst распределение нагрузки может происходить не только между вычислительными ядрами, но и графическими, а также подсистемой обработки изображения с камеры и дисплеем.\n\nАнимированная диаграмма ниже показывает принцип работы технологии:\n\nБолее тонкий техпроцесс, оптимизация архитектуры и новые технологии позволили добиться 2-6.5 кратного увеличения производительности процессора (в зависимости от приложения) при значительном снижении потребления. Это означает, что в 2014 году мы увидим мощные, долгоиграющие мобильные устройства на платформе Intel — по прогнозам разработчиков, реально говорить о 8-10 часах проигрывания видео для Bay Trail.\n\n_**При подготовке поста использовались материалы IDF 2013, автор — Shreekant (Ticky) Thakkar.**_\n\nКраткое содержание:\nСтатья обсуждает новые процессоры Intel Atom для планшетов под кодовым названием Bay Trail. Процессоры Bay Trail состоят из трех групп: N для ноутбуков, J для настольных ПК и Z для планшетов. Процессоры Z3700 и Z3600 с буквой Z предназначены для планшетов под Windows и Android. Они выполнены по 22-нм tri-gate технологии и имеют четыре ядра/четыре потока, 2 Мб L2 кэша, поддержку 64-битных ОС, новые векторные инструкции SSE4.1, поддержку двухканальной памяти LPDDR3-1067 (максимум 4 Гб), графическое ядро Intel HD Graphics с 8 потоками и динамической частотой.\n\nОсновные характеристики платформы предыдущего поколения Clover Trail+ и Bay Trail приведены в сравнительной табличке. Наиболее принципиальные изменения обведены синим.\n\nВ графической части наблюдается значительное улучшение. У графического ядра появился турбо режим. Теперь доступны DirectX11 и OpenGL ES 3.0 API на планшетах. Поддерживаются дисплеи высокого разрешения. Аппаратное кодирование HD H.264 может выполняться с самыми высокими профилями качества, энергопотребление при этом осталось небольшим.\n\nТестирование показало, что аппаратное транскодирование 1080p24@ 23Mbps выполняется при том же качестве в 3 раза быстрее, чем на iPhone 5. Подсистема видеовывода имеет гибкую двойную конфигурацию, включающую в себя два цифровых интерфейса DDI. Дисплей использует технологию Intel Display Power Saving Technology (Intel DPST) – осветление и улучшение выводимой картинки, за счет которого удается уменьшить яркость подсветки. Комбинированный контроллер памяти Bay Trail умеет работать как с DDR3L-RS, так и с LPDDR3. Поддерживается двухканальная память, применяются агрессивные алгоритмы энергосбережения. Пропускная способность памяти в Bay Trail в два раза превышает таковую у Clover Trail. Новый модуль камеры позволяет снимать видео 1","summary":"Статья обсуждает новые процессоры Intel Atom для планшетов под кодовым названием Bay Trail. Процессоры Bay Trail состоят из трех групп: N для ноутбуков, J для настольных ПК и Z для планшетов. Процессоры Z3700 и Z3600 с буквой Z предназначены для планшетов под Windows и Android. Они выполнены по 22-нм tri-gate технологии и имеют четыре ядра/четыре потока, 2 Мб L2 кэша, поддержку 64-битных ОС, новые векторные инструкции SSE4.1, поддержку двухканальной памяти LPDDR3-1067 (максимум 4 Гб), графическое ядро Intel HD Graphics с 8 потоками и динамической частотой.\n\nОсновные характеристики платформы предыдущего поколения Clover Trail+ и Bay Trail приведены в сравнительной табличке. Наиболее принципиальные изменения обведены синим.\n\nВ графической части наблюдается значительное улучшение. У графического ядра появился турбо режим. Теперь доступны DirectX11 и OpenGL ES 3.0 API на планшетах. Поддерживаются дисплеи высокого разрешения. Аппаратное кодирование HD H.264 может выполняться с самыми высокими профилями качества, энергопотребление при этом осталось небольшим.\n\nТестирование показало, что аппаратное транскодирование 1080p24@ 23Mbps выполняется при том же качестве в 3 раза быстрее, чем на iPhone 5. Подсистема видеовывода имеет гибкую двойную конфигурацию, включающую в себя два цифровых интерфейса DDI. Дисплей использует технологию Intel Display Power Saving Technology (Intel DPST) – осветление и улучшение выводимой картинки, за счет которого удается уменьшить яркость подсветки. Комбинированный контроллер памяти Bay Trail умеет работать как с DDR3L-RS, так и с LPDDR3. Поддерживается двухканальная память, применяются агрессивные алгоритмы энергосбережения. Пропускная способность памяти в Bay Trail в два раза превышает таковую у Clover Trail. Новый модуль камеры позволяет снимать видео 1"}
{"Text":"Вид и перспектива в дизайне уровней. Часть первая / Habr             \n\n22-07-2016\nНам, как дизайнерам уровней, часто приходится снабжать игроков большими объемами информации. Это необходимо по многим причинам. Иногда нам просто хочется блеснуть красивым гейм-артом, а иногда – направить игрока к выполнению цели, проработать историю или сюжет игры, создать или сбавить напряжение. Но прежде чем делать всё это, необходимо понять, как направлять внимание игроков туда, куда нам нужно. Подробнее об этом — в нашем переводе статьи Майка Стаута. Планета Гаспар из игры Ratchet & Clank Существует много методов, помогающих дизайнерам привлечь внимание пользователей. В некоторых играх используются видеозаставки (или пролет камеры), чтобы направить внимание игроков на важные, по задумке разработчиков, элементы. Но тем самым игроков лишают возможности управления. В других играх, наоборот, игроки могут получить доступ к важным элементам собственноручно, нажав на специальную кнопку. Так или иначе, в этой статье мы поговорим о моем любимом способе направлять внимание – создании изначально грамотной композиции уровня за счет умного использования вида и перспективы. Конечно, мне нравятся и другие методы (обычно используемые в комплексе), но лучше выполнить дизайнерскую задачу на этапе проектирования уровня, чтобы она не превратилась в проблему, которую придется решать уже на этапе разработки. Естественно, в таком случае решение (например, видеозаставка) потребует гораздо больше ресурсов и усилий. Что такое вид и перспектива? Вид – это взаиморасположение камеры и геометрии уровня, которое обеспечивает правильное построение композиции и подчеркивает важные элементы сцены. Здесь подразумевается любое место в уровне, устроенное таким образом, чтобы гарантированно обратить внимание пользователя на какую-либо важную деталь. Чаще всего для этого используется еще один особый инструмент – перспектива. Перспектива – это вид, который открывается через узкий коридор деревьев, зданий и т.д. Удачное построение вида и перспективы требует вдумчивого расположения ориентиров и акцентов игры, а также искусной расстановки камер. К примеру, в начале игры (или в любой момент после загрузки) дизайнер может разместить ориентиры и акценты в самом первом кадре, до того как игрок получит возможность управления. Но это не единственный способ их применения. Многие игры также используют узкие пространства, коридоры или другие элементы уровня, вынуждающие пользователя поворачивать камеру в определенном направлении. К примеру, если пользователь должен войти в дверь, камера, скорее всего, будет повернута в её сторону, и перспектива выстраивается через нее, по ту сторону двери. Вид и перспектива – очень мощные инструменты, особенно если использовать их в сочетании с другими упомянутыми выше средствами привлечения внимания (вроде видеозаставок). Вид на Тадж-Махал, открывающийся через узкий коридор (Diego Delso, Wikimedia Commons, License CC-BY-SA 3.0) Зачем использовать вид? 1. Чтобы продемонстрировать крутой гейм-арт Самое очевидное преимущество использования вида или перспективы – показать игрокам красивый гейм-арт: к примеру, как на этом скриншоте из Middle-earth: Shadows of Mordor. Контрольный пункт в игре Middle-earth: Shadows of Mordor Этот скриншот сделан на одном из контрольных пунктов игры, которые представлены там в виде высоких башен с колоннами (по одной колонне в каждом из четырех углов башни). Через каждую пару колонн игроку открывается шикарный вид на окружающий мир. 2. Чтобы подчеркнуть изюминку игры Если в вашей игре есть интересная особенность, неважно – в технологии, арте или геймплее, правильный вид и перспектива помогут вам сделать на ней акцент. Открывающийся вид на первую планету в игре Super Mario Galaxy Начальный открывающийся вид первой планеты в игре Super Mario Galaxy обращает внимание на её особую физику – изюминку игры. Камера расположена таким образом, чтобы округлый горизонт планеты и объекты, которые на нём появляются, акцентировали на себе внимание. 3. Чтобы направлять игрока Этот скриншот из игры Elder Scrolls: Oblivion был сделан сразу по окончании обучения в подземелье. После выхода наружу игроку открывается этот вид, причем камера располагается именно под таким углом. Прием довольно часто используется в играх серии Elder Scrolls. Он позволяет дизайнерам точно знать, куда будет направлен взгляд игрока, когда он войдет в очередную дверь. Вид при выходе из подземелья в игре Elder Scrolls: Oblivion Обратите внимание на то, как линия причала направляет ваше внимание к руинам. Руины не имеют никакого особенного значения в игре – просто разработчики Oblivion хотят научить игрока жить в открытом мире. Они побуждают его отправиться исследовать этот мир, не принуждая его делать что-либо конкретно. И такой вид прекрасно помогает донести эту идею. 4. Чтобы исследовать сюжет или историю игры Глядя на этот скриншот из игры Super Mario Galaxy, можно сделать два любопытных наблюдения: Обратите внимание, как тропинка впереди образует линию, которая подводит ваш взгляд к падающим звездам. Вспомните об этом немного позже, когда мы будем говорить об использовании линий для управления вниманием 1. Перспектива, создаваемая двумя холмами, направляет внимание игрока вдоль тропинки, которая ведет на следующий уровень. 2. Игрок видит небо в образовавшейся перспективе, а падающие звезды становятся ключом к пониманию всего сюжета игры. В начале Half-Life 2 игроку нужно как-то рассказать о том, что произошло с момента последней игры. Скриншот сделан на раннем этапе игры, когда игроку еще почти ничего не известно. На первом уровне игры часто используется перспектива для привлечения внимания игрока к видеоэкранам или диалогам неигровых персонажей. Это позволяет дизайнерам удостовериться в том, что игрок не пропустит важные моменты и будет постепенно узнавать, что происходит вокруг, не теряя при этом управление игрой. Совет: Управление камерой Дизайнеру не нужно полностью контролировать камеру, чтобы продемонстрировать игроку хорошие виды. К примеру, вид на приведенном выше скриншоте из Half-Life 2 не навязывается: персонаж просто выходит из поезда и поворачивает налево. Но поскольку ему и так нужно идти по этому узкому коридору между поездами, он гарантировано увидит такую картинку. 5. Чтобы создавать напряжение и интригу Как говорилось ранее, первый уровень в Half-Life 2 сделан таким образом, чтобы за счет правильного построения вида и перспективы дать игроку как можно больше информации через видеоэкраны и диалоги. Видеоэкраны, как правило, сообщают игроку, что происходит; диалоги между неигровыми персонажами нужны, в основном, чтобы припугнуть его. Возьмем для примера этот эпизод с пропускным пунктом охраны: Ограда выстроена в форме лабиринта с тремя параллельными секциями, образующими 3 отдельные перспективы. Из них 2 вырабатывают у игрока определенный паттерн – создают ощущение комфорта. Когда в третий раз этот паттерн нарушается, игрок чувствует диссонанс и тревогу. Проходя через первый коридор, сделанный из сетчатого ограждения, вы будете гарантировано смотреть прямо перед собой, как и было задумано. Пройдя второй коридор, вы увидите, как человек справа спокойно пересекает пропускной пункт. Когда вы уже почти прошли третий коридор, на ваших глазах второй человек так же спокойно преодолевает пропускной пункт. Вы видите, как они оба выходят через дверь в дальнем конце коридора. Наконец, дойдя до пропускного пункта, вы замечаете нарушение паттерна. Солдаты не нападают на вас, но один из них становится на пути к двери, через которую только что вышли 2 человека. Солдаты указывают вам на другую дверь, которую вы не видели, проходя через лабиринт. Нарушение паттерна сбивает вас с толку и побуждает задать себе вопрос: почему вас ведут в другую дверь? Не скрывается ли за ней какая-нибудь угроза? Начальный открывающийся вид дает игроку первое впечатление об уровне. А с использованием всех приведенных выше советов вы можете превратить его в мощное средство для создания хорошего первого впечатления.\n","metric":0.6806640625,"metric_input":"Текст статьи:\nВид и перспектива в дизайне уровней. Часть первая / Habr             \n\n22-07-2016\nНам, как дизайнерам уровней, часто приходится снабжать игроков большими объемами информации. Это необходимо по многим причинам. Иногда нам просто хочется блеснуть красивым гейм-артом, а иногда – направить игрока к выполнению цели, проработать историю или сюжет игры, создать или сбавить напряжение. Но прежде чем делать всё это, необходимо понять, как направлять внимание игроков туда, куда нам нужно. Подробнее об этом — в нашем переводе статьи Майка Стаута. Планета Гаспар из игры Ratchet & Clank Существует много методов, помогающих дизайнерам привлечь внимание пользователей. В некоторых играх используются видеозаставки (или пролет камеры), чтобы направить внимание игроков на важные, по задумке разработчиков, элементы. Но тем самым игроков лишают возможности управления. В других играх, наоборот, игроки могут получить доступ к важным элементам собственноручно, нажав на специальную кнопку. Так или иначе, в этой статье мы поговорим о моем любимом способе направлять внимание – создании изначально грамотной композиции уровня за счет умного использования вида и перспективы. Конечно, мне нравятся и другие методы (обычно используемые в комплексе), но лучше выполнить дизайнерскую задачу на этапе проектирования уровня, чтобы она не превратилась в проблему, которую придется решать уже на этапе разработки. Естественно, в таком случае решение (например, видеозаставка) потребует гораздо больше ресурсов и усилий. Что такое вид и перспектива? Вид – это взаиморасположение камеры и геометрии уровня, которое обеспечивает правильное построение композиции и подчеркивает важные элементы сцены. Здесь подразумевается любое место в уровне, устроенное таким образом, чтобы гарантированно обратить внимание пользователя на какую-либо важную деталь. Чаще всего для этого используется еще один особый инструмент – перспектива. Перспектива – это вид, который открывается через узкий коридор деревьев, зданий и т.д. Удачное построение вида и перспективы требует вдумчивого расположения ориентиров и акцентов игры, а также искусной расстановки камер. К примеру, в начале игры (или в любой момент после загрузки) дизайнер может разместить ориентиры и акценты в самом первом кадре, до того как игрок получит возможность управления. Но это не единственный способ их применения. Многие игры также используют узкие пространства, коридоры или другие элементы уровня, вынуждающие пользователя поворачивать камеру в определенном направлении. К примеру, если пользователь должен войти в дверь, камера, скорее всего, будет повернута в её сторону, и перспектива выстраивается через нее, по ту сторону двери. Вид и перспектива – очень мощные инструменты, особенно если использовать их в сочетании с другими упомянутыми выше средствами привлечения внимания (вроде видеозаставок). Вид на Тадж-Махал, открывающийся через узкий коридор (Diego Delso, Wikimedia Commons, License CC-BY-SA 3.0) Зачем использовать вид? 1. Чтобы продемонстрировать крутой гейм-арт Самое очевидное преимущество использования вида или перспективы – показать игрокам красивый гейм-арт: к примеру, как на этом скриншоте из Middle-earth: Shadows of Mordor. Контрольный пункт в игре Middle-earth: Shadows of Mordor Этот скриншот сделан на одном из контрольных пунктов игры, которые представлены там в виде высоких башен с колоннами (по одной колонне в каждом из четырех углов башни). Через каждую пару колонн игроку открывается шикарный вид на окружающий мир. 2. Чтобы подчеркнуть изюминку игры Если в вашей игре есть интересная особенность, неважно – в технологии, арте или геймплее, правильный вид и перспектива помогут вам сделать на ней акцент. Открывающийся вид на первую планету в игре Super Mario Galaxy Начальный открывающийся вид первой планеты в игре Super Mario Galaxy обращает внимание на её особую физику – изюминку игры. Камера расположена таким образом, чтобы округлый горизонт планеты и объекты, которые на нём появляются, акцентировали на себе внимание. 3. Чтобы направлять игрока Этот скриншот из игры Elder Scrolls: Oblivion был сделан сразу по окончании обучения в подземелье. После выхода наружу игроку открывается этот вид, причем камера располагается именно под таким углом. Прием довольно часто используется в играх серии Elder Scrolls. Он позволяет дизайнерам точно знать, куда будет направлен взгляд игрока, когда он войдет в очередную дверь. Вид при выходе из подземелья в игре Elder Scrolls: Oblivion Обратите внимание на то, как линия причала направляет ваше внимание к руинам. Руины не имеют никакого особенного значения в игре – просто разработчики Oblivion хотят научить игрока жить в открытом мире. Они побуждают его отправиться исследовать этот мир, не принуждая его делать что-либо конкретно. И такой вид прекрасно помогает донести эту идею. 4. Чтобы исследовать сюжет или историю игры Глядя на этот скриншот из игры Super Mario Galaxy, можно сделать два любопытных наблюдения: Обратите внимание, как тропинка впереди образует линию, которая подводит ваш взгляд к падающим звездам. Вспомните об этом немного позже, когда мы будем говорить об использовании линий для управления вниманием 1. Перспектива, создаваемая двумя холмами, направляет внимание игрока вдоль тропинки, которая ведет на следующий уровень. 2. Игрок видит небо в образовавшейся перспективе, а падающие звезды становятся ключом к пониманию всего сюжета игры. В начале Half-Life 2 игроку нужно как-то рассказать о том, что произошло с момента последней игры. Скриншот сделан на раннем этапе игры, когда игроку еще почти ничего не известно. На первом уровне игры часто используется перспектива для привлечения внимания игрока к видеоэкранам или диалогам неигровых персонажей. Это позволяет дизайнерам удостовериться в том, что игрок не пропустит важные моменты и будет постепенно узнавать, что происходит вокруг, не теряя при этом управление игрой. Совет: Управление камерой Дизайнеру не нужно полностью контролировать камеру, чтобы продемонстрировать игроку хорошие виды. К примеру, вид на приведенном выше скриншоте из Half-Life 2 не навязывается: персонаж просто выходит из поезда и поворачивает налево. Но поскольку ему и так нужно идти по этому узкому коридору между поездами, он гарантировано увидит такую картинку. 5. Чтобы создавать напряжение и интригу Как говорилось ранее, первый уровень в Half-Life 2 сделан таким образом, чтобы за счет правильного построения вида и перспективы дать игроку как можно больше информации через видеоэкраны и диалоги. Видеоэкраны, как правило, сообщают игроку, что происходит; диалоги между неигровыми персонажами нужны, в основном, чтобы припугнуть его. Возьмем для примера этот эпизод с пропускным пунктом охраны: Ограда выстроена в форме лабиринта с тремя параллельными секциями, образующими 3 отдельные перспективы. Из них 2 вырабатывают у игрока определенный паттерн – создают ощущение комфорта. Когда в третий раз этот паттерн нарушается, игрок чувствует диссонанс и тревогу. Проходя через первый коридор, сделанный из сетчатого ограждения, вы будете гарантировано смотреть прямо перед собой, как и было задумано. Пройдя второй коридор, вы увидите, как человек справа спокойно пересекает пропускной пункт. Когда вы уже почти прошли третий коридор, на ваших глазах второй человек так же спокойно преодолевает пропускной пункт. Вы видите, как они оба выходят через дверь в дальнем конце коридора. Наконец, дойдя до пропускного пункта, вы замечаете нарушение паттерна. Солдаты не нападают на вас, но один из них становится на пути к двери, через которую только что вышли 2 человека. Солдаты указывают вам на другую дверь, которую вы не видели, проходя через лабиринт. Нарушение паттерна сбивает вас с толку и побуждает задать себе вопрос: почему вас ведут в другую дверь? Не скрывается ли за ней какая-нибудь угроза? Начальный открывающийся вид дает игроку первое впечатление об уровне. А с использованием всех приведенных выше советов вы можете превратить его в мощное средство для создания хорошего первого впечатления.\n\n\nКраткое содержание:\nСтатья обсуждает важность вида и перспективы в дизайне уровней для компьютерных игр. Вид и перспектива используются для привлечения внимания игроков, демонстрации гейм-арта, акцентирования изюминки игры, направления игрока и создания напряжения. Советы включают использование видеозаставок, умных расположений камер и видов для акцентирования важных элементов.","summary":"Статья обсуждает важность вида и перспективы в дизайне уровней для компьютерных игр. Вид и перспектива используются для привлечения внимания игроков, демонстрации гейм-арта, акцентирования изюминки игры, направления игрока и создания напряжения. Советы включают использование видеозаставок, умных расположений камер и видов для акцентирования важных элементов."}
{"Text":"Светодиодная лампа Philips 7 Вт из Дикси / Хабр               \n\n26-10-2017\nВ магазинах Дикси по всей стране появились в продаже светодиодные лампы Philips 7 Вт по цене 99 рублей. Я тестирую много светодиодных ламп, но для того, чтобы их купить надо куда-то ехать или заказывать их в интернете, а эту лампочку будут покупать вместе с продуктами в магазинах у дома, поэтому очень важно, чтобы она была хорошей и безопасной, тем более что все предыдущие светодиодные лампы, продававшиеся в сетевых магазинах «у дома» сложно было назвать хорошими. Я зашёл в ближайший магазин и купил там лампу с тёплым светом. Производитель приводит на упаковке следующие параметры: Мощность: 7 Вт; Рабочее напряжение: 220-240 В; Световой поток: 480 лм; Заменяет лампу накаливания 55 Вт. Срок службы более 8 лет Гарантийный срок 2 года. Точных значений цветовой температуры, индекса цветопередачи и срока службы на упаковке нет. Заявлено, что «лампы полностью соответствуют заявленному световому потоку. Минимальный уровень пульсации. Лампы полностью безопасны и комфортны для глаз». Проверим. Световой поток лампы измерялся прибором Viso Light Spion, потребляемая мощность прибором Robiton PM-2, индекс цветопередачи, цветовая температура и пульсация прибором Uprtek MK350D. Перед тестированием лампа прогревалась в течение часа. Измеренный световой поток (464 лм) оказался ниже заявленного всего на 3% — это в пределах погрешности измерений. Потребляемая мощность даже меньше заявленной — всего 5.35 Вт. Цветовая температура около 3100К. Такой свет более белый, чем у ламп накаливания, но вполне комфортный и он гораздо «теплее» света ламп с белым светом 4000К. Пульсация света полностью отсутствует. Индекс цветопередачи 83.3 и это хорошо (считается, что для жилых помещений CRI должен быть больше 80). Удивительно, но на сайте производителя для этой лампы заявлен индекс цветопередачи 70. Впервые сталкиваюсь с тем, что производитель заявляет параметры хуже, чем они есть на самом деле. Возможно на сайте просто опечатка. Хоть на коробке указан диапазон рабочих напряжений 220-240 вольт, лампа работает при понижении напряжения питания до 98 вольт, при этом световой поток составляет не менее 90% от номинального. На упаковке указано (правда очень мелким шрифтом на наклейке), что лампу нельзя использовать с диммерами и выключателями с индикаторами. Я проверил работу лампы с выключателем, имеющим индикатор. Когда выключатель выключен, лампа слабо и равномерно светится. Скорее всего это никак не повредит лампе и её можно использовать с таким выключателем в режиме ночника, почти не потребляющего энергию. Результаты измерений прибором Viso Light Spion (в таблице я привожу результаты измерений более точными приборами, поэтому значения напряжения, мощности, цветовой температуры и индекса цветопередачи немного отличаются). Эта лампа, несмотря на низкую цену, действительно даёт качественный, комфортный и безопасный свет. При работе лампа нагревается не сильно — максимальная температура корпуса 49 градусов. Это означает, что у лампы есть шансы проработать достаточно долго, хоть на сайте и заявляется совсем небольшой для светодиодных ламп срок службы — 8000 часов. Единственный вопрос, который возникает к этой лампе, достаточно ли света она даёт и какую лампу накаливания она сможет полноценно заменить. Для того, чтобы получить точный ответ на этот вопрос я дополнительно протестировал две обычные матовые лампы накаливания на 60 Вт — OSRAM и Philips. С помощью стабилизатора Штиль Инстаб 500, ЛАТРа Suntek TDGC2-0.5 и точного мультиметра Aneng AN8001 я подавал на лампы различные напряжения и замерял световой поток. На упаковке обеих ламп указано рабочее напряжение 230 В и световой поток 710 лм. Фактически они дают меньше света и его количество очень сильно зависит от напряжения сети. Если напряжение в сети точно соответствует номинальному — 230 вольт, 60-ваттные лампы накаливания будет ярче светодиодной лампы, дающей 464 лм. Но далеко не везде в сети 230 вольт. По ГОСТ 29322-92 напряжение в сети может снижаться на 10%, то есть до 207 вольт. При таком напряжении светодиодная лампа, не меняющая яркость при снижении сетевого напряжения, уже будет ярче 60-ваттных ламп накаливания, а при напряжении 210 вольт их яркости будут приблизительно равны. В некоторых (обычно сельских) районах напряжение в сети иногда снижается даже до 180 вольт. На таком напряжении 60-ваттные лампы будут светить совсем тускло — около 250 лм (столько даёт 25-ваттная лампа накаливания на номинальном напряжении), а светодиодная лампа будет светить всё так же ярко, как и на номинальном напряжении. Не может не радовать, что на прилавках сети продуктовых магазинов, охватывающих всю страну, наконец-то появились дешёвые и качественные лампы. Пусть они не самые яркие, зато в отличие от того хлама, который продавался раньше и продолжает продаваться в других продуктовых магазинах, они не портят глаза покупателей пульсацией, не искажают цвета плохой цветопередачей и, надеюсь, прослужат долго. © 2017, Алексей Надёжин\n","metric":0.58837890630000000414,"metric_input":"Текст статьи:\nСветодиодная лампа Philips 7 Вт из Дикси / Хабр               \n\n26-10-2017\nВ магазинах Дикси по всей стране появились в продаже светодиодные лампы Philips 7 Вт по цене 99 рублей. Я тестирую много светодиодных ламп, но для того, чтобы их купить надо куда-то ехать или заказывать их в интернете, а эту лампочку будут покупать вместе с продуктами в магазинах у дома, поэтому очень важно, чтобы она была хорошей и безопасной, тем более что все предыдущие светодиодные лампы, продававшиеся в сетевых магазинах «у дома» сложно было назвать хорошими. Я зашёл в ближайший магазин и купил там лампу с тёплым светом. Производитель приводит на упаковке следующие параметры: Мощность: 7 Вт; Рабочее напряжение: 220-240 В; Световой поток: 480 лм; Заменяет лампу накаливания 55 Вт. Срок службы более 8 лет Гарантийный срок 2 года. Точных значений цветовой температуры, индекса цветопередачи и срока службы на упаковке нет. Заявлено, что «лампы полностью соответствуют заявленному световому потоку. Минимальный уровень пульсации. Лампы полностью безопасны и комфортны для глаз». Проверим. Световой поток лампы измерялся прибором Viso Light Spion, потребляемая мощность прибором Robiton PM-2, индекс цветопередачи, цветовая температура и пульсация прибором Uprtek MK350D. Перед тестированием лампа прогревалась в течение часа. Измеренный световой поток (464 лм) оказался ниже заявленного всего на 3% — это в пределах погрешности измерений. Потребляемая мощность даже меньше заявленной — всего 5.35 Вт. Цветовая температура около 3100К. Такой свет более белый, чем у ламп накаливания, но вполне комфортный и он гораздо «теплее» света ламп с белым светом 4000К. Пульсация света полностью отсутствует. Индекс цветопередачи 83.3 и это хорошо (считается, что для жилых помещений CRI должен быть больше 80). Удивительно, но на сайте производителя для этой лампы заявлен индекс цветопередачи 70. Впервые сталкиваюсь с тем, что производитель заявляет параметры хуже, чем они есть на самом деле. Возможно на сайте просто опечатка. Хоть на коробке указан диапазон рабочих напряжений 220-240 вольт, лампа работает при понижении напряжения питания до 98 вольт, при этом световой поток составляет не менее 90% от номинального. На упаковке указано (правда очень мелким шрифтом на наклейке), что лампу нельзя использовать с диммерами и выключателями с индикаторами. Я проверил работу лампы с выключателем, имеющим индикатор. Когда выключатель выключен, лампа слабо и равномерно светится. Скорее всего это никак не повредит лампе и её можно использовать с таким выключателем в режиме ночника, почти не потребляющего энергию. Результаты измерений прибором Viso Light Spion (в таблице я привожу результаты измерений более точными приборами, поэтому значения напряжения, мощности, цветовой температуры и индекса цветопередачи немного отличаются). Эта лампа, несмотря на низкую цену, действительно даёт качественный, комфортный и безопасный свет. При работе лампа нагревается не сильно — максимальная температура корпуса 49 градусов. Это означает, что у лампы есть шансы проработать достаточно долго, хоть на сайте и заявляется совсем небольшой для светодиодных ламп срок службы — 8000 часов. Единственный вопрос, который возникает к этой лампе, достаточно ли света она даёт и какую лампу накаливания она сможет полноценно заменить. Для того, чтобы получить точный ответ на этот вопрос я дополнительно протестировал две обычные матовые лампы накаливания на 60 Вт — OSRAM и Philips. С помощью стабилизатора Штиль Инстаб 500, ЛАТРа Suntek TDGC2-0.5 и точного мультиметра Aneng AN8001 я подавал на лампы различные напряжения и замерял световой поток. На упаковке обеих ламп указано рабочее напряжение 230 В и световой поток 710 лм. Фактически они дают меньше света и его количество очень сильно зависит от напряжения сети. Если напряжение в сети точно соответствует номинальному — 230 вольт, 60-ваттные лампы накаливания будет ярче светодиодной лампы, дающей 464 лм. Но далеко не везде в сети 230 вольт. По ГОСТ 29322-92 напряжение в сети может снижаться на 10%, то есть до 207 вольт. При таком напряжении светодиодная лампа, не меняющая яркость при снижении сетевого напряжения, уже будет ярче 60-ваттных ламп накаливания, а при напряжении 210 вольт их яркости будут приблизительно равны. В некоторых (обычно сельских) районах напряжение в сети иногда снижается даже до 180 вольт. На таком напряжении 60-ваттные лампы будут светить совсем тускло — около 250 лм (столько даёт 25-ваттная лампа накаливания на номинальном напряжении), а светодиодная лампа будет светить всё так же ярко, как и на номинальном напряжении. Не может не радовать, что на прилавках сети продуктовых магазинов, охватывающих всю страну, наконец-то появились дешёвые и качественные лампы. Пусть они не самые яркие, зато в отличие от того хлама, который продавался раньше и продолжает продаваться в других продуктовых магазинах, они не портят глаза покупателей пульсацией, не искажают цвета плохой цветопередачей и, надеюсь, прослужат долго. © 2017, Алексей Надёжин\n\n\nКраткое содержание:\nСветодиодная лампа Philips 7 Вт, продающаяся в магазинах Дикси, была протестирована автором статьи. Результаты показали, что лампа дает качественный, комфортный и безопасный свет. Несмотря на низкую цену, световой поток лампы оказался ниже заявленного всего на 3%, что является в пределах погрешности измерений. Потребляемая мощность даже меньше заявленной. Цветовая температура около 3100К, что более белый свет, чем у ламп накаливания, но вполне комфортный и он гораздо «теплее» света ламп с белым светом 4000К. Пульсация света полностью отсутствует. Индекс цветопередачи 83.3, что считается хорошим показателем для жилых помещений.\n\nДополнительно были протестированы две обычные матовые лампы накаливания на 60 Вт, которые дали меньше света и его количество сильно зависит от напряжения сети. Если напряжение в сети точно соответствует номинальному — 230 вольт, 60-ваттные лампы накаливания будут ярче светодиодной лампы, дающей 464 лм. Но далеко не везде в сети 230 вольт. По ГОСТ 29322-92 напряжение в сети может снижаться на 10%, то есть до 207 вольт. При таком напряжении светодиодная лампа, не меняющая яркость при снижении сетевого напряжения, уже будет ярче 60-ваттных ламп накаливания, а при напряжении 210 вольт их яркости будут приблизительно равны. В некоторых (обычно сельских) районах напряжение в сети иногда снижается даже до 180 вольт. На таком напряжении 60-ваттные лампы будут светить совсем тускло, а светодиодная лампа будет светить всё так же ярко, как и на номинальном напряжении.\n\nВ целом, автор статьи отмечает, что на прилавках сети продуктовых магазинов, охватывающих всю страну, наконец-то появились дешёвые и качественные лампы, которые не портят глаза покупателей пульсацией, не искажают цвета плохой цветопередачей и, надеюсь, прослужат долго.","summary":"Светодиодная лампа Philips 7 Вт, продающаяся в магазинах Дикси, была протестирована автором статьи. Результаты показали, что лампа дает качественный, комфортный и безопасный свет. Несмотря на низкую цену, световой поток лампы оказался ниже заявленного всего на 3%, что является в пределах погрешности измерений. Потребляемая мощность даже меньше заявленной. Цветовая температура около 3100К, что более белый свет, чем у ламп накаливания, но вполне комфортный и он гораздо «теплее» света ламп с белым светом 4000К. Пульсация света полностью отсутствует. Индекс цветопередачи 83.3, что считается хорошим показателем для жилых помещений.\n\nДополнительно были протестированы две обычные матовые лампы накаливания на 60 Вт, которые дали меньше света и его количество сильно зависит от напряжения сети. Если напряжение в сети точно соответствует номинальному — 230 вольт, 60-ваттные лампы накаливания будут ярче светодиодной лампы, дающей 464 лм. Но далеко не везде в сети 230 вольт. По ГОСТ 29322-92 напряжение в сети может снижаться на 10%, то есть до 207 вольт. При таком напряжении светодиодная лампа, не меняющая яркость при снижении сетевого напряжения, уже будет ярче 60-ваттных ламп накаливания, а при напряжении 210 вольт их яркости будут приблизительно равны. В некоторых (обычно сельских) районах напряжение в сети иногда снижается даже до 180 вольт. На таком напряжении 60-ваттные лампы будут светить совсем тускло, а светодиодная лампа будет светить всё так же ярко, как и на номинальном напряжении.\n\nВ целом, автор статьи отмечает, что на прилавках сети продуктовых магазинов, охватывающих всю страну, наконец-то появились дешёвые и качественные лампы, которые не портят глаза покупателей пульсацией, не искажают цвета плохой цветопередачей и, надеюсь, прослужат долго."}
{"Text":"Как ESLint анализирует код и борется с Legacy / Хабр                 \n\nРассказываем, как мы с фронтенд-разработчиком Дмитрием Балаевым @manmo убираем Legacy, какими Open Source конфигурациями для ESLint пользуемся и как статический анализатор кода повлиял на развитие разработчиков нашей компании.\n## Что такое Legacy и откуда он берется ##\nLegacy code — это тяжело поддерживаемый, некачественный или устаревший код, в котором невозможно разобраться (или можно, но очень долго). Часто под Legacy подразумевают код, которая команда получает в «наследство» от предыдущих разработчиков. Но не все так однозначно: код, который [хорошие] команды разработки передают заказчику, Legacy назвать нельзя, потому что в таком коде разобраться можно быстро и легко.\n\nОсновные причины Legacy:\n**Поддержка проекта с уже написанным кодом.**\n\nВ жизни любого разработчика наступает момент столкновения с «мамонтом». Мы работали над проектом, где код был написан еще в 2000 году. Тогда ни о каком ES6 нельзя было даже мечтать. За 20 лет на поддержке того проекта сменились сотни разработчиков. Каждый из них писал так, как ему удобно, используя свою методику и удобные технологии. Из-за подобных ситуаций не получается единый стиль написания кода, зато получаются костыли в JS-коде, неправильно используемые переменные в CSS и многое другое.\n\n**Лень разбираться в чужом коде и делать рефакторинг.**\n\nЭта причина вытекает из предыдущей. На готовых проектах Legacy практически неизбежен, но он может возникнуть и при создании абсолютно новых продуктов — об этом в следующем пункте.\n\n**Нет четкой схемы работы и должного контроля на проекте.**\n\nДля работы над новым проектом, как правило, собирается команда из свободных ресурсов. Это могут быть разработчики разных грейдов – от Junior до Senior, которые до этого работали с другими стеками и другими требованиями. Если за новый проект с самого начала не взялся хороший тимлид и не выстроил подходящие требования, код опять получится без единого стиля со всеми вытекающими последствиями.\n\n**Нет Code review со стороны тимлидов.**\n\nЧеткие инструкции не отменяют человеческий фактор, поэтому даже в очень сильных командах нужны Code review. Нет Code review — есть Legacy и постоянные ковыряния в Pull Request’ах. Нужно помнить стилистику каждого проекта, а для разработчиков это тяжело. Code review должно решать эту проблему.\nВ идеале необходимо избавиться от Legacy, сделать рефакторинг, привести код к единообразному читабельному виду. Для старого 20-летнего проекта это очень трудозатратно, поскольку изменение небольшого куска кода может повлечь за собой обрушение всего проекта, и порой проще и целесообразнее переписать его с нуля. Но для нового проекта, в котором только начали проявляться признаки Legacy, ситуацию можно исправить — выстроить работу под руководством опытного тимлида и написать четкие требования для разработчиков.\n## Как убрать Legacy ##\nЕсть ряд инструментов, которые помогут сделать рефакторинг и «подогнать» код под единые стандарты. В этой статье мы рассмотрим несколько статических анализаторов кода  (далее SCA) – они анализируют программный код без ее реального выполнения.\n**JSLint**  — это SCA с веб-интерфейсом для программ на языке JavaScript, проверяющий их соответствие стандартам оформления кода. Его разработал американский программист и предприниматель Дуглас Крокфорд ( _Douglas Crockford_ ) в 2002 году. Дуглас занимается разработкой языка JavaScript. Он популяризировал формат данных JSON и разработал различные инструменты, связанные с JavaScript, такие как JSLint и JSMin. После установки анализатор сразу готов к работе. Недостатком является то, что JSLint не настраивается и не расширяется. Официальная документация очень слабая.\n**JSHint**  используется при разработке программного обеспечения для проверки соответствия исходного кода JavaScript правилам кодирования. JSHint был создан в 2011 году Антоном Ковалевым как форк проекта JSLint. В данный момент Антон работает  инженером по безопасности в Medium. JSHint имеет хорошую документацию и простую интеграцию в редакторы. Из минусов — сложно определить, какие правила вызывают ошибки.\n**ESLint**  — инструмент для выявления проблемных шаблонов, обнаруженных в коде JavaScript. Он был создан Николасом Закасом ( _Nikolas, Zakas_ ) в 2013 году. Николас участвовал в разработке Yahoo! User Interface (YUI) library , создал Cookie Utility , Profiler , YUI Test , три его книги о JavaScript переведены  на русский язык. С помощью анализатора ESLint можно устанавливать правила поиска ошибок и легко их находить. Для него доступно множество плагинов, любое правило можно переключить, многие из них имеют дополнительные настройки. Результат получается предсказуемым, плюс ко всему у ESLint удобная поддержка ES6 и реактивных фреймворков.\nНа всех своих проектах мы пользуемся ESLint, поскольку он является эволюцией предыдущих линтеров. Расскажу, как мы его настраиваем и поделюсь результатами внедрения.\n## Как настроить ESLint ##\nПри создании проектов на современных фреймворках анализатор уже может быть встроен при первоначальных настройках. К примеру, при инициализации нового проекта на Nuxt система предлагает установить ESLint:\n\nЧтобы установить ESLint в уже существующий проект, нужно добавить его через npm “npm install eslint –save-dev” и создать файл настроек “.eslintrc”. Для мгновенной подсветки ошибок можно настроить редактор кода. Дополнительные плагины также устанавливаются через npm.\nВ сети можно найти  множество Open Source конфигураций для ESLint. Это обычные node.js пакеты с префиксом “eslint-config-”, которые содержат в себе только конфигурацию ESLint от конкретной компании/команды/разработчика. Например, можно легко использовать готовые конфигурации от Google  или Airbnb .\nИногда может понадобиться отключить правила в конкретном месте или файле. Это можно сделать несколькими способами:\n```\n/* eslint-disable */\n\tconsole.log(‘test’);\n/* eslint-enable */\n```\nили\n```\nconsole.log(‘test’); // eslint-disable-line\n```\nтакже можно отключать конкретные правила:\n```\n/* eslint-disable no-console */\n\tconsole.log(‘test’);\n/* eslint-enable no-console */\n```\nили\n```\nconsole.log(‘test’); // eslint-disable-line no-console\n```\n```\n// eslint-disable-next-line no-console\nconsole.log(‘test’);\n```\nНе все разработчики согласны с внедрением ESLint, так как привыкли писать код по-своему и не хотят перестраиваться. Бывали случаи, что некоторые разработчики просто забивали на правила, делали пулл с ошибками. В данном случае может помочь библиотека pre-commit , которая не позволяет сделать коммит, если есть ошибки. Но это может привести к тому, что кто-то отключит определенные настройки или сам ESLint. В таком случае поможет только должный контроль и Code review на проекте.\nКогда поддерживаешь проект с уже настроенным ESLint, необходимо следовать этим правилам. Не нужно что-то отключать или изменять. Как говорится, «со своей лопатой в чужой огород не лезут», даже если нам чужой конфиг не привычен.\n## Что в итоге ##\nЧтобы полностью избавиться от Legacy, нужно потратить немало времени на кропотливую ручную работу. Сам ESLint сделать этого не сможет, но вот помочь в этом и облегчить тяжелый труд – вполне.\n\nВот как на нашу компанию повлияло внедрение ESLint:\nЭкономим времени на поддержке проектов порядка 30%.\n\nБыстрее и легче обучаем новых сотрудников/стажеров/джунов. Они сразу видят как писать более чистый код.\n\nСовершенствуем код: он становится чище в плане количества библиотек, потому что не лень писать свой кастомный код и переносить наработки с прошлых проектов.\n\nУстраняем очевидные, банальные ошибки, на поиск которых иногда требуется время, например запись в уже ранее созданную переменную или пропущенный символ.\n\nОсвобождаем время на проверке своего кода. Теперь разработчики уделяют освободившиеся часы его на более качественную верстку макета, креативную анимацию и так далее. Это позволяет получать больше крутых проектов.","metric":0.6337890625,"metric_input":"Текст статьи:\nКак ESLint анализирует код и борется с Legacy / Хабр                 \n\nРассказываем, как мы с фронтенд-разработчиком Дмитрием Балаевым @manmo убираем Legacy, какими Open Source конфигурациями для ESLint пользуемся и как статический анализатор кода повлиял на развитие разработчиков нашей компании.\n## Что такое Legacy и откуда он берется ##\nLegacy code — это тяжело поддерживаемый, некачественный или устаревший код, в котором невозможно разобраться (или можно, но очень долго). Часто под Legacy подразумевают код, которая команда получает в «наследство» от предыдущих разработчиков. Но не все так однозначно: код, который [хорошие] команды разработки передают заказчику, Legacy назвать нельзя, потому что в таком коде разобраться можно быстро и легко.\n\nОсновные причины Legacy:\n**Поддержка проекта с уже написанным кодом.**\n\nВ жизни любого разработчика наступает момент столкновения с «мамонтом». Мы работали над проектом, где код был написан еще в 2000 году. Тогда ни о каком ES6 нельзя было даже мечтать. За 20 лет на поддержке того проекта сменились сотни разработчиков. Каждый из них писал так, как ему удобно, используя свою методику и удобные технологии. Из-за подобных ситуаций не получается единый стиль написания кода, зато получаются костыли в JS-коде, неправильно используемые переменные в CSS и многое другое.\n\n**Лень разбираться в чужом коде и делать рефакторинг.**\n\nЭта причина вытекает из предыдущей. На готовых проектах Legacy практически неизбежен, но он может возникнуть и при создании абсолютно новых продуктов — об этом в следующем пункте.\n\n**Нет четкой схемы работы и должного контроля на проекте.**\n\nДля работы над новым проектом, как правило, собирается команда из свободных ресурсов. Это могут быть разработчики разных грейдов – от Junior до Senior, которые до этого работали с другими стеками и другими требованиями. Если за новый проект с самого начала не взялся хороший тимлид и не выстроил подходящие требования, код опять получится без единого стиля со всеми вытекающими последствиями.\n\n**Нет Code review со стороны тимлидов.**\n\nЧеткие инструкции не отменяют человеческий фактор, поэтому даже в очень сильных командах нужны Code review. Нет Code review — есть Legacy и постоянные ковыряния в Pull Request’ах. Нужно помнить стилистику каждого проекта, а для разработчиков это тяжело. Code review должно решать эту проблему.\nВ идеале необходимо избавиться от Legacy, сделать рефакторинг, привести код к единообразному читабельному виду. Для старого 20-летнего проекта это очень трудозатратно, поскольку изменение небольшого куска кода может повлечь за собой обрушение всего проекта, и порой проще и целесообразнее переписать его с нуля. Но для нового проекта, в котором только начали проявляться признаки Legacy, ситуацию можно исправить — выстроить работу под руководством опытного тимлида и написать четкие требования для разработчиков.\n## Как убрать Legacy ##\nЕсть ряд инструментов, которые помогут сделать рефакторинг и «подогнать» код под единые стандарты. В этой статье мы рассмотрим несколько статических анализаторов кода  (далее SCA) – они анализируют программный код без ее реального выполнения.\n**JSLint**  — это SCA с веб-интерфейсом для программ на языке JavaScript, проверяющий их соответствие стандартам оформления кода. Его разработал американский программист и предприниматель Дуглас Крокфорд ( _Douglas Crockford_ ) в 2002 году. Дуглас занимается разработкой языка JavaScript. Он популяризировал формат данных JSON и разработал различные инструменты, связанные с JavaScript, такие как JSLint и JSMin. После установки анализатор сразу готов к работе. Недостатком является то, что JSLint не настраивается и не расширяется. Официальная документация очень слабая.\n**JSHint**  используется при разработке программного обеспечения для проверки соответствия исходного кода JavaScript правилам кодирования. JSHint был создан в 2011 году Антоном Ковалевым как форк проекта JSLint. В данный момент Антон работает  инженером по безопасности в Medium. JSHint имеет хорошую документацию и простую интеграцию в редакторы. Из минусов — сложно определить, какие правила вызывают ошибки.\n**ESLint**  — инструмент для выявления проблемных шаблонов, обнаруженных в коде JavaScript. Он был создан Николасом Закасом ( _Nikolas, Zakas_ ) в 2013 году. Николас участвовал в разработке Yahoo! User Interface (YUI) library , создал Cookie Utility , Profiler , YUI Test , три его книги о JavaScript переведены  на русский язык. С помощью анализатора ESLint можно устанавливать правила поиска ошибок и легко их находить. Для него доступно множество плагинов, любое правило можно переключить, многие из них имеют дополнительные настройки. Результат получается предсказуемым, плюс ко всему у ESLint удобная поддержка ES6 и реактивных фреймворков.\nНа всех своих проектах мы пользуемся ESLint, поскольку он является эволюцией предыдущих линтеров. Расскажу, как мы его настраиваем и поделюсь результатами внедрения.\n## Как настроить ESLint ##\nПри создании проектов на современных фреймворках анализатор уже может быть встроен при первоначальных настройках. К примеру, при инициализации нового проекта на Nuxt система предлагает установить ESLint:\n\nЧтобы установить ESLint в уже существующий проект, нужно добавить его через npm “npm install eslint –save-dev” и создать файл настроек “.eslintrc”. Для мгновенной подсветки ошибок можно настроить редактор кода. Дополнительные плагины также устанавливаются через npm.\nВ сети можно найти  множество Open Source конфигураций для ESLint. Это обычные node.js пакеты с префиксом “eslint-config-”, которые содержат в себе только конфигурацию ESLint от конкретной компании/команды/разработчика. Например, можно легко использовать готовые конфигурации от Google  или Airbnb .\nИногда может понадобиться отключить правила в конкретном месте или файле. Это можно сделать несколькими способами:\n```\n/* eslint-disable */\n\tconsole.log(‘test’);\n/* eslint-enable */\n```\nили\n```\nconsole.log(‘test’); // eslint-disable-line\n```\nтакже можно отключать конкретные правила:\n```\n/* eslint-disable no-console */\n\tconsole.log(‘test’);\n/* eslint-enable no-console */\n```\nили\n```\nconsole.log(‘test’); // eslint-disable-line no-console\n```\n```\n// eslint-disable-next-line no-console\nconsole.log(‘test’);\n```\nНе все разработчики согласны с внедрением ESLint, так как привыкли писать код по-своему и не хотят перестраиваться. Бывали случаи, что некоторые разработчики просто забивали на правила, делали пулл с ошибками. В данном случае может помочь библиотека pre-commit , которая не позволяет сделать коммит, если есть ошибки. Но это может привести к тому, что кто-то отключит определенные настройки или сам ESLint. В таком случае поможет только должный контроль и Code review на проекте.\nКогда поддерживаешь проект с уже настроенным ESLint, необходимо следовать этим правилам. Не нужно что-то отключать или изменять. Как говорится, «со своей лопатой в чужой огород не лезут», даже если нам чужой конфиг не привычен.\n## Что в итоге ##\nЧтобы полностью избавиться от Legacy, нужно потратить немало времени на кропотливую ручную работу. Сам ESLint сделать этого не сможет, но вот помочь в этом и облегчить тяжелый труд – вполне.\n\nВот как на нашу компанию повлияло внедрение ESLint:\nЭкономим времени на поддержке проектов порядка 30%.\n\nБыстрее и легче обучаем новых сотрудников/стажеров/джунов. Они сразу видят как писать более чистый код.\n\nСовершенствуем код: он становится чище в плане количества библиотек, потому что не лень писать свой кастомный код и переносить наработки с прошлых проектов.\n\nУстраняем очевидные, банальные ошибки, на поиск которых иногда требуется время, например запись в уже ранее созданную переменную или пропущенный символ.\n\nОсвобождаем время на проверке своего кода. Теперь разработчики уделяют освободившиеся часы его на более качественную верстку макета, креативную анимацию и так далее. Это позволяет получать больше крутых проектов.\n\nКраткое содержание:\nESLint - это статический анализатор кода, который используется для проверки и улучшения качества кода. Он помогает бороться с Legacy кодом, который является тяжело поддерживаемым, некачественным или устаревшим кодом, в котором невозможно разобраться.\n\nДля борьбы с Legacy кодом используются различные инструменты, такие как JSLint, JSHint и ESLint. ESLint является наиболее популярным и удобным инструментом для выявления проблемных шаблонов в коде JavaScript.\n\nESLint можно настроить под конкретные требования проекта, используя Open Source конфигурации. Не все разработчики согласны с внедрением ESLint, так как привыкли писать код по-своему и не хотят перестраиваться.\n\nВ итоге, ESLint помогает экономить время на поддержке проектов, быстрее и легче обучать новых сотрудников, совершенствовать код, устранять ошибки и освобождать время на проверку своего кода.","summary":"ESLint - это статический анализатор кода, который используется для проверки и улучшения качества кода. Он помогает бороться с Legacy кодом, который является тяжело поддерживаемым, некачественным или устаревшим кодом, в котором невозможно разобраться.\n\nДля борьбы с Legacy кодом используются различные инструменты, такие как JSLint, JSHint и ESLint. ESLint является наиболее популярным и удобным инструментом для выявления проблемных шаблонов в коде JavaScript.\n\nESLint можно настроить под конкретные требования проекта, используя Open Source конфигурации. Не все разработчики согласны с внедрением ESLint, так как привыкли писать код по-своему и не хотят перестраиваться.\n\nВ итоге, ESLint помогает экономить время на поддержке проектов, быстрее и легче обучать новых сотрудников, совершенствовать код, устранять ошибки и освобождать время на проверку своего кода."}
{"Text":"Правильное использование require в node.js / Habr                       \n\n15-06-2016\nПредисловие Не так давно проект, на котором я работаю в данный момент, начал использовать модульную систему ES2015. Я не буду заострять внимание на этой технологии JavaScript, т.к статья совсем не об этом, а о том как технология сподвигла меня к одной мысли. Как многие знают, ES2015 Modules представляют собой импортирование/экспортирование скриптов крайне схожее по синтаксису с python и многими другими языками программирования. Пример: // Helper.js export function includes(array, variable) { return array.indexOf(variable) !== -1; } // main.js import {includes} from 'Helper'; assets(includes([1,2,3], 2), true); Все, кто интересовался модулями JavaScript знают, что импортирование и экспортирование возможно только на верхнем уровне модуля (файла с кодом). Следующий грубый пример кода вызовет ошибки: // sendEmail.js export default function sendEmails(emails_list) { import sender from 'sender'; export sender; // сделать что-то } Exception: SyntaxError: import/export declarations may only appear at top level of a module В отличие от ES2015 Modules — в модульной системе node.js импортирование и экспортирование возможны на любом уровне вложенности. Аналогичный код на node.js не вызовет ошибку: // sendEmail.js module.exports = function sendEmails(emails_list) { const sender = require('sender'); exports.sender = sender; // сделать что-то } Преимущество такого способа в том, что модули необходимые в обработчике явно импортированы внутри и не засоряют пространство имен модуля (особенно актуально, если импортируемый модуль нужен только в одном обработчике). Так же появляется возможность отложенного экспортирования данных модуля. Основные минусы: Об отсутствии модуля вы узнаете только во время вызова соответствующего обработчика Путь к импортироемому модулю может измениться, что приведет к изменению в каждом месте импортирования (например, в вашем модуле, в различных обработчиках используется lodash/object/defaults и вы решили обновиться до 4.x версии, где подключать нужно lodash/defaults). Разбор полетов В большинстве задач для которых используется node.js — front-end или основной веб-сервер, и высокая нагрузка на node.js частое явление. Пропуская способность вашего сервера должны быть максимально возможная. Измерение пропускной способности Для измерения пропускной способности веб-сервера используется великолепная утилита от Apache — ab. Если вы еще с ней не знакомы, то настоятельно рекомендую это сделать. Код веб-сервера одинаков за исключением обработчиков. Тест запускался на node.js 6.0 с использованием модуля ifnode, сделанного на базе express Импортирование модулей непосредственно в обработчик Код: const app = require('ifnode')(); const RequireTestingController = app.Controller({ root: '/', map: { 'GET /not_imported': 'notImportedAction' } }); RequireTestingController.notImportedAction = function(request, response, next) { const data = { message: 'test internal and external require' }; const _defaults = require('lodash/object/defaults'); const _assign = require('lodash/object/assign'); const _clone = require('lodash/lang/clone'); response.ok({ _defaults: _defaults(data, { lodash: 'defaults' }), _assign: _assign(data, { lodash: 'assign' }), _clone: _clone(data) }); }; Результат: $ ab -n 15000 -c 30 -q \"http://localhost:8080/not_imported\" Server Hostname: localhost Server Port: 8080 Document Path: /not_imported Document Length: 233 bytes Concurrency Level: 30 Time taken for tests: 4.006 seconds Complete requests: 15000 Failed requests: 0 Total transferred: 6195000 bytes HTML transferred: 3495000 bytes Requests per second: 3744.32 [#/sec] (mean) Time per request: 8.012 [ms] (mean) Time per request: 0.267 [ms] (mean, across all concurrent requests) Transfer rate: 1510.16 [Kbytes/sec] received Percentage of the requests served within a certain time (ms) 50% 6 66% 7 75% 8 80% 8 90% 10 95% 15 98% 17 99% 20 100% 289 (longest request) Импортирование модулей в начале файла Код: const app = require('ifnode')(); const _defaults = require('lodash/object/defaults'); const _assign = require('lodash/object/assign'); const _clone = require('lodash/lang/clone'); const RequireTestingController = app.Controller({ root: '/', map: { 'GET /already_imported': 'alreadyImportedAction' } }); RequireTestingController.alreadyImportedAction = function(request, response, next) { const data = { message: 'test internal and external require' }; response.ok({ _defaults: _defaults(data, { lodash: 'defaults' }), _assign: _assign(data, { lodash: 'assign' }), _clone: _clone(data) }); }; Результат: $ ab -n 15000 -c 30 -q \"http://localhost:8080/already_imported\" Server Hostname: localhost Server Port: 8080 Document Path: /already_imported Document Length: 233 bytes Concurrency Level: 30 Time taken for tests: 3.241 seconds Complete requests: 15000 Failed requests: 0 Total transferred: 6195000 bytes HTML transferred: 3495000 bytes Requests per second: 4628.64 [#/sec] (mean) Time per request: 6.481 [ms] (mean) Time per request: 0.216 [ms] (mean, across all concurrent requests) Transfer rate: 1866.83 [Kbytes/sec] received Percentage of the requests served within a certain time (ms) 50% 5 66% 6 75% 6 80% 7 90% 8 95% 14 98% 17 99% 20 100% 38 (longest request) Анализ результатов Импортирование модулей в начале файла уменьшило время одного запроса на ~23%(!) (в сравнение с импортированием непосредственно в обработчик), что весьма существенно. Такая большая разница в результатах кроется в работе функции require. Перед импортированием, require обращается к алгоритму поиска абсолютного пути к запрашиваемому компоненту (алгоритм описан в документации node.js). Когда путь был найден, то require проверяет был ли закеширован модуль, и если нет — не делает ничего сверхестественного, кроме вызова обычного fs.readFileSync для .js и .json форматов, и недокументированного process.dlopen для загрузки C++ модулей. Note: пробовал \"прогревать\" кеш для случая с непосредственным импортированием модулей в обработчик (перед запуском утилиты ab, модули были уже закешированы) — производительность улучшалась на 1-2%. Выводы Если вы используете node.js, как сервер (нет разницы какой — TCP/UDP или HTTP(S)), то: Импортирование всех модулей необходимо делать в начале файла, чтобы избегать лишних синхронных операций связанных с загрузкой модулей (один из главных анти-паттернов использования node.js как асинхронного сервера). Вы можете не тратить ресурсы на вычисление абсолютного пути запрашиваемого модуля (это и есть основное место для потери производительности).\n","metric":0.56103515630000000414,"metric_input":"Текст статьи:\nПравильное использование require в node.js / Habr                       \n\n15-06-2016\nПредисловие Не так давно проект, на котором я работаю в данный момент, начал использовать модульную систему ES2015. Я не буду заострять внимание на этой технологии JavaScript, т.к статья совсем не об этом, а о том как технология сподвигла меня к одной мысли. Как многие знают, ES2015 Modules представляют собой импортирование/экспортирование скриптов крайне схожее по синтаксису с python и многими другими языками программирования. Пример: // Helper.js export function includes(array, variable) { return array.indexOf(variable) !== -1; } // main.js import {includes} from 'Helper'; assets(includes([1,2,3], 2), true); Все, кто интересовался модулями JavaScript знают, что импортирование и экспортирование возможно только на верхнем уровне модуля (файла с кодом). Следующий грубый пример кода вызовет ошибки: // sendEmail.js export default function sendEmails(emails_list) { import sender from 'sender'; export sender; // сделать что-то } Exception: SyntaxError: import/export declarations may only appear at top level of a module В отличие от ES2015 Modules — в модульной системе node.js импортирование и экспортирование возможны на любом уровне вложенности. Аналогичный код на node.js не вызовет ошибку: // sendEmail.js module.exports = function sendEmails(emails_list) { const sender = require('sender'); exports.sender = sender; // сделать что-то } Преимущество такого способа в том, что модули необходимые в обработчике явно импортированы внутри и не засоряют пространство имен модуля (особенно актуально, если импортируемый модуль нужен только в одном обработчике). Так же появляется возможность отложенного экспортирования данных модуля. Основные минусы: Об отсутствии модуля вы узнаете только во время вызова соответствующего обработчика Путь к импортироемому модулю может измениться, что приведет к изменению в каждом месте импортирования (например, в вашем модуле, в различных обработчиках используется lodash/object/defaults и вы решили обновиться до 4.x версии, где подключать нужно lodash/defaults). Разбор полетов В большинстве задач для которых используется node.js — front-end или основной веб-сервер, и высокая нагрузка на node.js частое явление. Пропуская способность вашего сервера должны быть максимально возможная. Измерение пропускной способности Для измерения пропускной способности веб-сервера используется великолепная утилита от Apache — ab. Если вы еще с ней не знакомы, то настоятельно рекомендую это сделать. Код веб-сервера одинаков за исключением обработчиков. Тест запускался на node.js 6.0 с использованием модуля ifnode, сделанного на базе express Импортирование модулей непосредственно в обработчик Код: const app = require('ifnode')(); const RequireTestingController = app.Controller({ root: '/', map: { 'GET /not_imported': 'notImportedAction' } }); RequireTestingController.notImportedAction = function(request, response, next) { const data = { message: 'test internal and external require' }; const _defaults = require('lodash/object/defaults'); const _assign = require('lodash/object/assign'); const _clone = require('lodash/lang/clone'); response.ok({ _defaults: _defaults(data, { lodash: 'defaults' }), _assign: _assign(data, { lodash: 'assign' }), _clone: _clone(data) }); }; Результат: $ ab -n 15000 -c 30 -q \"http://localhost:8080/not_imported\" Server Hostname: localhost Server Port: 8080 Document Path: /not_imported Document Length: 233 bytes Concurrency Level: 30 Time taken for tests: 4.006 seconds Complete requests: 15000 Failed requests: 0 Total transferred: 6195000 bytes HTML transferred: 3495000 bytes Requests per second: 3744.32 [#/sec] (mean) Time per request: 8.012 [ms] (mean) Time per request: 0.267 [ms] (mean, across all concurrent requests) Transfer rate: 1510.16 [Kbytes/sec] received Percentage of the requests served within a certain time (ms) 50% 6 66% 7 75% 8 80% 8 90% 10 95% 15 98% 17 99% 20 100% 289 (longest request) Импортирование модулей в начале файла Код: const app = require('ifnode')(); const _defaults = require('lodash/object/defaults'); const _assign = require('lodash/object/assign'); const _clone = require('lodash/lang/clone'); const RequireTestingController = app.Controller({ root: '/', map: { 'GET /already_imported': 'alreadyImportedAction' } }); RequireTestingController.alreadyImportedAction = function(request, response, next) { const data = { message: 'test internal and external require' }; response.ok({ _defaults: _defaults(data, { lodash: 'defaults' }), _assign: _assign(data, { lodash: 'assign' }), _clone: _clone(data) }); }; Результат: $ ab -n 15000 -c 30 -q \"http://localhost:8080/already_imported\" Server Hostname: localhost Server Port: 8080 Document Path: /already_imported Document Length: 233 bytes Concurrency Level: 30 Time taken for tests: 3.241 seconds Complete requests: 15000 Failed requests: 0 Total transferred: 6195000 bytes HTML transferred: 3495000 bytes Requests per second: 4628.64 [#/sec] (mean) Time per request: 6.481 [ms] (mean) Time per request: 0.216 [ms] (mean, across all concurrent requests) Transfer rate: 1866.83 [Kbytes/sec] received Percentage of the requests served within a certain time (ms) 50% 5 66% 6 75% 6 80% 7 90% 8 95% 14 98% 17 99% 20 100% 38 (longest request) Анализ результатов Импортирование модулей в начале файла уменьшило время одного запроса на ~23%(!) (в сравнение с импортированием непосредственно в обработчик), что весьма существенно. Такая большая разница в результатах кроется в работе функции require. Перед импортированием, require обращается к алгоритму поиска абсолютного пути к запрашиваемому компоненту (алгоритм описан в документации node.js). Когда путь был найден, то require проверяет был ли закеширован модуль, и если нет — не делает ничего сверхестественного, кроме вызова обычного fs.readFileSync для .js и .json форматов, и недокументированного process.dlopen для загрузки C++ модулей. Note: пробовал \"прогревать\" кеш для случая с непосредственным импортированием модулей в обработчик (перед запуском утилиты ab, модули были уже закешированы) — производительность улучшалась на 1-2%. Выводы Если вы используете node.js, как сервер (нет разницы какой — TCP/UDP или HTTP(S)), то: Импортирование всех модулей необходимо делать в начале файла, чтобы избегать лишних синхронных операций связанных с загрузкой модулей (один из главных анти-паттернов использования node.js как асинхронного сервера). Вы можете не тратить ресурсы на вычисление абсолютного пути запрашиваемого модуля (это и есть основное место для потери производительности).\n\n\nКраткое содержание:\nСтатья рассматривает использование require в Node.js, сравнивая два способа импортирования модулей: непосредственно в обработчик и в начале файла. Эксперименты показали, что импорт всех модулей в начале файла позволяет увеличить производительность сервера на 23%. Это связано с тем, что при импорте в начале файла не требуется вычислять абсолютный путь к модулю, что может быть ресурсоемким процессом. Автор рекомендует использовать такой подход для избежания потери производительности.","summary":"Статья рассматривает использование require в Node.js, сравнивая два способа импортирования модулей: непосредственно в обработчик и в начале файла. Эксперименты показали, что импорт всех модулей в начале файла позволяет увеличить производительность сервера на 23%. Это связано с тем, что при импорте в начале файла не требуется вычислять абсолютный путь к модулю, что может быть ресурсоемким процессом. Автор рекомендует использовать такой подход для избежания потери производительности."}
{"Text":"Пять Docker-утилит, о которых вам стоит узнать / Habr                            \n\n_Источник  изображения_\n\nНа просторах сети Интернет можно найти немало полезных утилит для Docker . Многие из них принадлежат к разряду Open Source и доступны на Github. В последние два года я достаточно активно использую Docker в большинстве своих проектов по разработке программного обеспечения. Однажды начав работать с Docker, вы осознаете, что он оказывается полезен для гораздо более широкого круга задач, нежели вы изначально предполагали. Вам захочется сделать с Docker еще больше, и он не разочарует!\n\nDocker-сообщество живет активной жизнью, ежедневно производя новые полезные инструменты. За этой бурной деятельностью достаточно сложно уследить. Поэтому я решил выбрать несколько наиболее интересных и полезных из ежедневно используемых мной Docker-утилит. Они делают работу более продуктивной, автоматизируя операции, которые пришлось бы выполнять вручную.\n\nДавайте посмотрим на утилиты, которые помогают мне в процессе докеризации всего и вся.\n\n## 1. Watchtower  — автоматическое обновление Docker-контейнеров ##\n\nWatchtower мониторит выполняющиеся контейнеры и отслеживает изменения в образах, на основе которых они были созданы. Если образ изменился, Watchtower автоматически перезапускает контейнер, используя новый образ. Это удобно при локальной разработке, если есть желание работать с самыми новыми версиями используемых инструментов.\n\nУтилита Watchtower также поставляется в виде Docker-образа и выполняется в контейнере. Для ее запуска введите следующую команду:\n\nМы запустили Watchtower с примонтированным файлом **/var/run/docker.sock** . Это нужно для того, чтобы Watchtower мог взаимодействовать с демоном Docker через соответствующий API. Мы также передали опцию **interval** , равную 30 секундам, которая определяет интервал опроса. У Watchtower есть и другие опции, которые описаны в документации .\n\nДавайте запустим контейнер и помониторим его с помощью Watchtower.\n\nТеперь Watchtower начнет мониторинг контейнера **friendlyhello** . Если я размещу новый образ на Docker Hub, Watchtower во время очередного запуска это обнаружит. Затем она корректно остановит контейнер и перезапустит его из нового образа. Также контейнеру будут переданы указанные нами в команде `run`  опции, то есть он будет запущен с **-p 4000:80** .\n\nПо умолчанию Watchtower будет искать новые версии образов в реестре Docker Hub. При этом Watchtower может опрашивать закрытые реестры, используя для входа учетные данные из переменных окружения REPO\\_USER и REPO\\_PASS.\n\nБолее подробную информацию о Watchtower вы можете найти в документации .\n\n## 2. Docker-gc  — удаление ненужных контейнеров и образов ##\n\nУтилита docker-gc помогает очистить Docker-хост, удалив более не нужные образы и контейнеры. Удаляются контейнеры, завершенные более часа назад, а также образы, не принадлежащие ни одному из оставшихся контейнеров.\n\nDocker-gc может запускаться в виде скрипта или контейнера. Мы воспользуемся вторым вариантом. Запустим docker-gc, чтобы узнать, какие контейнеры и образы можно удалить.\n\nЧтобы docker-gc мог взаимодействовать с Docker через его API, мы подмонтировали сокет-файл Docker. Для поиска контейнеров и образов, которые можно удалить, запустим docker-gc с переменной окружения DRY\\_RUN=1. При этом никаких изменений на диске сделано не будет. Всегда лучше сначала убедиться, что docker-gc не собирается удалить ничего лишнего. Вот вывод этой команды:\n\nЕсли вы согласны с предложенным docker-gc планом очистки, запустите ту же команду, но теперь уже без DRY\\_RUN.\n\nВ выводе будут перечислены удаленные docker-gc контейнеры и образы.\n\nУ docker-gc есть еще несколько опций. Чтобы получить более подробную информацию об этой утилите, рекомендую почитать соответствующую документацию .\n\n## 3. Docker-slim  — волшебная пилюля, которая поможет вашим контейнерам похудеть ##\n\nОбеспокоены размером ваших Docker-образов? Вам поможет docker-slim!\n\nЭта утилита использует статический и динамический анализ для создания легковесных вариантов Docker-образов. Чтобы поработать с docker-slim, загрузите бинарник с Github  (доступны версии для Linux и Mac), а затем добавьте путь к исполняемому файлу в переменную окружения PATH.\n\nС помощью инструкции из официальной документации Docker  я создал образ для демонстрационного приложения **friendlyhello** . Размер образа составил 194 МБ.\n\nДля очень простого приложения нам пришлось загрузить 194 МБ! Посмотрим, сколько мегабайт можно скинуть с docker-slim.\n\nИнспектируя исходный образ, docker-slim выполнит несколько шагов, а затем создаст его облегченную версию. Посмотрим, какой в итоге получился размер:\n\nНа иллюстрации видно, что размер образа уменьшился до 24.9 МБ. Если запустить контейнер, то он будет работать точно так же, как и его предыдущая версия. Docker-slim хорошо справляется с приложениями на Java, Python, Ruby и Node.js.\n\nПопробуйте docker-slim сами, и, возможно, это позволит вам освободить немало места. Эта утилита корректно отработала практически во всех моих проектах. Получить более подробную информацию о docker-slim можно из соответствующей документации .\n\n## 4. Rocker  — преодолеваем ограничения Dockerfile ##\n\nБольшинство разработчиков для сборки образов предпочитают использовать Dockerfile. Dockerfile — это декларативный способ определения команд по созданию образа, которые пользователь мог бы выполнить в командной строке.\n\nRocker  добавляет в Dockerfile набор новых инструкций. Rocker был создан в Grammarly  для решения проблем, с которыми эта компания столкнулась при использовании Dockerfile. В Grammarly написали содержательную статью , объясняющую причины создания Rocker. Если у вас есть желание лучше разобраться с Rocker, рекомендую эту статью к прочтению. В ней отмечены две проблемы:\n\nРазмер образов.\n\nСкорость сборки.\n\nТам также упоминается несколько добавленных Rocker инструкций. Для получения информации о всех поддерживаемых Rocker инструкциях см. документацию .\n\nMOUNT — позволяет настроить совместное использование томов различными сборками, что удобно, например, для инструментов управления зависимостями.\n\nFROM — эта инструкция существует и в стандартном Dockerfile. При этом Rocker позволяет добавить несколько FROM в один файл. Это значит, что с помощью одного Rockerfile можно сделать несколько образов, например, один для сборки, а другой для выполнения приложения. Первый набор инструкций соберет артефакт, используя все необходимые зависимости. Второй набор инструкций может использовать артефакт, созданный на первом шаге. С помощью этого приема достигается значительное уменьшение размера образа.\n\nTAG позволяет на разных этапах сборки создавать метки образа, то есть вручную это делать больше не нужно.\n\nPUSH используется для загрузки образов в реестр.\n\nATTACH позволяет интерактивно подключаться к контейнеру на промежуточных этапах сборки, что очень удобно для отладки.\n\nRocker требует установки. Для пользователей Mac команды следующие:\n\nRockerfile выглядит вот так:\n\nДля сборки образа и загрузки его на Docker Hub выполните:\n\nУ Rocker очень интересная функциональность. Чтобы узнать больше, обратитесь к его документации .\n\n## 5. Ctop  — top-подобная утилита для вывода информации о контейнерах ##\n\nЯ начал пользоваться ctop сравнительно недавно. Эта утилита в реальном времени отображает метрики сразу нескольких контейнеров. Если вы используете Mac, то для установки ctop можете воспользоваться следующей командой:\n\nДля работы ctop требуется установка переменной окружения DOCKER\\_HOST.\n\nВыполните `ctop` , чтобы отобразить состояние всех контейнеров.\n\nДля вывода информации только о запущенных контейнерах выполните `ctop -a` .\n\nCtop проста в использовании и при этом весьма полезна. Более подробную информацию вы можете найти в документации .\n\nСсылки:\n\nОригинал: 5 Docker Utilities You Should Know .","metric":0.58349609380000000414,"metric_input":"Текст статьи:\nПять Docker-утилит, о которых вам стоит узнать / Habr                            \n\n_Источник  изображения_\n\nНа просторах сети Интернет можно найти немало полезных утилит для Docker . Многие из них принадлежат к разряду Open Source и доступны на Github. В последние два года я достаточно активно использую Docker в большинстве своих проектов по разработке программного обеспечения. Однажды начав работать с Docker, вы осознаете, что он оказывается полезен для гораздо более широкого круга задач, нежели вы изначально предполагали. Вам захочется сделать с Docker еще больше, и он не разочарует!\n\nDocker-сообщество живет активной жизнью, ежедневно производя новые полезные инструменты. За этой бурной деятельностью достаточно сложно уследить. Поэтому я решил выбрать несколько наиболее интересных и полезных из ежедневно используемых мной Docker-утилит. Они делают работу более продуктивной, автоматизируя операции, которые пришлось бы выполнять вручную.\n\nДавайте посмотрим на утилиты, которые помогают мне в процессе докеризации всего и вся.\n\n## 1. Watchtower  — автоматическое обновление Docker-контейнеров ##\n\nWatchtower мониторит выполняющиеся контейнеры и отслеживает изменения в образах, на основе которых они были созданы. Если образ изменился, Watchtower автоматически перезапускает контейнер, используя новый образ. Это удобно при локальной разработке, если есть желание работать с самыми новыми версиями используемых инструментов.\n\nУтилита Watchtower также поставляется в виде Docker-образа и выполняется в контейнере. Для ее запуска введите следующую команду:\n\nМы запустили Watchtower с примонтированным файлом **/var/run/docker.sock** . Это нужно для того, чтобы Watchtower мог взаимодействовать с демоном Docker через соответствующий API. Мы также передали опцию **interval** , равную 30 секундам, которая определяет интервал опроса. У Watchtower есть и другие опции, которые описаны в документации .\n\nДавайте запустим контейнер и помониторим его с помощью Watchtower.\n\nТеперь Watchtower начнет мониторинг контейнера **friendlyhello** . Если я размещу новый образ на Docker Hub, Watchtower во время очередного запуска это обнаружит. Затем она корректно остановит контейнер и перезапустит его из нового образа. Также контейнеру будут переданы указанные нами в команде `run`  опции, то есть он будет запущен с **-p 4000:80** .\n\nПо умолчанию Watchtower будет искать новые версии образов в реестре Docker Hub. При этом Watchtower может опрашивать закрытые реестры, используя для входа учетные данные из переменных окружения REPO\\_USER и REPO\\_PASS.\n\nБолее подробную информацию о Watchtower вы можете найти в документации .\n\n## 2. Docker-gc  — удаление ненужных контейнеров и образов ##\n\nУтилита docker-gc помогает очистить Docker-хост, удалив более не нужные образы и контейнеры. Удаляются контейнеры, завершенные более часа назад, а также образы, не принадлежащие ни одному из оставшихся контейнеров.\n\nDocker-gc может запускаться в виде скрипта или контейнера. Мы воспользуемся вторым вариантом. Запустим docker-gc, чтобы узнать, какие контейнеры и образы можно удалить.\n\nЧтобы docker-gc мог взаимодействовать с Docker через его API, мы подмонтировали сокет-файл Docker. Для поиска контейнеров и образов, которые можно удалить, запустим docker-gc с переменной окружения DRY\\_RUN=1. При этом никаких изменений на диске сделано не будет. Всегда лучше сначала убедиться, что docker-gc не собирается удалить ничего лишнего. Вот вывод этой команды:\n\nЕсли вы согласны с предложенным docker-gc планом очистки, запустите ту же команду, но теперь уже без DRY\\_RUN.\n\nВ выводе будут перечислены удаленные docker-gc контейнеры и образы.\n\nУ docker-gc есть еще несколько опций. Чтобы получить более подробную информацию об этой утилите, рекомендую почитать соответствующую документацию .\n\n## 3. Docker-slim  — волшебная пилюля, которая поможет вашим контейнерам похудеть ##\n\nОбеспокоены размером ваших Docker-образов? Вам поможет docker-slim!\n\nЭта утилита использует статический и динамический анализ для создания легковесных вариантов Docker-образов. Чтобы поработать с docker-slim, загрузите бинарник с Github  (доступны версии для Linux и Mac), а затем добавьте путь к исполняемому файлу в переменную окружения PATH.\n\nС помощью инструкции из официальной документации Docker  я создал образ для демонстрационного приложения **friendlyhello** . Размер образа составил 194 МБ.\n\nДля очень простого приложения нам пришлось загрузить 194 МБ! Посмотрим, сколько мегабайт можно скинуть с docker-slim.\n\nИнспектируя исходный образ, docker-slim выполнит несколько шагов, а затем создаст его облегченную версию. Посмотрим, какой в итоге получился размер:\n\nНа иллюстрации видно, что размер образа уменьшился до 24.9 МБ. Если запустить контейнер, то он будет работать точно так же, как и его предыдущая версия. Docker-slim хорошо справляется с приложениями на Java, Python, Ruby и Node.js.\n\nПопробуйте docker-slim сами, и, возможно, это позволит вам освободить немало места. Эта утилита корректно отработала практически во всех моих проектах. Получить более подробную информацию о docker-slim можно из соответствующей документации .\n\n## 4. Rocker  — преодолеваем ограничения Dockerfile ##\n\nБольшинство разработчиков для сборки образов предпочитают использовать Dockerfile. Dockerfile — это декларативный способ определения команд по созданию образа, которые пользователь мог бы выполнить в командной строке.\n\nRocker  добавляет в Dockerfile набор новых инструкций. Rocker был создан в Grammarly  для решения проблем, с которыми эта компания столкнулась при использовании Dockerfile. В Grammarly написали содержательную статью , объясняющую причины создания Rocker. Если у вас есть желание лучше разобраться с Rocker, рекомендую эту статью к прочтению. В ней отмечены две проблемы:\n\nРазмер образов.\n\nСкорость сборки.\n\nТам также упоминается несколько добавленных Rocker инструкций. Для получения информации о всех поддерживаемых Rocker инструкциях см. документацию .\n\nMOUNT — позволяет настроить совместное использование томов различными сборками, что удобно, например, для инструментов управления зависимостями.\n\nFROM — эта инструкция существует и в стандартном Dockerfile. При этом Rocker позволяет добавить несколько FROM в один файл. Это значит, что с помощью одного Rockerfile можно сделать несколько образов, например, один для сборки, а другой для выполнения приложения. Первый набор инструкций соберет артефакт, используя все необходимые зависимости. Второй набор инструкций может использовать артефакт, созданный на первом шаге. С помощью этого приема достигается значительное уменьшение размера образа.\n\nTAG позволяет на разных этапах сборки создавать метки образа, то есть вручную это делать больше не нужно.\n\nPUSH используется для загрузки образов в реестр.\n\nATTACH позволяет интерактивно подключаться к контейнеру на промежуточных этапах сборки, что очень удобно для отладки.\n\nRocker требует установки. Для пользователей Mac команды следующие:\n\nRockerfile выглядит вот так:\n\nДля сборки образа и загрузки его на Docker Hub выполните:\n\nУ Rocker очень интересная функциональность. Чтобы узнать больше, обратитесь к его документации .\n\n## 5. Ctop  — top-подобная утилита для вывода информации о контейнерах ##\n\nЯ начал пользоваться ctop сравнительно недавно. Эта утилита в реальном времени отображает метрики сразу нескольких контейнеров. Если вы используете Mac, то для установки ctop можете воспользоваться следующей командой:\n\nДля работы ctop требуется установка переменной окружения DOCKER\\_HOST.\n\nВыполните `ctop` , чтобы отобразить состояние всех контейнеров.\n\nДля вывода информации только о запущенных контейнерах выполните `ctop -a` .\n\nCtop проста в использовании и при этом весьма полезна. Более подробную информацию вы можете найти в документации .\n\nСсылки:\n\nОригинал: 5 Docker Utilities You Should Know .\n\nКраткое содержание:\nВ статье рассматриваются пять Docker-утилит, которые помогают автоматизировать процессы и управлять контейнерами: Watchtower для автоматического обновления Docker-контейнеров, Docker-gc для очистки хоста, Docker-slim для уменьшения размера образов, Rocker для улучшения работы с Dockerfile и ctop для мониторинга состояния контейнеров.","summary":"В статье рассматриваются пять Docker-утилит, которые помогают автоматизировать процессы и управлять контейнерами: Watchtower для автоматического обновления Docker-контейнеров, Docker-gc для очистки хоста, Docker-slim для уменьшения размера образов, Rocker для улучшения работы с Dockerfile и ctop для мониторинга состояния контейнеров."}
{"Text":"Несколько причин, почему популярность минимализма постоянно растет (перевод) / Хабр                                                            \n\n19-08-2014\nАннотация Всем доброго времени суток. В рамках своей работы мне частенько приходится читать зарубежную периодику для дизайнеров ну и, естественно, часто переводить её. У меня скопилось несколько уже подготовленных переводов, с одним из них я решила поделиться с вами. Если кому-то будет интересно — продолжу. P.S. Некоторые переводы опубликованы в моём блоге, так что если со стороны читателей хабра будет интерес — для хабра буду оставлять эксклюзив. Почему популярность минимализма постоянно растет? Популярность минимализма неуклонно растет в последние годы. Задолго до появления флэт дизайна (плоского дизайна, flat design), минимализм оказывал большое влияние на художников, скульпторов, режиссеров, дизайнеров интерьера, дизайнеров и, конечно, веб-дизайнеров. В минимализме дизайн складывается из частей и измельчается. Хотя им трудно овладеть, основам минимализма легко научиться. Если вы хотите попробовать минимализм в любой художественной деятельности, вы должны очистить работу от всего, чего только возможно, оставив самые необходимые элементы. При правильном применении, минималистичный дизайн может стать наиболее эффективным и простым подходом в дизайне сайтов. Минимализм приведет к менее беспорядочному и шумному дизайну. Для этих целей дизайнеры сосредотачиваются на вычитании ненужных элементов, и в последнее время все больше дизайнеров следует этой концепции. Его популярность быстро выросла с распространением тренда плоского дизайна. Все больше и больше веб-сайтов были адаптированы с плоским дизайном, который применяет концепции минимализма, думая, что меньше – это действительно больше. Но почему же минимализм так популярен? Что кроется за популярностью этой дизайнерской концепции? И как можно использовать эту популярность, чтобы сделать дизайн более эффективным и красивым? Меньше значит больше Людвиг Мис ван дер Роэ Архитектор-модернист Людвиг Мис ван дер Роэ, возможно, никогда бы не догадался, какое влияние окажет сказанная им фраза «меньше значит больше» на блестящие умы, жившие после него. Эти три слова суммируют всё чем является минимализм – лаконично и прямо в точку. При разработке сайта, надо всегда задавать себе вопрос: «Что я имею в виду?». Таким образом, вы поймете куда вести зрителей. Загроможденный дизайн только заблокирует эту точку. Ненужные элементы, как правило становятся препятствием, нежели мостом для передачи вашей цели зрителю. Почему это плохо? Нагромождение элементов только отвлекает пользователя на другие вещи, нежели отвечает вашему призыву к действию. Оно делает информацию более трудной к пониманию. Дизайнер должен урезать свой контент, вырезать ненужные, менее полезные и нерелевантные элементы. Как достичь большего из меньшего? Планируйте свой веб-сайт более тщательно. Это предотвратит добавление ненужных элементов прежде, чем вы даже прикоснетесь к дизайну. Определите свою цель. Если один из элементов дизайна не служит этой цели, удалите его. Потратьте время, спрашивая себя: «Действительно ли мне это нужно?». Попробуйте использовать некоторые программы для отслеживания кликов на своем веб-сайте, чтобы определить элементы, которые никогда не используются зрителями. После чего, оцените стоит ли поменять место для этих элементов или удалить их. Отзывчивость дизайна С ежедневно увеличивающимся количеством пользователей смартфонов, веб-сайты должны быть отзывчивыми (Responsive). Как только ваш сайт станет просматриваемым и красивым на любых видах устройств, будут увеличены шансы по достижению поставленной цели. Почему здесь была упомянута отзывчивость дизайна? Какую роль она играет в популярности минимализма? Важная роль Отзывчивые сайты с применением единого прототипа и системы сетки могут спокойно переходить от настольного экрана к мобильным версиям. С меньшим контентом, меньшим количеством блоков и элементами дизайна (которые придерживаются стиля минимализма), с добавлением пустого пространства, минималистичные веб-сайты имеют больше шансов на перевод в красивые мобильные версии. Еще одна вещь об отзывчивости и её взаимосвязи с минимализмом – это пользователи. Пользователи мобильных устройств менее терпеливы, всегда заняты и всегда на ходу. Предоставление информации постоянному пользователю мобильных устройств может быть очень полезно. Запомните, если пользователь хочет это, дайте ему это как можно скорее. Вот несколько вещей, которые вы должны помнить об отзывчивом дизайне: Думаете отзывчиво. Существуют идеальные схемы для отзывчивого дизайна и существуют другие, которые не идеальны. Понимание этого, поможет вам создавать дизайн сайта для любого устройства. Обращайте внимание на контрольные точки. Разрешения экранов имеют набор разных контрольных точек. Эти точки должны стать ведущими для создания дизайна для различных экранов. Рисунки должны быть гибкими. При изменении ширины, сделайте ваши картинки настолько адаптивными, насколько это возможно. Здесь инструмент, которые может вам помочь: Adaptive Images. Более легкие веб-сайты Представьте себя на воздушном шаре. Вы включаете горелку, и шар надувается. Но вы не можете взлететь, потому что тяжелые мешки с песком тянут ваш шар к земле. Что вы будете делать? Ответ очевиден. Более легкие сайты стали одной из главных причин популярности минимализма. Наличие только нескольких элементов в дизайне будет значить более легкий веб-сайт. Меньшее количество контента, виджетов, дизайнерских элементов означает, что меньше данных необходимо загрузить вашему браузеру, давая вам более быстрый, легкий, эффективный и продуктивный пользовательский интерфейс. Вы сразу же конструируете более отзывчивый дизайн, думая насколько быстро ваш веб-сайт будет грузится на мобильных устройствах. Наличие более легкого сайта означает более лёгкое обновление, техническое обслуживание и устранение неполадок. Удалите неиспользуемые плагины. Если что-то не работает эффективно, выбросьте это. Если вы можете создать дизайнерские элементы с помощью кода, сделайте это. Подумайте в каких случаях лучше использовать PNG, GIF или JPG файлы. Вот несколько советов: Удалите неиспользуемые плагины. Если что-то не работает эффективно, выбросьте это. Если вы можете создать дизайнерские элементы с помощью кода, сделайте это. Подумайте в каких случаях лучше использовать PNG, GIF или JPG файлы. Нацеленность на контент Контент – главный. Понятно, что все это знают. Что движет людьми при выборе минимализма, это еще и его нацеленность на контент. Так как минималистичные сайты стремятся отбросить ненужные элементы и обнажить дизайн, как только это возможно, пользователь фокусируется на главном – на контенте. Без какого-либо смятения пользователи могут легко найти, то что им нужно, чтобы оперативно отреагировать на призыв к действию. Повышается эффективность сайтов, потому что это приводит к увеличению конверсии с минимальными усилиями в области дизайна и обслуживания. Как создавать дизайн для контента? Используйте подходящие выравнивание. Используйте пустое пространство. Используйте подходящие шрифты. Помните, что размер имеет значение. Заключение Почем минимализм так популярен? Он отражает стиль жизни, который стимулирует самоанализ. Какие вещи мне не нужны? Какие вещи мне необходимы? Ответ на эти вопросы приведет вас к осознаю, что действительно необходимо и что вы должны удержать и сохранить. Источник: http://www.1stwebdesigner.com/\n","metric":0.6435546875,"metric_input":"Текст статьи:\nНесколько причин, почему популярность минимализма постоянно растет (перевод) / Хабр                                                            \n\n19-08-2014\nАннотация Всем доброго времени суток. В рамках своей работы мне частенько приходится читать зарубежную периодику для дизайнеров ну и, естественно, часто переводить её. У меня скопилось несколько уже подготовленных переводов, с одним из них я решила поделиться с вами. Если кому-то будет интересно — продолжу. P.S. Некоторые переводы опубликованы в моём блоге, так что если со стороны читателей хабра будет интерес — для хабра буду оставлять эксклюзив. Почему популярность минимализма постоянно растет? Популярность минимализма неуклонно растет в последние годы. Задолго до появления флэт дизайна (плоского дизайна, flat design), минимализм оказывал большое влияние на художников, скульпторов, режиссеров, дизайнеров интерьера, дизайнеров и, конечно, веб-дизайнеров. В минимализме дизайн складывается из частей и измельчается. Хотя им трудно овладеть, основам минимализма легко научиться. Если вы хотите попробовать минимализм в любой художественной деятельности, вы должны очистить работу от всего, чего только возможно, оставив самые необходимые элементы. При правильном применении, минималистичный дизайн может стать наиболее эффективным и простым подходом в дизайне сайтов. Минимализм приведет к менее беспорядочному и шумному дизайну. Для этих целей дизайнеры сосредотачиваются на вычитании ненужных элементов, и в последнее время все больше дизайнеров следует этой концепции. Его популярность быстро выросла с распространением тренда плоского дизайна. Все больше и больше веб-сайтов были адаптированы с плоским дизайном, который применяет концепции минимализма, думая, что меньше – это действительно больше. Но почему же минимализм так популярен? Что кроется за популярностью этой дизайнерской концепции? И как можно использовать эту популярность, чтобы сделать дизайн более эффективным и красивым? Меньше значит больше Людвиг Мис ван дер Роэ Архитектор-модернист Людвиг Мис ван дер Роэ, возможно, никогда бы не догадался, какое влияние окажет сказанная им фраза «меньше значит больше» на блестящие умы, жившие после него. Эти три слова суммируют всё чем является минимализм – лаконично и прямо в точку. При разработке сайта, надо всегда задавать себе вопрос: «Что я имею в виду?». Таким образом, вы поймете куда вести зрителей. Загроможденный дизайн только заблокирует эту точку. Ненужные элементы, как правило становятся препятствием, нежели мостом для передачи вашей цели зрителю. Почему это плохо? Нагромождение элементов только отвлекает пользователя на другие вещи, нежели отвечает вашему призыву к действию. Оно делает информацию более трудной к пониманию. Дизайнер должен урезать свой контент, вырезать ненужные, менее полезные и нерелевантные элементы. Как достичь большего из меньшего? Планируйте свой веб-сайт более тщательно. Это предотвратит добавление ненужных элементов прежде, чем вы даже прикоснетесь к дизайну. Определите свою цель. Если один из элементов дизайна не служит этой цели, удалите его. Потратьте время, спрашивая себя: «Действительно ли мне это нужно?». Попробуйте использовать некоторые программы для отслеживания кликов на своем веб-сайте, чтобы определить элементы, которые никогда не используются зрителями. После чего, оцените стоит ли поменять место для этих элементов или удалить их. Отзывчивость дизайна С ежедневно увеличивающимся количеством пользователей смартфонов, веб-сайты должны быть отзывчивыми (Responsive). Как только ваш сайт станет просматриваемым и красивым на любых видах устройств, будут увеличены шансы по достижению поставленной цели. Почему здесь была упомянута отзывчивость дизайна? Какую роль она играет в популярности минимализма? Важная роль Отзывчивые сайты с применением единого прототипа и системы сетки могут спокойно переходить от настольного экрана к мобильным версиям. С меньшим контентом, меньшим количеством блоков и элементами дизайна (которые придерживаются стиля минимализма), с добавлением пустого пространства, минималистичные веб-сайты имеют больше шансов на перевод в красивые мобильные версии. Еще одна вещь об отзывчивости и её взаимосвязи с минимализмом – это пользователи. Пользователи мобильных устройств менее терпеливы, всегда заняты и всегда на ходу. Предоставление информации постоянному пользователю мобильных устройств может быть очень полезно. Запомните, если пользователь хочет это, дайте ему это как можно скорее. Вот несколько вещей, которые вы должны помнить об отзывчивом дизайне: Думаете отзывчиво. Существуют идеальные схемы для отзывчивого дизайна и существуют другие, которые не идеальны. Понимание этого, поможет вам создавать дизайн сайта для любого устройства. Обращайте внимание на контрольные точки. Разрешения экранов имеют набор разных контрольных точек. Эти точки должны стать ведущими для создания дизайна для различных экранов. Рисунки должны быть гибкими. При изменении ширины, сделайте ваши картинки настолько адаптивными, насколько это возможно. Здесь инструмент, которые может вам помочь: Adaptive Images. Более легкие веб-сайты Представьте себя на воздушном шаре. Вы включаете горелку, и шар надувается. Но вы не можете взлететь, потому что тяжелые мешки с песком тянут ваш шар к земле. Что вы будете делать? Ответ очевиден. Более легкие сайты стали одной из главных причин популярности минимализма. Наличие только нескольких элементов в дизайне будет значить более легкий веб-сайт. Меньшее количество контента, виджетов, дизайнерских элементов означает, что меньше данных необходимо загрузить вашему браузеру, давая вам более быстрый, легкий, эффективный и продуктивный пользовательский интерфейс. Вы сразу же конструируете более отзывчивый дизайн, думая насколько быстро ваш веб-сайт будет грузится на мобильных устройствах. Наличие более легкого сайта означает более лёгкое обновление, техническое обслуживание и устранение неполадок. Удалите неиспользуемые плагины. Если что-то не работает эффективно, выбросьте это. Если вы можете создать дизайнерские элементы с помощью кода, сделайте это. Подумайте в каких случаях лучше использовать PNG, GIF или JPG файлы. Вот несколько советов: Удалите неиспользуемые плагины. Если что-то не работает эффективно, выбросьте это. Если вы можете создать дизайнерские элементы с помощью кода, сделайте это. Подумайте в каких случаях лучше использовать PNG, GIF или JPG файлы. Нацеленность на контент Контент – главный. Понятно, что все это знают. Что движет людьми при выборе минимализма, это еще и его нацеленность на контент. Так как минималистичные сайты стремятся отбросить ненужные элементы и обнажить дизайн, как только это возможно, пользователь фокусируется на главном – на контенте. Без какого-либо смятения пользователи могут легко найти, то что им нужно, чтобы оперативно отреагировать на призыв к действию. Повышается эффективность сайтов, потому что это приводит к увеличению конверсии с минимальными усилиями в области дизайна и обслуживания. Как создавать дизайн для контента? Используйте подходящие выравнивание. Используйте пустое пространство. Используйте подходящие шрифты. Помните, что размер имеет значение. Заключение Почем минимализм так популярен? Он отражает стиль жизни, который стимулирует самоанализ. Какие вещи мне не нужны? Какие вещи мне необходимы? Ответ на эти вопросы приведет вас к осознаю, что действительно необходимо и что вы должны удержать и сохранить. Источник: http://www.1stwebdesigner.com/\n\n\nКраткое содержание:\nПопулярность минимализма в дизайне растет из-за его эффективности, простоты и способности создавать более эффективные и красивые веб-сайты. Минимализм требует удаления ненужных элементов и использования пустого пространства, что делает сайты более отзывчивыми и легкими. Контент является главным в минималистичных сайтах, и пользователи могут легко найти нужную информацию. Минимализм также стимулирует самоанализ и помогает определить, что действительно необходимо в дизайне.","summary":"Популярность минимализма в дизайне растет из-за его эффективности, простоты и способности создавать более эффективные и красивые веб-сайты. Минимализм требует удаления ненужных элементов и использования пустого пространства, что делает сайты более отзывчивыми и легкими. Контент является главным в минималистичных сайтах, и пользователи могут легко найти нужную информацию. Минимализм также стимулирует самоанализ и помогает определить, что действительно необходимо в дизайне."}
{"Text":"Wazer: первый настольный гидроабразивный станок для решения профессиональных задач / Habr             \n\n23-11-2021\nWazer - это профессиональный настольный станок для гидроабразивной резки с автоматизированным управлением, способный резать с высокой точностью даже прочные материалы, такие как сталь, титан, мрамор и стекло. В качестве режущего инструмента здесь используется смесь воды с абразивными частицами, выпускающаяся под высоким давлением. Настольный гидроабразивный станок с ЧПУ Wazer прост в обслуживании и не требует особых условий для работы. Все, что требуется - это кран с водой, наличие слива и мощность сети не менее 110 В Компактный дизайн, доступная цена и универсальность применения делают Wazer идеальным решением для: оборудования небольших производств и инженерных лабораторий, для малого и среднего бизнеса, каменщиков, учебных заведений, ЦМИТ. Параметры и материалы для резки Максимально доступная площадь реза составляет 305 x 460 мм, а ширина пропила не превышает 1,2 мм. Вейзер обрабатывает любые категории листовых материалов: металлы (низкоуглеродистая сталь, алюминий, титан, медь), пластики (поликарбонат, полиуретан), акрил, силикон, стекло, углеволокно, керамика, мрамор и др. Подключение и установка станка Вейзер поставляется в собранном виде, но для запуска станка потребуется несколько простых предварительных действий. В насосном агрегате необходимо заменить масляную пробку на воздушный фильтр с помощью гаечного ключа, который вы найдете в комплекте, и подключить его к электрической сети. Далее понадобится установить настольный блок и удалить фиксирующие элементы, использующиеся для безопасной транспортировки. После сборки станок подключается к источнику воды с помощью шланга, с задней стороны насосного агрегата. Насос всегда располагается внизу и соединяется с настольным блоком через кабель высокого давления. Вейзер работает от электрической сети 110 В, поэтому его можно подключить к любой домашней розетке. Абразивное сырье В торцевой части настольного режущего блока располагается бункер для абразива. В качестве абразивного материала используется минеральная горная порода - гранат, дробленая до состояния песчаной фракции с размером частиц 0,32 мм. Бункер заполняется сырьем и плотно закрывается. Одной порции абразива хватает на 3 часа работы станка, поэтому каждые три часа стоит проверять бункер на наличие материала и при необходимости добавлять новую порцию. Абразив рекомендуется хранить в шаговой доступности от станка, чтобы избежать неудобств с перемещением тяжелого материала на дальние расстояния и быстро среагировать на отсутствие сырья в бункере. Крепление материала и подготовка к запуску Заготовка материала закладывается в настольный блок и фиксируется внутри станка с помощью специального крепежа для обеспечения неподвижности во время резки. После этого необходимо убедиться, что сопло установлено на правильной высоте относительно поверхности материала. Для калибровки сопла винт накатной головки ослабляют, подтягивая режущую головку, и снова закрепляют. Прежде чем приступить к резке необходимо закрыть крышку, - функция безопасности не позволит осуществлять резку с открытой крышкой. После закрытия крышки можно нажать кнопку запуска. Принцип работы Wazer Отверстие насоса преобразует воду, находящуюся под давлением, в высокоскоростной поток и впрыскивает ее в смесительную камеру. В свою очередь гибкий шланг для абразива подает абразив из бункера в тот же смеситель. Абразив и вода с высокой скоростью смешиваются в одной камере и подаются в смесительную трубку. Внутри нее смесь воды с абразивом превращается в концентрированный поток и под давлением выбрасывается на материал. Устройство сопла сдерживает ненаправленные брызги воды и абразива, защищая ванну станка от чрезмерного износа. Лишняя вода откачивается через сливной шланг. После окончания резки деталь достается из машины. В передней части машины есть панель, которую можно снимать для очистки станка после работы. Под панелью располагаются 4 ведра, где собирается абразив. Абразив может быть утилизирован разными способами, так как это безвредный природный материал. Вейзер - не герметичная система, поэтому вода и абразив неизбежно будут протекать. Поверхность пола должна быть водонепроницаемой, нескользящей и достаточно гладкой, чтобы ее можно было легко очистить от разливов. Также рекомендуется установить слив в полу. Смотрите видеообзор Программное обеспечение Wazer В Вейзере установлено собственное программное обеспечение, простое и понятное в использовании. Оно интегрируется с любыми программами, способными экспортировать файлы в DXF или SVG формате. Модели для резки можно создавать в таких программах как как Solidworks, Adobe Illustrator или Google SketchUp, а затем загружать их в ПО Wazer Wam, где происходит настройка параметров материала и качества резки. Модели и характеристики Wazer В базовом исполнении Wazer Desktop представляет собой настольный режущий станок, но существует и напольная вариация станка - Wazer Standup. Он оснащен подставкой с полкой и с ножками из анодированного алюминия. Это эффективное решение для мастерских, лабораторий и предприятий, так как не требуется искать дополнительное место для размещения станка, - оно уже предусмотрено производителем. Наверху подставки размещается сам станок, на полке можно хранить абразив, заготовки и инструменты, а внизу устанавливается насосная станция. Преимущества гидроабразивной резки с Wazer: Небольшие размеры 864 x 635 x 534 мм; Простота установки и подключения ко всем системам; Автоматическая приостановка при открытой крышке; Изготовление деталей нестандартной и сложной формы; Обеспечение ровной поверхности и гладкой кромки; Незначительные затраты на обслуживание; Конкурентная себестоимость резки; Работы без перегрева и необходимости в вентиляции; Широкий перечень доступных для резки материалов. Применение и кейсы Гидроабразивный резак Wazer выдает превосходное качество резки и стоит почти в 10 раз дешевле традиционных стационарных машин. Он отлично подходит для прототипирования, производства подвижных механических деталей, изготовления элементов декора и обучающих целей. Резка фарфорового изделия - 8 минут Изготовление зубчатого колеса для ременной передачи - 168 минут Изготовление ножа: рукоять из углеволокна и лезвие из инструментальной стали - 118 минут Резка керамической плитки - 95 минут Настольный станок Wazer - это эффективное решение для частной мастерской, малого бизнеса или производственных цехов, использующих резку как промежуточный этап для изготовления прототипов, декоративных изделий, запчастей для ремонта и так далее.\n","metric":0.40673828130000000414,"metric_input":"Текст статьи:\nWazer: первый настольный гидроабразивный станок для решения профессиональных задач / Habr             \n\n23-11-2021\nWazer - это профессиональный настольный станок для гидроабразивной резки с автоматизированным управлением, способный резать с высокой точностью даже прочные материалы, такие как сталь, титан, мрамор и стекло. В качестве режущего инструмента здесь используется смесь воды с абразивными частицами, выпускающаяся под высоким давлением. Настольный гидроабразивный станок с ЧПУ Wazer прост в обслуживании и не требует особых условий для работы. Все, что требуется - это кран с водой, наличие слива и мощность сети не менее 110 В Компактный дизайн, доступная цена и универсальность применения делают Wazer идеальным решением для: оборудования небольших производств и инженерных лабораторий, для малого и среднего бизнеса, каменщиков, учебных заведений, ЦМИТ. Параметры и материалы для резки Максимально доступная площадь реза составляет 305 x 460 мм, а ширина пропила не превышает 1,2 мм. Вейзер обрабатывает любые категории листовых материалов: металлы (низкоуглеродистая сталь, алюминий, титан, медь), пластики (поликарбонат, полиуретан), акрил, силикон, стекло, углеволокно, керамика, мрамор и др. Подключение и установка станка Вейзер поставляется в собранном виде, но для запуска станка потребуется несколько простых предварительных действий. В насосном агрегате необходимо заменить масляную пробку на воздушный фильтр с помощью гаечного ключа, который вы найдете в комплекте, и подключить его к электрической сети. Далее понадобится установить настольный блок и удалить фиксирующие элементы, использующиеся для безопасной транспортировки. После сборки станок подключается к источнику воды с помощью шланга, с задней стороны насосного агрегата. Насос всегда располагается внизу и соединяется с настольным блоком через кабель высокого давления. Вейзер работает от электрической сети 110 В, поэтому его можно подключить к любой домашней розетке. Абразивное сырье В торцевой части настольного режущего блока располагается бункер для абразива. В качестве абразивного материала используется минеральная горная порода - гранат, дробленая до состояния песчаной фракции с размером частиц 0,32 мм. Бункер заполняется сырьем и плотно закрывается. Одной порции абразива хватает на 3 часа работы станка, поэтому каждые три часа стоит проверять бункер на наличие материала и при необходимости добавлять новую порцию. Абразив рекомендуется хранить в шаговой доступности от станка, чтобы избежать неудобств с перемещением тяжелого материала на дальние расстояния и быстро среагировать на отсутствие сырья в бункере. Крепление материала и подготовка к запуску Заготовка материала закладывается в настольный блок и фиксируется внутри станка с помощью специального крепежа для обеспечения неподвижности во время резки. После этого необходимо убедиться, что сопло установлено на правильной высоте относительно поверхности материала. Для калибровки сопла винт накатной головки ослабляют, подтягивая режущую головку, и снова закрепляют. Прежде чем приступить к резке необходимо закрыть крышку, - функция безопасности не позволит осуществлять резку с открытой крышкой. После закрытия крышки можно нажать кнопку запуска. Принцип работы Wazer Отверстие насоса преобразует воду, находящуюся под давлением, в высокоскоростной поток и впрыскивает ее в смесительную камеру. В свою очередь гибкий шланг для абразива подает абразив из бункера в тот же смеситель. Абразив и вода с высокой скоростью смешиваются в одной камере и подаются в смесительную трубку. Внутри нее смесь воды с абразивом превращается в концентрированный поток и под давлением выбрасывается на материал. Устройство сопла сдерживает ненаправленные брызги воды и абразива, защищая ванну станка от чрезмерного износа. Лишняя вода откачивается через сливной шланг. После окончания резки деталь достается из машины. В передней части машины есть панель, которую можно снимать для очистки станка после работы. Под панелью располагаются 4 ведра, где собирается абразив. Абразив может быть утилизирован разными способами, так как это безвредный природный материал. Вейзер - не герметичная система, поэтому вода и абразив неизбежно будут протекать. Поверхность пола должна быть водонепроницаемой, нескользящей и достаточно гладкой, чтобы ее можно было легко очистить от разливов. Также рекомендуется установить слив в полу. Смотрите видеообзор Программное обеспечение Wazer В Вейзере установлено собственное программное обеспечение, простое и понятное в использовании. Оно интегрируется с любыми программами, способными экспортировать файлы в DXF или SVG формате. Модели для резки можно создавать в таких программах как как Solidworks, Adobe Illustrator или Google SketchUp, а затем загружать их в ПО Wazer Wam, где происходит настройка параметров материала и качества резки. Модели и характеристики Wazer В базовом исполнении Wazer Desktop представляет собой настольный режущий станок, но существует и напольная вариация станка - Wazer Standup. Он оснащен подставкой с полкой и с ножками из анодированного алюминия. Это эффективное решение для мастерских, лабораторий и предприятий, так как не требуется искать дополнительное место для размещения станка, - оно уже предусмотрено производителем. Наверху подставки размещается сам станок, на полке можно хранить абразив, заготовки и инструменты, а внизу устанавливается насосная станция. Преимущества гидроабразивной резки с Wazer: Небольшие размеры 864 x 635 x 534 мм; Простота установки и подключения ко всем системам; Автоматическая приостановка при открытой крышке; Изготовление деталей нестандартной и сложной формы; Обеспечение ровной поверхности и гладкой кромки; Незначительные затраты на обслуживание; Конкурентная себестоимость резки; Работы без перегрева и необходимости в вентиляции; Широкий перечень доступных для резки материалов. Применение и кейсы Гидроабразивный резак Wazer выдает превосходное качество резки и стоит почти в 10 раз дешевле традиционных стационарных машин. Он отлично подходит для прототипирования, производства подвижных механических деталей, изготовления элементов декора и обучающих целей. Резка фарфорового изделия - 8 минут Изготовление зубчатого колеса для ременной передачи - 168 минут Изготовление ножа: рукоять из углеволокна и лезвие из инструментальной стали - 118 минут Резка керамической плитки - 95 минут Настольный станок Wazer - это эффективное решение для частной мастерской, малого бизнеса или производственных цехов, использующих резку как промежуточный этап для изготовления прототипов, декоративных изделий, запчастей для ремонта и так далее.\n\n\nКраткое содержание:\nWazer - это настольный гидроабразивный станок с ЧПУ, предназначенный для профессионального использования. Он способен резать прочные материалы, такие как сталь, титан, мрамор и стекло. Станок работает на основе смеси воды с абразивными частицами, которая под высоким давлением подается на материал. Wazer прост в обслуживании и не требует особых условий для работы. Станок имеет доступное программное обеспечение и может интегрироваться с различными программами для создания и редактирования моделей. Существует напольная вариация станка - Wazer Standup, который оснащен подставкой с полкой и ножками из анодированного алюминия. Wazer имеет небольшие размеры и высокую производительность.","summary":"Wazer - это настольный гидроабразивный станок с ЧПУ, предназначенный для профессионального использования. Он способен резать прочные материалы, такие как сталь, титан, мрамор и стекло. Станок работает на основе смеси воды с абразивными частицами, которая под высоким давлением подается на материал. Wazer прост в обслуживании и не требует особых условий для работы. Станок имеет доступное программное обеспечение и может интегрироваться с различными программами для создания и редактирования моделей. Существует напольная вариация станка - Wazer Standup, который оснащен подставкой с полкой и ножками из анодированного алюминия. Wazer имеет небольшие размеры и высокую производительность."}
{"Text":"Прозрачная аутентификация для OTRS при помощи mod_auth_ntlm_winbind на примере Cent OS 6 / Habr                              \n\n08-06-2013\nНекоторое время назад в нашей компании была внедрена система поддержки клиентов OTRS. OTRS легко интегрируется с Active Directory, существует масса пошаговых инструкций, в том числе на Хабре. Практика использования системы поддержки показала, что ввести пароль а еще и логин доменной учетной записи задача, для пользователя весьма сложная, для руководителей вообще не приемлемая. OTRS имеет возможность прозрачной авторизации, осталось научить Apache принимать\\отдавать такие данные. Вот тут и начались грабли, в одной части руководств описан ntlm_mod не обновлявшийся года этак с 2007, в другой части руководств описывается практика установки под windows. Хотелось чистого Open Source, поэтому Cent OS как одна из платформ рекомендуемая в руководстве по OTRS и mod_auth_ntlm_winbind присутствующий в стандартном репозитории. Сервер сущесвует и работает, необходимо лишь изменить способ аутентификации для пользователей (кастомеров) оставив без изменения способ авторизации для агентов. Winbind для проверки подлинности использует Kerberos. Для корректной работы Kerberos необходимо синхронизировать часы с доменом. Для это установим NTP: yum –y install ntp Настроим NTP: sed –i “s /^server /#server /g” /ect/ntp.conf echo “serever КОНТРОЛЕР_ДОМЕНА” /etc/ntp.conf Синхронизируем время: ntpdate КОНТРОЛЕР_ДОМЕНА Настроим автоматический запуск: chkconfig ntpd on И запустим NTP: service ntpd start Время синхронизированно. Установим необходимые пакеты: yum -y install mod_auth_ntlm_winbind httpd-devel autoconfig krb5-workstation samba samba-common samba-winbind Следующим шагом необходимо сконфигурировать установленные пакеты и ввести сервер в домен. Для это в консоли пишем: ADSERVER=FQDN контролера домена (например dc.company.local) DOMAIN=домен (company.local) WORKGROUP= company authconfig --enableshadow --enablemd5 --passalgo=md5 --krb5kdc=$ADSERVER --krb5realm=$DOMAIN --smbservers=$ADSERVER --smbworkgroup=$WORKGROUP --enablewinbind --enablewinbindauth --smbsecurity=ads --smbrealm=$DOMAIN --smbidmapuid=\"16777216-33554431\" --smbidmapgid=\"16777216-33554431\" --winbindseparator=\"+\" --winbindtemplateshell=\"/bin/false\" --enablewinbindusedefaultdomain --disablewinbindoffline --winbindjoin=Administrator --disablewins --disablecache --enablelocauthorize –updateall После этого мы должны получить сообщение о том, что наш сервер теперь является доменной машиной. Добавим правило для SE Linux: setsebool -P allow_httpd_mod_auth_ntlm_winbind on Запусим winbind service winbind start Настроим автоматический запуск: chkconfig winbind on Проверим правильность работы winbind: wbinfo –u получим список пользователей wbinfo –g получим список групп Проверить правильность работы Kerberos можно получив тикет: kinit administrator (любое имя доменного пользователя),по запросу вводим пароль. Полученный тикет можно посмотреть командой: klist Для работы mod_auth_ntlm_winbind необходимо в файле /etc/httpd/conf/httpd.conf изменить параметр KeepAlive=off на KeepAlive=on. В директории /etc/httpd/conf.d создаем файл ntlm_winbind.conf со следующим содержанием: LoadModule auth_ntlm_winbind_module /usr/lib64/httpd/modules/mod_auth_ntlm_winbind.so <Location ~ \"(otrs/customer.pl)\"> AuthName \"NTLM Authentication\" AuthType NTLM Require valid-user NTLMAuth on NTLMAuthHelper \"/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp\" NTLMBasicAuthoritative on </Location> Тем самым мы просим передать данные авторизации только при доступе к customer.pl Последние что нужно сделать, это изменить Config.pm закоментировать часть отвечающую за авторизацию LDAP и добавить NTLM авторизацию. # Авторизация LDAP #$Self->{'Customer::AuthModule'} = 'Kernel::System::Auth::LDAP'; #$Self->{'Customer::AuthModule::LDAP::Host'} = 'dc.company.local'; #$Self->{'Customer::AuthModule::LDAP::BaseDN'} = 'dc=COMPANY, dc=local'; #$Self->{'Customer::AuthModule::LDAP::UID'} = 'sAMAccountName'; #$Self->{'Customer::AuthModule::LDAP::SearchUserDN'} = 'read_ad_user'; #$Self->{'Customer::AuthModule::LDAP::SearchUserPw'} = 'pass_for_read_ad_user'; #Авторизация NTLM $Self->{'Customer::AuthModule'} = 'Kernel::System::Auth::HTTPBasicAuth'; Проверим коректно ли зарегистрировался сервер OTRS на DNS сервере выполнив команду nslookup otrs-server Настройка завершена! Открывать в браузере otrs-server-name/otrs/customer.pl и видим результат. Если же не видим результат, значит при настройке была допущена ошибка, внимательно смотрим настройки в файлах /etc/krb5.conf /etc/samba/smb.conf\n","metric":0.505859375,"metric_input":"Текст статьи:\nПрозрачная аутентификация для OTRS при помощи mod_auth_ntlm_winbind на примере Cent OS 6 / Habr                              \n\n08-06-2013\nНекоторое время назад в нашей компании была внедрена система поддержки клиентов OTRS. OTRS легко интегрируется с Active Directory, существует масса пошаговых инструкций, в том числе на Хабре. Практика использования системы поддержки показала, что ввести пароль а еще и логин доменной учетной записи задача, для пользователя весьма сложная, для руководителей вообще не приемлемая. OTRS имеет возможность прозрачной авторизации, осталось научить Apache принимать\\отдавать такие данные. Вот тут и начались грабли, в одной части руководств описан ntlm_mod не обновлявшийся года этак с 2007, в другой части руководств описывается практика установки под windows. Хотелось чистого Open Source, поэтому Cent OS как одна из платформ рекомендуемая в руководстве по OTRS и mod_auth_ntlm_winbind присутствующий в стандартном репозитории. Сервер сущесвует и работает, необходимо лишь изменить способ аутентификации для пользователей (кастомеров) оставив без изменения способ авторизации для агентов. Winbind для проверки подлинности использует Kerberos. Для корректной работы Kerberos необходимо синхронизировать часы с доменом. Для это установим NTP: yum –y install ntp Настроим NTP: sed –i “s /^server /#server /g” /ect/ntp.conf echo “serever КОНТРОЛЕР_ДОМЕНА” /etc/ntp.conf Синхронизируем время: ntpdate КОНТРОЛЕР_ДОМЕНА Настроим автоматический запуск: chkconfig ntpd on И запустим NTP: service ntpd start Время синхронизированно. Установим необходимые пакеты: yum -y install mod_auth_ntlm_winbind httpd-devel autoconfig krb5-workstation samba samba-common samba-winbind Следующим шагом необходимо сконфигурировать установленные пакеты и ввести сервер в домен. Для это в консоли пишем: ADSERVER=FQDN контролера домена (например dc.company.local) DOMAIN=домен (company.local) WORKGROUP= company authconfig --enableshadow --enablemd5 --passalgo=md5 --krb5kdc=$ADSERVER --krb5realm=$DOMAIN --smbservers=$ADSERVER --smbworkgroup=$WORKGROUP --enablewinbind --enablewinbindauth --smbsecurity=ads --smbrealm=$DOMAIN --smbidmapuid=\"16777216-33554431\" --smbidmapgid=\"16777216-33554431\" --winbindseparator=\"+\" --winbindtemplateshell=\"/bin/false\" --enablewinbindusedefaultdomain --disablewinbindoffline --winbindjoin=Administrator --disablewins --disablecache --enablelocauthorize –updateall После этого мы должны получить сообщение о том, что наш сервер теперь является доменной машиной. Добавим правило для SE Linux: setsebool -P allow_httpd_mod_auth_ntlm_winbind on Запусим winbind service winbind start Настроим автоматический запуск: chkconfig winbind on Проверим правильность работы winbind: wbinfo –u получим список пользователей wbinfo –g получим список групп Проверить правильность работы Kerberos можно получив тикет: kinit administrator (любое имя доменного пользователя),по запросу вводим пароль. Полученный тикет можно посмотреть командой: klist Для работы mod_auth_ntlm_winbind необходимо в файле /etc/httpd/conf/httpd.conf изменить параметр KeepAlive=off на KeepAlive=on. В директории /etc/httpd/conf.d создаем файл ntlm_winbind.conf со следующим содержанием: LoadModule auth_ntlm_winbind_module /usr/lib64/httpd/modules/mod_auth_ntlm_winbind.so <Location ~ \"(otrs/customer.pl)\"> AuthName \"NTLM Authentication\" AuthType NTLM Require valid-user NTLMAuth on NTLMAuthHelper \"/usr/bin/ntlm_auth --helper-protocol=squid-2.5-ntlmssp\" NTLMBasicAuthoritative on </Location> Тем самым мы просим передать данные авторизации только при доступе к customer.pl Последние что нужно сделать, это изменить Config.pm закоментировать часть отвечающую за авторизацию LDAP и добавить NTLM авторизацию. # Авторизация LDAP #$Self->{'Customer::AuthModule'} = 'Kernel::System::Auth::LDAP'; #$Self->{'Customer::AuthModule::LDAP::Host'} = 'dc.company.local'; #$Self->{'Customer::AuthModule::LDAP::BaseDN'} = 'dc=COMPANY, dc=local'; #$Self->{'Customer::AuthModule::LDAP::UID'} = 'sAMAccountName'; #$Self->{'Customer::AuthModule::LDAP::SearchUserDN'} = 'read_ad_user'; #$Self->{'Customer::AuthModule::LDAP::SearchUserPw'} = 'pass_for_read_ad_user'; #Авторизация NTLM $Self->{'Customer::AuthModule'} = 'Kernel::System::Auth::HTTPBasicAuth'; Проверим коректно ли зарегистрировался сервер OTRS на DNS сервере выполнив команду nslookup otrs-server Настройка завершена! Открывать в браузере otrs-server-name/otrs/customer.pl и видим результат. Если же не видим результат, значит при настройке была допущена ошибка, внимательно смотрим настройки в файлах /etc/krb5.conf /etc/samba/smb.conf\n\n\nКраткое содержание:\nВ статье описывается настройка прозрачной аутентификации для системы поддержки клиентов OTRS с использованием mod_auth_ntlm_winbind на примере Cent OS 6. Для этого необходимо установить и настроить пакеты mod_auth_ntlm_winbind, httpd-devel, autoconfig, krb5-workstation, samba, samba-common, samba-winbind, а также настроить синхронизацию времени с доменом. Затем необходимо настроить параметры SE Linux и автоматический запуск сервисов winbind и ntpd. Для работы mod_auth_ntlm_winbind необходимо изменить параметры в файле /etc/httpd/conf/httpd.conf и создать файл ntlm_winbind.conf в директории /etc/httpd/conf.d. В конце необходимо изменить настройки авторизации в файле Config.pm. Настройка завершена успешной регистрацией сервера OTRS на DNS сервере и проверкой работы прозрачной аутентификации.","summary":"В статье описывается настройка прозрачной аутентификации для системы поддержки клиентов OTRS с использованием mod_auth_ntlm_winbind на примере Cent OS 6. Для этого необходимо установить и настроить пакеты mod_auth_ntlm_winbind, httpd-devel, autoconfig, krb5-workstation, samba, samba-common, samba-winbind, а также настроить синхронизацию времени с доменом. Затем необходимо настроить параметры SE Linux и автоматический запуск сервисов winbind и ntpd. Для работы mod_auth_ntlm_winbind необходимо изменить параметры в файле /etc/httpd/conf/httpd.conf и создать файл ntlm_winbind.conf в директории /etc/httpd/conf.d. В конце необходимо изменить настройки авторизации в файле Config.pm. Настройка завершена успешной регистрацией сервера OTRS на DNS сервере и проверкой работы прозрачной аутентификации."}
{"Text":"Часть 3: Цифровой чеснок / Habr             \n\n07-12-2012\nТретья часть перевода официальной документации I2P. Ещё ближе к тексту. Если кто-то не в курсе, то добро пожаловать под кат, читать в порядке оглавления. Оглавление: Часть 1: «Всё что вы хотели знать и боялись спросить о I2P» Часть 2: Туннельная магия, NetDB и «жонглирование» протоколами Часть 3: Цифровой чеснок Криптография Минимальный набор криптографических примитивов складываются для обеспечения в I2P многоуровневой защиты от различных угроз.На самом нижнем уровне взаимодействие между маршрутизаторами защищено на транспортном уровне (TLS). Для этого используется протокол SSU (secure semireliable UDP), согласно которому каждый пакет зашифровывается при помощи AES256 в CBC режиме с явным указанием вектора инициализации (IV) и кода аутентичности сообщения (по протоколу HMAC-MD5-128). Предварительно создается сессионный 2048-битный ключ по алгоритму Диффи-Хэлмана. Аутентификация маршрутизаторов происходит при помощи DSA, также каждое сетевое сообщение имеет дополнительную проверку на целостность. Сообщения внутри туннеля также имеют собственное шифрование AES256/CBC с явным IV и проверкой целостности по хэш-алгоритму SHA256. Другие сообщения так же проходят через сеть находясь внутри чеснока, но зашифрованные с использованием алгоритмов Эль-Гемаля(ElGamal)/AES + SessionTags (см. ниже). “Честночные сообщения” Честночные сообщения, являются продолжением лукового («onion») шифрования, позволяет содержать в одном сообщении множество вложений («cloves»), которые являются полностью сформированными сообщениями со своими инструкциями к доставке. Сообщения заворачиваются в чеснок каждый раз, когда отправляются куда либо, в противном случае их содержимое будет видно посредническим узлам, а это не допустимо. В качестве примера, следующая ситуация: Роутер, отправляет запрос другому роутеру, для участия в создании туннеля, для этого он оборачивает сообщение в “чеснок” зашифровывает получая открытый ключ 2048 бит, по схеме Эль-Гамаля и отправляет сообщение через туннель. Другой пример, когда клиент хочет отправить сообщение получателю, роутер “отправителя” будет заворачивать данные сообщения (на ряду с другими сообщениями) в чеснок, шифровать чеснок с использованием 2048 битного открытого ключа по схеме Эль-Гамаля, опубликованного в leaseSet получателя и отправлять его через соответствующий туннель. Инструкции вложенные содержащиеся в дольках, дают возможность перенаправить локально (куда-либо), на удалённый роутер или в туннель к удалённому роутеру. В инструкциях есть специальные поля, позволяющие просить пиров отложить доставку до срабатывания определённого триггера (по времени или условию), если они не будут выполнены в пределах “нетривиальных задержек” (искусственно созданные задержки), то сообщение будет “развёрнуто” (are deployed). Если возможно, то чесночное сообщение может быть отправлено через любое количество hops (прыжков — промежуточных точек), без построенного в заранее туннеля, до цели. Находясь в чесноке оно пройдёт заданное количество шагов, до доставки на следующий hop в туннеле. (прим. пер. — как я понимаю, это и есть пример нетривиальной задержки). В текущей реализации сети и клиента, этот метод недоступен. Сессионные теги Как ненадежны и неупорядоченны сообщения системы основанной на I2P, I2P использует простую комбинацию асимметричных и симметричных алгоритмов шифрования, чтобы обеспечить конфиденциальность и целостность данных вложенных в чесночные сообщения. Это сочетание обозначается как Эль-Гемаль/AES+SessionTags, но это излишне подробный способ описания использованных простых алгоритмов 2048 битового “ElGamal”, AES256, SHA256 и 32 байтного одноразового номера. Сначала роутер шифрует сообщение для другого роутера посредством AES256, в качестве ключа служит ElGamal и после него добавляется AES256/CBC шифрованная “полезная” (Дополнительная, транзитная) нагрузка. В дополнительной нагрузке, в зашифрованной AES секции содержится длинна нагрузки, SHA256 хеш, не шифрованной нагрузки и ряд сессионный тег — случайные 32 байта. В следующий раз, отправитель захочет зашифровать “чеснок” для другого роутера, но без генерации по ElGamal нового ключа, он просто может выбрать один из ранее поставленных сессионных тегов и AES шифрования полезной нагрузки, как и прежде, с помощью ключа использующего сессионный тег, который был добавлен до этого. Когда роутер получает сообщение, он проверяет первые 32 байта, чтобы узнать совпадают ли они с доступными сессионными тегами, если совпадают, то используется простая дешифровка AES, но если нет, то для первого блока используется алгоритм ElGamal. Каждый сессионный тег, может быть использован только один раз, для предотвращения внутренних конфликтов и из необходимости корреляции сообщений между маршрутизаторами. Чесночные сообщения позволяют контролировать состояние доставки, если доставка успешна, то выполняется триггер: Попадание к получателю -> успешная дешифровка -> поиск инструкции для обратного ответа, если найдена отправить ответ, по входящему туннелю обратно к отправителю. Когда отправитель получает сообщение о статусе доставки, он узнаёт, что сессионный тег в сообщении было успешно доставлены. Сессионные теги, сами по себе живут очень мало, после выхода срока жизни они удаляются, если не используются. Кроме того, количество сохранений для каждого ключа ограничено, ровно как и число самих ключей. Если тегов слишком много, то новые или старые могут быть “выкинуты”. Отправитель отслеживает свои сообщения с помощью тегов сессии, если нет необходимой связи они могут быть удалены перед тем, как сообщение будет раскрыто и сообщение вернётся назад благодаря дорожке из Эль-Гамаля шифрования. (прим. пер: PRNG — Псевдослучайный генератор чисел) Одним из вариантов является отправка, одного тега и прослушивание PRNG для определения, какие теги использовались и какие могут быть использованы. Поддерживая PRNG примерно синхронизированным между отправителем и получателем (получателем предварительно вычисляется размер следующего окна, например в 50 тегов), при слишком больших накладных расходах, теги удаляются что позволяет получить больше вариантов и получить компромисс между временем и объёмом, и сокращение количества ElGamal шифрований. Тем не менее, от сложности PRNG будет зависеть степень защиты от внутренних противников, возможно путём ограничения количество использований PRNG, любые слабости могут быть минимизированы. Но на данный момент, в обозримом будущем, нет планов двигаться в сторону развития синхронного PRNG.\n","metric":0.5703125,"metric_input":"Текст статьи:\nЧасть 3: Цифровой чеснок / Habr             \n\n07-12-2012\nТретья часть перевода официальной документации I2P. Ещё ближе к тексту. Если кто-то не в курсе, то добро пожаловать под кат, читать в порядке оглавления. Оглавление: Часть 1: «Всё что вы хотели знать и боялись спросить о I2P» Часть 2: Туннельная магия, NetDB и «жонглирование» протоколами Часть 3: Цифровой чеснок Криптография Минимальный набор криптографических примитивов складываются для обеспечения в I2P многоуровневой защиты от различных угроз.На самом нижнем уровне взаимодействие между маршрутизаторами защищено на транспортном уровне (TLS). Для этого используется протокол SSU (secure semireliable UDP), согласно которому каждый пакет зашифровывается при помощи AES256 в CBC режиме с явным указанием вектора инициализации (IV) и кода аутентичности сообщения (по протоколу HMAC-MD5-128). Предварительно создается сессионный 2048-битный ключ по алгоритму Диффи-Хэлмана. Аутентификация маршрутизаторов происходит при помощи DSA, также каждое сетевое сообщение имеет дополнительную проверку на целостность. Сообщения внутри туннеля также имеют собственное шифрование AES256/CBC с явным IV и проверкой целостности по хэш-алгоритму SHA256. Другие сообщения так же проходят через сеть находясь внутри чеснока, но зашифрованные с использованием алгоритмов Эль-Гемаля(ElGamal)/AES + SessionTags (см. ниже). “Честночные сообщения” Честночные сообщения, являются продолжением лукового («onion») шифрования, позволяет содержать в одном сообщении множество вложений («cloves»), которые являются полностью сформированными сообщениями со своими инструкциями к доставке. Сообщения заворачиваются в чеснок каждый раз, когда отправляются куда либо, в противном случае их содержимое будет видно посредническим узлам, а это не допустимо. В качестве примера, следующая ситуация: Роутер, отправляет запрос другому роутеру, для участия в создании туннеля, для этого он оборачивает сообщение в “чеснок” зашифровывает получая открытый ключ 2048 бит, по схеме Эль-Гамаля и отправляет сообщение через туннель. Другой пример, когда клиент хочет отправить сообщение получателю, роутер “отправителя” будет заворачивать данные сообщения (на ряду с другими сообщениями) в чеснок, шифровать чеснок с использованием 2048 битного открытого ключа по схеме Эль-Гамаля, опубликованного в leaseSet получателя и отправлять его через соответствующий туннель. Инструкции вложенные содержащиеся в дольках, дают возможность перенаправить локально (куда-либо), на удалённый роутер или в туннель к удалённому роутеру. В инструкциях есть специальные поля, позволяющие просить пиров отложить доставку до срабатывания определённого триггера (по времени или условию), если они не будут выполнены в пределах “нетривиальных задержек” (искусственно созданные задержки), то сообщение будет “развёрнуто” (are deployed). Если возможно, то чесночное сообщение может быть отправлено через любое количество hops (прыжков — промежуточных точек), без построенного в заранее туннеля, до цели. Находясь в чесноке оно пройдёт заданное количество шагов, до доставки на следующий hop в туннеле. (прим. пер. — как я понимаю, это и есть пример нетривиальной задержки). В текущей реализации сети и клиента, этот метод недоступен. Сессионные теги Как ненадежны и неупорядоченны сообщения системы основанной на I2P, I2P использует простую комбинацию асимметричных и симметричных алгоритмов шифрования, чтобы обеспечить конфиденциальность и целостность данных вложенных в чесночные сообщения. Это сочетание обозначается как Эль-Гемаль/AES+SessionTags, но это излишне подробный способ описания использованных простых алгоритмов 2048 битового “ElGamal”, AES256, SHA256 и 32 байтного одноразового номера. Сначала роутер шифрует сообщение для другого роутера посредством AES256, в качестве ключа служит ElGamal и после него добавляется AES256/CBC шифрованная “полезная” (Дополнительная, транзитная) нагрузка. В дополнительной нагрузке, в зашифрованной AES секции содержится длинна нагрузки, SHA256 хеш, не шифрованной нагрузки и ряд сессионный тег — случайные 32 байта. В следующий раз, отправитель захочет зашифровать “чеснок” для другого роутера, но без генерации по ElGamal нового ключа, он просто может выбрать один из ранее поставленных сессионных тегов и AES шифрования полезной нагрузки, как и прежде, с помощью ключа использующего сессионный тег, который был добавлен до этого. Когда роутер получает сообщение, он проверяет первые 32 байта, чтобы узнать совпадают ли они с доступными сессионными тегами, если совпадают, то используется простая дешифровка AES, но если нет, то для первого блока используется алгоритм ElGamal. Каждый сессионный тег, может быть использован только один раз, для предотвращения внутренних конфликтов и из необходимости корреляции сообщений между маршрутизаторами. Чесночные сообщения позволяют контролировать состояние доставки, если доставка успешна, то выполняется триггер: Попадание к получателю -> успешная дешифровка -> поиск инструкции для обратного ответа, если найдена отправить ответ, по входящему туннелю обратно к отправителю. Когда отправитель получает сообщение о статусе доставки, он узнаёт, что сессионный тег в сообщении было успешно доставлены. Сессионные теги, сами по себе живут очень мало, после выхода срока жизни они удаляются, если не используются. Кроме того, количество сохранений для каждого ключа ограничено, ровно как и число самих ключей. Если тегов слишком много, то новые или старые могут быть “выкинуты”. Отправитель отслеживает свои сообщения с помощью тегов сессии, если нет необходимой связи они могут быть удалены перед тем, как сообщение будет раскрыто и сообщение вернётся назад благодаря дорожке из Эль-Гамаля шифрования. (прим. пер: PRNG — Псевдослучайный генератор чисел) Одним из вариантов является отправка, одного тега и прослушивание PRNG для определения, какие теги использовались и какие могут быть использованы. Поддерживая PRNG примерно синхронизированным между отправителем и получателем (получателем предварительно вычисляется размер следующего окна, например в 50 тегов), при слишком больших накладных расходах, теги удаляются что позволяет получить больше вариантов и получить компромисс между временем и объёмом, и сокращение количества ElGamal шифрований. Тем не менее, от сложности PRNG будет зависеть степень защиты от внутренних противников, возможно путём ограничения количество использований PRNG, любые слабости могут быть минимизированы. Но на данный момент, в обозримом будущем, нет планов двигаться в сторону развития синхронного PRNG.\n\n\nКраткое содержание:\nВ статье рассматривается протокол I2P, который обеспечивает анонимность и конфиденциальность в сети. Протокол использует многоуровневую защиту от различных угроз, включая шифрование на транспортном уровне (TLS) с использованием протокола SSU. Сообщения внутри туннеля также имеют собственное шифрование AES256/CBC с явным IV и проверкой целостности по хэш-алгоритму SHA256. Для обеспечения конфиденциальности данных используется сочетание асимметричных и симметричных алгоритмов шифрования, таких как ElGamal/AES+SessionTags. Сессионные теги позволяют контролировать состояние доставки и предотвращать внутренние конфликты.","summary":"В статье рассматривается протокол I2P, который обеспечивает анонимность и конфиденциальность в сети. Протокол использует многоуровневую защиту от различных угроз, включая шифрование на транспортном уровне (TLS) с использованием протокола SSU. Сообщения внутри туннеля также имеют собственное шифрование AES256/CBC с явным IV и проверкой целостности по хэш-алгоритму SHA256. Для обеспечения конфиденциальности данных используется сочетание асимметричных и симметричных алгоритмов шифрования, таких как ElGamal/AES+SessionTags. Сессионные теги позволяют контролировать состояние доставки и предотвращать внутренние конфликты."}
{"Text":"5 способов улучшить ваш CSS / Хабр                                                           \n\n25-02-2009\nПрактически любой человек сможет написать CSS-код, в наше время любая программа сделает это за вас. Но будет ли это хороший CSS? Далее представлены пять советов по совершенствованию вашего CSS 1. Reset Обязательно используете сброс настроек в том или ином виде. Вы можете использовать уже готовые решения ( Eric Meyer, YUI), или придумать свое, выбирайте что больше нравится. Это может быть обычное удаление полей и отступов, у всех элементов: html, body, div, h1, h2, h3, h4, h5, h6, ul, ol, dl, li, dt, dd, p, blockquote, pre, form, fieldset, table, th, td { margin: 0; padding: 0; } Указанные выше готовые решения, конечно весьма впечатляют, но мне кажется что они немного чрезмерны. Я так и представляю себе, как вы сбрасываете все настройки у всех элементов, а затем заново их устанавливаете. Поэтому если вы решили использовать один из предложенных вариантов, не стоит полностью копировать весь CSS-файл, лучше измените его так, чтобы он максимально отвечал требованиям вашего проекта. И пожалуйста не делайте следующего: * { margin: 0; padding: 0; } Этот прием увеличивает время обработки, и при удалении паддинга, некоторые элементы будут отображаться некорректно (например radio button). Элементы форм, при удалении всех настроек, могут вести себя непредсказуемо, поэтому сброс к ним лучше не применять. 2. Алфавитный порядок В каком из примеров по вашему мнению быстрее можно найти свойство margin-right? Пример 1 div#header h1 { z-index: 101; color: #000; position: relative; line-height: 24px; margin-right: 48px; border-bottom: 1px solid #dedede; font-size: 18px; } Пример 2 div#header h1 { border-bottom: 1px solid #dedede; color: #000; font-size: 18px; line-height: 24px; margin-right: 48px; position: relative; z-index: 101; } Согласитесь, что во втором примере свойство находится быстрее. Расположив свойства в алфавитном порядке, вы создадите эту последовательность, которая поможет вам сократить время, затрачиваемое на поиск специализированного свойства. Я знаю множество людей, располагающих CSS-свойства разными способами, но в нашей компании мы пришли к соглашению, располагать все свойства в алфавитном порядке. Это помогает, когда приходится работать с кодом, написанным другими людьми. Мне досадно каждый раз, когда приходится работать с css-файлом, в котором свойства не расположены по алфавиту. 3. Группировка Вы должны организовать ваш CSS-файл так, чтобы искомые объекты и связанные с ними свойства располагались рядом, также эффективно использование комментариев. Вот например мой способ группировки: /*****Reset*****/ Remove margin and padding from elements /*****Basic Elements*****/ Define styles for basic elements: body, h1-h6, ul, ol, a, p, etc. /*****Generic Classes*****/ Define styles for simple things like floating to the sides, removing a bottom margin on elements, etc Yes, these may not be as semantic as we would all like, but they are necessary for coding efficiently /*****Basic Layout*****/ Define the basic template: header, footer, etc. Elements that help to define the basic layout of the site /*****Header*****/ Define all elements in the header /*****Content*****/ Define all elements in the content area /*****Footer*****/ Define all elements in the footer /*****Etc*****/ Continue to define the other sections one by one Использование комментариев и группировка схожих элементов, помогает быстро находить необходимые объекты и их свойства. 4. Последовательность Какой бы способ написания кода вы не выбрали, придерживайтесь его. Меня уже тошнит от CSS-дебатов посвященных выбору правильного способа написания кода, 1-строчное против многострочного. Каждый имеет право на свое собственное мнение, так что выберите наиболее удобный для вас и используйте его во всех CSS-файлах. Лично я использую комбинацию обоих способов. Если селектор содержит больше трех свойств, я разбиваю его на несколько строк. div#header { float: left; width: 100%; } div#header div.column { border-right: 1px solid #ccc; float: right; margin-right: 50px; padding: 10px; width: 300px; } div#header h1 { float: left; position: relative; width: 250px; } 5. Начните правильно Не начинайте писать css-стиль, пока не закончена разметка страницы. Прежде чем создавать CSS-файл, я пишу всю разметку страницы начиная от открывающего тега body, до закрывающего. Я не добавляю лишних div-ов, id и классов, только некоторые характерные блоки, типа header, content, и footer. Используйте наследование CSS-селекторов, для расположения дочерних элементов, не добавляйте автоматически классы и id к элементам. Помните главное: CSS ничего не стоит без хорошо отоформатированного документа. Это далеко не полный список некоторых советов, которые помогают мне писать лучший код. А какие советы используете вы?\n","metric":0.57373046880000000414,"metric_input":"Текст статьи:\n5 способов улучшить ваш CSS / Хабр                                                           \n\n25-02-2009\nПрактически любой человек сможет написать CSS-код, в наше время любая программа сделает это за вас. Но будет ли это хороший CSS? Далее представлены пять советов по совершенствованию вашего CSS 1. Reset Обязательно используете сброс настроек в том или ином виде. Вы можете использовать уже готовые решения ( Eric Meyer, YUI), или придумать свое, выбирайте что больше нравится. Это может быть обычное удаление полей и отступов, у всех элементов: html, body, div, h1, h2, h3, h4, h5, h6, ul, ol, dl, li, dt, dd, p, blockquote, pre, form, fieldset, table, th, td { margin: 0; padding: 0; } Указанные выше готовые решения, конечно весьма впечатляют, но мне кажется что они немного чрезмерны. Я так и представляю себе, как вы сбрасываете все настройки у всех элементов, а затем заново их устанавливаете. Поэтому если вы решили использовать один из предложенных вариантов, не стоит полностью копировать весь CSS-файл, лучше измените его так, чтобы он максимально отвечал требованиям вашего проекта. И пожалуйста не делайте следующего: * { margin: 0; padding: 0; } Этот прием увеличивает время обработки, и при удалении паддинга, некоторые элементы будут отображаться некорректно (например radio button). Элементы форм, при удалении всех настроек, могут вести себя непредсказуемо, поэтому сброс к ним лучше не применять. 2. Алфавитный порядок В каком из примеров по вашему мнению быстрее можно найти свойство margin-right? Пример 1 div#header h1 { z-index: 101; color: #000; position: relative; line-height: 24px; margin-right: 48px; border-bottom: 1px solid #dedede; font-size: 18px; } Пример 2 div#header h1 { border-bottom: 1px solid #dedede; color: #000; font-size: 18px; line-height: 24px; margin-right: 48px; position: relative; z-index: 101; } Согласитесь, что во втором примере свойство находится быстрее. Расположив свойства в алфавитном порядке, вы создадите эту последовательность, которая поможет вам сократить время, затрачиваемое на поиск специализированного свойства. Я знаю множество людей, располагающих CSS-свойства разными способами, но в нашей компании мы пришли к соглашению, располагать все свойства в алфавитном порядке. Это помогает, когда приходится работать с кодом, написанным другими людьми. Мне досадно каждый раз, когда приходится работать с css-файлом, в котором свойства не расположены по алфавиту. 3. Группировка Вы должны организовать ваш CSS-файл так, чтобы искомые объекты и связанные с ними свойства располагались рядом, также эффективно использование комментариев. Вот например мой способ группировки: /*****Reset*****/ Remove margin and padding from elements /*****Basic Elements*****/ Define styles for basic elements: body, h1-h6, ul, ol, a, p, etc. /*****Generic Classes*****/ Define styles for simple things like floating to the sides, removing a bottom margin on elements, etc Yes, these may not be as semantic as we would all like, but they are necessary for coding efficiently /*****Basic Layout*****/ Define the basic template: header, footer, etc. Elements that help to define the basic layout of the site /*****Header*****/ Define all elements in the header /*****Content*****/ Define all elements in the content area /*****Footer*****/ Define all elements in the footer /*****Etc*****/ Continue to define the other sections one by one Использование комментариев и группировка схожих элементов, помогает быстро находить необходимые объекты и их свойства. 4. Последовательность Какой бы способ написания кода вы не выбрали, придерживайтесь его. Меня уже тошнит от CSS-дебатов посвященных выбору правильного способа написания кода, 1-строчное против многострочного. Каждый имеет право на свое собственное мнение, так что выберите наиболее удобный для вас и используйте его во всех CSS-файлах. Лично я использую комбинацию обоих способов. Если селектор содержит больше трех свойств, я разбиваю его на несколько строк. div#header { float: left; width: 100%; } div#header div.column { border-right: 1px solid #ccc; float: right; margin-right: 50px; padding: 10px; width: 300px; } div#header h1 { float: left; position: relative; width: 250px; } 5. Начните правильно Не начинайте писать css-стиль, пока не закончена разметка страницы. Прежде чем создавать CSS-файл, я пишу всю разметку страницы начиная от открывающего тега body, до закрывающего. Я не добавляю лишних div-ов, id и классов, только некоторые характерные блоки, типа header, content, и footer. Используйте наследование CSS-селекторов, для расположения дочерних элементов, не добавляйте автоматически классы и id к элементам. Помните главное: CSS ничего не стоит без хорошо отоформатированного документа. Это далеко не полный список некоторых советов, которые помогают мне писать лучший код. А какие советы используете вы?\n\n\nКраткое содержание:\nВ данной статье представлены пять советов по улучшению CSS-кода:\n\n1. Используйте сброс настроек для удаления ненужных стилей и упрощения работы с кодом.\n2. Располагайте свойства в алфавитном порядке для ускорения поиска нужного свойства.\n3. Группируйте схожие элементы и используйте комментарии для организации кода.\n4. Придерживайтесь выбранной последовательности написания кода и используйте разные способы написания (однострочный и многострочный) в зависимости от контекста.\n5. Начинайте писать CSS-стили только после завершения разметки страницы.\n\nАвтор статьи предлагает использовать эти советы для создания более качественного и удобного для чтения кода.","summary":"В данной статье представлены пять советов по улучшению CSS-кода:\n\n1. Используйте сброс настроек для удаления ненужных стилей и упрощения работы с кодом.\n2. Располагайте свойства в алфавитном порядке для ускорения поиска нужного свойства.\n3. Группируйте схожие элементы и используйте комментарии для организации кода.\n4. Придерживайтесь выбранной последовательности написания кода и используйте разные способы написания (однострочный и многострочный) в зависимости от контекста.\n5. Начинайте писать CSS-стили только после завершения разметки страницы.\n\nАвтор статьи предлагает использовать эти советы для создания более качественного и удобного для чтения кода."}
{"Text":"Сказ о виртуализации-кластеризации и СХД Fujitsu / Habr             \n\n15-08-2016\nДело было так. Одна маленькая организация государственных масштабов решила обновить во владении своем местном оборудование серверное. И обратились ее мужи к нашим доблестным менеджерам, мол, хотим мы серверов с дисками для сервисов наших важных. И узнали они от менеджеров о виртуализации заморской, отказоустойчивой да с кластеризацией. Собственно, долго сказка сказывается, да быстро дело делается. Так и появилась у организации той связка из двух серверов Fujitsu PRIMERGY RX 200 S8 и системы хранения данных ETERNUS DX 100 S3. И никто на тот момент не думал, что очень скоро ресурсов серверов тех не станет хватать. А наоборот, посчитали и были уверены, что хватит, и надолго. Но быстро машины виртуальные плодится стали и прожорливы они были да тучны. И тогда расширенно было пространство дисковое, а к двум RX 200 подселился брат их юный Fujitsu PRIMERGY RX2530 M1. О подселении нового сервера в кластер, а также о добавлении в контроллерные блоки новых карт расширения с дополнительными FC-интерфейсами мы и расскажем. А вернее, о простоте, безопасности и удобстве всех этих и других инженерных манипуляций, которые возможны (в нашем случае) с виртуализацией VMware и оборудованием Fujitsu. Те, кто уже имеет СХД и ту или иную реализацию виртуализации серверов (или просто в теме) не удивятся и, наверное, не почерпнут ничего нового из этого текста. Но есть еще много организаций, где какие-либо действия с оборудованием, его перезагрузкой и остановкой вызывают ужас и заставляют скрупулезно планировать и минимизировать предстоящий downtime. О системе хранения данных Как известно, для того чтобы добавить или убрать какой-либо компонент (не поддерживающий «горячую» замену) в уже работающем устройстве, предоставляющем сервисы и постоянно обменивающимся данными, это устройство требуется выключить. В случае с сервером такая остановка неизбежна, в случае с современными СХД, имеющими два контроллера, все делается без остановок и пауз. Действительно, помимо отказоустойчивости посредством многопутевого ввода-вывода (MultiPathing – каждый из хостов (серверов) имеет по несколько каналов передачи данных от своих I/O портов к портам каждого из контроллеров СХД), современные системы хранения данных (в частности, речь идет о Fujitsu ETERNUS DX 100 S3) умеют распределять/распараллеливать передачу данных по этим каналам, уменьшая нагрузку на канал и увеличивая пропускную способность. Время простоя при отказе одного из каналов сведено к нулю (active / active режим) либо минимизировано при автоматическом переключении с активного (но «потерянного») канала на пассивный – «ожидающий» (active / passive режим). Собственно, эти возможности, а также особенности операционной системы позволяют нам вносить изменения в контроллерные блоки (либо заменять их полностью) без простоев и перезагрузок. С DX 100 S3 это легко и просто выполняется следующим образом, отражающим всю безопасность и продуманность манипуляций: Войдя в GUI под учетной записью сервисного инженера (f.ce), всю систему переводим в режим обслуживания Далее, выбрав нужный нам контроллерный модуль, переводим его в режим обслуживания Система перед вводом искомого блока в maintenance mod выполнит ряд тестов безопасности и реализует запрос только если второй контроллер и зависимые компоненты работают штатно и целостности данных ничего не угрожает. Все логические диски, для которых отключенный контроллер являлся «владельцем», перейдут в обслуживание второго контроллера практически мгновенно и бесшовно после синхронизации кэша и смены путей передачи данных. В графическом интерфейсе станет видно, что контроллер больше не доступен и может быть извлечен. По окончании манипуляций, так же в онлайн режиме возвращаем его на место, система автоматически, без перезагрузок самих контроллерных блоков, распознает и вернет прежнюю схему работы. Появившиеся новые устройства (у нас это FC-адаптеры) нужно задействовать, добавив в систему. Вот и все, воистину у ETERNUS DX операционная система «реального времени». У линейки IBM (Lenovo) Storwize, например, данные манипуляции тоже проходят онлайн, но сами контроллеры выполняют цикл из нескольких поочередных перезагрузок после указания того, что это вовсе не ошибка и это мы добавили тот или иной компонент. И перезагрузки могут длиться, по показаниям системы, до 30 (!!!) минут каждая. Хочется добавить еще несколько слов о реализованном здесь алгоритме безопасности, можно сказать, защите от необдуманных действий. А суть его проста и изящна: нельзя что-либо удалить нечаянным нажатием не туда, куда надо. Например, мы не можем удалить RAID-массив или логический диск пока не разберем всю цепочку взаимосвязей с самого «конца», а именно – сначала нужно изъять LUN-группы из настроек связи с хостом (Host Affinity), далее сами логические диски убрать из LUN-групп, удалить логические диски и только потом разрушить RAID. Согласитесь, сделать такое случайно и непреднамеренно практически невозможно. Вкладки \"Delete\" LUN Group и RAID Group не активны до разрыва \"связей\" Стоит оговориться, что из командной строки (SSH) при необходимости принудительное удаление любых элементов доступно сразу. О серверах и виртуализации Как уже упоминалось ранее, серверная часть кластера изначально состояла из 2-х серверов Fujitsu PRIMERGY RX200 S8, к которым в дальнейшем добавили аналогичное по классу и производительности устройство Fujitsu PRIMERGY RX 2530 M1. По сути, архитектурно устройства весьма схожи, но новая линейка процессоров Intel (как следствие и новые функции, инструкции, протоколы) в RX 2530 M1 сулила нам проблемы на уровне кластера VMware, а именно, проблемы с миграцией машин между хостами. Что и проявилось после внедрения: некоторые из уже имеющихся машин при миграции выдавали ошибку, связанную как раз с отличием CPU целевого сервера. Конечно же, VMware предусмотрела решение для подобного рода проблем, ее функция EVC (Enhanced vMotion Compatibility) предназначена для «маскировки» различий в процессорах разных поколений. Увы, изначально кластер поднимался на пятой версии виртуальной сферы VMware (VMware vSphere 5.5), которая тогда еще «понятия не имела» о новых процессорах Intel. Однако суть в том, что для нашей ситуации эта беда – вовсе не беда. Имея отказоустойчивый кластер, обновление до версии VMware vSphere 6.0 (EVC которой имеет возможность работать со всеми версиями процессоров) занимает всего нескольких часов. При правильном использовании и планировании кластера любой из имеющихся в нем серверов можно свободно обслуживать, распределив его машины на оставшихся хостах, используя «живую» миграцию. Миграция машины производилась со сменой не только хоста, но и хранилища данных, так как выполнялась реорганизация дискового пространства СХД. Данный процесс протекает путем полного клонирования машины, оригинал которой после синхронизации автоматически удаляется. Одновременная миграция (сервер и диск) возможна только при выключенной виртуальной машине. В VMware vSphere 5.5 поддержка процессоров заканчивается поколением \"Sandy Bridge\" В VMware vSphere 6.0 мы можем наблюдать \"Ivy Bridge\" и \"Haswell\" Подводя итог этой небольшой истории о кластерной виртуализации, хочется процитировать строчку из песни – «Летать с тобой мне было трудно, но без тебя я не могу дышать!». Ибо решиться на это трудно, да и дорого. Но зато потом страшно представить, как раньше без всего этого обходились, вспоминая потраченные выходные, обеды и ночи, нервы, переживания и надежду успеть завершить задуманное до конца перерыва или начала рабочего дня. Подготовлено по материалу Третьякова Вячеслава, инженера компании «Парадигма». Полную версию статьи смотрите здесь.\n","metric":0.51806640630000000414,"metric_input":"Текст статьи:\nСказ о виртуализации-кластеризации и СХД Fujitsu / Habr             \n\n15-08-2016\nДело было так. Одна маленькая организация государственных масштабов решила обновить во владении своем местном оборудование серверное. И обратились ее мужи к нашим доблестным менеджерам, мол, хотим мы серверов с дисками для сервисов наших важных. И узнали они от менеджеров о виртуализации заморской, отказоустойчивой да с кластеризацией. Собственно, долго сказка сказывается, да быстро дело делается. Так и появилась у организации той связка из двух серверов Fujitsu PRIMERGY RX 200 S8 и системы хранения данных ETERNUS DX 100 S3. И никто на тот момент не думал, что очень скоро ресурсов серверов тех не станет хватать. А наоборот, посчитали и были уверены, что хватит, и надолго. Но быстро машины виртуальные плодится стали и прожорливы они были да тучны. И тогда расширенно было пространство дисковое, а к двум RX 200 подселился брат их юный Fujitsu PRIMERGY RX2530 M1. О подселении нового сервера в кластер, а также о добавлении в контроллерные блоки новых карт расширения с дополнительными FC-интерфейсами мы и расскажем. А вернее, о простоте, безопасности и удобстве всех этих и других инженерных манипуляций, которые возможны (в нашем случае) с виртуализацией VMware и оборудованием Fujitsu. Те, кто уже имеет СХД и ту или иную реализацию виртуализации серверов (или просто в теме) не удивятся и, наверное, не почерпнут ничего нового из этого текста. Но есть еще много организаций, где какие-либо действия с оборудованием, его перезагрузкой и остановкой вызывают ужас и заставляют скрупулезно планировать и минимизировать предстоящий downtime. О системе хранения данных Как известно, для того чтобы добавить или убрать какой-либо компонент (не поддерживающий «горячую» замену) в уже работающем устройстве, предоставляющем сервисы и постоянно обменивающимся данными, это устройство требуется выключить. В случае с сервером такая остановка неизбежна, в случае с современными СХД, имеющими два контроллера, все делается без остановок и пауз. Действительно, помимо отказоустойчивости посредством многопутевого ввода-вывода (MultiPathing – каждый из хостов (серверов) имеет по несколько каналов передачи данных от своих I/O портов к портам каждого из контроллеров СХД), современные системы хранения данных (в частности, речь идет о Fujitsu ETERNUS DX 100 S3) умеют распределять/распараллеливать передачу данных по этим каналам, уменьшая нагрузку на канал и увеличивая пропускную способность. Время простоя при отказе одного из каналов сведено к нулю (active / active режим) либо минимизировано при автоматическом переключении с активного (но «потерянного») канала на пассивный – «ожидающий» (active / passive режим). Собственно, эти возможности, а также особенности операционной системы позволяют нам вносить изменения в контроллерные блоки (либо заменять их полностью) без простоев и перезагрузок. С DX 100 S3 это легко и просто выполняется следующим образом, отражающим всю безопасность и продуманность манипуляций: Войдя в GUI под учетной записью сервисного инженера (f.ce), всю систему переводим в режим обслуживания Далее, выбрав нужный нам контроллерный модуль, переводим его в режим обслуживания Система перед вводом искомого блока в maintenance mod выполнит ряд тестов безопасности и реализует запрос только если второй контроллер и зависимые компоненты работают штатно и целостности данных ничего не угрожает. Все логические диски, для которых отключенный контроллер являлся «владельцем», перейдут в обслуживание второго контроллера практически мгновенно и бесшовно после синхронизации кэша и смены путей передачи данных. В графическом интерфейсе станет видно, что контроллер больше не доступен и может быть извлечен. По окончании манипуляций, так же в онлайн режиме возвращаем его на место, система автоматически, без перезагрузок самих контроллерных блоков, распознает и вернет прежнюю схему работы. Появившиеся новые устройства (у нас это FC-адаптеры) нужно задействовать, добавив в систему. Вот и все, воистину у ETERNUS DX операционная система «реального времени». У линейки IBM (Lenovo) Storwize, например, данные манипуляции тоже проходят онлайн, но сами контроллеры выполняют цикл из нескольких поочередных перезагрузок после указания того, что это вовсе не ошибка и это мы добавили тот или иной компонент. И перезагрузки могут длиться, по показаниям системы, до 30 (!!!) минут каждая. Хочется добавить еще несколько слов о реализованном здесь алгоритме безопасности, можно сказать, защите от необдуманных действий. А суть его проста и изящна: нельзя что-либо удалить нечаянным нажатием не туда, куда надо. Например, мы не можем удалить RAID-массив или логический диск пока не разберем всю цепочку взаимосвязей с самого «конца», а именно – сначала нужно изъять LUN-группы из настроек связи с хостом (Host Affinity), далее сами логические диски убрать из LUN-групп, удалить логические диски и только потом разрушить RAID. Согласитесь, сделать такое случайно и непреднамеренно практически невозможно. Вкладки \"Delete\" LUN Group и RAID Group не активны до разрыва \"связей\" Стоит оговориться, что из командной строки (SSH) при необходимости принудительное удаление любых элементов доступно сразу. О серверах и виртуализации Как уже упоминалось ранее, серверная часть кластера изначально состояла из 2-х серверов Fujitsu PRIMERGY RX200 S8, к которым в дальнейшем добавили аналогичное по классу и производительности устройство Fujitsu PRIMERGY RX 2530 M1. По сути, архитектурно устройства весьма схожи, но новая линейка процессоров Intel (как следствие и новые функции, инструкции, протоколы) в RX 2530 M1 сулила нам проблемы на уровне кластера VMware, а именно, проблемы с миграцией машин между хостами. Что и проявилось после внедрения: некоторые из уже имеющихся машин при миграции выдавали ошибку, связанную как раз с отличием CPU целевого сервера. Конечно же, VMware предусмотрела решение для подобного рода проблем, ее функция EVC (Enhanced vMotion Compatibility) предназначена для «маскировки» различий в процессорах разных поколений. Увы, изначально кластер поднимался на пятой версии виртуальной сферы VMware (VMware vSphere 5.5), которая тогда еще «понятия не имела» о новых процессорах Intel. Однако суть в том, что для нашей ситуации эта беда – вовсе не беда. Имея отказоустойчивый кластер, обновление до версии VMware vSphere 6.0 (EVC которой имеет возможность работать со всеми версиями процессоров) занимает всего нескольких часов. При правильном использовании и планировании кластера любой из имеющихся в нем серверов можно свободно обслуживать, распределив его машины на оставшихся хостах, используя «живую» миграцию. Миграция машины производилась со сменой не только хоста, но и хранилища данных, так как выполнялась реорганизация дискового пространства СХД. Данный процесс протекает путем полного клонирования машины, оригинал которой после синхронизации автоматически удаляется. Одновременная миграция (сервер и диск) возможна только при выключенной виртуальной машине. В VMware vSphere 5.5 поддержка процессоров заканчивается поколением \"Sandy Bridge\" В VMware vSphere 6.0 мы можем наблюдать \"Ivy Bridge\" и \"Haswell\" Подводя итог этой небольшой истории о кластерной виртуализации, хочется процитировать строчку из песни – «Летать с тобой мне было трудно, но без тебя я не могу дышать!». Ибо решиться на это трудно, да и дорого. Но зато потом страшно представить, как раньше без всего этого обходились, вспоминая потраченные выходные, обеды и ночи, нервы, переживания и надежду успеть завершить задуманное до конца перерыва или начала рабочего дня. Подготовлено по материалу Третьякова Вячеслава, инженера компании «Парадигма». Полную версию статьи смотрите здесь.\n\n\nКраткое содержание:\nСтатья рассказывает о том, как организация обновила свое оборудование серверного и систем хранения данных (СХД) Fujitsu ETERNUS DX 100 S3. Организация использовала виртуализацию VMware для создания отказоустойчивого кластера из двух серверов Fujitsu PRIMERGY RX 200 S8. В статье описываются возможности СХД Fujitsu по добавлению и удалению компонентов без остановки работы устройства. Также описываются особенности виртуализации VMware и особенности работы с оборудованием Fujitsu.","summary":"Статья рассказывает о том, как организация обновила свое оборудование серверного и систем хранения данных (СХД) Fujitsu ETERNUS DX 100 S3. Организация использовала виртуализацию VMware для создания отказоустойчивого кластера из двух серверов Fujitsu PRIMERGY RX 200 S8. В статье описываются возможности СХД Fujitsu по добавлению и удалению компонентов без остановки работы устройства. Также описываются особенности виртуализации VMware и особенности работы с оборудованием Fujitsu."}
{"Text":"Законы для жизни / Хабр                                                              \n\n27-01-2009\nСтрасть к машинам Информатика и программирование в частности уже неотделимы от нашей жизни. Это «наш хлеб» и «наше зрелище». Они делают нашу жизнь… Проще? Сложнее? Остановимся на том факте, что они делают нашу жизнь. Информатика влияет на человечество подобно словесности, философии, физике, математике. Решая свои проблемы с её помощью, мы в очередной раз (как в случае со всеми науками, теориями) описываем саму жизнь, её законы. Я не хочу делать далеко идущие умозаключения, разводить пустую демагогию. Постараюсь просто провести, кажущиеся интересными, некоторые параллели между законами информатики и законами мира, в котором мы и придумали эту информатику. Этот текст будет даже немного с юмором, но пищу для размышлений, думаю, вы все же найдете. Возможно, это не последний текст на данную тему. Поэтому здесь мы просто вспомним немного Дискретной математики. Результат жесткого совокупления Математики и Логики, из которого впоследствии и вырастет Информатика... На игрушки новорожденному не скупились, и дядя Тьюринг подарил малышу машину. Мальчик рос, изучал машинку, развивался. Научившись говорить хоть что-то разумное, чадо сказало дяде Тьюрингу и дяде Чёрчу (а те потом нам), что любая, интуитивно вычислимая функция, является частично вычислимой, или, что тоже самое, может быть вычислена некоторой машиной Тьюринга. Ну, сказали и сказали… А потом Маккаллок и Питс математически строго доказали, что из нейроноподобных элементов можно построить сеть, способную осуществить любое алгоритмизируемое вычисление. Иными словами, было показано, что, по крайней мере, потенциально, мозг является универсальным компьютером, способным вычислить (в соответствии с тезисом Черча) все, что вычислимо в интуитивном смысле (в частности, вычислимо с помощью машины Тьюринга). С другой стороны, функцию любой нервной сети можно (как показывает практика) имитировать с помощью соответствующей компьютерной программы. Это означает, что мозг по своим функциональным возможностям тождественен машине Тьюринга и может быть оценен только по количественным параметрам: объему памяти и быстродействию (т.к. все универсальные компьютеры потенциально обладают одинаковыми возможностями — решают один и тот же класс проблем, известный как класс алгоритмически разрешимых проблем). Т.е. мозг, в силу своего устройства, способен вычислять любые алгоритмически вычислимые функции, если ему на это хватит объема памяти и быстродействия. Но вернёмся к уже подросшему чаду, к Дискретной математике. Я не знаю, чем его тогда обидели его родители, но в письме, которое подросток, уходя из отчего дома, передал Гёделю, и которое позже Гёдель выдал за свои научные работы, было написано примерно следующее: «Мама и Папа, во всякой достаточно богатой непротиворечивой теории первого порядка (в частности, во всякой непротиворечивой теории, включающей формальную арифметику), существует такая замкнутая формула F, что ни F, ни -|Fне являются выводимыми в этой теории». Гедель назвал это «Первой теоремой о неполноте». Понятия не имею, каким образом, но тогда поняли, что иначе говоря, в любой достаточно сложной непротиворечивой теории существует утверждение, которое средствами самой теории невозможно ни доказать, ни опровергнуть. Например, такое утверждение можно добавить к системе аксиом, оставив её непротиворечивой. При этом для новой теории (с увеличенным количеством аксиом) также будет существовать недоказуемое и неопровержимое утверждение. И всё сразу стало понятно, обиженный подросток спрятался у Философии. Надо сказать, что как раз тогда Дискретка и познакомился с Религией и впоследствии частенько вспоминал, как они на троих расписывали пульку у камина. Потом в 1931 году Гёдель найдет в дневнике заметку Дискретной матиматики о Религии: «Во всякой достаточно богатой непротиворечивой теории первого порядка (в частности, во всякой непротиворечивой теории, включающей формальную арифметику), формула, утверждающая непротиворечивость этой теории, не является выводимой в ней». Гедель назвал это «Второй теоремой о неполноте». Иными словами, непротиворечивость достаточно богатой теории не может быть доказана средствами этой теории. Однако вполне может оказаться, что непротиворечивость одной конкретной теории может быть установлена средствами другой, более мощной формальной теории. Но тогда встаёт вопрос о непротиворечивости этой второй теории, и т.д. Эта теорема имеет широкие последствия как для математики, так и для философии, в частности, для онтологии и философии науки. Более того — «мощность множества истинных утверждений больше мощности множества доказуемых утверждений». А если перевести с математического на человеческий — есть бесконечно много утверждений, доказать которые принципиально невозможно, но которые тем не менее верны! Так можно ли требовать от верующих доказательств существования Бога и, не получив таковые, утверждать, будто Бога нет? Страсть к машинам в дальнейшем у Дискретной математики переросла в серьезное занятие, можно сказать юноша был настоящим гонщиком. У него было несколько машин похожих на ту, что когда-то подарил ему дядя Тьюринг. Видели бы вы эти машины! Я бы близко не подошел. Но гениальный юноша на всех без исключения доезжал до финиша, какой сложности трасса бы не была! Понимаете теперь те, самые первые слова? «На машине Тьюринга я проеду любую трассу, которую сможет построить человечество когда либо»! Себастьян Лёб «нервно курил в сторонке», когда Дискретка подходил к этой машине. Да, Дискретка был знаменитым гонщиком! Сам Георг Кантор когда-то хвастался, что общался с Дискретной математикой о несчетности множества всех действительных чисел, существенном различии бесконечных множеств по их мощности, то есть по количеству содержащихся в них элементов. Но по моему, и тогда Дискретка гнал )! Вот такой Дискретной математикой была Информатика в юности. Родители технари и атеисты, а он верил в Бога, влюбился в Религию, обожал машины. Далее: «Без дураков»\n","metric":0.5185546875,"metric_input":"Текст статьи:\nЗаконы для жизни / Хабр                                                              \n\n27-01-2009\nСтрасть к машинам Информатика и программирование в частности уже неотделимы от нашей жизни. Это «наш хлеб» и «наше зрелище». Они делают нашу жизнь… Проще? Сложнее? Остановимся на том факте, что они делают нашу жизнь. Информатика влияет на человечество подобно словесности, философии, физике, математике. Решая свои проблемы с её помощью, мы в очередной раз (как в случае со всеми науками, теориями) описываем саму жизнь, её законы. Я не хочу делать далеко идущие умозаключения, разводить пустую демагогию. Постараюсь просто провести, кажущиеся интересными, некоторые параллели между законами информатики и законами мира, в котором мы и придумали эту информатику. Этот текст будет даже немного с юмором, но пищу для размышлений, думаю, вы все же найдете. Возможно, это не последний текст на данную тему. Поэтому здесь мы просто вспомним немного Дискретной математики. Результат жесткого совокупления Математики и Логики, из которого впоследствии и вырастет Информатика... На игрушки новорожденному не скупились, и дядя Тьюринг подарил малышу машину. Мальчик рос, изучал машинку, развивался. Научившись говорить хоть что-то разумное, чадо сказало дяде Тьюрингу и дяде Чёрчу (а те потом нам), что любая, интуитивно вычислимая функция, является частично вычислимой, или, что тоже самое, может быть вычислена некоторой машиной Тьюринга. Ну, сказали и сказали… А потом Маккаллок и Питс математически строго доказали, что из нейроноподобных элементов можно построить сеть, способную осуществить любое алгоритмизируемое вычисление. Иными словами, было показано, что, по крайней мере, потенциально, мозг является универсальным компьютером, способным вычислить (в соответствии с тезисом Черча) все, что вычислимо в интуитивном смысле (в частности, вычислимо с помощью машины Тьюринга). С другой стороны, функцию любой нервной сети можно (как показывает практика) имитировать с помощью соответствующей компьютерной программы. Это означает, что мозг по своим функциональным возможностям тождественен машине Тьюринга и может быть оценен только по количественным параметрам: объему памяти и быстродействию (т.к. все универсальные компьютеры потенциально обладают одинаковыми возможностями — решают один и тот же класс проблем, известный как класс алгоритмически разрешимых проблем). Т.е. мозг, в силу своего устройства, способен вычислять любые алгоритмически вычислимые функции, если ему на это хватит объема памяти и быстродействия. Но вернёмся к уже подросшему чаду, к Дискретной математике. Я не знаю, чем его тогда обидели его родители, но в письме, которое подросток, уходя из отчего дома, передал Гёделю, и которое позже Гёдель выдал за свои научные работы, было написано примерно следующее: «Мама и Папа, во всякой достаточно богатой непротиворечивой теории первого порядка (в частности, во всякой непротиворечивой теории, включающей формальную арифметику), существует такая замкнутая формула F, что ни F, ни -|Fне являются выводимыми в этой теории». Гедель назвал это «Первой теоремой о неполноте». Понятия не имею, каким образом, но тогда поняли, что иначе говоря, в любой достаточно сложной непротиворечивой теории существует утверждение, которое средствами самой теории невозможно ни доказать, ни опровергнуть. Например, такое утверждение можно добавить к системе аксиом, оставив её непротиворечивой. При этом для новой теории (с увеличенным количеством аксиом) также будет существовать недоказуемое и неопровержимое утверждение. И всё сразу стало понятно, обиженный подросток спрятался у Философии. Надо сказать, что как раз тогда Дискретка и познакомился с Религией и впоследствии частенько вспоминал, как они на троих расписывали пульку у камина. Потом в 1931 году Гёдель найдет в дневнике заметку Дискретной матиматики о Религии: «Во всякой достаточно богатой непротиворечивой теории первого порядка (в частности, во всякой непротиворечивой теории, включающей формальную арифметику), формула, утверждающая непротиворечивость этой теории, не является выводимой в ней». Гедель назвал это «Второй теоремой о неполноте». Иными словами, непротиворечивость достаточно богатой теории не может быть доказана средствами этой теории. Однако вполне может оказаться, что непротиворечивость одной конкретной теории может быть установлена средствами другой, более мощной формальной теории. Но тогда встаёт вопрос о непротиворечивости этой второй теории, и т.д. Эта теорема имеет широкие последствия как для математики, так и для философии, в частности, для онтологии и философии науки. Более того — «мощность множества истинных утверждений больше мощности множества доказуемых утверждений». А если перевести с математического на человеческий — есть бесконечно много утверждений, доказать которые принципиально невозможно, но которые тем не менее верны! Так можно ли требовать от верующих доказательств существования Бога и, не получив таковые, утверждать, будто Бога нет? Страсть к машинам в дальнейшем у Дискретной математики переросла в серьезное занятие, можно сказать юноша был настоящим гонщиком. У него было несколько машин похожих на ту, что когда-то подарил ему дядя Тьюринг. Видели бы вы эти машины! Я бы близко не подошел. Но гениальный юноша на всех без исключения доезжал до финиша, какой сложности трасса бы не была! Понимаете теперь те, самые первые слова? «На машине Тьюринга я проеду любую трассу, которую сможет построить человечество когда либо»! Себастьян Лёб «нервно курил в сторонке», когда Дискретка подходил к этой машине. Да, Дискретка был знаменитым гонщиком! Сам Георг Кантор когда-то хвастался, что общался с Дискретной математикой о несчетности множества всех действительных чисел, существенном различии бесконечных множеств по их мощности, то есть по количеству содержащихся в них элементов. Но по моему, и тогда Дискретка гнал )! Вот такой Дискретной математикой была Информатика в юности. Родители технари и атеисты, а он верил в Бога, влюбился в Религию, обожал машины. Далее: «Без дураков»\n\n\nКраткое содержание:\nСтатья обсуждает параллели между законами информатики и законами мира, а также некоторые интересные факты из истории информатики. В частности, рассматривается связь между информатикой и математикой, включая работу Алана Тьюринга, Джона Маккалока и Уильяма Питса, а также теоремы Геделя о неполноте. Статья также упоминает связь между информатикой и философией, а также роль Дискретной математики в развитии информатики.","summary":"Статья обсуждает параллели между законами информатики и законами мира, а также некоторые интересные факты из истории информатики. В частности, рассматривается связь между информатикой и математикой, включая работу Алана Тьюринга, Джона Маккалока и Уильяма Питса, а также теоремы Геделя о неполноте. Статья также упоминает связь между информатикой и философией, а также роль Дискретной математики в развитии информатики."}
{"Text":"8 навыков, которые делают вас шеф-поваром (или любым другим бизнес-лидером) / Habr            \n\n23-12-2015\nДовольно сложно объяснить, как функционирует благополучная, хорошо работающая кухня — здесь слишком много движущихся элементов. Чтобы стать шефом, нужно уметь превосходно готовить. Но одна эта способность не сделает вас лидером. Для создания собственного стиля лидерства, который за вас скажет: «Вот, кто я!», придется потратить время и силы, овладевая мастерством. Как это сделать? Издалека, вероятно, обычная ресторанная кухня выглядит, как множество людей, занятых созданием одной и той же вещи, и очень похожа на конвейер. Но если вы приглядитесь получше, то увидите, что все компоненты сочетаются определенным образом и направлены на выполнение конкретной цели. Заметьте, превращение в шефа не освобождает вас от готовки, напротив, эти обязанности дополняют друг друга и являются частями одного набора навыков. Поэтому настоящий вопрос вот в чем: как отличный повар превращается в шефа? Шеф — это человек, умеющий прекрасно готовить и в то же время обладающий способностью управлять, руководить и создавать успешный производственный процесс на кухне в ресторане или где-то еще. Одна из основных проблем заключается в том, что перечень навыков, необходимых для выполнения практической, технической части работы, которую львиная доля из нас любит больше всего, существенно отличается от требований остальных жизненно важных компонентов должности. Поэтому да, нужно обладать не только очевидными практическими навыками и техниками, но и многими другими умениями. Шефы отличаются друг на друга, как и представители других креативных профессий, ведь, чтобы выразить творческий замысел, им приходится использовать много разных факторов. Они отличаются от других творцов, деятельность которых напрямую зависит от того, что они создают: автор — пишет, художник — рисует, а актер — играет. В руках кулинаров – масса постоянно изменяющихся параметров, влияющих на результат, и ими необходимо управлять соответствующим образом. Учитывая все вышесказанное, существует несколько талантов и навыков, необходимых для того, чтобы быть шефом. 1. Умение готовить. Вы должны быть лучшим поваром в здании в любой момент времени. Если у ваших линейных поваров или су-шефов есть вопросы, или они не уверены в технике, вы должны быть там и помогать им. Это не означает, что вам нужно знать ответы на все вопросы, но, разумеется, чем больше времени вы проведете на кухне, оттачивая свое мастерство, тем меньше тайн для вас останется. Пока вы не освоите техническую сторону приготовления блюд в острых, напряженных и стрессовых ситуациях, никто, даже ваш штат, не станет воспринимать вас всерьез. 2. Замысел. Одна из основных способностей творца — умение видеть еще несуществующие вещи и находить способы претворить их в жизнь. Однако замысел может вызывать страх, ведь вам нужно самим вмешиваться в процесс и осуществлять его. Это пугает, потому что мы не знаем, как отреагируют клиенты, боссы и сотрудники. В любом случае именно это делает вас творцами, и является частью того, что заставляет людей верить вам. Чтобы добиться результата, нужно уметь точно формулировать свои идеи, ведь команда должна четко представлять себе, куда она движется. 3. Знание цифр. Если вы не можете в уме подсчитать стоимость блюда за 15—20 секунд, вы не шеф. Это говорит о том, что вы слишком мало времени провели за оценкой меню, составлением рецептов и раскладыванием блюд на тарелки. Вам необходимо знать, сколько стоит каждая порция, чтобы потом рассчитывать меню. Но это лишь часть цифр: вы должны уметь нормировать рецепты, оценивать труд, проводить инвентаризацию, сокращать ее длительность, когда время поджимает, и этот список можно продолжать бесконечно. 4. Систематичность. Большинство поваров от природы не так уж организованы и ужасно суетливы, часто позволяют синдрому дефицита внимания брать верх над собой. Но со временем мы понимаем: чтобы стать лучшими кулинарами, нам нужно научиться организовывать собственное время и рабочее место. Необходимо создавать рецепты и процессы, которые помогут командам достигать успеха. Вы должны постоянно стремиться к повышению эффективности и производительности, не жертвуя при этом качеством. 5. Способность к развитию эмоциональной отзывчивости. Мы все знакомы с мнением, что шефы — это обыкновенные сволочи на кухне, и, разумеется, такие персонажи иногда встречаются. Но чем больше кулинарная индустрия и профессия превращаются в востребованный вариант самовыражения, тем быстрее число подобных лиц сходит на нет. Действительно, это здорово. Будучи шефами и руководителями, вы должны уметь управлять людьми, создавать у них ощущение востребованности и, главное, своими действиями доказывать им, что вы на их стороне и поддерживаете их. Чтобы быть успешным шефом, вам необходимо научиться устанавливать связи с окружающими людьми — с теми, на кого вы полагаетесь. Ведь если вы не сумеете этого сделать, они никогда не станут искренне симпатизировать вам и не будут чувствовать себя частью того, что вы строите — а это ключевой момент. Ведите свою команду вперед, при любом удобном случае интересуйтесь сотрудниками, дисциплинируйте их, если это необходимо. Позволяйте им ошибаться и разбирайте недочеты так, чтобы они могли учиться и расти над собой, как повара и как личности, а не с точностью до наоборот (как это часто бывает). В распоряжении шефа может быть все, что угодно, но если у него/нее нет сплоченной группы людей, сосредоточенной на ежедневном выполнении своей миссии, то у него/нее нет ничего. 6. Дисциплина (показатель выдающегося мастерства). Все начинается сверху. Большинство кулинаров не готовы становиться шефами, потому что не знают, как установить стандарт. Корпоративная культура, чистота, обслуживание клиентов, подача блюд — все это начинается с шефа. Почему во многих ресторанах стандарты для продуктов, отпускаемых с кухни, так низки? Может быть, так повелось с самого начала или сформировалось со временем, но, в любом случае, каждый божий день множество людей допускает, чтобы тарелки с неаппетитными блюдами покидали их кухню. Как говорит тренер Джон Вуден: «Если у вас нет времени, чтобы делать хорошо, откуда возьмется время на исправление?» 7. Самый усердный человек в здании. Есть только один способ доказать, что вы хотите чего-то добиться: изо всех сил спешить сделать нечто, имеющее для вас значение. Именно этим занимаются великие шефы во всем мире. Они первыми входят в дверь по утрам, и часто покидают рабочее место одними из последних. Все просто и понятно: если вы чего-то хотите, нужно выкладываться. 8. Точка зрения. Для создания собственного стиля, который за вас скажет: «Вот, кто я!», придется потратить время и силы, овладевая мастерством. И будучи шефами, вы должны быть в состоянии добиться этого. Чтобы выяснить, какие кухни, приправы и стили вдохновляют и говорят о том, кто мы и кем хотим быть, нужно учиться у окружающих и трудиться для других. Вам просто нужно обращать внимание на работу, которой вы занимаетесь, время от времени пытаться отстраняться от нее, внимательно прислушиваться к своему сердцу и говорить себе: «Я хочу этим заниматься — значит, это я и есть!» Итак, на каком вы этапе превращения в шефа? P.S. Рекомендуем ещё одну статью по теме — Забудьте о постановке целей и сосредоточьтесь на Этом. Автор перевода — Давиденко Вячеслав, основатель компании MBA Consult.\n","metric":0.578125,"metric_input":"Текст статьи:\n8 навыков, которые делают вас шеф-поваром (или любым другим бизнес-лидером) / Habr            \n\n23-12-2015\nДовольно сложно объяснить, как функционирует благополучная, хорошо работающая кухня — здесь слишком много движущихся элементов. Чтобы стать шефом, нужно уметь превосходно готовить. Но одна эта способность не сделает вас лидером. Для создания собственного стиля лидерства, который за вас скажет: «Вот, кто я!», придется потратить время и силы, овладевая мастерством. Как это сделать? Издалека, вероятно, обычная ресторанная кухня выглядит, как множество людей, занятых созданием одной и той же вещи, и очень похожа на конвейер. Но если вы приглядитесь получше, то увидите, что все компоненты сочетаются определенным образом и направлены на выполнение конкретной цели. Заметьте, превращение в шефа не освобождает вас от готовки, напротив, эти обязанности дополняют друг друга и являются частями одного набора навыков. Поэтому настоящий вопрос вот в чем: как отличный повар превращается в шефа? Шеф — это человек, умеющий прекрасно готовить и в то же время обладающий способностью управлять, руководить и создавать успешный производственный процесс на кухне в ресторане или где-то еще. Одна из основных проблем заключается в том, что перечень навыков, необходимых для выполнения практической, технической части работы, которую львиная доля из нас любит больше всего, существенно отличается от требований остальных жизненно важных компонентов должности. Поэтому да, нужно обладать не только очевидными практическими навыками и техниками, но и многими другими умениями. Шефы отличаются друг на друга, как и представители других креативных профессий, ведь, чтобы выразить творческий замысел, им приходится использовать много разных факторов. Они отличаются от других творцов, деятельность которых напрямую зависит от того, что они создают: автор — пишет, художник — рисует, а актер — играет. В руках кулинаров – масса постоянно изменяющихся параметров, влияющих на результат, и ими необходимо управлять соответствующим образом. Учитывая все вышесказанное, существует несколько талантов и навыков, необходимых для того, чтобы быть шефом. 1. Умение готовить. Вы должны быть лучшим поваром в здании в любой момент времени. Если у ваших линейных поваров или су-шефов есть вопросы, или они не уверены в технике, вы должны быть там и помогать им. Это не означает, что вам нужно знать ответы на все вопросы, но, разумеется, чем больше времени вы проведете на кухне, оттачивая свое мастерство, тем меньше тайн для вас останется. Пока вы не освоите техническую сторону приготовления блюд в острых, напряженных и стрессовых ситуациях, никто, даже ваш штат, не станет воспринимать вас всерьез. 2. Замысел. Одна из основных способностей творца — умение видеть еще несуществующие вещи и находить способы претворить их в жизнь. Однако замысел может вызывать страх, ведь вам нужно самим вмешиваться в процесс и осуществлять его. Это пугает, потому что мы не знаем, как отреагируют клиенты, боссы и сотрудники. В любом случае именно это делает вас творцами, и является частью того, что заставляет людей верить вам. Чтобы добиться результата, нужно уметь точно формулировать свои идеи, ведь команда должна четко представлять себе, куда она движется. 3. Знание цифр. Если вы не можете в уме подсчитать стоимость блюда за 15—20 секунд, вы не шеф. Это говорит о том, что вы слишком мало времени провели за оценкой меню, составлением рецептов и раскладыванием блюд на тарелки. Вам необходимо знать, сколько стоит каждая порция, чтобы потом рассчитывать меню. Но это лишь часть цифр: вы должны уметь нормировать рецепты, оценивать труд, проводить инвентаризацию, сокращать ее длительность, когда время поджимает, и этот список можно продолжать бесконечно. 4. Систематичность. Большинство поваров от природы не так уж организованы и ужасно суетливы, часто позволяют синдрому дефицита внимания брать верх над собой. Но со временем мы понимаем: чтобы стать лучшими кулинарами, нам нужно научиться организовывать собственное время и рабочее место. Необходимо создавать рецепты и процессы, которые помогут командам достигать успеха. Вы должны постоянно стремиться к повышению эффективности и производительности, не жертвуя при этом качеством. 5. Способность к развитию эмоциональной отзывчивости. Мы все знакомы с мнением, что шефы — это обыкновенные сволочи на кухне, и, разумеется, такие персонажи иногда встречаются. Но чем больше кулинарная индустрия и профессия превращаются в востребованный вариант самовыражения, тем быстрее число подобных лиц сходит на нет. Действительно, это здорово. Будучи шефами и руководителями, вы должны уметь управлять людьми, создавать у них ощущение востребованности и, главное, своими действиями доказывать им, что вы на их стороне и поддерживаете их. Чтобы быть успешным шефом, вам необходимо научиться устанавливать связи с окружающими людьми — с теми, на кого вы полагаетесь. Ведь если вы не сумеете этого сделать, они никогда не станут искренне симпатизировать вам и не будут чувствовать себя частью того, что вы строите — а это ключевой момент. Ведите свою команду вперед, при любом удобном случае интересуйтесь сотрудниками, дисциплинируйте их, если это необходимо. Позволяйте им ошибаться и разбирайте недочеты так, чтобы они могли учиться и расти над собой, как повара и как личности, а не с точностью до наоборот (как это часто бывает). В распоряжении шефа может быть все, что угодно, но если у него/нее нет сплоченной группы людей, сосредоточенной на ежедневном выполнении своей миссии, то у него/нее нет ничего. 6. Дисциплина (показатель выдающегося мастерства). Все начинается сверху. Большинство кулинаров не готовы становиться шефами, потому что не знают, как установить стандарт. Корпоративная культура, чистота, обслуживание клиентов, подача блюд — все это начинается с шефа. Почему во многих ресторанах стандарты для продуктов, отпускаемых с кухни, так низки? Может быть, так повелось с самого начала или сформировалось со временем, но, в любом случае, каждый божий день множество людей допускает, чтобы тарелки с неаппетитными блюдами покидали их кухню. Как говорит тренер Джон Вуден: «Если у вас нет времени, чтобы делать хорошо, откуда возьмется время на исправление?» 7. Самый усердный человек в здании. Есть только один способ доказать, что вы хотите чего-то добиться: изо всех сил спешить сделать нечто, имеющее для вас значение. Именно этим занимаются великие шефы во всем мире. Они первыми входят в дверь по утрам, и часто покидают рабочее место одними из последних. Все просто и понятно: если вы чего-то хотите, нужно выкладываться. 8. Точка зрения. Для создания собственного стиля, который за вас скажет: «Вот, кто я!», придется потратить время и силы, овладевая мастерством. И будучи шефами, вы должны быть в состоянии добиться этого. Чтобы выяснить, какие кухни, приправы и стили вдохновляют и говорят о том, кто мы и кем хотим быть, нужно учиться у окружающих и трудиться для других. Вам просто нужно обращать внимание на работу, которой вы занимаетесь, время от времени пытаться отстраняться от нее, внимательно прислушиваться к своему сердцу и говорить себе: «Я хочу этим заниматься — значит, это я и есть!» Итак, на каком вы этапе превращения в шефа? P.S. Рекомендуем ещё одну статью по теме — Забудьте о постановке целей и сосредоточьтесь на Этом. Автор перевода — Давиденко Вячеслав, основатель компании MBA Consult.\n\n\nКраткое содержание:\nСтатья обсуждает процесс превращения обычного повара в успешного шеф-повара или бизнес-лидера. Она выделяет восемь ключевых навыков, необходимых для этого:\n\n1. Отличное кулинарное мастерство: Быть лучшим поваром в здании в любой момент времени.\n2. Способность видеть и формулировать идеи: Уметь видеть еще несуществующие вещи и находить способы претворить их в жизнь.\n3. Знание цифр: Уметь подсчитывать стоимость блюд и управлять процессами.\n4. Систематичность: Быть организованным и эффективным в организации рабочего времени и пространства.\n5. Эмоциональная отзывчивость: Уметь управлять людьми и создавать у них ощущение востребованности.\n6. Дисциплина: Установить стандарты и поддерживать корпоративную культуру.\n7. Усердие: Вкладывать все силы в работу.\n8. Творческий подход: Изучать и вдохновляться разными стилями и кухнями.\n\nСтатья призывает не останавливаться на одном кулинарном мастерстве, а развивать все эти навыки, чтобы стать успешным шеф-поваром и лидером.","summary":"Статья обсуждает процесс превращения обычного повара в успешного шеф-повара или бизнес-лидера. Она выделяет восемь ключевых навыков, необходимых для этого:\n\n1. Отличное кулинарное мастерство: Быть лучшим поваром в здании в любой момент времени.\n2. Способность видеть и формулировать идеи: Уметь видеть еще несуществующие вещи и находить способы претворить их в жизнь.\n3. Знание цифр: Уметь подсчитывать стоимость блюд и управлять процессами.\n4. Систематичность: Быть организованным и эффективным в организации рабочего времени и пространства.\n5. Эмоциональная отзывчивость: Уметь управлять людьми и создавать у них ощущение востребованности.\n6. Дисциплина: Установить стандарты и поддерживать корпоративную культуру.\n7. Усердие: Вкладывать все силы в работу.\n8. Творческий подход: Изучать и вдохновляться разными стилями и кухнями.\n\nСтатья призывает не останавливаться на одном кулинарном мастерстве, а развивать все эти навыки, чтобы стать успешным шеф-поваром и лидером."}
{"Text":"Затишье на рынке первичного размещения акций технологических компаний — проверка инвестиционной реальности / Habr             \n\nРынок первичного размещения акций (IPO) технологических компаний заморожен. Ситуация с поставщиком систем памяти Nutanix дополнительно подтверждает это.\n\nПрошло более трёх месяцев, как «дебютировали» компании Square и Match Group, но ни одна компания, занимающаяся программным обеспечением и Интернетом, не последовала за ними. Семь из последних 10-ти технологических компаний, вышедших на публичный рынок, торговали ниже их цены размещения, а четыре упали более чем на 20 процентов (по данным FactSet).\n\nКомпания Nutanix, разместившая свой\nпроспект\nэмиссии акций в декабре, решила воздержаться от продажи акций, пока нестабильность фондового рынка не уменьшится, — так сообщают знающие источники. Разработчик технологии дата-центров, объединяющих хранение данных, серверы и виртуализацию, собирался выйти на публичный рынок в конце января, но его банки посоветовали ему подождать, когда рынок остынет, — продолжают источники, которые просили их не называть, поскольку обсуждения были приватными.\n\nПресс-секретарь подразделения Nutanix, расположенного в г. Сан-Хосе, Калифорния, отказался от комментариев. Среди упомянутых банков — Goldman Sachs, Morgan Stanley и JPMorgan.\n\nБольше никакие другие высококлассные технологические компании публично не заявлены. В прошедшем месяце онлайн-кредитор Elevate Credit задержал свою эмиссию акций из-за условий рынка, поскольку этот январь оказался наихудшим месяцем по индексу Nasdaq с 2010 года. Этот индекс опустился, в общей сложности, на 8,5 процентов относительно начала года, хотя и продемонстрировал некоторый рост на последней неделе.\n\n«Компании не ощущают перспективу, поэтому говорят: „Давайте воздержимся“», так считает Афтэб Джамиль, партнёр в бухгалтерской и консалтинговой фирме BDO, США, и руководитель подразделения технологии и прикладных медико-биологических разработок в этой фирме. «Такое настроение, определённо, присутствует.»\n\nВ соответствии с ежегодным обзором, подготавливаемым фирмой BDO по 100 технологическим компаниям, менее половины финансовых директоров ожидает, что оценки капитализации возрастут в 2016 году.\n\nУ финансистов Кремниевой долины всё это вызывает беспокойство. Большие IPO являются их источником жизненной силы. Именно на них венчурные инвесторы получают гигантские прибыли и возвращают себе миллиарды долларов, ежегодно вкладываемых ими в такие стартапы, которые, в конечном счёте, банкротятся или не приносят доход.\n\nПрошлый год должен был быть рекордным для IPO. Индекс Nasdaq преодолел 5 000 впервые с 2000 года и рос четвёртый год подряд. Инвесторы проявили желание рисковать, поднимая цену для Amazon.com и Netflix — двух лучших перформеров года c большой капитализацией.\n\nИ всё же технологические компании шли на публичном рынке с самым низким темпом с 2009 года.\n\nВместо IPO самые быстрорастущие компании поднимались в ходе мегациклов наличных средств на частном рынке в оценке капитализации и кратности годового дохода, что превысило вклады инвесторов публичного рынка. Хедж-фонды и фирмы, занимающиеся операциями с частными акциями, исказили цикл IPO, направив уйму средств в компании, но не вознаграждая ранних инвесторов и сотрудников так, как это делают при публичном предложении акций.\n\nЧастное финансирование на поздней стадии почти удвоилось — с 8,9 млрд. долларов в 2013 году до 16-ти в прошлом — по данным Национальной ассоциации венчурного капитала; среди компаний были Uber, Airbnb, SpaceX и SoFi, каждая из которых оценивалась, как минимум, в 1 млрд. долларов.\n\nТакие компании, возможно, собрали достаточно средств, чтобы действовать длительно, но многие компании с венчурным капиталом из списка «140-плюс», оценённые в 1 млрд. долларов или больше, оказываются перед необходимостью принимать более рациональное окружение — высказывается венчурный инвестор Давид Голден.\n\nЭто могло бы означать цикл крайне разводняющих частных инвестиций, продажу бизнеса в убыток некоторым инвесторам или выход на публичный рынок при цене размещения ниже той, что заплатили более ранние инвесторы.\n\n«Люди просто должны вернуться к реальности», говорит г-н Голден, управляющий партнёр в компании Revolution Ventures в Сан-Франциско и бывший руководитель подразделения технологических инвестиций банка JPMorgan. «Когда рынки говорят, нам может не нравиться то, что они говорят, но мы должны слушать.»\n\nЭмиссия акций Nutanix, которая — по информации источников — всё ещё может произойти, когда рынки застабилизируются, будет поучительным событием. Одна из самых быстрорастущих технологических компаний Кремниевой долины выросла на 145 млн. долларов в 2014 году и достигла оценки капитализации в 2 млрд. долларов (13,40 доллара за акцию).\n\nСогласно проспекту Nutanix, реальная стоимость её конвертируемой привилегированной акции серии D (начало финансового года 2014) поднялась на 53 процента — с 10,72 доллара в июле 2014 до 16,36 в октябре 2015.\n\nЛюбое сравнение с недавней активностью на публичном рынке подсказывает, что Nutanix будет вынужден считаться с новой реальностью. Компания Nimble Storage потеряла почти три четверти своей стоимости с октября, а Pure Storage — прим. 25 процентов от своей цены выхода на публичный рынок 7 октября.\n\nРассматривая более широко, можно сказать, что любое действие софтверной фирмы, вызывающее высокий рост без доходности, ведёт к краху. Облачный индекс, подготавливаемый компанией Bessemer Venture Partners и отслеживающий поставщиков облачного программного обеспечения (43 компании), представленных на публичном рынке, провалился с января 2016 г. на 29 процентов.\n\nПравление Nutanix должно будет принять трудное решение. Если компания выйдет, всё-таки, со своим IPO и с ценой за акцию 10,93 доллара, то средства группы инвесторов, вложивших в 2013-14 гг., рассматривались бы как дополнительные выпущенные акции, ещё более разводняя остаток её базы акционеров.\n\nЭто — тип защиты, известный как «храповик», который был встроен в последние годы во многие поздние дорогостоящие этапы, особенно инвесторами в Box и Square.\n\nНо одновременно с тем, как начались трудности на рынке IPO, частные туры также стали более сложными; нетрадиционные инвесторы начали покидать арену.\n\nДля Nutanix нет какого-либо лёгкого пути выхода. И учитывая их количество, если компания не сокращает издержки и не жертвует ростом, то она, в конечном счёте, начинает искать заёмный капитал. В 2015 году Nutanix потратил 161,8 млн. долларов на продажи и маркетинг, что составляет 67 процентов от дохода. Чистый убыток — 126,1 млн. долларов, что лишь немного меньше всей наличности и эквивалентов на счетах.\n\nС июля 2013 года по октябрь 2015 число сотрудников возросло больше, чем впятеро — от 247 до 1 368, «и мы ожидаем значительное увеличение общего количества сотрудников в будущем», сказано в бумагах на регистрацию.\n\n#### Dow против Nasdaq ####\n\nЗдесь главная причина того, почему компания заявила о росте на 200 млн. долларов в своём первоначальном проспекте эмиссии.\n\nДоходы от IPO плюс наличные деньги компании, краткосрочные инвестиции и возможность получения кредита должны быть достаточными, чтобы удовлетворить её потребности в капитале на следующие 12 месяцев согласно проспекту.\n\nГлавное, что требуется компании Nutanix или любой другой компании с венчурным капиталом, — это менее изменчивый фондовый рынок, говорит г-н Голден из Revolution. Девятнадцать раз в этом году индекс Доу-Джонса поднимался или опускался, как минимум, на 200 пунктов.\n\n«Когда качание вверх-вниз достигает пары сотен пунктов в день, инвесторы теряют доверие, и андерайтерам нелегко оценивать фондовый рынок», говорит г-н Голден.\n\nПосле стабилизации рынка руководители компаний могут пройти через болезненный процесс разрешения инвесторам публичного рынка повторно оценить их бизнес и могут тогда определить, стоит ли предложение риска.\n\n«Можно продать акции хорошей компании по хорошей цене», продолжает г-н Голден. «То, что продающий акционер готов взять, определяет, открыт или закрыт рынок IPO.»","metric":0.28955078130000000414,"metric_input":"Текст статьи:\nЗатишье на рынке первичного размещения акций технологических компаний — проверка инвестиционной реальности / Habr             \n\nРынок первичного размещения акций (IPO) технологических компаний заморожен. Ситуация с поставщиком систем памяти Nutanix дополнительно подтверждает это.\n\nПрошло более трёх месяцев, как «дебютировали» компании Square и Match Group, но ни одна компания, занимающаяся программным обеспечением и Интернетом, не последовала за ними. Семь из последних 10-ти технологических компаний, вышедших на публичный рынок, торговали ниже их цены размещения, а четыре упали более чем на 20 процентов (по данным FactSet).\n\nКомпания Nutanix, разместившая свой\nпроспект\nэмиссии акций в декабре, решила воздержаться от продажи акций, пока нестабильность фондового рынка не уменьшится, — так сообщают знающие источники. Разработчик технологии дата-центров, объединяющих хранение данных, серверы и виртуализацию, собирался выйти на публичный рынок в конце января, но его банки посоветовали ему подождать, когда рынок остынет, — продолжают источники, которые просили их не называть, поскольку обсуждения были приватными.\n\nПресс-секретарь подразделения Nutanix, расположенного в г. Сан-Хосе, Калифорния, отказался от комментариев. Среди упомянутых банков — Goldman Sachs, Morgan Stanley и JPMorgan.\n\nБольше никакие другие высококлассные технологические компании публично не заявлены. В прошедшем месяце онлайн-кредитор Elevate Credit задержал свою эмиссию акций из-за условий рынка, поскольку этот январь оказался наихудшим месяцем по индексу Nasdaq с 2010 года. Этот индекс опустился, в общей сложности, на 8,5 процентов относительно начала года, хотя и продемонстрировал некоторый рост на последней неделе.\n\n«Компании не ощущают перспективу, поэтому говорят: „Давайте воздержимся“», так считает Афтэб Джамиль, партнёр в бухгалтерской и консалтинговой фирме BDO, США, и руководитель подразделения технологии и прикладных медико-биологических разработок в этой фирме. «Такое настроение, определённо, присутствует.»\n\nВ соответствии с ежегодным обзором, подготавливаемым фирмой BDO по 100 технологическим компаниям, менее половины финансовых директоров ожидает, что оценки капитализации возрастут в 2016 году.\n\nУ финансистов Кремниевой долины всё это вызывает беспокойство. Большие IPO являются их источником жизненной силы. Именно на них венчурные инвесторы получают гигантские прибыли и возвращают себе миллиарды долларов, ежегодно вкладываемых ими в такие стартапы, которые, в конечном счёте, банкротятся или не приносят доход.\n\nПрошлый год должен был быть рекордным для IPO. Индекс Nasdaq преодолел 5 000 впервые с 2000 года и рос четвёртый год подряд. Инвесторы проявили желание рисковать, поднимая цену для Amazon.com и Netflix — двух лучших перформеров года c большой капитализацией.\n\nИ всё же технологические компании шли на публичном рынке с самым низким темпом с 2009 года.\n\nВместо IPO самые быстрорастущие компании поднимались в ходе мегациклов наличных средств на частном рынке в оценке капитализации и кратности годового дохода, что превысило вклады инвесторов публичного рынка. Хедж-фонды и фирмы, занимающиеся операциями с частными акциями, исказили цикл IPO, направив уйму средств в компании, но не вознаграждая ранних инвесторов и сотрудников так, как это делают при публичном предложении акций.\n\nЧастное финансирование на поздней стадии почти удвоилось — с 8,9 млрд. долларов в 2013 году до 16-ти в прошлом — по данным Национальной ассоциации венчурного капитала; среди компаний были Uber, Airbnb, SpaceX и SoFi, каждая из которых оценивалась, как минимум, в 1 млрд. долларов.\n\nТакие компании, возможно, собрали достаточно средств, чтобы действовать длительно, но многие компании с венчурным капиталом из списка «140-плюс», оценённые в 1 млрд. долларов или больше, оказываются перед необходимостью принимать более рациональное окружение — высказывается венчурный инвестор Давид Голден.\n\nЭто могло бы означать цикл крайне разводняющих частных инвестиций, продажу бизнеса в убыток некоторым инвесторам или выход на публичный рынок при цене размещения ниже той, что заплатили более ранние инвесторы.\n\n«Люди просто должны вернуться к реальности», говорит г-н Голден, управляющий партнёр в компании Revolution Ventures в Сан-Франциско и бывший руководитель подразделения технологических инвестиций банка JPMorgan. «Когда рынки говорят, нам может не нравиться то, что они говорят, но мы должны слушать.»\n\nЭмиссия акций Nutanix, которая — по информации источников — всё ещё может произойти, когда рынки застабилизируются, будет поучительным событием. Одна из самых быстрорастущих технологических компаний Кремниевой долины выросла на 145 млн. долларов в 2014 году и достигла оценки капитализации в 2 млрд. долларов (13,40 доллара за акцию).\n\nСогласно проспекту Nutanix, реальная стоимость её конвертируемой привилегированной акции серии D (начало финансового года 2014) поднялась на 53 процента — с 10,72 доллара в июле 2014 до 16,36 в октябре 2015.\n\nЛюбое сравнение с недавней активностью на публичном рынке подсказывает, что Nutanix будет вынужден считаться с новой реальностью. Компания Nimble Storage потеряла почти три четверти своей стоимости с октября, а Pure Storage — прим. 25 процентов от своей цены выхода на публичный рынок 7 октября.\n\nРассматривая более широко, можно сказать, что любое действие софтверной фирмы, вызывающее высокий рост без доходности, ведёт к краху. Облачный индекс, подготавливаемый компанией Bessemer Venture Partners и отслеживающий поставщиков облачного программного обеспечения (43 компании), представленных на публичном рынке, провалился с января 2016 г. на 29 процентов.\n\nПравление Nutanix должно будет принять трудное решение. Если компания выйдет, всё-таки, со своим IPO и с ценой за акцию 10,93 доллара, то средства группы инвесторов, вложивших в 2013-14 гг., рассматривались бы как дополнительные выпущенные акции, ещё более разводняя остаток её базы акционеров.\n\nЭто — тип защиты, известный как «храповик», который был встроен в последние годы во многие поздние дорогостоящие этапы, особенно инвесторами в Box и Square.\n\nНо одновременно с тем, как начались трудности на рынке IPO, частные туры также стали более сложными; нетрадиционные инвесторы начали покидать арену.\n\nДля Nutanix нет какого-либо лёгкого пути выхода. И учитывая их количество, если компания не сокращает издержки и не жертвует ростом, то она, в конечном счёте, начинает искать заёмный капитал. В 2015 году Nutanix потратил 161,8 млн. долларов на продажи и маркетинг, что составляет 67 процентов от дохода. Чистый убыток — 126,1 млн. долларов, что лишь немного меньше всей наличности и эквивалентов на счетах.\n\nС июля 2013 года по октябрь 2015 число сотрудников возросло больше, чем впятеро — от 247 до 1 368, «и мы ожидаем значительное увеличение общего количества сотрудников в будущем», сказано в бумагах на регистрацию.\n\n#### Dow против Nasdaq ####\n\nЗдесь главная причина того, почему компания заявила о росте на 200 млн. долларов в своём первоначальном проспекте эмиссии.\n\nДоходы от IPO плюс наличные деньги компании, краткосрочные инвестиции и возможность получения кредита должны быть достаточными, чтобы удовлетворить её потребности в капитале на следующие 12 месяцев согласно проспекту.\n\nГлавное, что требуется компании Nutanix или любой другой компании с венчурным капиталом, — это менее изменчивый фондовый рынок, говорит г-н Голден из Revolution. Девятнадцать раз в этом году индекс Доу-Джонса поднимался или опускался, как минимум, на 200 пунктов.\n\n«Когда качание вверх-вниз достигает пары сотен пунктов в день, инвесторы теряют доверие, и андерайтерам нелегко оценивать фондовый рынок», говорит г-н Голден.\n\nПосле стабилизации рынка руководители компаний могут пройти через болезненный процесс разрешения инвесторам публичного рынка повторно оценить их бизнес и могут тогда определить, стоит ли предложение риска.\n\n«Можно продать акции хорошей компании по хорошей цене», продолжает г-н Голден. «То, что продающий акционер готов взять, определяет, открыт или закрыт рынок IPO.»\n\nКраткое содержание:\nРынок первичного размещения акций технологических компаний заморожен. Компания Nutanix, разработчик технологии дата-центров, собиралась выйти на публичный рынок в конце января, но его банки посоветовали ему подождать, когда рынок остынет.\n\nБольше никакие другие высококлассные технологические компании публично не заявлены. В прошедшем месяце онлайн-кредитор Elevate Credit задержал свою эмиссию акций из-за условий рынка.\n\nВместо IPO самые быстрорастущие компании поднимались в ходе мегациклов наличных средств на частном рынке в оценке капитализации и кратности годового дохода.\n\nЧастное финансирование на поздней стадии почти удвоилось — с 8,9 млрд. долларов в 2013 году до 16-ти в прошлом — по данным Национальной ассоциации венчурного капитала.\n\nЭмиссия акций Nutanix, которая — по информации источников — всё ещё может произойти, когда рынки застабилизируются, будет поучительным событием.\n\nПравление Nutanix должно будет принять трудное решение. Если компания выйдет, всё-таки, со своим IPO и с ценой за акцию 10,93 доллара, то средства группы инвесторов, вложивших в 2013-14 гг., рассматривались бы как дополнительные выпущенные акции, ещё более разводняя остаток её базы акционеров.","summary":"Рынок первичного размещения акций технологических компаний заморожен. Компания Nutanix, разработчик технологии дата-центров, собиралась выйти на публичный рынок в конце января, но его банки посоветовали ему подождать, когда рынок остынет.\n\nБольше никакие другие высококлассные технологические компании публично не заявлены. В прошедшем месяце онлайн-кредитор Elevate Credit задержал свою эмиссию акций из-за условий рынка.\n\nВместо IPO самые быстрорастущие компании поднимались в ходе мегациклов наличных средств на частном рынке в оценке капитализации и кратности годового дохода.\n\nЧастное финансирование на поздней стадии почти удвоилось — с 8,9 млрд. долларов в 2013 году до 16-ти в прошлом — по данным Национальной ассоциации венчурного капитала.\n\nЭмиссия акций Nutanix, которая — по информации источников — всё ещё может произойти, когда рынки застабилизируются, будет поучительным событием.\n\nПравление Nutanix должно будет принять трудное решение. Если компания выйдет, всё-таки, со своим IPO и с ценой за акцию 10,93 доллара, то средства группы инвесторов, вложивших в 2013-14 гг., рассматривались бы как дополнительные выпущенные акции, ещё более разводняя остаток её базы акционеров."}
{"Text":"Найм в 2022: меньше релокации и удаленки, но прежние планы. Немного цифр и комментарии HR / Habr             \n\n22-11-2022\nЧерез наш сервис регулярно проходят тысячи вакансий, и в 2022 году многое в них изменилось. В getmatch 300+ компаний. Мы опросили часть из тех, с кем регулярно работаем, и собрали статистику изменений рынка найма в условиях ситуации в мире. Делимся этой статистикой и пытаемся объяснить ее с нашей точки зрения. Релокейт от международных компаний 61% компаний раньше помогали с релокацией кандидатам из России, но больше этого не делают. Большая их часть готова нанимать тех, кто уже переехал самостоятельно, но теперь не предлагает релокейт. Главная причина — в проблемном оформлении: в некоторых странах теперь сложнее сделать рабочую визу, а куда-то уже не получится въехать по туристической, чтобы спокойно оформить документы на месте. Из-за всей этой бумажной волокиты, проблем с границами и подорожавшими авиабилетами перевозить кандидатов стало дороже. В итоге многим компаниям стало выгоднее нанимать местных либо тех, кто уже переехал своими силами — тем более что их стало больше. 28% продолжают помогать россиянам с переездом. В основном это те, у кого уже отлажен процесс перевоза и кого ограничения не слишком беспокоят. Плюс есть еще две причины: Раньше некоторые компании не могли себе позволить перевозить специалистов из России. Сейчас желающих уехать больше, некоторые согласны на более низкие зарплаты, и небольшие компании готовы этим воспользоваться. На волне «хайпа релокейта» больше классных специалистов готовы переехать. Компании пользуются возможностью собрать крутую команду. 11% компаний не перевозили кандидатов раньше и не перевозят сейчас. В основном это те, кто успешно набирают штат из уже переехавших и не хотят тратить деньги и время на релокейт. Удаленка от международных компаний 58% компаний раньше нанимали на полную удаленку кандидатов из России, но теперь перестали. Практически все готовы предложить удаленку, но только тем, кто переедет в любую страну (кроме России, Белоруссии и Украины). В основном это связано с трудностями перевода денег: приходится изобретать костыли и выдумывать обходные пути, чтобы выплаты сотрудникам шли нормально. Есть и ряд других проблем, начиная от отправки документов почтой и заканчивая трудностями из-за государственных санкций. Особенно это может быть критично для крупных компаний. Чаще всего международные компании сейчас нанимают уже переехавших в Грузии, Армении, Турции и странах ЕС. 27% нанимают кандидатов в России на полную удаленку, как раньше. Это те, для кого кандидаты из России ценны. У них нашлись некоторые обходные пути: платить в крипте, переводить деньги на зарубежные карты, открыть юрлицо в РФ, нанять в уже существующее российское лицо в аутстаф. Варианты есть, и ими пользуются также те, кто раньше не мог себе позволить найм сотрудников из-за рубежа — а теперь может, поскольку спрос на иностранные вакансии вырос. 15% компаний не нанимали кандидатов в России на удаленку раньше и не нанимают сейчас. Для них все осталось по-прежнему, так как они и раньше предпочитали не ввязываться в заграничные переводы и оформление иностранцев. Удаленка в российских компаниях 34% компаний готовы предоставлять удаленку только кандидатам, которые находятся на территории России. Особенно это характерно для банков и компаний, в которых критично относятся к информационной безопасности. Удаленка только по стране связана с более простым процессом оплаты и юридическими нюансами — при найме резидентов других стран все не так просто с оформлением и налогами. Кроме того, есть риск потерять контроль. Оформил сотрудника, выдал ему корпоративную технику — а он пропал где-то в Грузии, не отвечает на звонки, и непонятно, как его искать. 42% российских компаний готовы нанимать кандидатов, которые переехали, временно находятся за границей и планируют работать удаленно. Для них вопрос сохранения специалистов более важен, и они готовы решать проблемы с оформлением и оплатой. В целом при краткосрочном отъезде вопрос решается довольно легко: можно просто продолжать платить на старую карту и сохранить старый договор. Если отъезд более длительный, переоформление тоже возможно — например, в виде заключения договора с сотрудником как с ИП, найма на аутсорс или по ГПХ. Отработанные схемы уже есть, и их успешно применяют. 24% готовы обсуждать этот вопрос с кандидатами индивидуально и в порядке исключения нанимать кандидатов на удаленную работу из других стран. Во многом это вопрос договоренности, и зачастую решение зависит от сотрудника. Если с джуном компаниям проще будет расстаться, то сеньора или специалиста по редкой технологии лучше оставить, даже если придется перетерпеть некоторые неудобства. Планы по найму 72% компаний сообщили, что их план по найму не изменился. 20% сократили потребность в найме на последний квартал года. 8% увеличили количество вакансий. То есть в целом количество вакансий в getmatch скорее упало, но не так сильно, как об этом принято говорить. Многие, наоборот, сейчас стараются вырваться вперед, чтобы в период кризиса нарастить штат и войти в новый год более развитыми и подготовленными. Посмотреть актуальные для себя вакансии, в том числе с релокейтом и удаленкой, вы всегда можете в нашем телеграм-боте getmatch.\n","metric":0.6455078125,"metric_input":"Текст статьи:\nНайм в 2022: меньше релокации и удаленки, но прежние планы. Немного цифр и комментарии HR / Habr             \n\n22-11-2022\nЧерез наш сервис регулярно проходят тысячи вакансий, и в 2022 году многое в них изменилось. В getmatch 300+ компаний. Мы опросили часть из тех, с кем регулярно работаем, и собрали статистику изменений рынка найма в условиях ситуации в мире. Делимся этой статистикой и пытаемся объяснить ее с нашей точки зрения. Релокейт от международных компаний 61% компаний раньше помогали с релокацией кандидатам из России, но больше этого не делают. Большая их часть готова нанимать тех, кто уже переехал самостоятельно, но теперь не предлагает релокейт. Главная причина — в проблемном оформлении: в некоторых странах теперь сложнее сделать рабочую визу, а куда-то уже не получится въехать по туристической, чтобы спокойно оформить документы на месте. Из-за всей этой бумажной волокиты, проблем с границами и подорожавшими авиабилетами перевозить кандидатов стало дороже. В итоге многим компаниям стало выгоднее нанимать местных либо тех, кто уже переехал своими силами — тем более что их стало больше. 28% продолжают помогать россиянам с переездом. В основном это те, у кого уже отлажен процесс перевоза и кого ограничения не слишком беспокоят. Плюс есть еще две причины: Раньше некоторые компании не могли себе позволить перевозить специалистов из России. Сейчас желающих уехать больше, некоторые согласны на более низкие зарплаты, и небольшие компании готовы этим воспользоваться. На волне «хайпа релокейта» больше классных специалистов готовы переехать. Компании пользуются возможностью собрать крутую команду. 11% компаний не перевозили кандидатов раньше и не перевозят сейчас. В основном это те, кто успешно набирают штат из уже переехавших и не хотят тратить деньги и время на релокейт. Удаленка от международных компаний 58% компаний раньше нанимали на полную удаленку кандидатов из России, но теперь перестали. Практически все готовы предложить удаленку, но только тем, кто переедет в любую страну (кроме России, Белоруссии и Украины). В основном это связано с трудностями перевода денег: приходится изобретать костыли и выдумывать обходные пути, чтобы выплаты сотрудникам шли нормально. Есть и ряд других проблем, начиная от отправки документов почтой и заканчивая трудностями из-за государственных санкций. Особенно это может быть критично для крупных компаний. Чаще всего международные компании сейчас нанимают уже переехавших в Грузии, Армении, Турции и странах ЕС. 27% нанимают кандидатов в России на полную удаленку, как раньше. Это те, для кого кандидаты из России ценны. У них нашлись некоторые обходные пути: платить в крипте, переводить деньги на зарубежные карты, открыть юрлицо в РФ, нанять в уже существующее российское лицо в аутстаф. Варианты есть, и ими пользуются также те, кто раньше не мог себе позволить найм сотрудников из-за рубежа — а теперь может, поскольку спрос на иностранные вакансии вырос. 15% компаний не нанимали кандидатов в России на удаленку раньше и не нанимают сейчас. Для них все осталось по-прежнему, так как они и раньше предпочитали не ввязываться в заграничные переводы и оформление иностранцев. Удаленка в российских компаниях 34% компаний готовы предоставлять удаленку только кандидатам, которые находятся на территории России. Особенно это характерно для банков и компаний, в которых критично относятся к информационной безопасности. Удаленка только по стране связана с более простым процессом оплаты и юридическими нюансами — при найме резидентов других стран все не так просто с оформлением и налогами. Кроме того, есть риск потерять контроль. Оформил сотрудника, выдал ему корпоративную технику — а он пропал где-то в Грузии, не отвечает на звонки, и непонятно, как его искать. 42% российских компаний готовы нанимать кандидатов, которые переехали, временно находятся за границей и планируют работать удаленно. Для них вопрос сохранения специалистов более важен, и они готовы решать проблемы с оформлением и оплатой. В целом при краткосрочном отъезде вопрос решается довольно легко: можно просто продолжать платить на старую карту и сохранить старый договор. Если отъезд более длительный, переоформление тоже возможно — например, в виде заключения договора с сотрудником как с ИП, найма на аутсорс или по ГПХ. Отработанные схемы уже есть, и их успешно применяют. 24% готовы обсуждать этот вопрос с кандидатами индивидуально и в порядке исключения нанимать кандидатов на удаленную работу из других стран. Во многом это вопрос договоренности, и зачастую решение зависит от сотрудника. Если с джуном компаниям проще будет расстаться, то сеньора или специалиста по редкой технологии лучше оставить, даже если придется перетерпеть некоторые неудобства. Планы по найму 72% компаний сообщили, что их план по найму не изменился. 20% сократили потребность в найме на последний квартал года. 8% увеличили количество вакансий. То есть в целом количество вакансий в getmatch скорее упало, но не так сильно, как об этом принято говорить. Многие, наоборот, сейчас стараются вырваться вперед, чтобы в период кризиса нарастить штат и войти в новый год более развитыми и подготовленными. Посмотреть актуальные для себя вакансии, в том числе с релокейтом и удаленкой, вы всегда можете в нашем телеграм-боте getmatch.\n\n\nКраткое содержание:\nВ 2022 году многие компании перестали помогать с релокацией кандидатам из России, так как оформление стало сложнее из-за проблем с визами и границами. При этом компании готовы нанимать тех, кто уже переехал самостоятельно.\n\nТакже многие компании перестали нанимать кандидатов из России на полную удаленку, но готовы предложить удаленную работу тем, кто переехал или временно находится за границей.\n\nПланы по найму большинства компаний не изменились, но некоторые сократили потребность в найме на последний квартал года или увеличили количество вакансий.","summary":"В 2022 году многие компании перестали помогать с релокацией кандидатам из России, так как оформление стало сложнее из-за проблем с визами и границами. При этом компании готовы нанимать тех, кто уже переехал самостоятельно.\n\nТакже многие компании перестали нанимать кандидатов из России на полную удаленку, но готовы предложить удаленную работу тем, кто переехал или временно находится за границей.\n\nПланы по найму большинства компаний не изменились, но некоторые сократили потребность в найме на последний квартал года или увеличили количество вакансий."}
{"Text":"Универсальное чтение ячеек в PHPExcel / Habr             \n\n19-01-2012\nПривет, Хабр! Мне по работе часто приходится импортировать Excel-документы средствами PHP. Для этого я использую библиотеку PHPExcel, которая на сегодняшний день является весьма удобным инструментом. Но есть несколько «скользких» моментов, связанных с чтением данных из ячеек, о которых я хочу рассказать хабрачитателям, а также разобрать функцию, решающую эти проблемы. 1. Адресация ячеек Адресовать ячейки при работе с excel можно разными способами: колонка и ряд в виде строки: «A1» колонка буквой, ряд числом: («A», 1) колонка и ряд числом: (1, 1) Первый способ удобнее для статических ячеек, а второй и третий для циклов. Но в PHPExcel нет универсальной функции получения ячейки любым из этих способов, есть только отдельные функции. Что ж, исправляем это упущение: public function getCellValue($cellOrCol, $row = null) { //column set by index if(is_numeric($cellOrCol)) { $cell = $this->activeSheet->getCellByColumnAndRow($cellOrCol, $row); } else { $lastChar = substr($cellOrCol, -1, 1); if(!is_numeric($lastChar)) { //column contains only letter, e.g. \"A\" $cellOrCol .= $row; } $cell = $this->activeSheet->getCell($cellOrCol); } $val = $cell->getValue(); return $val; } Сразу оговорюсь, что в приводимых мной примерах кода есть ссылки на $this, т.к. это методы моего класса-обертки над PHPExcel. В этом кусочке реализуются все три способа получения объекта ячейки. 2. Объединение ячеек При чтении объединенных ячеек, PHPExcel возращает пустое значение для всех, кроме первой. Т.е. для рисунка ниже значения B3 и C3 будут пустыми строками: Мне всегда было неудобно такое поведение. Гораздо удобнее (и логичнее!) для любой объединенной ячейки возвращать общее для них значение «mergedvalue». Для этого при запросе значения нужно пройтись по всем объединенным диапазонам листа, и если заданная ячейка попадает в диапазон, то вернуть первую: $this->mergedCellsRange = $this->activeSheet->getMergeCells(); foreach($this->mergedCellsRange as $currMergedRange) { if($cell->isInRange($currMergedRange)) { $currMergedCellsArray = PHPExcel_Cell::splitRange($currMergedRange); $cell = $this->activeSheet->getCell($currMergedCellsArray[0][0]); break; } } 3. Даты Как известно, Excel хранит даты как число дней с 1 января 1900 года. Поэтому при чтении ячейки B2 на скриншоте выше мы получим бесполезное 41044. Но есть и хорошая новость — в PHPExcel присутствует удобная функция PHPExcel_Shared_Date::ExcelToPHP(), которая превращает дату в формат php. Осталось только применить эту функцию в нужный момент: $val = $cell->getValue(); if(PHPExcel_Shared_Date::isDateTime($cell)) { $val = date($format, PHPExcel_Shared_Date::ExcelToPHP($val)); } 4. Формулы В большинстве случаев стандартная функция $cell->getValue() корректно обрабатывает формулы и возвращает рассчитанное значение. Но бывают ситуации, когда формула ссылается на несуществующий лист или другой файл, который локально хранится у того, кто отправил вам excel-документ. Тогда getValue() возвратит ошибку, хотя визуально в экселе вы можете увидеть верное значение, если не было пересчета листа. Дело в том, что Excel сохраняет oldCalculatedValue, которое и используется, если не пересчитывать лист. На рисунке выше я показал это в ячейке B4 — она отображает старое значение, хотя ссылка в ней нерабочая. PHPExcel, к счастью, тоже умеет хранить старое значение формулы. Это удобно использовать, когда getValue() не смогла отработать и вернула не значение, а саму формулу (первый символ \"=\"): $val = $cell->getValue(); if((substr($val,0,1) === '=' ) && (strlen($val) > 1)){ $val = $cell->getOldCalculatedValue(); } Результат В итоге мы получили функцию, которая позволяет универсально считывать значения ячеек: public function getCellValue($cellOrCol, $row = null, $format = 'd.m.Y') { //column set by index if(is_numeric($cellOrCol)) { $cell = $this->activeSheet->getCellByColumnAndRow($cellOrCol, $row); } else { $lastChar = substr($cellOrCol, -1, 1); if(!is_numeric($lastChar)) { //column contains only letter, e.g. \"A\" $cellOrCol .= $row; } $cell = $this->activeSheet->getCell($cellOrCol); } //try to find current coordinate in all merged cells ranges //if find -> get value from head cell foreach($this->mergedCellsRange as $currMergedRange){ if($cell->isInRange($currMergedRange)) { $currMergedCellsArray = PHPExcel_Cell::splitRange($currMergedRange); $cell = $this->activeSheet->getCell($currMergedCellsArray[0][0]); break; } } //simple value $val = $cell->getValue(); //date if(PHPExcel_Shared_Date::isDateTime($cell)) { $val = date($format, PHPExcel_Shared_Date::ExcelToPHP($val)); } //for incorrect formulas take old value if((substr($val,0,1) === '=' ) && (strlen($val) > 1)){ $val = $cell->getOldCalculatedValue(); } return $val; } Тест Для проверки прочитаем эксель со скриншота двумя способами: стандартной getValue (#1) и с использованием вышеописанной функции (#2): Тест #1: Тест #2: Как видно, во втором случае все считалось корректно. Ложка дегтя Важно отметить, что использование пунктов 2, 3 и 4 работает только в режиме ReadDataOnly = false. Это режим PHPExcel по умолчанию, когда он считывает всю мета-информацию о книге. Подходит для стандартных небольших документов, например счета, накладные и.т.д. Включение ReadDataOnly = true может потребоваться для громоздких файлов, когда нужны только значения ячеек. По моей практике, такие файлы содержат отформатированные таблицы и там такой функционал не нужен. Установка режима чтения в PHPExcel делается так: $objReader = PHPExcel_IOFactory::createReaderForFile($filename); $objReader->setReadDataOnly(false); $this->PHPExcel = $objReader->load($filename); Спасибо за внимание!\n","metric":0.53662109380000000414,"metric_input":"Текст статьи:\nУниверсальное чтение ячеек в PHPExcel / Habr             \n\n19-01-2012\nПривет, Хабр! Мне по работе часто приходится импортировать Excel-документы средствами PHP. Для этого я использую библиотеку PHPExcel, которая на сегодняшний день является весьма удобным инструментом. Но есть несколько «скользких» моментов, связанных с чтением данных из ячеек, о которых я хочу рассказать хабрачитателям, а также разобрать функцию, решающую эти проблемы. 1. Адресация ячеек Адресовать ячейки при работе с excel можно разными способами: колонка и ряд в виде строки: «A1» колонка буквой, ряд числом: («A», 1) колонка и ряд числом: (1, 1) Первый способ удобнее для статических ячеек, а второй и третий для циклов. Но в PHPExcel нет универсальной функции получения ячейки любым из этих способов, есть только отдельные функции. Что ж, исправляем это упущение: public function getCellValue($cellOrCol, $row = null) { //column set by index if(is_numeric($cellOrCol)) { $cell = $this->activeSheet->getCellByColumnAndRow($cellOrCol, $row); } else { $lastChar = substr($cellOrCol, -1, 1); if(!is_numeric($lastChar)) { //column contains only letter, e.g. \"A\" $cellOrCol .= $row; } $cell = $this->activeSheet->getCell($cellOrCol); } $val = $cell->getValue(); return $val; } Сразу оговорюсь, что в приводимых мной примерах кода есть ссылки на $this, т.к. это методы моего класса-обертки над PHPExcel. В этом кусочке реализуются все три способа получения объекта ячейки. 2. Объединение ячеек При чтении объединенных ячеек, PHPExcel возращает пустое значение для всех, кроме первой. Т.е. для рисунка ниже значения B3 и C3 будут пустыми строками: Мне всегда было неудобно такое поведение. Гораздо удобнее (и логичнее!) для любой объединенной ячейки возвращать общее для них значение «mergedvalue». Для этого при запросе значения нужно пройтись по всем объединенным диапазонам листа, и если заданная ячейка попадает в диапазон, то вернуть первую: $this->mergedCellsRange = $this->activeSheet->getMergeCells(); foreach($this->mergedCellsRange as $currMergedRange) { if($cell->isInRange($currMergedRange)) { $currMergedCellsArray = PHPExcel_Cell::splitRange($currMergedRange); $cell = $this->activeSheet->getCell($currMergedCellsArray[0][0]); break; } } 3. Даты Как известно, Excel хранит даты как число дней с 1 января 1900 года. Поэтому при чтении ячейки B2 на скриншоте выше мы получим бесполезное 41044. Но есть и хорошая новость — в PHPExcel присутствует удобная функция PHPExcel_Shared_Date::ExcelToPHP(), которая превращает дату в формат php. Осталось только применить эту функцию в нужный момент: $val = $cell->getValue(); if(PHPExcel_Shared_Date::isDateTime($cell)) { $val = date($format, PHPExcel_Shared_Date::ExcelToPHP($val)); } 4. Формулы В большинстве случаев стандартная функция $cell->getValue() корректно обрабатывает формулы и возвращает рассчитанное значение. Но бывают ситуации, когда формула ссылается на несуществующий лист или другой файл, который локально хранится у того, кто отправил вам excel-документ. Тогда getValue() возвратит ошибку, хотя визуально в экселе вы можете увидеть верное значение, если не было пересчета листа. Дело в том, что Excel сохраняет oldCalculatedValue, которое и используется, если не пересчитывать лист. На рисунке выше я показал это в ячейке B4 — она отображает старое значение, хотя ссылка в ней нерабочая. PHPExcel, к счастью, тоже умеет хранить старое значение формулы. Это удобно использовать, когда getValue() не смогла отработать и вернула не значение, а саму формулу (первый символ \"=\"): $val = $cell->getValue(); if((substr($val,0,1) === '=' ) && (strlen($val) > 1)){ $val = $cell->getOldCalculatedValue(); } Результат В итоге мы получили функцию, которая позволяет универсально считывать значения ячеек: public function getCellValue($cellOrCol, $row = null, $format = 'd.m.Y') { //column set by index if(is_numeric($cellOrCol)) { $cell = $this->activeSheet->getCellByColumnAndRow($cellOrCol, $row); } else { $lastChar = substr($cellOrCol, -1, 1); if(!is_numeric($lastChar)) { //column contains only letter, e.g. \"A\" $cellOrCol .= $row; } $cell = $this->activeSheet->getCell($cellOrCol); } //try to find current coordinate in all merged cells ranges //if find -> get value from head cell foreach($this->mergedCellsRange as $currMergedRange){ if($cell->isInRange($currMergedRange)) { $currMergedCellsArray = PHPExcel_Cell::splitRange($currMergedRange); $cell = $this->activeSheet->getCell($currMergedCellsArray[0][0]); break; } } //simple value $val = $cell->getValue(); //date if(PHPExcel_Shared_Date::isDateTime($cell)) { $val = date($format, PHPExcel_Shared_Date::ExcelToPHP($val)); } //for incorrect formulas take old value if((substr($val,0,1) === '=' ) && (strlen($val) > 1)){ $val = $cell->getOldCalculatedValue(); } return $val; } Тест Для проверки прочитаем эксель со скриншота двумя способами: стандартной getValue (#1) и с использованием вышеописанной функции (#2): Тест #1: Тест #2: Как видно, во втором случае все считалось корректно. Ложка дегтя Важно отметить, что использование пунктов 2, 3 и 4 работает только в режиме ReadDataOnly = false. Это режим PHPExcel по умолчанию, когда он считывает всю мета-информацию о книге. Подходит для стандартных небольших документов, например счета, накладные и.т.д. Включение ReadDataOnly = true может потребоваться для громоздких файлов, когда нужны только значения ячеек. По моей практике, такие файлы содержат отформатированные таблицы и там такой функционал не нужен. Установка режима чтения в PHPExcel делается так: $objReader = PHPExcel_IOFactory::createReaderForFile($filename); $objReader->setReadDataOnly(false); $this->PHPExcel = $objReader->load($filename); Спасибо за внимание!\n\n\nКраткое содержание:\nВ статье рассматривается проблема с чтением данных из ячеек в библиотеке PHPExcel. Автор описывает несколько способов адресации ячеек и указывает на некоторые недостатки стандартной функции getValue(). \n\nТакже обсуждаются проблемы с объединенными ячейками и датами, хранящимися в Excel. Для решения этих проблем предлагается использовать дополнительные функции и обрабатывать значения ячеек особым образом.\n\nВ статье также упоминается о режиме ReadDataOnly, который влияет на функциональность чтения данных из файлов. В целом, статья предоставляет полезные рекомендации и примеры для работы с данными в Excel с помощью PHPExcel.","summary":"В статье рассматривается проблема с чтением данных из ячеек в библиотеке PHPExcel. Автор описывает несколько способов адресации ячеек и указывает на некоторые недостатки стандартной функции getValue(). \n\nТакже обсуждаются проблемы с объединенными ячейками и датами, хранящимися в Excel. Для решения этих проблем предлагается использовать дополнительные функции и обрабатывать значения ячеек особым образом.\n\nВ статье также упоминается о режиме ReadDataOnly, который влияет на функциональность чтения данных из файлов. В целом, статья предоставляет полезные рекомендации и примеры для работы с данными в Excel с помощью PHPExcel."}
{"Text":"«Ростех» разработал комплексы для охоты на беспилотники / Habr             \n\n_Источник: Cnews_\n\nДочерние предприятия «Ростеха» — «Российская электроника» и концерн «Автоматика»\nпредставили комплексы, предназначенные для борьбы с беспилотниками\n. Эти устройства могут быть как носимыми, портативными, так и передвижными. Созданы они как для охоты за отдельными устройствами, так и для разворачивания защитного периметра вокруг какой-то территории.\n\nПо словам разработчиков, они создали свои устройства для борьбы с опасными дронами, которые могут нести взрывчатые вещества, отравляющие и радиоактивные вещества. Кроме того, их можно использовать для защиты от шпионажа (то есть направлять против дронов, которые оснащены фото- и видеокамерами, средствами разведки и радиоэлектронной борьбы), а также для ограничения полетов в беспилотной зоне — например, вокруг АЭС, аэропортов, больниц и т.п.\n\nКроме силовых структур, такие комплексы могут использовать промышленные потребители. Разработанные системы уже закупили ряд предприятий топливно-энергетического комплекса России, включая «Татнефть», «Славнефть», «Сургутнефтегаз».\n\nЧто касается первого из представленных комплексов, то его показала «Россэлектроника». Комплекс автономный, он позволяет обнаружить беспилотник без участия оператора, идентифицируя его по принципу «свой-чужой». Если оказывается, что полет БПЛА над охраняемой зоной не разрешен, система автоматически заблокирует каналы связи и спутниковой навигации охраняемого аппарата.\n\nВремя реакции системы составляет около 0,1 сек. Работа комплекса не нарушает функционирование окружающего коммуникационного и навигационного оборудования. Зато комплекс способен обнаружить и заблокировать каналы управления в диапазоне частот от 2 до 6 ГГц, который является наиболее популярным каналом связи между дроном и приемо-передающим оборудованием оператора. При обнаружении нарушителя система оповещает список внесенных в контакты лиц SMS или электронным сообщением.\n\n«Атака-DBS» обладает масштабируемой архитектурой. Она позволяет использовать одну ячейку подавления, также система может работать с комплексом взаимосвязанных ячеек, которые закрывают целый периметр, о чем и говорилось выше.\n\nВторое предприятие «Ростеха» представило сразу три разных комплекса борьбы с беспилотниками: носимый, переносной и мобильный. Эти комплексы поддерживают подавление навигационных систем ГЛОНАСС, GPS, Galileo и Beidou, а также систем связи и управления в диапазонах частот 300 МГц, 433 МГЦ, 915 МГц, 1,5 ГГц, 2,4 ГГц и 5,8 ГГц (этот же диапазон используется для сетей Wi-Fi).\n\nНосимый комплекс «Пищаль-ПРО» способен сорвать полетное задание БПЛА, подавив каналы связи, управления и навигации. Этот комплекс не требует специальной подготовки и готов к боевому применению в режим реального времени.\n\nКаналы связи система забивает помехами, причем не во всех направлениях, а прицельно. Он выводит из строя один или несколько БПЛА, которые летят группой. Комплекс может работать беспрерывно в течение 1 часа. Дальность его работы — 2,5 км. Правда, при условии прямой видимости.\n\nФункционирует этот комплекс благодаря подсистеме РТР, которая предназначена для обнаружения и пеленгации радиосигналов, которые создаются БПЛА и пунктами наземного управления БПЛА в процессе их применения. Дальность обнаружения — 10 км.\n\nВторое решение получило название «Таран-про». Это переносной комплекс, который обеспечивает воздействие на каналы навигации, управления и передачи информации БПЛА с тем, чтобы не дать беспилотнику нормально функционировать. Дальность радиоподавления -не менее 2,7 км.\n\nВ состав этого комплекса входят сразу две подсистемы. Первая создает «купол», то есть объемный защитный периметр над определенным объектом, блокируя полеты всех БПЛА. Диаметр основания созданного защитного купола составляет около 5,6 км, высота — 1,7км.\n\nВторая подсистема способна действовать направлено. Она прерывает канал связи беспилотника или их группы передачей помех в определенном частотном диапазоне. Время реакции при создании прицельной по частоте помехи не превышает 10 секунд. Передающие антенны обеспечивают излучение помех с линейной вертикальной поляризацией. Дальность противодействия составляет не менее 5 км при условии прямой видимости.\n\nРаботает этот комплекс в гражданском и военном варианте. Гражданский комплекс не может подавлять частоты, которые используют военные. Стоимость комплекса начинается с 1 млн рублей. По словам разработчиков, для того, чтобы перекрыть территорию нефтеперерабатывающего завода, понадобится около 25 млн рублей\n\nНаконец, третья система — мобильный комплекс «Сапсан-бекас». Он устанавливается внутри транспортного средства, обеспечивая защиту охраняемых объектов и территорий от противоправных действий, которые совершаются с использованием БПЛА.\n\nКомплекс состоит из трех компонентов. Это станция радиотехнической разведки и пеленгования средств связи и управления БПЛА, радиолокационная станция ближней зоны и станция радиолокации и оптоэлектронной разведки. Они предназначены для контроля воздушного пространства, обнаружения и траекторного сопровождения сверхмалых воздушных объектов в ближней зоне до 10 км.","metric":0.703125,"metric_input":"Текст статьи:\n«Ростех» разработал комплексы для охоты на беспилотники / Habr             \n\n_Источник: Cnews_\n\nДочерние предприятия «Ростеха» — «Российская электроника» и концерн «Автоматика»\nпредставили комплексы, предназначенные для борьбы с беспилотниками\n. Эти устройства могут быть как носимыми, портативными, так и передвижными. Созданы они как для охоты за отдельными устройствами, так и для разворачивания защитного периметра вокруг какой-то территории.\n\nПо словам разработчиков, они создали свои устройства для борьбы с опасными дронами, которые могут нести взрывчатые вещества, отравляющие и радиоактивные вещества. Кроме того, их можно использовать для защиты от шпионажа (то есть направлять против дронов, которые оснащены фото- и видеокамерами, средствами разведки и радиоэлектронной борьбы), а также для ограничения полетов в беспилотной зоне — например, вокруг АЭС, аэропортов, больниц и т.п.\n\nКроме силовых структур, такие комплексы могут использовать промышленные потребители. Разработанные системы уже закупили ряд предприятий топливно-энергетического комплекса России, включая «Татнефть», «Славнефть», «Сургутнефтегаз».\n\nЧто касается первого из представленных комплексов, то его показала «Россэлектроника». Комплекс автономный, он позволяет обнаружить беспилотник без участия оператора, идентифицируя его по принципу «свой-чужой». Если оказывается, что полет БПЛА над охраняемой зоной не разрешен, система автоматически заблокирует каналы связи и спутниковой навигации охраняемого аппарата.\n\nВремя реакции системы составляет около 0,1 сек. Работа комплекса не нарушает функционирование окружающего коммуникационного и навигационного оборудования. Зато комплекс способен обнаружить и заблокировать каналы управления в диапазоне частот от 2 до 6 ГГц, который является наиболее популярным каналом связи между дроном и приемо-передающим оборудованием оператора. При обнаружении нарушителя система оповещает список внесенных в контакты лиц SMS или электронным сообщением.\n\n«Атака-DBS» обладает масштабируемой архитектурой. Она позволяет использовать одну ячейку подавления, также система может работать с комплексом взаимосвязанных ячеек, которые закрывают целый периметр, о чем и говорилось выше.\n\nВторое предприятие «Ростеха» представило сразу три разных комплекса борьбы с беспилотниками: носимый, переносной и мобильный. Эти комплексы поддерживают подавление навигационных систем ГЛОНАСС, GPS, Galileo и Beidou, а также систем связи и управления в диапазонах частот 300 МГц, 433 МГЦ, 915 МГц, 1,5 ГГц, 2,4 ГГц и 5,8 ГГц (этот же диапазон используется для сетей Wi-Fi).\n\nНосимый комплекс «Пищаль-ПРО» способен сорвать полетное задание БПЛА, подавив каналы связи, управления и навигации. Этот комплекс не требует специальной подготовки и готов к боевому применению в режим реального времени.\n\nКаналы связи система забивает помехами, причем не во всех направлениях, а прицельно. Он выводит из строя один или несколько БПЛА, которые летят группой. Комплекс может работать беспрерывно в течение 1 часа. Дальность его работы — 2,5 км. Правда, при условии прямой видимости.\n\nФункционирует этот комплекс благодаря подсистеме РТР, которая предназначена для обнаружения и пеленгации радиосигналов, которые создаются БПЛА и пунктами наземного управления БПЛА в процессе их применения. Дальность обнаружения — 10 км.\n\nВторое решение получило название «Таран-про». Это переносной комплекс, который обеспечивает воздействие на каналы навигации, управления и передачи информации БПЛА с тем, чтобы не дать беспилотнику нормально функционировать. Дальность радиоподавления -не менее 2,7 км.\n\nВ состав этого комплекса входят сразу две подсистемы. Первая создает «купол», то есть объемный защитный периметр над определенным объектом, блокируя полеты всех БПЛА. Диаметр основания созданного защитного купола составляет около 5,6 км, высота — 1,7км.\n\nВторая подсистема способна действовать направлено. Она прерывает канал связи беспилотника или их группы передачей помех в определенном частотном диапазоне. Время реакции при создании прицельной по частоте помехи не превышает 10 секунд. Передающие антенны обеспечивают излучение помех с линейной вертикальной поляризацией. Дальность противодействия составляет не менее 5 км при условии прямой видимости.\n\nРаботает этот комплекс в гражданском и военном варианте. Гражданский комплекс не может подавлять частоты, которые используют военные. Стоимость комплекса начинается с 1 млн рублей. По словам разработчиков, для того, чтобы перекрыть территорию нефтеперерабатывающего завода, понадобится около 25 млн рублей\n\nНаконец, третья система — мобильный комплекс «Сапсан-бекас». Он устанавливается внутри транспортного средства, обеспечивая защиту охраняемых объектов и территорий от противоправных действий, которые совершаются с использованием БПЛА.\n\nКомплекс состоит из трех компонентов. Это станция радиотехнической разведки и пеленгования средств связи и управления БПЛА, радиолокационная станция ближней зоны и станция радиолокации и оптоэлектронной разведки. Они предназначены для контроля воздушного пространства, обнаружения и траекторного сопровождения сверхмалых воздушных объектов в ближней зоне до 10 км.\n\nКраткое содержание:\nСтатья рассказывает о новых комплексах для борьбы с беспилотниками, разработанных компанией \"Ростех\". Эти устройства могут обнаруживать и блокировать каналы управления беспилотников, а также создавать защитные периметры вокруг определенных территорий. Комплексы могут использоваться как силовыми структурами, так и промышленными потребителями.","summary":"Статья рассказывает о новых комплексах для борьбы с беспилотниками, разработанных компанией \"Ростех\". Эти устройства могут обнаруживать и блокировать каналы управления беспилотников, а также создавать защитные периметры вокруг определенных территорий. Комплексы могут использоваться как силовыми структурами, так и промышленными потребителями."}
{"Text":"Установка Quartus II на CentOS / Habr                         \n\n16-09-2016\nЕсли вы, как и я, убежденный линуксойд да еще и электронщик, желающий приобщиться к миру ПЛИС, то помимо непонимания со стороны коллег вы с большой долей вероятности столкнетесь с проблемами установки соответствующего ПО. Поэтому я постараюсь рассказать о своем пути и облегчить путь последователей. Добро пожаловать под кат. Альтера утверждает, что их программное обеспечение способно работать под управлением RHEL-based систем. Поэтому будем устанавливать Altera Quartus II 13.0 sp1 Free (самая «свежая» версия, поддерживающая интересующие меня кристаллы) на свежую CentOS 7. И первая особенность: для установки подойдет только х64 версия системы. Чтож, этот пункт у нас выполнен, идем на сайт Альтера, проходим нудную регистрацию, на странице загрузки выбираем нужную нам версию и скачиваем ее: После продолжительного ожидания (4,8Гб архива не шутки) распаковываем архив и радостно вбиваем: ./setup.sh И!.. и ничего. Т.е. вообще ничего, никаких ошибок равно как и никаких признаков жизни. Лезем в гугл и узнаем, узнаем, что альтера, несмотря на работоспособность только в х64 системах, требует некоторые 32-разрядные библиотеки. Гуглим дальше и наконец собираем список этих самых некоторых библиотек. Устанавливаем их (не забываем залогиниться под рутом): # yum install compat-libstdc++-33.i686 expat.i686 fontconfig.i686 freetype.i686 glibc.i686 gtk2.i686 libcanberra-gtk2.i686 gtk2-engines-2.18.4-5.el6.centos.i686 libpng.i686 libICE.i686 libSM.i686 libuuid.i686 ncurses-devel.i686 ncurses-libs.i686 PackageKit-gtk-module.i686 tcl-devel.i686 tcl.i686 zlib.i686 # yum install libX11.i686 libXau.i686 libXdmcp.i686 libXext.i686 libXft-devel.i686 libXft.i686 libXrender.i686 libXt.i686 libXtst.i686 Тут может возникнуть следующая проблема: Error: Multilib version problems found. This often means that the root cause is something else and multilib version checking is just pointing out that there is a problem. Eg.: 1. You have an upgrade for libgcc which is missing some dependency that another package requires. Yum is trying to solve this by installing an older version of libgcc of the different architecture. If you exclude the bad architecture yum will tell you what the root cause is (which package requires what). You can try redoing the upgrade with --exclude libgcc.otherarch ... this should give you an error message showing the root cause of the problem. 2. You have multiple architectures of libgcc installed, but yum can only see an upgrade for one of those arcitectures. If you don't want/need both architectures anymore then you can remove the one with the missing update and everything will work. 3. You have duplicate versions of libgcc installed already. You can use \"yum check\" to get yum show these errors. ...you can also use --setopt=protected_multilib=false to remove this checking, however this is almost never the correct thing to do as something else is very likely to go wrong (often causing much more problems). Protected multilib versions: libgcc-4.4.7-4.el6.i686 != libgcc-4.4.7-3.el6.x86_64 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles –nodigest Вместо libgcc у вас может быть что-то другое. У меня, например, это была libuuid. Это означает, что устанавливаемая 32-разрядная библиотека имеет более новую версию, чем имеющаяся в системе 64-разрядная. Решается проблема обновлением библиотеки. В моем случае это выглядело так: # yum install libuuid.i686 libuuid.x86_64 (Скриншот я сразу не сделал, а потом лень сносить библиотеку было.) Вот теперь, когда все необходимые библиотеки стоят, можно запускать setup: ./setup.sh Открывается windows-style окошко инсталляции: Внимательно читаем лицензионное соглашение: Выбираем директорию для установки, предварительно убедившись, что она доступна на запись: Решаем, поддержка каких кристаллов нам нужна: Инсталлятор оценивает доступное место: Идем и завариваем чай… После чаепития возвращаемся к компьютеру и видим картинку: Справедливости ради замечу, что независимо от расстановки крестиков окно закроется без каких-либо иных действий. То есть ни ярлычки не создадуться, ни прога не запустится. Собственно все — Quartus установлен. Что с ним делать дальше и дополнительные приколы, ожидающие линуксойдов, в последующих публикациях.\n","metric":0.35400390630000000414,"metric_input":"Текст статьи:\nУстановка Quartus II на CentOS / Habr                         \n\n16-09-2016\nЕсли вы, как и я, убежденный линуксойд да еще и электронщик, желающий приобщиться к миру ПЛИС, то помимо непонимания со стороны коллег вы с большой долей вероятности столкнетесь с проблемами установки соответствующего ПО. Поэтому я постараюсь рассказать о своем пути и облегчить путь последователей. Добро пожаловать под кат. Альтера утверждает, что их программное обеспечение способно работать под управлением RHEL-based систем. Поэтому будем устанавливать Altera Quartus II 13.0 sp1 Free (самая «свежая» версия, поддерживающая интересующие меня кристаллы) на свежую CentOS 7. И первая особенность: для установки подойдет только х64 версия системы. Чтож, этот пункт у нас выполнен, идем на сайт Альтера, проходим нудную регистрацию, на странице загрузки выбираем нужную нам версию и скачиваем ее: После продолжительного ожидания (4,8Гб архива не шутки) распаковываем архив и радостно вбиваем: ./setup.sh И!.. и ничего. Т.е. вообще ничего, никаких ошибок равно как и никаких признаков жизни. Лезем в гугл и узнаем, узнаем, что альтера, несмотря на работоспособность только в х64 системах, требует некоторые 32-разрядные библиотеки. Гуглим дальше и наконец собираем список этих самых некоторых библиотек. Устанавливаем их (не забываем залогиниться под рутом): # yum install compat-libstdc++-33.i686 expat.i686 fontconfig.i686 freetype.i686 glibc.i686 gtk2.i686 libcanberra-gtk2.i686 gtk2-engines-2.18.4-5.el6.centos.i686 libpng.i686 libICE.i686 libSM.i686 libuuid.i686 ncurses-devel.i686 ncurses-libs.i686 PackageKit-gtk-module.i686 tcl-devel.i686 tcl.i686 zlib.i686 # yum install libX11.i686 libXau.i686 libXdmcp.i686 libXext.i686 libXft-devel.i686 libXft.i686 libXrender.i686 libXt.i686 libXtst.i686 Тут может возникнуть следующая проблема: Error: Multilib version problems found. This often means that the root cause is something else and multilib version checking is just pointing out that there is a problem. Eg.: 1. You have an upgrade for libgcc which is missing some dependency that another package requires. Yum is trying to solve this by installing an older version of libgcc of the different architecture. If you exclude the bad architecture yum will tell you what the root cause is (which package requires what). You can try redoing the upgrade with --exclude libgcc.otherarch ... this should give you an error message showing the root cause of the problem. 2. You have multiple architectures of libgcc installed, but yum can only see an upgrade for one of those arcitectures. If you don't want/need both architectures anymore then you can remove the one with the missing update and everything will work. 3. You have duplicate versions of libgcc installed already. You can use \"yum check\" to get yum show these errors. ...you can also use --setopt=protected_multilib=false to remove this checking, however this is almost never the correct thing to do as something else is very likely to go wrong (often causing much more problems). Protected multilib versions: libgcc-4.4.7-4.el6.i686 != libgcc-4.4.7-3.el6.x86_64 You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles –nodigest Вместо libgcc у вас может быть что-то другое. У меня, например, это была libuuid. Это означает, что устанавливаемая 32-разрядная библиотека имеет более новую версию, чем имеющаяся в системе 64-разрядная. Решается проблема обновлением библиотеки. В моем случае это выглядело так: # yum install libuuid.i686 libuuid.x86_64 (Скриншот я сразу не сделал, а потом лень сносить библиотеку было.) Вот теперь, когда все необходимые библиотеки стоят, можно запускать setup: ./setup.sh Открывается windows-style окошко инсталляции: Внимательно читаем лицензионное соглашение: Выбираем директорию для установки, предварительно убедившись, что она доступна на запись: Решаем, поддержка каких кристаллов нам нужна: Инсталлятор оценивает доступное место: Идем и завариваем чай… После чаепития возвращаемся к компьютеру и видим картинку: Справедливости ради замечу, что независимо от расстановки крестиков окно закроется без каких-либо иных действий. То есть ни ярлычки не создадуться, ни прога не запустится. Собственно все — Quartus установлен. Что с ним делать дальше и дополнительные приколы, ожидающие линуксойдов, в последующих публикациях.\n\n\nКраткое содержание:\nСтатья рассказывает о процессе установки программного обеспечения Altera Quartus II 13.0 sp1 Free на операционную систему CentOS 7, которая является дистрибутивом Linux. Автор отмечает, что для установки требуется 64-разрядная версия системы, и предоставляет инструкции по загрузке и установке программного обеспечения. Однако, при попытке запуска установки возникает проблема с 32-разрядными библиотеками, которые необходимо установить. После установки этих библиотек, установка Quartus II проходит успешно.","summary":"Статья рассказывает о процессе установки программного обеспечения Altera Quartus II 13.0 sp1 Free на операционную систему CentOS 7, которая является дистрибутивом Linux. Автор отмечает, что для установки требуется 64-разрядная версия системы, и предоставляет инструкции по загрузке и установке программного обеспечения. Однако, при попытке запуска установки возникает проблема с 32-разрядными библиотеками, которые необходимо установить. После установки этих библиотек, установка Quartus II проходит успешно."}
{"Text":"НАСА выставило на продажу свой старый компьютер Amiga 2500 / Habr             \n\n11-12-2017\nАгентство НАСА выставило свой старый компьютер Amiga на продажу. Приобрести раритетную технику с историей можно на eBay. По словам представителей агентства, компьютер представляет собой «рабочую лошадку», которая долгое время находилась в телеметрической лаборатории агентства. Речь идет о вполне конкретной модели устройства — Amiga 2500. Рабочая лошадка очень активно использовалась в 80-е годы прошлого века. Система не в идеальном состоянии, но на ней есть все необходимые наклейки, удостоверяющие, что это собственность агентства. На фото, выставленных в интернет-аукционе, видно, что рабочее ПО зарегистрировано на имя Дейв Браун (в 90-х он занимал должность главного программиста в телеметрической лаборатории НАСА). Большинство персональных компьютеров такого класса были выведены НАСА из эксплуатации в 2006 году. Часть из них не была выброшена, а осталась в коллекциях сотрудников и самого агентства. Модель, которая выставлена на продажу, простояла в хранилище несколько лет, после чего у представителей телеметрической лаборатории и созрела идея продать компьютер. Эта система была создана в 1989 году. Компьютеры Amiga 2500 были очень популярными в США и Канаде. Модель представляет собой Amiga 2000 с модифицированным аппаратным обеспечением. В частности, внутри установлена карта расширения, в которой находится более мощный процессор 68RC030 25 МГц (первый акселератор Commodore). Незначительное отличие 2500 от 2000 — это видоизмененный шильдик на системном блоке. Сам компьютер для своего времени был достаточно мощным, и его после выхода в продажу начали активно раскупать. Тем не менее, очень популярным он не стал в силу своей сложности и дороговизны. Например, на материнские платы Amiga 2500 устанавливался процессор 68000, который полноценно не использовался, поскольку его заменял стоящий на акселераторе процессор 68020 или 68030. Таким образом, потенциал системы был не раскрыт полностью. Модель 2500 уступала в производительности модели Amiga 3000. Стоит отметить, что даже после выхода обновленного «трехтысячника» продажи 2500 еще шли некоторое время. Дело в том, что с этим компьютером была совместима карта Video Toaster. А вот 3000 ее не поддерживал. Как сообщает Википедия, существовало три различные модификации Amiga 2500: Amiga 2500/020 (предустановлена карта A2620 содержащая процессор 68020 14,3 МГц и сопроцессор 68881). Amiga 2500/030 (предустановлена карта A2630 содержащая процессор 68030 25 МГц и сопроцессор 68882). Amiga 2500UX (прилагался дистрибутив AT&T Unix System V и, (по желанию) стример и Ethernet-адаптер). В экземпляре, принадлежавшем НАСА, было установлено две специализированные карты, которые позволяли принимать и записывать телеметрические данные разных космических систем. Одна из карт называлась Amiga 2000 Serial Interface. Вторая — 2000 Remote DMA Output Controller. Они обе работали с набором, выглядевшим, как SCSI-кабели с 50-ю пинами. Насколько можно понять, к разъемам подключались внешние накопители или другое оборудование, позволявшее записывать крупные блоки информации. Еще сохранились файлы проекта, датированные 1988-м годом. По мнению сотрудника НАСА, выставившего компьютер на eBay, этим устройством, возможно, пользовался один из астронавтов НАСА, который трагически погиб во время катастрофы на Мысе Канаверал в 2003 в момент запуска шаттла Колумбия. Самое интересное — это цена лота. Его продают за $5200. На eBay аналогичные модели продаются по $150 или около того. Доставка в Европу обойдется еще в 80 долларов США. По словам продавца, в системе хранятся файлы таких проектов, как AC.135, 141, 164, AURA, GP8, MS10, MS11, MS7, MS9, P91, TITAN, PEGASUS, TAURUS. Для каждого из проекта есть исходники на «С». Эти исходники при желании могут быть превращены в работающий софт. Ранее он использовался для связи с разными космическими аппаратами. На сайте Канадского музея ПК указано, что цена такого рода компьютеров достигала $3000 в конце 80-х годов прошлого века. Таким образом, персональный компьютер продается по цене, значительно превышающей стоимость покупки.\n","metric":0.58544921880000000414,"metric_input":"Текст статьи:\nНАСА выставило на продажу свой старый компьютер Amiga 2500 / Habr             \n\n11-12-2017\nАгентство НАСА выставило свой старый компьютер Amiga на продажу. Приобрести раритетную технику с историей можно на eBay. По словам представителей агентства, компьютер представляет собой «рабочую лошадку», которая долгое время находилась в телеметрической лаборатории агентства. Речь идет о вполне конкретной модели устройства — Amiga 2500. Рабочая лошадка очень активно использовалась в 80-е годы прошлого века. Система не в идеальном состоянии, но на ней есть все необходимые наклейки, удостоверяющие, что это собственность агентства. На фото, выставленных в интернет-аукционе, видно, что рабочее ПО зарегистрировано на имя Дейв Браун (в 90-х он занимал должность главного программиста в телеметрической лаборатории НАСА). Большинство персональных компьютеров такого класса были выведены НАСА из эксплуатации в 2006 году. Часть из них не была выброшена, а осталась в коллекциях сотрудников и самого агентства. Модель, которая выставлена на продажу, простояла в хранилище несколько лет, после чего у представителей телеметрической лаборатории и созрела идея продать компьютер. Эта система была создана в 1989 году. Компьютеры Amiga 2500 были очень популярными в США и Канаде. Модель представляет собой Amiga 2000 с модифицированным аппаратным обеспечением. В частности, внутри установлена карта расширения, в которой находится более мощный процессор 68RC030 25 МГц (первый акселератор Commodore). Незначительное отличие 2500 от 2000 — это видоизмененный шильдик на системном блоке. Сам компьютер для своего времени был достаточно мощным, и его после выхода в продажу начали активно раскупать. Тем не менее, очень популярным он не стал в силу своей сложности и дороговизны. Например, на материнские платы Amiga 2500 устанавливался процессор 68000, который полноценно не использовался, поскольку его заменял стоящий на акселераторе процессор 68020 или 68030. Таким образом, потенциал системы был не раскрыт полностью. Модель 2500 уступала в производительности модели Amiga 3000. Стоит отметить, что даже после выхода обновленного «трехтысячника» продажи 2500 еще шли некоторое время. Дело в том, что с этим компьютером была совместима карта Video Toaster. А вот 3000 ее не поддерживал. Как сообщает Википедия, существовало три различные модификации Amiga 2500: Amiga 2500/020 (предустановлена карта A2620 содержащая процессор 68020 14,3 МГц и сопроцессор 68881). Amiga 2500/030 (предустановлена карта A2630 содержащая процессор 68030 25 МГц и сопроцессор 68882). Amiga 2500UX (прилагался дистрибутив AT&T Unix System V и, (по желанию) стример и Ethernet-адаптер). В экземпляре, принадлежавшем НАСА, было установлено две специализированные карты, которые позволяли принимать и записывать телеметрические данные разных космических систем. Одна из карт называлась Amiga 2000 Serial Interface. Вторая — 2000 Remote DMA Output Controller. Они обе работали с набором, выглядевшим, как SCSI-кабели с 50-ю пинами. Насколько можно понять, к разъемам подключались внешние накопители или другое оборудование, позволявшее записывать крупные блоки информации. Еще сохранились файлы проекта, датированные 1988-м годом. По мнению сотрудника НАСА, выставившего компьютер на eBay, этим устройством, возможно, пользовался один из астронавтов НАСА, который трагически погиб во время катастрофы на Мысе Канаверал в 2003 в момент запуска шаттла Колумбия. Самое интересное — это цена лота. Его продают за $5200. На eBay аналогичные модели продаются по $150 или около того. Доставка в Европу обойдется еще в 80 долларов США. По словам продавца, в системе хранятся файлы таких проектов, как AC.135, 141, 164, AURA, GP8, MS10, MS11, MS7, MS9, P91, TITAN, PEGASUS, TAURUS. Для каждого из проекта есть исходники на «С». Эти исходники при желании могут быть превращены в работающий софт. Ранее он использовался для связи с разными космическими аппаратами. На сайте Канадского музея ПК указано, что цена такого рода компьютеров достигала $3000 в конце 80-х годов прошлого века. Таким образом, персональный компьютер продается по цене, значительно превышающей стоимость покупки.\n\n\nКраткое содержание:\nАгентство НАСА выставило на продажу свой старый компьютер Amiga 2500 на eBay. Это рабочая лошадка, которая долгое время использовалась в телеметрической лаборатории агентства. Модель была создана в 1989 году и стала популярной в США и Канаде. Компьютер имеет модифицированное аппаратное обеспечение и более мощный процессор 68RC030 25 МГц. Он использовался для связи с разными космическими аппаратами и был совместим с картой Video Toaster. Цена лота составляет $5200, что значительно превышает стоимость покупки.","summary":"Агентство НАСА выставило на продажу свой старый компьютер Amiga 2500 на eBay. Это рабочая лошадка, которая долгое время использовалась в телеметрической лаборатории агентства. Модель была создана в 1989 году и стала популярной в США и Канаде. Компьютер имеет модифицированное аппаратное обеспечение и более мощный процессор 68RC030 25 МГц. Он использовался для связи с разными космическими аппаратами и был совместим с картой Video Toaster. Цена лота составляет $5200, что значительно превышает стоимость покупки."}
{"Text":"Руль для PC на базе Wii Remote / Habr                        \n\n25-02-2011\nБольшинство из нас знают о такой игровой консоли, как Wii, отличительной чертой которой является беспроводной контроллер Wii Remote. Сегодня я расскажу вам о моем опыте по превращению этого контроллера в удобный, функциональный, и при этом очень дешевый руль для компьютера. Возможности самого контроллера впечатляют: Трехосевой акселерометр Инфракрасная камера Подключение по Bluetooth Вибромотор и динамик 4 светодиодных индикатора Подключение дополнительных контроллеров 12 кнопок Для нас, важными являются акселерометр и Bluetooth. А сам порядок действий будет выглядеть примерно следующим образом: Найти контроллер Wiimote (альтернативное название Wii Remote). Подключить его к компьютеру. Наладить преобразование данных, поступающих с акселерометра контроллера в понятный играм формат. Где взять контроллер Wiimote Те, у кого есть приставка Wii, могут сразу переходить к следующему пункту. У меня же таковой не оказалось, и я решил купить контроллер отдельно. В России он стоит около 2000 руб., что, на мой взгляд, неоправданно дорого. Так что, недолго думая, я отправился на аукцион Ebay. И не ошибся! Год назад, новый оригинальный Wii remote + Nunchuk с бесплатной доставкой обошёлся мне всего в $18.50. При беглом осмотре сегодняшних результатов поиска , цены стали еще ниже. Подключение к компьютеру На первый взгляд, никаких трудностей здесь возникнуть не должно: главное иметь встроенный или внешний Bluetooth приемник. В интернете легко можно найти несколько инструкций по подключению Wiimote к компьютеру. Однако, то ли по причине старенькой Windows Vista, то ли из-за еще более старого Bluetooth-адаптера, я так и не смог заставить нормально работать контроллер, используя стандартный стек Bluetooth от Microsoft. Но там, где споткнулась Microsoft, преуспела IVT с её продуктом BlueSoleil (к сожалению, платным). Без особых усилий, мне удалось подключить контроллер и заставить его стабильно работать. Подробную инструкцию можно найти, например, здесь . Краткая инструкция: Устанавливаем BlueSoleil (или при удачном стечении обстоятельств используем стандартные средства Windows). Активируем Bluetooth-адаптер. Зажимаем кнопки 1 и 2 на контроллере и запускаем поиск устройств. Контроллер опознается как Nintendo RVL-CNT-01. Превращаем Wiimote в руль Для работы Wiimote с PC написано несколько программ, например GlovePIE – универсальная программа для эмуляции устройств ввода, позволяющая, в т.ч. связывать компьютер и Wii Remote используя внутренний язык скриптов. Поискав, можно найти несколько таких скриптов, но и здесь не все так гладко ;) У меня получилось заставить работать контроллер виртуальным джойстиком, однако работа с акселерометром была очень нестабильна: чуть повернешь контроллер по определенной оси (или слишком быстро/на слишком большой угол/и т.п.), и он зависает, переставая отправлять данные. Я не стал мириться с этим безобразием, и продолжил поиски. Через некоторое время я набрел на замечательную .NET библиотеку: WiimoteLib . Она позволяет обращаться почти к любым функциям контроллера, и получать данные с его сенсоров. В комплекте идет пример программы, отображающий всю информацию, поступающую с контроллера. Теперь, когда у нас есть данные с акселерометра, нам нужно как-то привести их к понятному играм формату. Большинство гоночных игр поддерживает железную реализацию руля. А мы подсунем им виртуальный. Для этого воспользуемся универсальным драйвером эмуляции джойстиков – PPJoy . Официальная ссылка на скачивание: PPJoySetup-0.8.4.5-early-release.exe Добавим в систему новый виртуальный джойстик: Устанавливаем PPJoy В панели управления находим и открывем «Parallel Port Joysticks». Щелкаем по кнопке Add Параметр Parallel Port меняем на Virtual joysticks Щелкаем по кнопке Add Выбираем в списке созданный джойстик и щелкаем по «Mapping...» Next > Заполняем это и последующие окна как на скриншотах, и заканчиваем настройку. Я написал небольшую программку на C#, которая, фактически являясь мостом между Wiimote и PPJoy, нажимает виртуальные кнопки и крутит виртуальный руль джойстика. При запуске можно задать чувствительность. Скачать её вместе с исходными кодами вы можете по этой ссылке: WiiController.zip Испытания в реальном мире Все испытания проводились на гоночном симуляторе Colin McRae: DiRT 2. Заранее прошу прощения за качество фото и видео: все снималось на мыльницу пятилетней давности. Примерный вид настроек игры: А вот, как выглядит реальная игра с контроллером: Впечатления Сугубо положительные! Прошел всю игру, используя только Wiimote. Все работает настолько стабильно и быстро, что уже через несколько минут забываешь, что у тебя в руках устройство, предназначенное совсем не для этого. Из плюсов: огромный руль не закрывает пол-экрана, и не мешает перемещению рук, все управление выведено на кнопки контроллера. p.s. Вообще, можно сделать очень много прикольных штук с Wii Remote, вот некоторые из них: http://johnnylee.net/projects/wii/ Трекинг пальцев, мультитач доска, трекинг головы (для создания иллюзии трехмерности, заглянуть за край монитора). Фактически, я купил Wiimote под впечатлением от этих проектов. http://habrahabr.ru/blogs/DIY/91149/ Виртуальное окно в мир Winscape с трекингом положения головы используя Wiimote http://habrahabr.ru/blogs/subconsciousness/2877/ 5 самых креативных способов использования Wiimote.\n","metric":0.5771484375,"metric_input":"Текст статьи:\nРуль для PC на базе Wii Remote / Habr                        \n\n25-02-2011\nБольшинство из нас знают о такой игровой консоли, как Wii, отличительной чертой которой является беспроводной контроллер Wii Remote. Сегодня я расскажу вам о моем опыте по превращению этого контроллера в удобный, функциональный, и при этом очень дешевый руль для компьютера. Возможности самого контроллера впечатляют: Трехосевой акселерометр Инфракрасная камера Подключение по Bluetooth Вибромотор и динамик 4 светодиодных индикатора Подключение дополнительных контроллеров 12 кнопок Для нас, важными являются акселерометр и Bluetooth. А сам порядок действий будет выглядеть примерно следующим образом: Найти контроллер Wiimote (альтернативное название Wii Remote). Подключить его к компьютеру. Наладить преобразование данных, поступающих с акселерометра контроллера в понятный играм формат. Где взять контроллер Wiimote Те, у кого есть приставка Wii, могут сразу переходить к следующему пункту. У меня же таковой не оказалось, и я решил купить контроллер отдельно. В России он стоит около 2000 руб., что, на мой взгляд, неоправданно дорого. Так что, недолго думая, я отправился на аукцион Ebay. И не ошибся! Год назад, новый оригинальный Wii remote + Nunchuk с бесплатной доставкой обошёлся мне всего в $18.50. При беглом осмотре сегодняшних результатов поиска , цены стали еще ниже. Подключение к компьютеру На первый взгляд, никаких трудностей здесь возникнуть не должно: главное иметь встроенный или внешний Bluetooth приемник. В интернете легко можно найти несколько инструкций по подключению Wiimote к компьютеру. Однако, то ли по причине старенькой Windows Vista, то ли из-за еще более старого Bluetooth-адаптера, я так и не смог заставить нормально работать контроллер, используя стандартный стек Bluetooth от Microsoft. Но там, где споткнулась Microsoft, преуспела IVT с её продуктом BlueSoleil (к сожалению, платным). Без особых усилий, мне удалось подключить контроллер и заставить его стабильно работать. Подробную инструкцию можно найти, например, здесь . Краткая инструкция: Устанавливаем BlueSoleil (или при удачном стечении обстоятельств используем стандартные средства Windows). Активируем Bluetooth-адаптер. Зажимаем кнопки 1 и 2 на контроллере и запускаем поиск устройств. Контроллер опознается как Nintendo RVL-CNT-01. Превращаем Wiimote в руль Для работы Wiimote с PC написано несколько программ, например GlovePIE – универсальная программа для эмуляции устройств ввода, позволяющая, в т.ч. связывать компьютер и Wii Remote используя внутренний язык скриптов. Поискав, можно найти несколько таких скриптов, но и здесь не все так гладко ;) У меня получилось заставить работать контроллер виртуальным джойстиком, однако работа с акселерометром была очень нестабильна: чуть повернешь контроллер по определенной оси (или слишком быстро/на слишком большой угол/и т.п.), и он зависает, переставая отправлять данные. Я не стал мириться с этим безобразием, и продолжил поиски. Через некоторое время я набрел на замечательную .NET библиотеку: WiimoteLib . Она позволяет обращаться почти к любым функциям контроллера, и получать данные с его сенсоров. В комплекте идет пример программы, отображающий всю информацию, поступающую с контроллера. Теперь, когда у нас есть данные с акселерометра, нам нужно как-то привести их к понятному играм формату. Большинство гоночных игр поддерживает железную реализацию руля. А мы подсунем им виртуальный. Для этого воспользуемся универсальным драйвером эмуляции джойстиков – PPJoy . Официальная ссылка на скачивание: PPJoySetup-0.8.4.5-early-release.exe Добавим в систему новый виртуальный джойстик: Устанавливаем PPJoy В панели управления находим и открывем «Parallel Port Joysticks». Щелкаем по кнопке Add Параметр Parallel Port меняем на Virtual joysticks Щелкаем по кнопке Add Выбираем в списке созданный джойстик и щелкаем по «Mapping...» Next > Заполняем это и последующие окна как на скриншотах, и заканчиваем настройку. Я написал небольшую программку на C#, которая, фактически являясь мостом между Wiimote и PPJoy, нажимает виртуальные кнопки и крутит виртуальный руль джойстика. При запуске можно задать чувствительность. Скачать её вместе с исходными кодами вы можете по этой ссылке: WiiController.zip Испытания в реальном мире Все испытания проводились на гоночном симуляторе Colin McRae: DiRT 2. Заранее прошу прощения за качество фото и видео: все снималось на мыльницу пятилетней давности. Примерный вид настроек игры: А вот, как выглядит реальная игра с контроллером: Впечатления Сугубо положительные! Прошел всю игру, используя только Wiimote. Все работает настолько стабильно и быстро, что уже через несколько минут забываешь, что у тебя в руках устройство, предназначенное совсем не для этого. Из плюсов: огромный руль не закрывает пол-экрана, и не мешает перемещению рук, все управление выведено на кнопки контроллера. p.s. Вообще, можно сделать очень много прикольных штук с Wii Remote, вот некоторые из них: http://johnnylee.net/projects/wii/ Трекинг пальцев, мультитач доска, трекинг головы (для создания иллюзии трехмерности, заглянуть за край монитора). Фактически, я купил Wiimote под впечатлением от этих проектов. http://habrahabr.ru/blogs/DIY/91149/ Виртуальное окно в мир Winscape с трекингом положения головы используя Wiimote http://habrahabr.ru/blogs/subconsciousness/2877/ 5 самых креативных способов использования Wiimote.\n\n\nКраткое содержание:\nСтатья рассказывает о создании руля для PC с использованием беспроводного контроллера Wii Remote. Автор подключает контроллер к компьютеру и использует различные программы и библиотеки для преобразования данных с акселерометра в понятный играм формат. Затем он настраивает эмуляцию виртуального джойстика с помощью PPJoy, что позволяет использовать контроллер в гоночных играх. В статье также приводятся ссылки на программы и исходные коды для создания подобного руля.","summary":"Статья рассказывает о создании руля для PC с использованием беспроводного контроллера Wii Remote. Автор подключает контроллер к компьютеру и использует различные программы и библиотеки для преобразования данных с акселерометра в понятный играм формат. Затем он настраивает эмуляцию виртуального джойстика с помощью PPJoy, что позволяет использовать контроллер в гоночных играх. В статье также приводятся ссылки на программы и исходные коды для создания подобного руля."}
{"Text":"IT-хардкор: конференция о самых сложных ситуациях и проектах / Хабр                                                          \n\nПривет, Geektimes! Сегодня мы анонсируем самую суровую из наших конференций, которая пройдет 8 октября в Санкт-Петербурге. Она посвящена экстремальным ситуациям в IT-проектах: в планировании, менеджменте, разработке и поддержке. В программе — доклады от «Медузы» и «Вконтакте», история создания системы видеонаблюдения на выборах, разбор провалов в менеджменте проектов, предсказательное моделирование критических ситуаций с серьезными социальными последствиями, противостояние дизайнеров с женами заказчиков и еще несколько историй. Все по хардкору, даже кофе-брейки!\n\n**IT NonStop**\n— серия международных IT-конференций, которые проводит компания DataArt в 10 городах России, Украины и Польши. В каждом городе у IT NonStop — своя тема и свой формат, но все конференции объединяет единый подход\n_case after case_\n— докладчики рассказывают о конкретных решениях и кейсах.\n\n**Сайт конференции**\n\nit-nonstop.net\n\n**ПРОГРАММА**\n\n**Самат Галимов**\n, технический директор «Медузы», ранее был техническим директором Bookmate.\n\n**Дмитрий Захаров**\nраньше работал в Undev и банковских структурах, а теперь управляет софтом и железом в «Медузе».\n\n««Система на кэше», ИЛИ КАК «Медуза» обслуживает 5миллионов читателей и более 100 терабайт трафика, тратя на это меньше 500 долларов в месяц».\n\nВ докладе мы расскажем об архитектуре Медузы и внимательно посмотрим на одно ее из ключевых звеньев — систему кэширования.\n\n**Алексей Кривенков**\n\nРуководитель всех проектов видеонаблюдения в АО «Рестрим». Cтоял у истоков DataArt и Mail.ru. В последние годы занимается проектами, которые так или иначе связаны с передачей, хранением и распознаванием аудио и видео. Среди них — Moskva.fm, Piter.fm, Zombobox, Admonitor.\n\n«Как за три месяца написать систему, которая будет транслировать видео с двухсот камер и показывать его пяти миллионам россиян».\n\nВ 2012 году компания Кривенкова по заказу Ростелекома за рекордно короткий срок создала систему видеонаблюдения за выборами, которую теперь используют, например, и во время ЕГЭ. Как им это удалось — в докладе на IT Hardcore.\n\n**Денис Насонов**\n\nПреподаватель, старший разработчик Института наукоемких компьютерных технологий Университета ИТМО. Денис разрабатывает системы поддержки принятия решений в критических ситуациях с серьезными социальными последствиями (акты терроризма, преступная деятельность) и моделирует реакцию групп людей на внешние раздражители (паника при пожарах, наводнениях или эпидемиях).\n\n«От больших данных к предсказательному моделированию».\n\nТолпа — сложная система, и прогнозирование поведения большого количества людей до недавнего времени было невыполнимой задачей. Революцию здесь произвело сочетание компьютерного моделирования и методов работы с Big Data. Почему религиозные фестивали в Индии, собирающие миллионы паломников, проходят без давки, а малейший просчет в логистике музыкального фестиваля может привести к страшной трагедии? Как моделирование потоков толпы помогает в городском здравоохранении?\n\n**Евгений Ефимов**\n\nQA lead, DataArt. Начал карьеру в 10-м классе в качестве IT-директора, в 2003 г. устал и дауншифтнул до QA-практиканта в DataArt. Потом работал в «БриджКвесте», но зов Родины оказался сильнее, и в 2010 вернулся в DataArt. Всегда работал в области QA на разных ролях и позициях: QA, BA, PM, pre-sales.\n\n«Краш-тест для менеджера. Какие основные ошибки может совершить PM, взяв на себя больше, чем может вынести».\n\nЧтобы предотвратить крушение поезда, недостаточно знать, как проектировать поезда. Очень полезно анализировать причины известных крушений, учитывать их и стараться предотвратить в дальнейшем. Я на своем примере расскажу про крушения и почти крушения нескольких проектов и о том, какие выводы я из этого смог сделать.\n\n**Вячеслав Михайлов**\n\nSolutions Architect, DataArt. Занимается разработкой ПО более 18 лет. Работал в различных индустриях: финансы, информационная безопасность, путешествия. Начинал разработку на.NET в 2000 году, когда появились первые бета-версии платформы.\n\n«Limited Concurrency Multitasking».\n\nВ процессе работы над большим продуктом возникла интересная задача — запускать много тестов, которые имели между собой различные нетривиальные зависимости. Тесты должны были выполняться с максимальной эффективностью, занимая все разрешенные вычислительные ресурсы. При этом внутри тесты использовали асинхронное сетевое взаимодействие и могли работать долго. В докладе я расскажу о том, как мы справились с проблемами в реализации, которые возникли из-за ограничения количества используемых ядер и особенностей работы Task Parallel Library.\n\n**Валентина Крупадерова**\n\nТренес-консультант по проектному управлению в компании «СКАУТ-Академия». В IT c 2006 года как разработчик, с 2011 как менеджер. Лидер сообщества IT-тренеров Санкт-Петербурга. Член программного комитета конференции CEE-SECR 2016. Регулярный спикер на конференциях по управлению проектами.\n\n«Как мы здесь оказались? Как менеджеры усложняют проект»\n\nНе каждый проект в самом начале выглядит как жесткий, но почти каждый выглядит таким под конец. Почему? Ладно, мы все знаем про то, что окружение проекта непредсказуемо, но как-то неловко взрослым людям сваливать все на то, что «мир сложный». Какие решения менеджеров могут превратить и превращают нормальный проект в хардкор с переработками, выходом из бюджета, стрессом и конфликтами в команде? Какие незыблемые правила, применяемые без понимания, к этому приводят? Если вы уверены, что точно знаете, почему нельзя добавлять разработчиков в опаздывающий проект, приходите, нам есть, о чем поговорить.\n\n**Анастасия Режепп**\n\nРуководитель дизайн-студии DataArt. Пришла в дизайн-студию DataArt девять лет назад. До этого работала арт-директором дизайн-студии «Петерлинк.Веб», провела год в «Баухаус-Дессау» — центре исследований, обучения и экспериментального дизайна, преподавала веб-дизайн, писала статьи о нем и на другие темы.\n\n**Артем Санжаревский**\n\nSenior designer, DataArt. 15 лет в дизайне, 7 лет в IT. Проповедует интерактивный маркетинг и брендинг. Верит в любовь между дизайнерами и фронтенд-разработчиками.\n\n«О борьбе с хаосом, или как получить хороший продукт, когда против тебя человеческий фактор».\n\nКлиент решил создать новое бомбическое приложение, но сам пока не знает, о чем? Начали проект с одними стейкхолдерами, а по пути они сменились на других с совершенно иными идеями? На стороне клиента тоже есть дизайнер? Жена директора постоянно дает советы по внешнему виду приложения? Получится ли у дизайн-студии сделать хорошую вещь во всех этих ситуациях, или придется последовательно уничтожить жену, другого дизайнера и стейкхолдеров?\n\n**Вячеслав Шебанов**\n\nРазработчик «Вконтакте». Увлекается функциональным и реактивным программированием, программированием распределенных систем. Сооснователь интернет-газеты «Бумага», один из организаторов ежемесячных встреч SPb Frontend.\n\n**Александр Чистяков**\n\nSenior Developer, DataArt. Соорганизатор митапа петербургских девопсов «DevOps-40». Трижды работал в DataArt. Занимается эксплуатацией и поддержкой процесса разработки сложных информационных систем: больших веб-сайтов, аналитических кластеров. Снова работает в DataArt.\n\nЗарегистрироваться можно\nздесь\n.","metric":0.6220703125,"metric_input":"Текст статьи:\nIT-хардкор: конференция о самых сложных ситуациях и проектах / Хабр                                                          \n\nПривет, Geektimes! Сегодня мы анонсируем самую суровую из наших конференций, которая пройдет 8 октября в Санкт-Петербурге. Она посвящена экстремальным ситуациям в IT-проектах: в планировании, менеджменте, разработке и поддержке. В программе — доклады от «Медузы» и «Вконтакте», история создания системы видеонаблюдения на выборах, разбор провалов в менеджменте проектов, предсказательное моделирование критических ситуаций с серьезными социальными последствиями, противостояние дизайнеров с женами заказчиков и еще несколько историй. Все по хардкору, даже кофе-брейки!\n\n**IT NonStop**\n— серия международных IT-конференций, которые проводит компания DataArt в 10 городах России, Украины и Польши. В каждом городе у IT NonStop — своя тема и свой формат, но все конференции объединяет единый подход\n_case after case_\n— докладчики рассказывают о конкретных решениях и кейсах.\n\n**Сайт конференции**\n\nit-nonstop.net\n\n**ПРОГРАММА**\n\n**Самат Галимов**\n, технический директор «Медузы», ранее был техническим директором Bookmate.\n\n**Дмитрий Захаров**\nраньше работал в Undev и банковских структурах, а теперь управляет софтом и железом в «Медузе».\n\n««Система на кэше», ИЛИ КАК «Медуза» обслуживает 5миллионов читателей и более 100 терабайт трафика, тратя на это меньше 500 долларов в месяц».\n\nВ докладе мы расскажем об архитектуре Медузы и внимательно посмотрим на одно ее из ключевых звеньев — систему кэширования.\n\n**Алексей Кривенков**\n\nРуководитель всех проектов видеонаблюдения в АО «Рестрим». Cтоял у истоков DataArt и Mail.ru. В последние годы занимается проектами, которые так или иначе связаны с передачей, хранением и распознаванием аудио и видео. Среди них — Moskva.fm, Piter.fm, Zombobox, Admonitor.\n\n«Как за три месяца написать систему, которая будет транслировать видео с двухсот камер и показывать его пяти миллионам россиян».\n\nВ 2012 году компания Кривенкова по заказу Ростелекома за рекордно короткий срок создала систему видеонаблюдения за выборами, которую теперь используют, например, и во время ЕГЭ. Как им это удалось — в докладе на IT Hardcore.\n\n**Денис Насонов**\n\nПреподаватель, старший разработчик Института наукоемких компьютерных технологий Университета ИТМО. Денис разрабатывает системы поддержки принятия решений в критических ситуациях с серьезными социальными последствиями (акты терроризма, преступная деятельность) и моделирует реакцию групп людей на внешние раздражители (паника при пожарах, наводнениях или эпидемиях).\n\n«От больших данных к предсказательному моделированию».\n\nТолпа — сложная система, и прогнозирование поведения большого количества людей до недавнего времени было невыполнимой задачей. Революцию здесь произвело сочетание компьютерного моделирования и методов работы с Big Data. Почему религиозные фестивали в Индии, собирающие миллионы паломников, проходят без давки, а малейший просчет в логистике музыкального фестиваля может привести к страшной трагедии? Как моделирование потоков толпы помогает в городском здравоохранении?\n\n**Евгений Ефимов**\n\nQA lead, DataArt. Начал карьеру в 10-м классе в качестве IT-директора, в 2003 г. устал и дауншифтнул до QA-практиканта в DataArt. Потом работал в «БриджКвесте», но зов Родины оказался сильнее, и в 2010 вернулся в DataArt. Всегда работал в области QA на разных ролях и позициях: QA, BA, PM, pre-sales.\n\n«Краш-тест для менеджера. Какие основные ошибки может совершить PM, взяв на себя больше, чем может вынести».\n\nЧтобы предотвратить крушение поезда, недостаточно знать, как проектировать поезда. Очень полезно анализировать причины известных крушений, учитывать их и стараться предотвратить в дальнейшем. Я на своем примере расскажу про крушения и почти крушения нескольких проектов и о том, какие выводы я из этого смог сделать.\n\n**Вячеслав Михайлов**\n\nSolutions Architect, DataArt. Занимается разработкой ПО более 18 лет. Работал в различных индустриях: финансы, информационная безопасность, путешествия. Начинал разработку на.NET в 2000 году, когда появились первые бета-версии платформы.\n\n«Limited Concurrency Multitasking».\n\nВ процессе работы над большим продуктом возникла интересная задача — запускать много тестов, которые имели между собой различные нетривиальные зависимости. Тесты должны были выполняться с максимальной эффективностью, занимая все разрешенные вычислительные ресурсы. При этом внутри тесты использовали асинхронное сетевое взаимодействие и могли работать долго. В докладе я расскажу о том, как мы справились с проблемами в реализации, которые возникли из-за ограничения количества используемых ядер и особенностей работы Task Parallel Library.\n\n**Валентина Крупадерова**\n\nТренес-консультант по проектному управлению в компании «СКАУТ-Академия». В IT c 2006 года как разработчик, с 2011 как менеджер. Лидер сообщества IT-тренеров Санкт-Петербурга. Член программного комитета конференции CEE-SECR 2016. Регулярный спикер на конференциях по управлению проектами.\n\n«Как мы здесь оказались? Как менеджеры усложняют проект»\n\nНе каждый проект в самом начале выглядит как жесткий, но почти каждый выглядит таким под конец. Почему? Ладно, мы все знаем про то, что окружение проекта непредсказуемо, но как-то неловко взрослым людям сваливать все на то, что «мир сложный». Какие решения менеджеров могут превратить и превращают нормальный проект в хардкор с переработками, выходом из бюджета, стрессом и конфликтами в команде? Какие незыблемые правила, применяемые без понимания, к этому приводят? Если вы уверены, что точно знаете, почему нельзя добавлять разработчиков в опаздывающий проект, приходите, нам есть, о чем поговорить.\n\n**Анастасия Режепп**\n\nРуководитель дизайн-студии DataArt. Пришла в дизайн-студию DataArt девять лет назад. До этого работала арт-директором дизайн-студии «Петерлинк.Веб», провела год в «Баухаус-Дессау» — центре исследований, обучения и экспериментального дизайна, преподавала веб-дизайн, писала статьи о нем и на другие темы.\n\n**Артем Санжаревский**\n\nSenior designer, DataArt. 15 лет в дизайне, 7 лет в IT. Проповедует интерактивный маркетинг и брендинг. Верит в любовь между дизайнерами и фронтенд-разработчиками.\n\n«О борьбе с хаосом, или как получить хороший продукт, когда против тебя человеческий фактор».\n\nКлиент решил создать новое бомбическое приложение, но сам пока не знает, о чем? Начали проект с одними стейкхолдерами, а по пути они сменились на других с совершенно иными идеями? На стороне клиента тоже есть дизайнер? Жена директора постоянно дает советы по внешнему виду приложения? Получится ли у дизайн-студии сделать хорошую вещь во всех этих ситуациях, или придется последовательно уничтожить жену, другого дизайнера и стейкхолдеров?\n\n**Вячеслав Шебанов**\n\nРазработчик «Вконтакте». Увлекается функциональным и реактивным программированием, программированием распределенных систем. Сооснователь интернет-газеты «Бумага», один из организаторов ежемесячных встреч SPb Frontend.\n\n**Александр Чистяков**\n\nSenior Developer, DataArt. Соорганизатор митапа петербургских девопсов «DevOps-40». Трижды работал в DataArt. Занимается эксплуатацией и поддержкой процесса разработки сложных информационных систем: больших веб-сайтов, аналитических кластеров. Снова работает в DataArt.\n\nЗарегистрироваться можно\nздесь\n.\n\nКраткое содержание:\nКонференция IT NonStop, которая пройдет 8 октября в Санкт-Петербурге, посвящена экстремальным ситуациям в IT-проектах. В программе доклады от компаний \"Медуза\", \"Вконтакте\", история создания системы видеонаблюдения на выборах, разбор провалов в менеджменте проектов, предсказательное моделирование критических ситуаций с серьезными социальными последствиями, противостояние дизайнеров с женами заказчиков и другие истории.","summary":"Конференция IT NonStop, которая пройдет 8 октября в Санкт-Петербурге, посвящена экстремальным ситуациям в IT-проектах. В программе доклады от компаний \"Медуза\", \"Вконтакте\", история создания системы видеонаблюдения на выборах, разбор провалов в менеджменте проектов, предсказательное моделирование критических ситуаций с серьезными социальными последствиями, противостояние дизайнеров с женами заказчиков и другие истории."}
{"Text":"habr.com\n== Распознавание непрерывного рукописного текста в режиме off-line\n\n\n13 янв 2012 в 15:37Предисловие\n\n Как известно, задача распознавания непрерывного рукописного текста в режиме off-line пока считается нерешённой. \n\n Мне удалось решить эту задачу теоретически и практически. Практическая часть сейчас имеет вид демонстрационной версии программы. Решение общее, оно не ограничивается какой-либо областью применения, языком или размером словаря.О программе\n\n Программа полностью обучаемая. Процесс обучения выглядит просто: вы пишете символы в режиме on-line, программа их обобщает и выделяет алгоритм написания. Это первый этап обучения. Второй этап происходит во время работы. Если встречается символ, общий алгоритм написания которого совпадает с одним из имеющихся в наличии, а значения некоторых свойств выходят за рамки вычисленных на первом этапе диапазонов, то диапазоны расширяются. Конечно, только после подтверждения пользователем общего результата распознавания. К слову сказать, на первом этапе достаточно от трёх до семи предъявлений символа, и алгоритм готов.Теория\n\n Немного о теории. Существует несколько подходов к решению указанной задачи. Их обычно делят на два вида: структурные и эталонные. Первый основан на выделении и анализе различных структурных элементов символа и их признаков, свойств. Второй предполагает сравнение распознаваемого символа с набором заданных эталонов. Эти методы не позволяют решить задачу в общем виде. \n\n Задача рукописного ввода в режиме on-line полностью и успешно решена. Это решение основано, в любом случае, на создании алгоритмов написания символов, учитывающих траекторию движения пера. То есть, последовательность смены его координат. Были предложения свести задачу распознавания в off-line режиме к распознаванию в режиме on-line. Для этого достаточно правильно считать линии с графической копии текста. Но сделать это принципиально невозможно. Можно считать отрезки линий между пересечениями, но чтобы их правильно соединить, уже нужна интерпретация. \n\n Остаётся только одно решение — восстанавливать символы в процессе интерпретации отрезков, полученных на этапе считывания с цифровой графической копии текста. Для этого нужны две составляющие: специальное представление алгоритма написания символа, позволяющее это делать, и алгоритм интерпретации отрезков, способный проанализировать все возможные варианты интерпретации.Практика\n\n Это удалось сделать в полной мере. Как известно, главная задача демоверсии — продемонстрировать принципиальное решение поставленной задачи. На что в этом смысле способен тот прототип, который имеется сейчас? Программа способна распознавать одно слово, написанное произвольным непрерывным почерком на белой бумаге. Для перевода в цифровой файл слово может быть либо отсканировано, либо сфотографировано веб-камерой или цифровым фотоаппаратом. В принципе, уже сделано и распознавание текста, но эта функция требует доработки. \n\n Ниже приведены примеры распознаваемых слов. Как видно, здесь не только обычное написание, но и «усложнённые» варианты: перечёркнутые слова, символы, написанные отрезками, имеющие лишние части и тому подобное. Это показывает, что в полностью готовом виде программа будет способна распознавать достаточно зашумлённые тексты. \n\n Очевидно, что уверенно распознать можно только те символы, которые имеют все необходимые части примерно на своих местах. Если имеются недостающие или сильно искажённые части, то необходима интерпретация на уровне слов. Наличие словаря повышает процент распознавания, но не решает всех проблем. Бывают такие случаи, когда без понимания смысла фразы некоторые слова однозначно интерпретировать невозможно. Для этого нужна система искусственного интеллекта, способная понимать смысл фраз естественного языка. До недавнего времени информации о наличии таких систем на рынке не было. Сейчас уже есть: фирма ABBYY объявила о создании [системы «Compreno»](http://www.kommersant.ru/doc/1822898?stamp=634592536239114016) , в которой для перевода используется смысловая интерпретация фраз на основе «модели мира», не зависимой от конкретного языка. \n\n У меня тоже есть прототип системы ИИ, способной понимать смысл текста. Если судить по той информации о «Compreno», которая сейчас есть в СМИ, моя система функционально гораздо шире. Она обучаема, способна к обобщению информации и к активному поиску знаний в случае, когда их не хватает для выполнения поставленной задачи. Другими словами, такая система вполне способна работать личным секретарём. Но у неё есть один серьёзный недостаток по сравнению с «Compreno» — по степени общей готовности она пока не дотягивает даже до демоверсии.Коммерция\n\n И в конце немного о коммерческой стороне проекта. В интернете есть [интервью](http://www.docflow.ru/analitics/detail.php?ID=15199) вице-президента компании ABBYY Lingvo Арама Пахчаняна. В отношении задачи распознавания непрерывного рукописного текста в режиме off-line там сказано, по сути, что эту задачу и не надо решать. Затраты на её решение (надо полагать, очень большие) не окупятся. И, похоже, в основном потому, что непрерывное писание фирма ABBYY Lingvo практически уже сделала неактуальным. Она полностью решила проблему распознавания раздельного рукописного текста, и на все случаи жизни разработала соответствующие бланки. \n\n Возможно, это была шутка. Но всё равно, имеет смысл сказать следующее. Писать привычным непрерывным почерком удобнее и легче, чем вписывать буквы в квадратики. Если компьютер будет распознавать первое не хуже второго, то второе уйдёт в прошлое так же как перфокарты, чёрно-белые телевизоры и плёнки для фотоаппаратов. \n\n В следующем короткое видео можно увидеть программу в действии. Возможно, это будет интересно.Заключение\n\n И ещё один важный момент – показатели эффективности, а именно, время и процент распознавания. Конечно, в демоверсии основное внимание уделялось второму критерию. Сейчас достигнут уровень не ниже 70%. В готовом варианте этот показатель можно сформулировать так: если человек сможет прочитать текст, то и программа тоже. О времени распознавания пока можно сказать только то, что его удастся довести до приемлемых величин. \n\n Если всё пойдёт хорошо, будут ещё статьи о некоторых технических аспектах распознавания текста и об ИИ. \n\n Благодарю за внимание.  ____________  **Update.** Дорогие хабравчане! Спасибо всем за фидбек, нам это очень важно и полезно. В целом топик был встречен положительно, что не может не радовать. \n\n Негодующим личностям хотелось бы сказать: уважаемые, мы не ярмарочные фокусники. Мы отдаём отчёт в своих словах. Если мы написали, что в готовом продукте точность распознавания будет стремиться к 100%, значит мы в этом уверены. \n\n Эту статью можете считать анонсом, у неё не было цели подробно раскрыть все технические подробности. Однако учитывая проявленный интерес, через некоторое время будет ещё одна статья, более подробно описывающая процесс распознавания. \n\n Также будет доступная для скачивания демонстрационная версия программы.","metric":0.638671875,"metric_input":"Текст статьи:\nhabr.com\n== Распознавание непрерывного рукописного текста в режиме off-line\n\n\n13 янв 2012 в 15:37Предисловие\n\n Как известно, задача распознавания непрерывного рукописного текста в режиме off-line пока считается нерешённой. \n\n Мне удалось решить эту задачу теоретически и практически. Практическая часть сейчас имеет вид демонстрационной версии программы. Решение общее, оно не ограничивается какой-либо областью применения, языком или размером словаря.О программе\n\n Программа полностью обучаемая. Процесс обучения выглядит просто: вы пишете символы в режиме on-line, программа их обобщает и выделяет алгоритм написания. Это первый этап обучения. Второй этап происходит во время работы. Если встречается символ, общий алгоритм написания которого совпадает с одним из имеющихся в наличии, а значения некоторых свойств выходят за рамки вычисленных на первом этапе диапазонов, то диапазоны расширяются. Конечно, только после подтверждения пользователем общего результата распознавания. К слову сказать, на первом этапе достаточно от трёх до семи предъявлений символа, и алгоритм готов.Теория\n\n Немного о теории. Существует несколько подходов к решению указанной задачи. Их обычно делят на два вида: структурные и эталонные. Первый основан на выделении и анализе различных структурных элементов символа и их признаков, свойств. Второй предполагает сравнение распознаваемого символа с набором заданных эталонов. Эти методы не позволяют решить задачу в общем виде. \n\n Задача рукописного ввода в режиме on-line полностью и успешно решена. Это решение основано, в любом случае, на создании алгоритмов написания символов, учитывающих траекторию движения пера. То есть, последовательность смены его координат. Были предложения свести задачу распознавания в off-line режиме к распознаванию в режиме on-line. Для этого достаточно правильно считать линии с графической копии текста. Но сделать это принципиально невозможно. Можно считать отрезки линий между пересечениями, но чтобы их правильно соединить, уже нужна интерпретация. \n\n Остаётся только одно решение — восстанавливать символы в процессе интерпретации отрезков, полученных на этапе считывания с цифровой графической копии текста. Для этого нужны две составляющие: специальное представление алгоритма написания символа, позволяющее это делать, и алгоритм интерпретации отрезков, способный проанализировать все возможные варианты интерпретации.Практика\n\n Это удалось сделать в полной мере. Как известно, главная задача демоверсии — продемонстрировать принципиальное решение поставленной задачи. На что в этом смысле способен тот прототип, который имеется сейчас? Программа способна распознавать одно слово, написанное произвольным непрерывным почерком на белой бумаге. Для перевода в цифровой файл слово может быть либо отсканировано, либо сфотографировано веб-камерой или цифровым фотоаппаратом. В принципе, уже сделано и распознавание текста, но эта функция требует доработки. \n\n Ниже приведены примеры распознаваемых слов. Как видно, здесь не только обычное написание, но и «усложнённые» варианты: перечёркнутые слова, символы, написанные отрезками, имеющие лишние части и тому подобное. Это показывает, что в полностью готовом виде программа будет способна распознавать достаточно зашумлённые тексты. \n\n Очевидно, что уверенно распознать можно только те символы, которые имеют все необходимые части примерно на своих местах. Если имеются недостающие или сильно искажённые части, то необходима интерпретация на уровне слов. Наличие словаря повышает процент распознавания, но не решает всех проблем. Бывают такие случаи, когда без понимания смысла фразы некоторые слова однозначно интерпретировать невозможно. Для этого нужна система искусственного интеллекта, способная понимать смысл фраз естественного языка. До недавнего времени информации о наличии таких систем на рынке не было. Сейчас уже есть: фирма ABBYY объявила о создании [системы «Compreno»](http://www.kommersant.ru/doc/1822898?stamp=634592536239114016) , в которой для перевода используется смысловая интерпретация фраз на основе «модели мира», не зависимой от конкретного языка. \n\n У меня тоже есть прототип системы ИИ, способной понимать смысл текста. Если судить по той информации о «Compreno», которая сейчас есть в СМИ, моя система функционально гораздо шире. Она обучаема, способна к обобщению информации и к активному поиску знаний в случае, когда их не хватает для выполнения поставленной задачи. Другими словами, такая система вполне способна работать личным секретарём. Но у неё есть один серьёзный недостаток по сравнению с «Compreno» — по степени общей готовности она пока не дотягивает даже до демоверсии.Коммерция\n\n И в конце немного о коммерческой стороне проекта. В интернете есть [интервью](http://www.docflow.ru/analitics/detail.php?ID=15199) вице-президента компании ABBYY Lingvo Арама Пахчаняна. В отношении задачи распознавания непрерывного рукописного текста в режиме off-line там сказано, по сути, что эту задачу и не надо решать. Затраты на её решение (надо полагать, очень большие) не окупятся. И, похоже, в основном потому, что непрерывное писание фирма ABBYY Lingvo практически уже сделала неактуальным. Она полностью решила проблему распознавания раздельного рукописного текста, и на все случаи жизни разработала соответствующие бланки. \n\n Возможно, это была шутка. Но всё равно, имеет смысл сказать следующее. Писать привычным непрерывным почерком удобнее и легче, чем вписывать буквы в квадратики. Если компьютер будет распознавать первое не хуже второго, то второе уйдёт в прошлое так же как перфокарты, чёрно-белые телевизоры и плёнки для фотоаппаратов. \n\n В следующем короткое видео можно увидеть программу в действии. Возможно, это будет интересно.Заключение\n\n И ещё один важный момент – показатели эффективности, а именно, время и процент распознавания. Конечно, в демоверсии основное внимание уделялось второму критерию. Сейчас достигнут уровень не ниже 70%. В готовом варианте этот показатель можно сформулировать так: если человек сможет прочитать текст, то и программа тоже. О времени распознавания пока можно сказать только то, что его удастся довести до приемлемых величин. \n\n Если всё пойдёт хорошо, будут ещё статьи о некоторых технических аспектах распознавания текста и об ИИ. \n\n Благодарю за внимание.  ____________  **Update.** Дорогие хабравчане! Спасибо всем за фидбек, нам это очень важно и полезно. В целом топик был встречен положительно, что не может не радовать. \n\n Негодующим личностям хотелось бы сказать: уважаемые, мы не ярмарочные фокусники. Мы отдаём отчёт в своих словах. Если мы написали, что в готовом продукте точность распознавания будет стремиться к 100%, значит мы в этом уверены. \n\n Эту статью можете считать анонсом, у неё не было цели подробно раскрыть все технические подробности. Однако учитывая проявленный интерес, через некоторое время будет ещё одна статья, более подробно описывающая процесс распознавания. \n\n Также будет доступная для скачивания демонстрационная версия программы.\n\nКраткое содержание:\nВ статье рассматривается проблема распознавания непрерывного рукописного текста в режиме off-line. Автор предлагает теоретическое и практическое решение этой задачи.\n\nПрограмма полностью обучаемая и способна распознавать одно слово, написанное произвольным непрерывным почерком на белой бумаге. В полностью готовом виде программа будет способна распознавать достаточно зашумленные тексты.\n\nВ интервью вице-президент компании ABBYY Lingvo Арам Пахчанян говорит, что задачу распознавания непрерывного рукописного текста в режиме off-line не нужно решать, так как затраты на ее решение не окупятся. Однако, автор считает, что писать привычным непрерывным почерком удобнее и легче, чем вписывать буквы в квадратики. Если компьютер будет распознавать первое не хуже второго, то второе уйдёт в прошлое.\n\nАвтор обещает опубликовать статьи о некоторых технических аспектах распознавания текста и об ИИ.","summary":"В статье рассматривается проблема распознавания непрерывного рукописного текста в режиме off-line. Автор предлагает теоретическое и практическое решение этой задачи.\n\nПрограмма полностью обучаемая и способна распознавать одно слово, написанное произвольным непрерывным почерком на белой бумаге. В полностью готовом виде программа будет способна распознавать достаточно зашумленные тексты.\n\nВ интервью вице-президент компании ABBYY Lingvo Арам Пахчанян говорит, что задачу распознавания непрерывного рукописного текста в режиме off-line не нужно решать, так как затраты на ее решение не окупятся. Однако, автор считает, что писать привычным непрерывным почерком удобнее и легче, чем вписывать буквы в квадратики. Если компьютер будет распознавать первое не хуже второго, то второе уйдёт в прошлое.\n\nАвтор обещает опубликовать статьи о некоторых технических аспектах распознавания текста и об ИИ."}
{"Text":"Tesla продала 285 тыс. клиентам в Северной Америке доступ к системе Full Self-Driving (FSD) за всю историю проекта / Хабр                 \n\nTesla\nпродала\n285 тыс. клиентам в Северной Америке доступ к бета-версии автопилота Full Self-Driving (FSD) за всю историю проекта.\n\nПо\nданным\nElectrek, всего в Серверной Америке было продано около 1,5 млн электромобилей Tesla, обладающих необходимой для работы с FSD версией бортового компьютера. Производитель также за дополнительную плату предлагает апгрейд бортовых систем, чтобы на старые электромобили можно было установить FSD.\n\nЭксперты считают, что Tesla фактически в рамках внутреннего тестирования своей системы получила с клиентов более \\$2 млрд за возможность активировать FSD. Это с учётом, что ранний доступ к FSD продавался за \\$6 тыс., а сейчас эта опция стоит \\$15 тыс.\n\nTesla считает, что работа бета-версии системы помощи водителю FSD полностью безопасна при выполнении всех рекомендаций компании: руки на руле и внимательно смотреть на дорожную обстановку. В компании уточнили, что ответственность за использование FSD остаётся за водителем, который должен всегда быть внимательным и быть готовым взять на себя управление.\n\nНа своём сайте Tesla сообщает, что FSD предназначена для поездок на короткие и дальние расстояния без каких-либо действий со стороны человека на сидении водителя. В компании отмечают, что система прошла согласование регулирующих органов и прошла миллиарды миль испытаний. По данным директора по ПО Tesla Autopilot Ашока Эллусвами, автопилот ежедневно предотвращает около 40 ДТП.\n\n_Tesla предупреждает, что руки водителя при использовании FSD должны быть на руле._\n\nИзначально Tesla не имела возможности обнаружения рук водителя на руле. Бортовой компьютер может отслеживать приложенный к рулю крутящий момент. Автопроизводитель регулярно просит водителей держать руль и иногда им двигать, чтобы продолжать использовать автопилот. Водители электромобилей Tesla нашли способ обманывать ранние версии системы FSD с помощью специальных обманок на руле, вплоть до установки на руль бутылки с водой на скотче, которая иногда двигает рулевое колесо на кочках и автопилот далее работает штатным образом. Теперь Tesla научилась\nотслеживать\nи отключать FSD при использовании таких устройств.\n\n---\n\nВ конце декабря штат Калифорния принял  закон, который запрещает американской компании Tesla называть программное обеспечение для электромобилей Full Self-Driving (FSD). Вступающее в силу со следующего года положение запретит Tesla называть автопилот FSD, поскольку компания вводит в заблуждение владельцев и покупателей электромобилей, утверждают законодатели.\n\n24 ноября Tesla открыла  доступ к бета-версии автопилота Full Self-Driving (FSD) в Северной Америке для всех желающих купить эту опцию или сделал это ранее, но ждал своей очереди.\n\nРанее доступ к FSD был ограничен, а также предоставлялся  тем водителям, кто смог получить  проходные баллы  во внутренней системе оценки безопасности вождения от Tesla.\n\nТеперь эти ограничения сняты, а компания уже установила  FSD более чем на 160 тыс. электромобилей.\n\n5 сентября Tesla увеличила  стоимость доступа к бета-версии системы автопилота Full Self-Driving ( FSD ) до \\$15 тысяч.\n\nРанее единоразовый платёж за активацию помощника водителя FSD составлял \\$10 тысяч, а потом её стоимость поднялась до \\$12 тысяч.\n\nБортовую систему электромобиля Tesla можно удалённо обновить до FSD за 2 минуты через приложение производителя. Стоимость месячной подписки на FSD не поменялась и составляет , как и ранее, \\$99 или \\$199, в зависимости от комплектации бортовой системы электромобиля.\n\nОснователь Tesla Илон Маск ранее заявлял , что стоимость доступа к FSD все ещё «смехотворно дешёвая». По его мнению, по мере того, как FSD будет получать автономные возможности, плата за подключение этой услуги будет только повышаться.\n\nЭксперты считают, что до сих пор эта функция не оправдала своего названия, а компания фактически собирает оплату с водителей за тестирование и доработку сервиса. FSD находится в стадии бета-тестирования уже два года. Данная система не предоставляет клиенту Tesla возможность полного автономного вождения.\n\nСистема навигации автопилота Tesla пакета FSD обучена  выполнять большое количество полуавтономных функций. Например, она умеет автоматически менять полосы движения, самостоятельно парковаться на стоянке и останавливаться на запрещающий сигнал светофора, двигаться по сложному маршруту в городе. При активации FSD водители всё равно должны держать руки на руле и постоянно следить за дорожной ситуацией. Разработчики улучшили алгоритм обнаружения животных, пешеходов  и велосипедистов, обновили систему ограничения скорости, внесли правки в работу алгоритма городского движения, обновили модуль для более плавного перестроения между полосами движения. Tesla FSD Beta v11 включает  в себя слияние стека Tesla для городского и шоссейного вождения, что станет большим шагом на пути к развитию этой автономной системы вождения.\n\nВ июле 2021 года Tesla запустила  подписку на сервис помощи водителю Full Self-Driving за \\$199 в месяц. Подписка на сервис FSD доступна не всем владельцам Tesla. Её можно активировать, если в электромобиле установлена с завода компьютерная бортовая система FSD 3.0 (HW3) или выше. Если электромобиль ей не оснащён, то владелец может её поставить дополнительно за \\$1500. Если у владельца Tesla есть пакет Enhanced Autopilot, то подписка на FSD будет стоить \\$99 в месяц.","metric":0.369140625,"metric_input":"Текст статьи:\nTesla продала 285 тыс. клиентам в Северной Америке доступ к системе Full Self-Driving (FSD) за всю историю проекта / Хабр                 \n\nTesla\nпродала\n285 тыс. клиентам в Северной Америке доступ к бета-версии автопилота Full Self-Driving (FSD) за всю историю проекта.\n\nПо\nданным\nElectrek, всего в Серверной Америке было продано около 1,5 млн электромобилей Tesla, обладающих необходимой для работы с FSD версией бортового компьютера. Производитель также за дополнительную плату предлагает апгрейд бортовых систем, чтобы на старые электромобили можно было установить FSD.\n\nЭксперты считают, что Tesla фактически в рамках внутреннего тестирования своей системы получила с клиентов более \\$2 млрд за возможность активировать FSD. Это с учётом, что ранний доступ к FSD продавался за \\$6 тыс., а сейчас эта опция стоит \\$15 тыс.\n\nTesla считает, что работа бета-версии системы помощи водителю FSD полностью безопасна при выполнении всех рекомендаций компании: руки на руле и внимательно смотреть на дорожную обстановку. В компании уточнили, что ответственность за использование FSD остаётся за водителем, который должен всегда быть внимательным и быть готовым взять на себя управление.\n\nНа своём сайте Tesla сообщает, что FSD предназначена для поездок на короткие и дальние расстояния без каких-либо действий со стороны человека на сидении водителя. В компании отмечают, что система прошла согласование регулирующих органов и прошла миллиарды миль испытаний. По данным директора по ПО Tesla Autopilot Ашока Эллусвами, автопилот ежедневно предотвращает около 40 ДТП.\n\n_Tesla предупреждает, что руки водителя при использовании FSD должны быть на руле._\n\nИзначально Tesla не имела возможности обнаружения рук водителя на руле. Бортовой компьютер может отслеживать приложенный к рулю крутящий момент. Автопроизводитель регулярно просит водителей держать руль и иногда им двигать, чтобы продолжать использовать автопилот. Водители электромобилей Tesla нашли способ обманывать ранние версии системы FSD с помощью специальных обманок на руле, вплоть до установки на руль бутылки с водой на скотче, которая иногда двигает рулевое колесо на кочках и автопилот далее работает штатным образом. Теперь Tesla научилась\nотслеживать\nи отключать FSD при использовании таких устройств.\n\n---\n\nВ конце декабря штат Калифорния принял  закон, который запрещает американской компании Tesla называть программное обеспечение для электромобилей Full Self-Driving (FSD). Вступающее в силу со следующего года положение запретит Tesla называть автопилот FSD, поскольку компания вводит в заблуждение владельцев и покупателей электромобилей, утверждают законодатели.\n\n24 ноября Tesla открыла  доступ к бета-версии автопилота Full Self-Driving (FSD) в Северной Америке для всех желающих купить эту опцию или сделал это ранее, но ждал своей очереди.\n\nРанее доступ к FSD был ограничен, а также предоставлялся  тем водителям, кто смог получить  проходные баллы  во внутренней системе оценки безопасности вождения от Tesla.\n\nТеперь эти ограничения сняты, а компания уже установила  FSD более чем на 160 тыс. электромобилей.\n\n5 сентября Tesla увеличила  стоимость доступа к бета-версии системы автопилота Full Self-Driving ( FSD ) до \\$15 тысяч.\n\nРанее единоразовый платёж за активацию помощника водителя FSD составлял \\$10 тысяч, а потом её стоимость поднялась до \\$12 тысяч.\n\nБортовую систему электромобиля Tesla можно удалённо обновить до FSD за 2 минуты через приложение производителя. Стоимость месячной подписки на FSD не поменялась и составляет , как и ранее, \\$99 или \\$199, в зависимости от комплектации бортовой системы электромобиля.\n\nОснователь Tesla Илон Маск ранее заявлял , что стоимость доступа к FSD все ещё «смехотворно дешёвая». По его мнению, по мере того, как FSD будет получать автономные возможности, плата за подключение этой услуги будет только повышаться.\n\nЭксперты считают, что до сих пор эта функция не оправдала своего названия, а компания фактически собирает оплату с водителей за тестирование и доработку сервиса. FSD находится в стадии бета-тестирования уже два года. Данная система не предоставляет клиенту Tesla возможность полного автономного вождения.\n\nСистема навигации автопилота Tesla пакета FSD обучена  выполнять большое количество полуавтономных функций. Например, она умеет автоматически менять полосы движения, самостоятельно парковаться на стоянке и останавливаться на запрещающий сигнал светофора, двигаться по сложному маршруту в городе. При активации FSD водители всё равно должны держать руки на руле и постоянно следить за дорожной ситуацией. Разработчики улучшили алгоритм обнаружения животных, пешеходов  и велосипедистов, обновили систему ограничения скорости, внесли правки в работу алгоритма городского движения, обновили модуль для более плавного перестроения между полосами движения. Tesla FSD Beta v11 включает  в себя слияние стека Tesla для городского и шоссейного вождения, что станет большим шагом на пути к развитию этой автономной системы вождения.\n\nВ июле 2021 года Tesla запустила  подписку на сервис помощи водителю Full Self-Driving за \\$199 в месяц. Подписка на сервис FSD доступна не всем владельцам Tesla. Её можно активировать, если в электромобиле установлена с завода компьютерная бортовая система FSD 3.0 (HW3) или выше. Если электромобиль ей не оснащён, то владелец может её поставить дополнительно за \\$1500. Если у владельца Tesla есть пакет Enhanced Autopilot, то подписка на FSD будет стоить \\$99 в месяц.\n\nКраткое содержание:\nTesla продала 285 тыс. клиентам в Северной Америке доступ к бета-версии автопилота Full Self-Driving (FSD) за всю историю проекта. Эксперты считают, что Tesla фактически в рамках внутреннего тестирования своей системы получила с клиентов более \\$2 млрд за возможность активировать FSD. Tesla предупреждает, что руки водителя при использовании FSD должны быть на руле.\n\nВ конце декабря штат Калифорния принял закон, который запрещает американской компании Tesla называть программное обеспечение для электромобилей Full Self-Driving (FSD). Вступающее в силу со следующего года положение запретит Tesla называть автопилот FSD, поскольку компания вводит в заблуждение владельцев и покупателей электромобилей, утверждают законодатели.\n\nTesla открыла доступ к бета-версии автопилота Full Self-Driving (FSD) в Северной Америке для всех желающих купить эту опцию или сделал это ранее, но ждал своей очереди. Теперь эти ограничения сняты, а компания уже установила FSD более чем на 160 тыс. электромобилей.\n\n5 сентября Tesla увеличила стоимость доступа к бета-версии системы автопилота Full Self-Driving ( FSD ) до \\$15 тысяч. Основатель Tesla Илон Маск ранее заявлял, что стоимость доступа к FSD все ещё «смехотворно дешёвая». Эксперты считают, что до сих пор эта функция не оправдала своего названия, а компания фактически собирает оплату с водителей за тестирование и доработку сервиса. FSD находится в стадии бета-тестирования уже два года.","summary":"Tesla продала 285 тыс. клиентам в Северной Америке доступ к бета-версии автопилота Full Self-Driving (FSD) за всю историю проекта. Эксперты считают, что Tesla фактически в рамках внутреннего тестирования своей системы получила с клиентов более \\$2 млрд за возможность активировать FSD. Tesla предупреждает, что руки водителя при использовании FSD должны быть на руле.\n\nВ конце декабря штат Калифорния принял закон, который запрещает американской компании Tesla называть программное обеспечение для электромобилей Full Self-Driving (FSD). Вступающее в силу со следующего года положение запретит Tesla называть автопилот FSD, поскольку компания вводит в заблуждение владельцев и покупателей электромобилей, утверждают законодатели.\n\nTesla открыла доступ к бета-версии автопилота Full Self-Driving (FSD) в Северной Америке для всех желающих купить эту опцию или сделал это ранее, но ждал своей очереди. Теперь эти ограничения сняты, а компания уже установила FSD более чем на 160 тыс. электромобилей.\n\n5 сентября Tesla увеличила стоимость доступа к бета-версии системы автопилота Full Self-Driving ( FSD ) до \\$15 тысяч. Основатель Tesla Илон Маск ранее заявлял, что стоимость доступа к FSD все ещё «смехотворно дешёвая». Эксперты считают, что до сих пор эта функция не оправдала своего названия, а компания фактически собирает оплату с водителей за тестирование и доработку сервиса. FSD находится в стадии бета-тестирования уже два года."}
{"Text":"Страйкбол: Немного обо всём / Хабр                                                  \n\n30-08-2009\nНаверное, уже многие наслышаны о такой игре, как страйкбол (StrikeBall, AirSoft). Тем более в свете последних событий, связанных с выходом компьютерной игры S.T.A.L.K.E.R., и проводимыми по ее мотивам так называемыми СталкерСтрайками. Итак, в течение нескольких топиков я хотел бы немного рассказать об истории страйкбола, о его появлении в России, на Урале, ну и, конечно же, о самом СталкерСтрайке. Итак, пожалуй, начнем пол порядку. КРАТКАЯ ИСТОРИЯ После окончания второй мировой войны Япония, как страна побежденных, не имела права иметь собственные вооруженные войска и проводить военные учения с применением настоящего оружия. В конце 80-х, начале 90-х японцы создали полные копии боевого оружия, принцип действия которых основан на пневматической стрельбе пластиковыми шарами калибра 6 мм. Большинство из данного вооружения соответствовало своим реальным прототипам как по размерам, так и по массе. Причем оружие подразделяется по принципу действия на три категории: механическое, газобаллонное и электропневматическое. СТРАЙКБОЛ, КАК ОН ЕСТЬ Страйкбол – командная игра, в которой стрельба ведется из массогабаритных аналогов реального оружия, изготовленного из пластика либо из металла, пластиковыми шариками калибра 6 мм. Игра представляет собой имитацию боевых действий, максимально приближенную к реальности. Игрок — настоящий солдат, ни больше ни меньше, в полной экипировке, за исключением оружия. Игры могут проходить совершенно в любой обстановке: лес, грязь, слякоть, дождь либо город с полуразрушенными постройками, чистое поле и т.д. Обычно, игроки объединяются в команды, которые вместе ездят на выездные соревнования, тренируются и т.д. Некоторые команды стараются отыгрывать реально существовавшие боевые подразделения вплоть до мельчайших подробностей: пиковые тузы на касках времен Вьетнамской войны и т.д. Естественно, в играх присутствуют некоторые правила. Страйкбол не является исключением. Свод правил можно прочитать на любом страйкбольном портале (ссылки приведу в конце статьи). ЭКИПИРОВКА Экипировка игрока может быть различной, благо вариантов – вагон и маленькая тележка. Никто не запрещает носить бронежилеты, каски, ПНВ и другие примочки. Главное, чтобы это не мешало в самой игре. Смотрится это все, конечно, впечатляюще, но носить это не всегда удобно. Если игрок состоит в команде, которая старается отыгрывать те или иные войска, то ему придется находить себе соответствующий эквип, ибо довольно нелепо будет смотреться в рядах морских пехотинцев, человек в кирзовых сапогах и бушлате, с АК74 наперевес. Вот так, небольшой, но минус. Но самыми главными элементами экипировки являются, в первую очередь, защитные очки либо «штурмовая» маска, ну и другие элементы защиты: налокотники, наколенники, каска, перчатки. Без них – никуда. Сам я предпочитаю бегать налегке: форма, балаклава, «штурмовая» маска, разгрузочный жилет, ботинки, налокотники. Просто и со вкусом. ВООРУЖЕНИЕ Все вооружение в страйкболе можно разделить на 3 категории: механика, газовое, электромеханика, если не считать гранат, различного рода минометов и гранатометов. Механическое оружие Наиболее многочисленная категория. Включает огромное количество моделей пистолетов, пистолетов — пулемётов и штурмовых винтовок. Выстрел производится путём спуска с боевого взвода подпружиненного поршня, перемещающегося внутри пневматической камеры. Перед каждым выстрелом поршень требуется взводить мускульной силой стрелка, передёргивая при этом затвор, или отводя назад рукоятку заряжания (для моделей автоматов). Основной материал корпуса — ударопрочная пластмасса. Пневматическая камера, поршень, спусковой механизм — полиэтилен высокого давления. Ствол — латунный, канал ствола полированный. Прицельная дальность стрельбы составляет 10 — 15 м для пистолетов и 20 -25 м для автоматов. Скорострельность — 1 прицельный выстрел в секунду. Думаю, все когда-то играли или видели китайские пистолетики и автоматы. Вот, примерно, об этом и идет речь. Газобаллонное оружие Копируются, как правило курковые модели, которые приводятся в действие энергией сжатого газа, закачиваемого через клапан в рукоятку. Принцип действия схож с работой газобаллонного пистолета под стальные шарики калибра 4.5 мм. При отводе назад затворной рамы вместе с ней отходит подаватель, который при обратном ходе досылает в патронник очередной шар и удерживает его там, надёжно запирая канал ствола резиновой прокладкой. Задним срезом затвора взводится курок и шепталом удерживается в этом положении. При спуске курок своим бойком бьёт по клапану заполненного газом дозатора, энергично открывая его на непродолжительное время. Расширяясь, газ выталкивает из патронника шарик, который закручивается вверх, благодаря специальной резиновой прокладке особой формы, расположенной в начале ствольного канала. Эта система закрутки шарика, которой, кстати, оборудованы и некоторые модели «механической» серии получила непереводимое название «Хоп-ап». Она позволяет шарику достигнуть более устойчивой траектории, повышая при этом прицельную дальность, которая составляет уже более 20 метров. Также, стрельба из газобалонного оружия может происходить и самовзводом, что значительно повышает скорострельность, правда, в ущерб точности. Электромеханическое оружие Наиболее интересная разновидность 6 мм страйкбольного оружия — модели пистолетов-пулемётов и автоматов с электрическим приводом на воздушный компрессор. Это устройство позволяет вести автоматический огонь со скоростью порядка 700 — 800 выстрелов в минуту на максимальное расстояние около 80 метров. Все автоматы этой категории оснащены регулируемой системой «Хоп-ап», т.е. по своему желанию стрелок может изменять силу закручивания шарика, достигая наибольшей дальности при наивысшей кучности. По исполнению «электропривода» выполнены значительно интересней предыдущих категорий за счёт максимального оснащения металлическими деталями. Приводятся в действие такие автоматы высокоточными электромоторами напряжением 8.4В, питаемых от никель-кадмиевых аккумуляторов ёмкостью 600 или 1300 миллиампер/час. Одной зарядки аккумулятора 1300 ма/ч хватает примерно на 3500 — 4000 выстрелов, 600 ма/ч втрое меньше. ДОСТОИНСТВА СТРАЙКБОЛА ПО СРАВНЕНИЮ С ДРУГИМИ ВОЕННЫМИ ИГРАМИ 1. Реальность оружия, (имеется в виду практически полное соответствие страйкбольного оружия, с действительным оружием). 2. Малое травматическое воздействие на игрока. В отличие от пейнтбола, где игрок должен быть защищен достаточно громоздкими доспехами, которые снижают подвижность, зрение и слух, игрок в страйкболе может ограничиться защитными приспособлениями для глаз, будь то легкие пластиковые очки или сетка-накомарник. 3. Неограниченный район для игры, которым может быть любой лес, поле, площадки для игр или городские условия (здания, стройки). 4. Относительная дешевизна и легкость боекомплекта (по сравнению с пейнтболом). 5. Принцип действия моделей позволяет вести огонь из любого положения оружия. 6. Минимальное воздействие на страйкбольное оружие климатических условий. ССЫЛКИ НА МАТЕРИАЛЫ 1. Страйкбол. www.airsoftgun.ru 2. Страйкбол в Перми. www.airsoft59.ru P.S. В следующей статье хотелось бы поподробней рассказать о страйкболе в России и на Урале в частности. Если читающим интересно, то точно напишу =)\n","metric":0.6640625,"metric_input":"Текст статьи:\nСтрайкбол: Немного обо всём / Хабр                                                  \n\n30-08-2009\nНаверное, уже многие наслышаны о такой игре, как страйкбол (StrikeBall, AirSoft). Тем более в свете последних событий, связанных с выходом компьютерной игры S.T.A.L.K.E.R., и проводимыми по ее мотивам так называемыми СталкерСтрайками. Итак, в течение нескольких топиков я хотел бы немного рассказать об истории страйкбола, о его появлении в России, на Урале, ну и, конечно же, о самом СталкерСтрайке. Итак, пожалуй, начнем пол порядку. КРАТКАЯ ИСТОРИЯ После окончания второй мировой войны Япония, как страна побежденных, не имела права иметь собственные вооруженные войска и проводить военные учения с применением настоящего оружия. В конце 80-х, начале 90-х японцы создали полные копии боевого оружия, принцип действия которых основан на пневматической стрельбе пластиковыми шарами калибра 6 мм. Большинство из данного вооружения соответствовало своим реальным прототипам как по размерам, так и по массе. Причем оружие подразделяется по принципу действия на три категории: механическое, газобаллонное и электропневматическое. СТРАЙКБОЛ, КАК ОН ЕСТЬ Страйкбол – командная игра, в которой стрельба ведется из массогабаритных аналогов реального оружия, изготовленного из пластика либо из металла, пластиковыми шариками калибра 6 мм. Игра представляет собой имитацию боевых действий, максимально приближенную к реальности. Игрок — настоящий солдат, ни больше ни меньше, в полной экипировке, за исключением оружия. Игры могут проходить совершенно в любой обстановке: лес, грязь, слякоть, дождь либо город с полуразрушенными постройками, чистое поле и т.д. Обычно, игроки объединяются в команды, которые вместе ездят на выездные соревнования, тренируются и т.д. Некоторые команды стараются отыгрывать реально существовавшие боевые подразделения вплоть до мельчайших подробностей: пиковые тузы на касках времен Вьетнамской войны и т.д. Естественно, в играх присутствуют некоторые правила. Страйкбол не является исключением. Свод правил можно прочитать на любом страйкбольном портале (ссылки приведу в конце статьи). ЭКИПИРОВКА Экипировка игрока может быть различной, благо вариантов – вагон и маленькая тележка. Никто не запрещает носить бронежилеты, каски, ПНВ и другие примочки. Главное, чтобы это не мешало в самой игре. Смотрится это все, конечно, впечатляюще, но носить это не всегда удобно. Если игрок состоит в команде, которая старается отыгрывать те или иные войска, то ему придется находить себе соответствующий эквип, ибо довольно нелепо будет смотреться в рядах морских пехотинцев, человек в кирзовых сапогах и бушлате, с АК74 наперевес. Вот так, небольшой, но минус. Но самыми главными элементами экипировки являются, в первую очередь, защитные очки либо «штурмовая» маска, ну и другие элементы защиты: налокотники, наколенники, каска, перчатки. Без них – никуда. Сам я предпочитаю бегать налегке: форма, балаклава, «штурмовая» маска, разгрузочный жилет, ботинки, налокотники. Просто и со вкусом. ВООРУЖЕНИЕ Все вооружение в страйкболе можно разделить на 3 категории: механика, газовое, электромеханика, если не считать гранат, различного рода минометов и гранатометов. Механическое оружие Наиболее многочисленная категория. Включает огромное количество моделей пистолетов, пистолетов — пулемётов и штурмовых винтовок. Выстрел производится путём спуска с боевого взвода подпружиненного поршня, перемещающегося внутри пневматической камеры. Перед каждым выстрелом поршень требуется взводить мускульной силой стрелка, передёргивая при этом затвор, или отводя назад рукоятку заряжания (для моделей автоматов). Основной материал корпуса — ударопрочная пластмасса. Пневматическая камера, поршень, спусковой механизм — полиэтилен высокого давления. Ствол — латунный, канал ствола полированный. Прицельная дальность стрельбы составляет 10 — 15 м для пистолетов и 20 -25 м для автоматов. Скорострельность — 1 прицельный выстрел в секунду. Думаю, все когда-то играли или видели китайские пистолетики и автоматы. Вот, примерно, об этом и идет речь. Газобаллонное оружие Копируются, как правило курковые модели, которые приводятся в действие энергией сжатого газа, закачиваемого через клапан в рукоятку. Принцип действия схож с работой газобаллонного пистолета под стальные шарики калибра 4.5 мм. При отводе назад затворной рамы вместе с ней отходит подаватель, который при обратном ходе досылает в патронник очередной шар и удерживает его там, надёжно запирая канал ствола резиновой прокладкой. Задним срезом затвора взводится курок и шепталом удерживается в этом положении. При спуске курок своим бойком бьёт по клапану заполненного газом дозатора, энергично открывая его на непродолжительное время. Расширяясь, газ выталкивает из патронника шарик, который закручивается вверх, благодаря специальной резиновой прокладке особой формы, расположенной в начале ствольного канала. Эта система закрутки шарика, которой, кстати, оборудованы и некоторые модели «механической» серии получила непереводимое название «Хоп-ап». Она позволяет шарику достигнуть более устойчивой траектории, повышая при этом прицельную дальность, которая составляет уже более 20 метров. Также, стрельба из газобалонного оружия может происходить и самовзводом, что значительно повышает скорострельность, правда, в ущерб точности. Электромеханическое оружие Наиболее интересная разновидность 6 мм страйкбольного оружия — модели пистолетов-пулемётов и автоматов с электрическим приводом на воздушный компрессор. Это устройство позволяет вести автоматический огонь со скоростью порядка 700 — 800 выстрелов в минуту на максимальное расстояние около 80 метров. Все автоматы этой категории оснащены регулируемой системой «Хоп-ап», т.е. по своему желанию стрелок может изменять силу закручивания шарика, достигая наибольшей дальности при наивысшей кучности. По исполнению «электропривода» выполнены значительно интересней предыдущих категорий за счёт максимального оснащения металлическими деталями. Приводятся в действие такие автоматы высокоточными электромоторами напряжением 8.4В, питаемых от никель-кадмиевых аккумуляторов ёмкостью 600 или 1300 миллиампер/час. Одной зарядки аккумулятора 1300 ма/ч хватает примерно на 3500 — 4000 выстрелов, 600 ма/ч втрое меньше. ДОСТОИНСТВА СТРАЙКБОЛА ПО СРАВНЕНИЮ С ДРУГИМИ ВОЕННЫМИ ИГРАМИ 1. Реальность оружия, (имеется в виду практически полное соответствие страйкбольного оружия, с действительным оружием). 2. Малое травматическое воздействие на игрока. В отличие от пейнтбола, где игрок должен быть защищен достаточно громоздкими доспехами, которые снижают подвижность, зрение и слух, игрок в страйкболе может ограничиться защитными приспособлениями для глаз, будь то легкие пластиковые очки или сетка-накомарник. 3. Неограниченный район для игры, которым может быть любой лес, поле, площадки для игр или городские условия (здания, стройки). 4. Относительная дешевизна и легкость боекомплекта (по сравнению с пейнтболом). 5. Принцип действия моделей позволяет вести огонь из любого положения оружия. 6. Минимальное воздействие на страйкбольное оружие климатических условий. ССЫЛКИ НА МАТЕРИАЛЫ 1. Страйкбол. www.airsoftgun.ru 2. Страйкбол в Перми. www.airsoft59.ru P.S. В следующей статье хотелось бы поподробней рассказать о страйкболе в России и на Урале в частности. Если читающим интересно, то точно напишу =)\n\n\nКраткое содержание:\nСтрайкбол - это командная игра, в которой игроки используют массогабаритные аналоги реального оружия, стреляя пластиковыми шариками калибра 6 мм. Игра имитирует боевые действия, максимально приближенные к реальности. Экипировка игроков может быть различной, но защитные очки и элементы защиты обязательны. Оружие в страйкболе делится на механическое, газовое и электромеханическое. Страйкбол отличается от других военных игр своей реалистичностью, малым травматическим воздействием на игрока, возможностью игры в любом месте и относительной дешевизной боекомплекта.","summary":"Страйкбол - это командная игра, в которой игроки используют массогабаритные аналоги реального оружия, стреляя пластиковыми шариками калибра 6 мм. Игра имитирует боевые действия, максимально приближенные к реальности. Экипировка игроков может быть различной, но защитные очки и элементы защиты обязательны. Оружие в страйкболе делится на механическое, газовое и электромеханическое. Страйкбол отличается от других военных игр своей реалистичностью, малым травматическим воздействием на игрока, возможностью игры в любом месте и относительной дешевизной боекомплекта."}
{"Text":"Выбор хостинга: скорость и надёжность / Habr           \n\n07-12-2016\nИндустрия хостинга существует в России более 20 лет. Казалось бы, достаточно времени, чтобы рынок стабилизировался, появились четкие лидеры и простые критерии выбора для заказчиков. Однако, при решении задачи выбора хостинга до сих пор приходится опираться на собственный опыт, рекомендации и «метод тыка». В этой статье мы рассмотрим методичный подход к выбору провайдера хостинга с позиций качественной работы веб-проекта как с точки зрения скорости, так и надёжности. Уточню, что речь пойдёт только о выделенных серверах или VPS (или их облачных аналогах), полностью виртуальный (shared) хостинг оставляем за скобками как неприемлемый вариант. 1. Компания, какой она должна быть? При выборе компании, предоставляющей услуги хостинга, нужно смотреть на несколько критериев. Специализация: желательно, чтобы компания занималась именно хостингом и/или управлением дата-центрами. Наличие специализации даёт два преимущества: более высокие компетенции и ориентированность на результат. Например, часто можно встретить ситуацию, когда телеком-компания предоставляет в аренду свои дата-центры, при этом такая услуга не является ключевой для бизнеса и получает соответствующее внимание со стороны менеджмента. Прямой поставщик: лучше работать напрямую с компанией, управляющей дата-центром. Только в этом случае возможно наиболее оперативное решение проблем (меньше звеньев в цепи реагирования на инцидент). Опыт: любые ИТ-услуги предполагают динамичное развитие рынка, поэтому не стоит бояться новых игроков. Однако, стоит обходить стороной компании, созданные менее двух лет назад: в лучшем случае столкнётесь с «детскими» проблемами, в худшем – компания пропадёт с рынка (живы примеры Оверсан, Макхост и некоторых других). Масштаб: оптимальный размер компании от малого до среднего масштаба. Стоит избегать микро-компаний, так как у них может не оказаться нужного персонала, резервных ресурсов и скорее всего они будут экономить на качестве услуг. Крупные компании будут сложнее для взаимодействия: в случае потребности индивидуального решения и в общем менее чувствительны к проблемам клиентов. Открытость: дополнительным плюсом будет активное участие компании в коммуникациях с клиентами. Эта активность может принимать формы ведения блогов, опросов, участия в профессиональных конференциях и других мероприятиях. Как правило, это признак желания повышать качество услуг и готовности вести диалог с клиентами. География: важно, где расположены дата-центры компании – данные должны быть как можно ближе к пользователям. В случае, если требуется распределённая инфраструктура от одного поставщика, компания должна обладать сетью дата-центров. 2. Дата-центр Именно дата-центр имеет ключевое значение на качество работы вашего проекта. Никакая организация или техническая поддержка не сможет компенсировать слабую связность или сбои по питанию в ДЦ. Для оценки критериев выбора дата-центра требуется, чтобы компания раскрывала данные о своём ДЦ. В случае, если таких данных нет, стоит задаться вопросом: почему эта информация скрыта? Связность: одна из важнейших характеристик. Определяется количеством и качеством интернет-каналов в дата-центре, а также политикой обмена трафиком (пирингом). Важно, чтобы дата-центр располагался как можно ближе к вашим пользователям (посетителям сайта). Проверить задержки в сети до ДЦ можно простой командой ping, которую нужно выполнять из различных сетей в регионе целевой аудитории. Для более глубокого анализа связности можно использовать глобальный граф связности, который представлен на сервисе radar.qrator.net (Radar by Qrator). Этот граф показывает политики маршрутизации между всеми автономными системами (AS) и Интернете. Здесь стоит посмотреть сам граф (раздел Graph), провайдеров, пиринг. Также можно посмотреть крупных клиентов хостера (тех, у которых есть собственная AS). Стоит опасаться компаний, у которых граф связности упирается в единственный узел – это значит, что хостер подключен через чужую сеть и полностью зависит от её работы. Дата запуска: предпочтение стоит отдавать новым дата-центрам, так как сейчас применяются более продуманные проекты, выделяются большие мощности, закладывается большая пропускная способность. При этом, нужно помнить о возможности «детских» проблем у дата-центров, запущенных «вчера». Для старых дата-центров также возможна ситуация замены/обслуживания инфраструктурного оборудования, которая часто приводит к перерывам в обслуживании. Сертификация: по идее уровень сертификации (Tier II,III,IV) должен показывать надёжность дата-центра. В целом идея правильная, наличие такой сертификации (Tier II, лучше Tier III) можно рассматривать как преимущество. Однако, нужно помнить, что как правило, сертифицируется только проект дата-центра. Совсем не факт, что проект будет в полной мере реализован. Кроме того, важнейший аспект – обслуживание и эксплуатация – как правило не сертифицируется в России. Электропитание: подключаемой мощности должно быть достаточно для ваших потребностей (в случае аренды стойки). Резервирование должно быть организовано либо в классической схеме: электрические ИБП + дизель-генератор, либо в виде динамического ИБП. Желательно питание ДЦ от двух независимых лучей. Реально надёжность питания можно проверить только на собственном опыте (бывали случаи отключения ДЦ из-за ИБП). Охлаждение и вентиляция: существуют различные стандарты по температуре в машинном зале ДЦ. Текущий тренд в целях энергоэффективности повышать рабочую температуру воздуха, поэтому стоит уточнить потребности вашего оборудования по температурному режиму. Важно, чтобы система кондиционирования имела резерв для обслуживания и запас по мощности. Кроме того, важно, чтобы воздух в машинном зале проходил фильтрацию от пыли, иначе оборудование забивается грязью очень быстро. Дополнительная отказоустойчивость: для критичных приложений могут потребоваться дополнительные меры повышения надёжности – подключение сервера по двум лучам электропитания, несколько независимых сетевых линков или организация отказоустойчивости между ДЦ (например, VRRP). Доступность и стоимость каналов: при оценке стоимости размещения нужно точно выяснить условия подключения. Во-первых, канал может быть с гарантированной или негарантированной полосой. Во-вторых, может накладываться ограничение на количество трафика или соотношения входящий/исходящий. Также стоит заранее выяснить возможность и стоимость расширения каналов (стандарт это 100 Мбит/с). Процессы и безопасность: при решении о размещении оборудования в дата-центре полезно заранее его навестить и посмотреть на систему безопасности, а также организацию работы с оборудованием. Вход в машинный зал должен быть строго регламентирован и ограничен. В зоне обслуживания оборудования – порядок и чистота. Если при визите в ДЦ вы обнаружили бардак и отсутствие организации, не стоит ожидать высокой доступности. 3. Услуги Собственно, за услугами мы и приходим к провайдеру хостнига. На что стоит обратить внимание? Специализация: посмотрите на спектр предлагаемых услуг, выделите основное направление и сравните со своими задачами. Если флагманские услуги, продвигаемые компанией как раз вам и нужны – отлично. И наоборот, если вы хотите разместить свой сервер у компании, которая в основном занимается VPS – ждите проблем. Запас на вырост: потребности вашего проекта сейчас и через могут сильно различаться. Есть ли в линейке услуг запас для роста вашего проекта? Если есть, каким образом организована миграция с одной услуги на другую (автоматизированно, с помощью хостера, самостоятельно)? Противодействие DDoS: для любого коммерческого проекта актуален риск DDoS-атак. Что может предложить провайдер, есть ли готовое решение в рамках ДЦ или облака? Как конкретно организована защита, какие тарифы и сроки подключения? В случае отсутствия защиты на стороне ДЦ, нужно выяснить возможность быстрой смены IP-адреса и доступа к машине по запасному каналу связи. 4. Поддержка и SLA Качество поддержки сложно оценить заранее, но можно выделить несколько признаков. Режим работы поддержки: должен быть действительно круглосуточным. Косвенно это можно проверить по времени реакции на запросы ночью и в выходные дни. Соглашение об уровне сервиса (SLA): это полезная вещь, но не стоит полностью на него уповать. Дело в том, что компенсация за простой сервиса как правило ограничена стоимостью услуги за период, то есть с финансовой точки зрения никак не покрывает реальные потери бизнеса. С другой стороны, само наличие SLA дисциплинирует компанию и показывает системный подход к качеству услуг. Учет заявок: лучше, чтобы работа с заявками была реализована в виде веб-панели с дублированием на e-mail. История переписки должна сохраняться. Системы, работающие только на почте недостаточно надёжные, к тому же ваша контактная почта может находится на проблемном сервере. Итоги Приведённые выше критерии основаны на нашем опыте взаимодействия с хостинг-провайдерами. Надеюсь, этот опыт будет полезен. Если у вас есть свои наработки по выбору хостинг-провайдера, поделитесь ими в комментариях.\n","metric":0.69677734380000000414,"metric_input":"Текст статьи:\nВыбор хостинга: скорость и надёжность / Habr           \n\n07-12-2016\nИндустрия хостинга существует в России более 20 лет. Казалось бы, достаточно времени, чтобы рынок стабилизировался, появились четкие лидеры и простые критерии выбора для заказчиков. Однако, при решении задачи выбора хостинга до сих пор приходится опираться на собственный опыт, рекомендации и «метод тыка». В этой статье мы рассмотрим методичный подход к выбору провайдера хостинга с позиций качественной работы веб-проекта как с точки зрения скорости, так и надёжности. Уточню, что речь пойдёт только о выделенных серверах или VPS (или их облачных аналогах), полностью виртуальный (shared) хостинг оставляем за скобками как неприемлемый вариант. 1. Компания, какой она должна быть? При выборе компании, предоставляющей услуги хостинга, нужно смотреть на несколько критериев. Специализация: желательно, чтобы компания занималась именно хостингом и/или управлением дата-центрами. Наличие специализации даёт два преимущества: более высокие компетенции и ориентированность на результат. Например, часто можно встретить ситуацию, когда телеком-компания предоставляет в аренду свои дата-центры, при этом такая услуга не является ключевой для бизнеса и получает соответствующее внимание со стороны менеджмента. Прямой поставщик: лучше работать напрямую с компанией, управляющей дата-центром. Только в этом случае возможно наиболее оперативное решение проблем (меньше звеньев в цепи реагирования на инцидент). Опыт: любые ИТ-услуги предполагают динамичное развитие рынка, поэтому не стоит бояться новых игроков. Однако, стоит обходить стороной компании, созданные менее двух лет назад: в лучшем случае столкнётесь с «детскими» проблемами, в худшем – компания пропадёт с рынка (живы примеры Оверсан, Макхост и некоторых других). Масштаб: оптимальный размер компании от малого до среднего масштаба. Стоит избегать микро-компаний, так как у них может не оказаться нужного персонала, резервных ресурсов и скорее всего они будут экономить на качестве услуг. Крупные компании будут сложнее для взаимодействия: в случае потребности индивидуального решения и в общем менее чувствительны к проблемам клиентов. Открытость: дополнительным плюсом будет активное участие компании в коммуникациях с клиентами. Эта активность может принимать формы ведения блогов, опросов, участия в профессиональных конференциях и других мероприятиях. Как правило, это признак желания повышать качество услуг и готовности вести диалог с клиентами. География: важно, где расположены дата-центры компании – данные должны быть как можно ближе к пользователям. В случае, если требуется распределённая инфраструктура от одного поставщика, компания должна обладать сетью дата-центров. 2. Дата-центр Именно дата-центр имеет ключевое значение на качество работы вашего проекта. Никакая организация или техническая поддержка не сможет компенсировать слабую связность или сбои по питанию в ДЦ. Для оценки критериев выбора дата-центра требуется, чтобы компания раскрывала данные о своём ДЦ. В случае, если таких данных нет, стоит задаться вопросом: почему эта информация скрыта? Связность: одна из важнейших характеристик. Определяется количеством и качеством интернет-каналов в дата-центре, а также политикой обмена трафиком (пирингом). Важно, чтобы дата-центр располагался как можно ближе к вашим пользователям (посетителям сайта). Проверить задержки в сети до ДЦ можно простой командой ping, которую нужно выполнять из различных сетей в регионе целевой аудитории. Для более глубокого анализа связности можно использовать глобальный граф связности, который представлен на сервисе radar.qrator.net (Radar by Qrator). Этот граф показывает политики маршрутизации между всеми автономными системами (AS) и Интернете. Здесь стоит посмотреть сам граф (раздел Graph), провайдеров, пиринг. Также можно посмотреть крупных клиентов хостера (тех, у которых есть собственная AS). Стоит опасаться компаний, у которых граф связности упирается в единственный узел – это значит, что хостер подключен через чужую сеть и полностью зависит от её работы. Дата запуска: предпочтение стоит отдавать новым дата-центрам, так как сейчас применяются более продуманные проекты, выделяются большие мощности, закладывается большая пропускная способность. При этом, нужно помнить о возможности «детских» проблем у дата-центров, запущенных «вчера». Для старых дата-центров также возможна ситуация замены/обслуживания инфраструктурного оборудования, которая часто приводит к перерывам в обслуживании. Сертификация: по идее уровень сертификации (Tier II,III,IV) должен показывать надёжность дата-центра. В целом идея правильная, наличие такой сертификации (Tier II, лучше Tier III) можно рассматривать как преимущество. Однако, нужно помнить, что как правило, сертифицируется только проект дата-центра. Совсем не факт, что проект будет в полной мере реализован. Кроме того, важнейший аспект – обслуживание и эксплуатация – как правило не сертифицируется в России. Электропитание: подключаемой мощности должно быть достаточно для ваших потребностей (в случае аренды стойки). Резервирование должно быть организовано либо в классической схеме: электрические ИБП + дизель-генератор, либо в виде динамического ИБП. Желательно питание ДЦ от двух независимых лучей. Реально надёжность питания можно проверить только на собственном опыте (бывали случаи отключения ДЦ из-за ИБП). Охлаждение и вентиляция: существуют различные стандарты по температуре в машинном зале ДЦ. Текущий тренд в целях энергоэффективности повышать рабочую температуру воздуха, поэтому стоит уточнить потребности вашего оборудования по температурному режиму. Важно, чтобы система кондиционирования имела резерв для обслуживания и запас по мощности. Кроме того, важно, чтобы воздух в машинном зале проходил фильтрацию от пыли, иначе оборудование забивается грязью очень быстро. Дополнительная отказоустойчивость: для критичных приложений могут потребоваться дополнительные меры повышения надёжности – подключение сервера по двум лучам электропитания, несколько независимых сетевых линков или организация отказоустойчивости между ДЦ (например, VRRP). Доступность и стоимость каналов: при оценке стоимости размещения нужно точно выяснить условия подключения. Во-первых, канал может быть с гарантированной или негарантированной полосой. Во-вторых, может накладываться ограничение на количество трафика или соотношения входящий/исходящий. Также стоит заранее выяснить возможность и стоимость расширения каналов (стандарт это 100 Мбит/с). Процессы и безопасность: при решении о размещении оборудования в дата-центре полезно заранее его навестить и посмотреть на систему безопасности, а также организацию работы с оборудованием. Вход в машинный зал должен быть строго регламентирован и ограничен. В зоне обслуживания оборудования – порядок и чистота. Если при визите в ДЦ вы обнаружили бардак и отсутствие организации, не стоит ожидать высокой доступности. 3. Услуги Собственно, за услугами мы и приходим к провайдеру хостнига. На что стоит обратить внимание? Специализация: посмотрите на спектр предлагаемых услуг, выделите основное направление и сравните со своими задачами. Если флагманские услуги, продвигаемые компанией как раз вам и нужны – отлично. И наоборот, если вы хотите разместить свой сервер у компании, которая в основном занимается VPS – ждите проблем. Запас на вырост: потребности вашего проекта сейчас и через могут сильно различаться. Есть ли в линейке услуг запас для роста вашего проекта? Если есть, каким образом организована миграция с одной услуги на другую (автоматизированно, с помощью хостера, самостоятельно)? Противодействие DDoS: для любого коммерческого проекта актуален риск DDoS-атак. Что может предложить провайдер, есть ли готовое решение в рамках ДЦ или облака? Как конкретно организована защита, какие тарифы и сроки подключения? В случае отсутствия защиты на стороне ДЦ, нужно выяснить возможность быстрой смены IP-адреса и доступа к машине по запасному каналу связи. 4. Поддержка и SLA Качество поддержки сложно оценить заранее, но можно выделить несколько признаков. Режим работы поддержки: должен быть действительно круглосуточным. Косвенно это можно проверить по времени реакции на запросы ночью и в выходные дни. Соглашение об уровне сервиса (SLA): это полезная вещь, но не стоит полностью на него уповать. Дело в том, что компенсация за простой сервиса как правило ограничена стоимостью услуги за период, то есть с финансовой точки зрения никак не покрывает реальные потери бизнеса. С другой стороны, само наличие SLA дисциплинирует компанию и показывает системный подход к качеству услуг. Учет заявок: лучше, чтобы работа с заявками была реализована в виде веб-панели с дублированием на e-mail. История переписки должна сохраняться. Системы, работающие только на почте недостаточно надёжные, к тому же ваша контактная почта может находится на проблемном сервере. Итоги Приведённые выше критерии основаны на нашем опыте взаимодействия с хостинг-провайдерами. Надеюсь, этот опыт будет полезен. Если у вас есть свои наработки по выбору хостинг-провайдера, поделитесь ими в комментариях.\n\n\nКраткое содержание:\nСтатья предлагает методичный подход к выбору провайдера хостинга с позиций качественной работы веб-проекта как с точки зрения скорости, так и надёжности. \n\nКлючевые моменты:\n\n1. При выборе компании, предоставляющей услуги хостинга, нужно смотреть на несколько критериев: специализация, опыт, масштаб, открытость, география.\n2. Дата-центр имеет ключевое значение на качество работы вашего проекта. Для оценки критериев выбора дата-центра требуется, чтобы компания раскрывала данные о своём ДЦ.\n3. При выборе услуг стоит обратить внимание на спектр предлагаемых услуг, наличие запаса на вырост, противодействие DDoS-атак, качество поддержки.\n4. В статье приведены критерии, основанные на опыте взаимодействия с хостинг-провайдерами.","summary":"Статья предлагает методичный подход к выбору провайдера хостинга с позиций качественной работы веб-проекта как с точки зрения скорости, так и надёжности. \n\nКлючевые моменты:\n\n1. При выборе компании, предоставляющей услуги хостинга, нужно смотреть на несколько критериев: специализация, опыт, масштаб, открытость, география.\n2. Дата-центр имеет ключевое значение на качество работы вашего проекта. Для оценки критериев выбора дата-центра требуется, чтобы компания раскрывала данные о своём ДЦ.\n3. При выборе услуг стоит обратить внимание на спектр предлагаемых услуг, наличие запаса на вырост, противодействие DDoS-атак, качество поддержки.\n4. В статье приведены критерии, основанные на опыте взаимодействия с хостинг-провайдерами."}
{"Text":"Project Blue: небольшой космический телескоп для поиска экзопланет в системе Альфа Центавра / Хабр                \n\n11-10-2016\nБлижайшая к Земле звездная система — Альфа Центавра. Она привлекает внимание астрономов всей нашей планеты уже долгое время. В этой системе не одна звезда, а сразу три: α Центавра А, α Центавра B и α Центавра C. Самая маленькая из трех — α Центавра C. Как оказалось, около нее есть потенциально обитаемая экзопланета. Есть экозопланеты и у двух других звезд, и за ними ученые очень хотят вести наблюдение. Возможно, на этих планетах тоже есть жизнь. Пока что доказать или опровергнуть это нельзя, поскольку наблюдать за экзопланетами для человека — чрезвычайно сложная задача. НАСА неоднократно заявляла о необходимости наблюдения за этим регионом. И сейчас, похоже, эти планы могут быть реализованы. Характеристики компонентов системы α Центавра α Центавра А α Центавра B Проксима Центавра Абсолютная звёздная величина 4,38 5,71 15,53 Спектральный класс G2V K1V M5,5Ve Светимость (в солнечных) 1,519 0,5 6·10−5 Диаметр (в солнечных) 1,227 0,865 0,14 Расстояние до Солнца, св. лет 4,36 4,22 Только не силами агентства, а группой ученых, которые собираются запустить на орбиту Земли небольшой оптический телескоп, позволяющий вести наблюдения за экопланетами ближайшей к нам звездной системы. Причем эти ученые говорят, что телескоп не нужно делать огромным, хватит и прибора относительно небольшого размера. Авторы идеи говорят о том, что такой телескоп не будет больше обычный стиральной машины. Проект по созданию такой системы будет довольно продолжительным, но уже к концу этого десятилетия экзопланеты окажутся в пределах «оптической досягаемости». Название нового проекта — Project Blue. Pale Blue Dot («бледно-синяя точка»). Источник: НАСА Это название он получил благодаря известному снимку Pale Blue Dot, полученному «Вояджером» в 1980 году. Цель нового проекта — получение похожих снимков для экзопланет из обитаемой зоны звезд α Центавра А и α Центавра B. «Мы не считаем возможным получить детальный снимок экзопланеты, но есть все шансы на то, что мы сможем получить фотографию, подобную Pale Blue Dot», — заявил Джон Морзе ( John Morse), один из участников и инициаторов Project Blue. В отличие от телескопа Кеплер, который отслеживает сотни тысяч звезд в поисках слабого изменения их светимости (что может быть признаком наличия экзопланеты у периодически изменяющей свою яркость звезды), новый телескоп будет предоставлять ученым высококонтрастные фотографии звездной системы. Телескоп новой конструкции будет включать и коронограф, который будет блокировать свет звезд, что позволит обнаруживать слабо светящиеся объекты на орбите этих звезд. Как и говорилось выше, речь идет о наблюдении лишь за звездной системой α Центавра. К сожалению, такой телескоп не сможет наблюдать за потенциально обитаемыми планетами на орбитах α Центавра С. Эта звезда гораздо холоднее своих «сестер», и за ее планетами с Земли наблюдать практически невозможно. Плюс ко всему, если экзопланета звезды находится очень близко к ней (а скорее всего, так и есть), то коронограф не поможет блокировать свет звезды для получения изображения планеты. Интересно, что ученые пока не имеют доказательств того, что у α Центавра А и α Центавра В есть планеты в обитаемой зоне. Возможно, такие планеты существуют, но в этом никто не уверен. Точно известно, что у α Центавра В есть планета, которая, впрочем, расположена очень близко к звезде. Так что, скорее всего, это горячий мир, находящийся вне обитаемой зоны своей звезды, на котором нет никакой жизни. Экзопланета была обнаружена телескопом Кеплер в 2012 году. Авторы «Project Blue» утверждают, что телескоп предлагаемой ими конструкции сможет обнаружить наличие экзопланет размером от 0,5 до 1,5 диаметров Земли, которые находятся в обитаемой зоне своих звезд. «Учитывая информацию, полученную с телескопа Кеплер, вероятность существования планеты в обитаемой зоне α Центавра А и α Центавра B — около 80%», — говорит Морзе. Предполагаемая продолжительность работы инструмента на орбите — 2 года. Не телескопом единым В апреле этого года на пресс-конференции в Нью-Йорке была озвучена идея проекта по отправке к Альфе Центавра большого количества зондов, которые должны заняться изучением этой звездной системы. Одна из задач — поиск и детальное изучение экзопланет. В рамках этого проекта планируется запуск корабля, который должен вывести на орбиту космические аппараты гораздо меньшего размера. С земли на солнечные паруса этих аппаратов будут воздействовать лазерным лучом. Лазером эти зонды будут разгоняться до большой скорости, которая позволяет достичь цели не за тысячи, а за десятки лет. Всего к Альфе Центавра планируется отправить 1000 миниатюрных зондов. Полученные в ходе путешествия данные такие аппараты будут передавать при помощи все того же лазера. В роли антенны будет выступать солнечный парус. Структура зонда проста — это камера, солнечный парус, лазерная система передачи данных и плутониевый элемент питания. У этого проекта есть ряд нерешенных проблем. Основная — это технические сложности. У человека пока нет всех технологий (или они не достигли нужного уровня развития) для реализации этого проекта. По мнению The Economist, реализация проекта станет возможной только после улучшения ряда современных технологий на несколько порядков. К числу других проблем относится необходимости защиты аппарата от встреченных в открытом космосе элементарных частиц и космической пыли.\n","metric":0.5859375,"metric_input":"Текст статьи:\nProject Blue: небольшой космический телескоп для поиска экзопланет в системе Альфа Центавра / Хабр                \n\n11-10-2016\nБлижайшая к Земле звездная система — Альфа Центавра. Она привлекает внимание астрономов всей нашей планеты уже долгое время. В этой системе не одна звезда, а сразу три: α Центавра А, α Центавра B и α Центавра C. Самая маленькая из трех — α Центавра C. Как оказалось, около нее есть потенциально обитаемая экзопланета. Есть экозопланеты и у двух других звезд, и за ними ученые очень хотят вести наблюдение. Возможно, на этих планетах тоже есть жизнь. Пока что доказать или опровергнуть это нельзя, поскольку наблюдать за экзопланетами для человека — чрезвычайно сложная задача. НАСА неоднократно заявляла о необходимости наблюдения за этим регионом. И сейчас, похоже, эти планы могут быть реализованы. Характеристики компонентов системы α Центавра α Центавра А α Центавра B Проксима Центавра Абсолютная звёздная величина 4,38 5,71 15,53 Спектральный класс G2V K1V M5,5Ve Светимость (в солнечных) 1,519 0,5 6·10−5 Диаметр (в солнечных) 1,227 0,865 0,14 Расстояние до Солнца, св. лет 4,36 4,22 Только не силами агентства, а группой ученых, которые собираются запустить на орбиту Земли небольшой оптический телескоп, позволяющий вести наблюдения за экопланетами ближайшей к нам звездной системы. Причем эти ученые говорят, что телескоп не нужно делать огромным, хватит и прибора относительно небольшого размера. Авторы идеи говорят о том, что такой телескоп не будет больше обычный стиральной машины. Проект по созданию такой системы будет довольно продолжительным, но уже к концу этого десятилетия экзопланеты окажутся в пределах «оптической досягаемости». Название нового проекта — Project Blue. Pale Blue Dot («бледно-синяя точка»). Источник: НАСА Это название он получил благодаря известному снимку Pale Blue Dot, полученному «Вояджером» в 1980 году. Цель нового проекта — получение похожих снимков для экзопланет из обитаемой зоны звезд α Центавра А и α Центавра B. «Мы не считаем возможным получить детальный снимок экзопланеты, но есть все шансы на то, что мы сможем получить фотографию, подобную Pale Blue Dot», — заявил Джон Морзе ( John Morse), один из участников и инициаторов Project Blue. В отличие от телескопа Кеплер, который отслеживает сотни тысяч звезд в поисках слабого изменения их светимости (что может быть признаком наличия экзопланеты у периодически изменяющей свою яркость звезды), новый телескоп будет предоставлять ученым высококонтрастные фотографии звездной системы. Телескоп новой конструкции будет включать и коронограф, который будет блокировать свет звезд, что позволит обнаруживать слабо светящиеся объекты на орбите этих звезд. Как и говорилось выше, речь идет о наблюдении лишь за звездной системой α Центавра. К сожалению, такой телескоп не сможет наблюдать за потенциально обитаемыми планетами на орбитах α Центавра С. Эта звезда гораздо холоднее своих «сестер», и за ее планетами с Земли наблюдать практически невозможно. Плюс ко всему, если экзопланета звезды находится очень близко к ней (а скорее всего, так и есть), то коронограф не поможет блокировать свет звезды для получения изображения планеты. Интересно, что ученые пока не имеют доказательств того, что у α Центавра А и α Центавра В есть планеты в обитаемой зоне. Возможно, такие планеты существуют, но в этом никто не уверен. Точно известно, что у α Центавра В есть планета, которая, впрочем, расположена очень близко к звезде. Так что, скорее всего, это горячий мир, находящийся вне обитаемой зоны своей звезды, на котором нет никакой жизни. Экзопланета была обнаружена телескопом Кеплер в 2012 году. Авторы «Project Blue» утверждают, что телескоп предлагаемой ими конструкции сможет обнаружить наличие экзопланет размером от 0,5 до 1,5 диаметров Земли, которые находятся в обитаемой зоне своих звезд. «Учитывая информацию, полученную с телескопа Кеплер, вероятность существования планеты в обитаемой зоне α Центавра А и α Центавра B — около 80%», — говорит Морзе. Предполагаемая продолжительность работы инструмента на орбите — 2 года. Не телескопом единым В апреле этого года на пресс-конференции в Нью-Йорке была озвучена идея проекта по отправке к Альфе Центавра большого количества зондов, которые должны заняться изучением этой звездной системы. Одна из задач — поиск и детальное изучение экзопланет. В рамках этого проекта планируется запуск корабля, который должен вывести на орбиту космические аппараты гораздо меньшего размера. С земли на солнечные паруса этих аппаратов будут воздействовать лазерным лучом. Лазером эти зонды будут разгоняться до большой скорости, которая позволяет достичь цели не за тысячи, а за десятки лет. Всего к Альфе Центавра планируется отправить 1000 миниатюрных зондов. Полученные в ходе путешествия данные такие аппараты будут передавать при помощи все того же лазера. В роли антенны будет выступать солнечный парус. Структура зонда проста — это камера, солнечный парус, лазерная система передачи данных и плутониевый элемент питания. У этого проекта есть ряд нерешенных проблем. Основная — это технические сложности. У человека пока нет всех технологий (или они не достигли нужного уровня развития) для реализации этого проекта. По мнению The Economist, реализация проекта станет возможной только после улучшения ряда современных технологий на несколько порядков. К числу других проблем относится необходимости защиты аппарата от встреченных в открытом космосе элементарных частиц и космической пыли.\n\n\nКраткое содержание:\nПроект Blue - это идея создания небольшого космического телескопа для наблюдения за экзопланетами в системе Альфа Центавра. Цель проекта - получение фотографий подобных знаменитой \"бледно-голубой точке\" с \"Вояджера\". Телескоп будет вести наблюдение за звездами Альфа Центавра А и В, но не сможет обнаружить планеты у звезды Альфа Центавра С. Ученые предполагают, что у этих звезд могут быть планеты в обитаемой зоне, но доказательств этому пока нет. Проект будет реализован к концу десятилетия.","summary":"Проект Blue - это идея создания небольшого космического телескопа для наблюдения за экзопланетами в системе Альфа Центавра. Цель проекта - получение фотографий подобных знаменитой \"бледно-голубой точке\" с \"Вояджера\". Телескоп будет вести наблюдение за звездами Альфа Центавра А и В, но не сможет обнаружить планеты у звезды Альфа Центавра С. Ученые предполагают, что у этих звезд могут быть планеты в обитаемой зоне, но доказательств этому пока нет. Проект будет реализован к концу десятилетия."}
{"Text":"История о двух миллиметрах / Habr              \n\n22-11-2012\nЭтот пост посвящен 7-миллиметровым жестким дискам форм-фактора 2,5 дюйма: именно такие модели совсем недавно (начиная с прошлого года) начали появляться в ноутбуках различных производителей. Изменение, по сравнению с накопителями стандартной высоты в 9,5 миллиметра, вроде бы небольшое, но у наиболее продвинутых пользователей оно вызвало массу эмоций. Прежде всего – из-за невозможности (забегая вперед скажу – временной) обеспечить в своем лаптопе хранилище объемом более 500 гигабайт. Действительно, а есть ли смысл? Ладно, если бы новые модели имели толщину три миллиметра и помещались даже в планшеты. А тут – какие-то два миллиметра. Поэтому в блоге компании HGST, крупнейшего производителя «семимиллиметровок», я хотел бы уделить таким винчестерам особое внимание. «Минус два миллиметра» — это и вправду не революционное изменение, но тонкие винчестеры появились на рынке как раз вовремя. Уже сейчас благодаря им в продаже становится больше моделей с гибридным хранилищем (SSD+HDD), обеспечивающим и скорость, и объем, да и просто – ноутбуков с достаточно быстрым и экономичным жестким диском. Такие модели стоят недорого (SSD хоть и подешевели, но все же дают ощутимую наценку), и не требуют от владельца таскать с собой внешний накопитель – на случай если захочется выбрать что-то из собственной видеотеки. Ожидая в комментариях неумеренный холивар «SSD против HDD», начну с небольшого экскурса в историю. Наш размер? Вся современная компьютерная история (уже после того, как для перевозки одного жесткого диска перестал требоваться грузовик) сопровождается накопителями двух основных форм-факторов – 3,5 дюйма (унаследованного от Floppy-дисков) для настольных компьютеров и 2,5 – для ноутбуков. Впрочем, были и исключения – например, жесткие диски формата 5,25 дюйма, появившиеся в начале 80-х и ненадолго возрожденные в середине 90-х. Для ноутбуков одно время рассматривался 3-дюймовый форм-фактор – компромисс между объемом и габаритами, который, правда, так и не взлетел. Жесткие диски размерностью 1,8 дюйма и менее исчезли совсем недавно – такую кроху и правда можно разместить где угодно, только вот маленькие пластины не обеспечивают ни скорость передачи данных, ни объем. Поэтому – для мобильных устройств стандартом был и остается 2,5-дюймовый жесткий диск. Впрочем, почему только для мобильных? Аналогичные винчестеры используются во внешних накопителях (3,5-дюймовые боксы все же мало соответствуют понятию «переносной жесткий диск»), в игровых приставках и в корпоративных системах хранения данных. А это значит, что в исследовательских центрах всех (ныне – немногочисленных :) производителей жестких дисков данному формату будет уделяться не меньшее, а может и большее внимание, чем стандартным 3,5-дюймовым моделям. Что предлагает HGST? Модель Travelstar Z7K500 в настоящий момент доступна в модификациях объемом 250, 320 и 500 гигабайт. Общие для всех версий характеристики следующие: Интерфейс: SATA 3 6 Гб/c Объем буфера данных: 32 МБ Скорость вращения шпинделя: 7200 оборотов в минуту Время поиска: 13 мс Количество циклов включения/выключения: 600 000 Энергопотребление в режиме чтения/записи: 1,8 Вт Энергопотребление в режиме ожидания: 1,7 Вт Полный список характеристик – в PDF на сайте HGST. Все модели имеют одну пластину для записи данных, и в настоящее время являются одними из наиболее быстрых «ноутбучных» дисков (более 130 мегабайт в секунду в тесте последовательного чтения-записи). Но гораздо важнее даже не скорость, а параметры энергопотребления. В режиме чтения-записи потребляемая мощность модели составляет всего 1,8 Ватта. 2 миллиметра – это много! Нет, конечно для монстров толщиной 8 сантиметров более тонкий жесткий диск погоды не сделает. А вот для ультрабуков – вполне. Согласно Яндекс.Маркету на момент подготовки текста было известно 90 моделей ультрабуков, доступных в продаже в России, с ценником от 18 до 40+ тысяч рублей. Толщина моделей – от 17 до 21 миллиметра. В таких моделях каждый миллиметр на счету, поэтому примерно в половине случаев используется только SSD объемом до 128 гигабайт (а минимальная цена поднимается до 25 тысяч рублей). В других случаях используется либо один жесткий диск, либо гибридное хранилище, предоставляющее и скорость, и объем, при сохранении минимальных размеров и веса. Среди таких моделей есть даже 15-дюймовые компьютеры весом в 1,8 килограмма. И всем, кто постоянно носит компьютер с собой, прекрасно известно, что эти параметры лаптопов неплохо бы еще уменьшить. Тонкие винчестеры – это одна из технологий, которая позволяет это сделать, не заставляя разбираться в том, как выключить в Windows гибернацию и файл подкачки (или же иначе ничего не поместится :). Что дальше? Тонкие жесткие диски предоставляют владельцам ультрабуков и просто компактных моделей возможность не ограничивать себя в объеме хранимых данных – в разумных пределах, но и без суровых ограничений типового SSD на 128 гигабайт. Конечно, сейчас можно купить ноутбук с SSD на 750 и более гигабайт, только вот цена у него будет соответствующая. Распространение моделей с гибридным хранилищем снимает и вопрос скорости работы. Более того, такие модели как Z7K500 обеспечивают и весьма быстрый перенос данных на жесткий диск и обратно, со скоростями более 120 мегабайт в секунду на «крупных» файлах. В результате, изменение высоты жесткого диска всего на 2 мм – на самом деле продлевает жизнь жестких дисков. Если кому-то кажется, что еще чуть-чуть и останутся только твердотельные накопители, то это не так :) Более того, даже 7 миллиметров – это еще не предел возможностей. Совсем недавно были анонсированы пятимиллиметровые жесткие диски объемом до 500 гигабайт. Конкуренция SSD и HDD продолжится, ну а нам остается только пользоваться ее плодами. Из всех доступных конфигураций для ноутбучной дисковой системы, лично для меня наиболее выгодной является 80-гигабайтный твердотельный накопитель и 500-гигабайтный, постоянно доступный «архив» со всеми нужными файлами. А для вас?\n","metric":0.51220703130000000414,"metric_input":"Текст статьи:\nИстория о двух миллиметрах / Habr              \n\n22-11-2012\nЭтот пост посвящен 7-миллиметровым жестким дискам форм-фактора 2,5 дюйма: именно такие модели совсем недавно (начиная с прошлого года) начали появляться в ноутбуках различных производителей. Изменение, по сравнению с накопителями стандартной высоты в 9,5 миллиметра, вроде бы небольшое, но у наиболее продвинутых пользователей оно вызвало массу эмоций. Прежде всего – из-за невозможности (забегая вперед скажу – временной) обеспечить в своем лаптопе хранилище объемом более 500 гигабайт. Действительно, а есть ли смысл? Ладно, если бы новые модели имели толщину три миллиметра и помещались даже в планшеты. А тут – какие-то два миллиметра. Поэтому в блоге компании HGST, крупнейшего производителя «семимиллиметровок», я хотел бы уделить таким винчестерам особое внимание. «Минус два миллиметра» — это и вправду не революционное изменение, но тонкие винчестеры появились на рынке как раз вовремя. Уже сейчас благодаря им в продаже становится больше моделей с гибридным хранилищем (SSD+HDD), обеспечивающим и скорость, и объем, да и просто – ноутбуков с достаточно быстрым и экономичным жестким диском. Такие модели стоят недорого (SSD хоть и подешевели, но все же дают ощутимую наценку), и не требуют от владельца таскать с собой внешний накопитель – на случай если захочется выбрать что-то из собственной видеотеки. Ожидая в комментариях неумеренный холивар «SSD против HDD», начну с небольшого экскурса в историю. Наш размер? Вся современная компьютерная история (уже после того, как для перевозки одного жесткого диска перестал требоваться грузовик) сопровождается накопителями двух основных форм-факторов – 3,5 дюйма (унаследованного от Floppy-дисков) для настольных компьютеров и 2,5 – для ноутбуков. Впрочем, были и исключения – например, жесткие диски формата 5,25 дюйма, появившиеся в начале 80-х и ненадолго возрожденные в середине 90-х. Для ноутбуков одно время рассматривался 3-дюймовый форм-фактор – компромисс между объемом и габаритами, который, правда, так и не взлетел. Жесткие диски размерностью 1,8 дюйма и менее исчезли совсем недавно – такую кроху и правда можно разместить где угодно, только вот маленькие пластины не обеспечивают ни скорость передачи данных, ни объем. Поэтому – для мобильных устройств стандартом был и остается 2,5-дюймовый жесткий диск. Впрочем, почему только для мобильных? Аналогичные винчестеры используются во внешних накопителях (3,5-дюймовые боксы все же мало соответствуют понятию «переносной жесткий диск»), в игровых приставках и в корпоративных системах хранения данных. А это значит, что в исследовательских центрах всех (ныне – немногочисленных :) производителей жестких дисков данному формату будет уделяться не меньшее, а может и большее внимание, чем стандартным 3,5-дюймовым моделям. Что предлагает HGST? Модель Travelstar Z7K500 в настоящий момент доступна в модификациях объемом 250, 320 и 500 гигабайт. Общие для всех версий характеристики следующие: Интерфейс: SATA 3 6 Гб/c Объем буфера данных: 32 МБ Скорость вращения шпинделя: 7200 оборотов в минуту Время поиска: 13 мс Количество циклов включения/выключения: 600 000 Энергопотребление в режиме чтения/записи: 1,8 Вт Энергопотребление в режиме ожидания: 1,7 Вт Полный список характеристик – в PDF на сайте HGST. Все модели имеют одну пластину для записи данных, и в настоящее время являются одними из наиболее быстрых «ноутбучных» дисков (более 130 мегабайт в секунду в тесте последовательного чтения-записи). Но гораздо важнее даже не скорость, а параметры энергопотребления. В режиме чтения-записи потребляемая мощность модели составляет всего 1,8 Ватта. 2 миллиметра – это много! Нет, конечно для монстров толщиной 8 сантиметров более тонкий жесткий диск погоды не сделает. А вот для ультрабуков – вполне. Согласно Яндекс.Маркету на момент подготовки текста было известно 90 моделей ультрабуков, доступных в продаже в России, с ценником от 18 до 40+ тысяч рублей. Толщина моделей – от 17 до 21 миллиметра. В таких моделях каждый миллиметр на счету, поэтому примерно в половине случаев используется только SSD объемом до 128 гигабайт (а минимальная цена поднимается до 25 тысяч рублей). В других случаях используется либо один жесткий диск, либо гибридное хранилище, предоставляющее и скорость, и объем, при сохранении минимальных размеров и веса. Среди таких моделей есть даже 15-дюймовые компьютеры весом в 1,8 килограмма. И всем, кто постоянно носит компьютер с собой, прекрасно известно, что эти параметры лаптопов неплохо бы еще уменьшить. Тонкие винчестеры – это одна из технологий, которая позволяет это сделать, не заставляя разбираться в том, как выключить в Windows гибернацию и файл подкачки (или же иначе ничего не поместится :). Что дальше? Тонкие жесткие диски предоставляют владельцам ультрабуков и просто компактных моделей возможность не ограничивать себя в объеме хранимых данных – в разумных пределах, но и без суровых ограничений типового SSD на 128 гигабайт. Конечно, сейчас можно купить ноутбук с SSD на 750 и более гигабайт, только вот цена у него будет соответствующая. Распространение моделей с гибридным хранилищем снимает и вопрос скорости работы. Более того, такие модели как Z7K500 обеспечивают и весьма быстрый перенос данных на жесткий диск и обратно, со скоростями более 120 мегабайт в секунду на «крупных» файлах. В результате, изменение высоты жесткого диска всего на 2 мм – на самом деле продлевает жизнь жестких дисков. Если кому-то кажется, что еще чуть-чуть и останутся только твердотельные накопители, то это не так :) Более того, даже 7 миллиметров – это еще не предел возможностей. Совсем недавно были анонсированы пятимиллиметровые жесткие диски объемом до 500 гигабайт. Конкуренция SSD и HDD продолжится, ну а нам остается только пользоваться ее плодами. Из всех доступных конфигураций для ноутбучной дисковой системы, лично для меня наиболее выгодной является 80-гигабайтный твердотельный накопитель и 500-гигабайтный, постоянно доступный «архив» со всеми нужными файлами. А для вас?\n\n\nКраткое содержание:\nСтатья обсуждает историю и текущее состояние жестких дисков с форм-фактором 2,5 дюйма, которые стали тоньше на 2 миллиметра по сравнению с традиционными моделями. Автор отмечает, что такие диски стали популярны благодаря гибридному хранению данных, обеспечивая скорость и объем, и их использование в ультрабуках и компактных ноутбуках позволяет увеличить объем хранения данных без ущерба для размера и веса устройства. Тонкие жесткие диски также способствуют распространению моделей с гибридным хранилищем, обеспечивая быструю передачу данных между твердотельным накопителем и жестким диском.","summary":"Статья обсуждает историю и текущее состояние жестких дисков с форм-фактором 2,5 дюйма, которые стали тоньше на 2 миллиметра по сравнению с традиционными моделями. Автор отмечает, что такие диски стали популярны благодаря гибридному хранению данных, обеспечивая скорость и объем, и их использование в ультрабуках и компактных ноутбуках позволяет увеличить объем хранения данных без ущерба для размера и веса устройства. Тонкие жесткие диски также способствуют распространению моделей с гибридным хранилищем, обеспечивая быструю передачу данных между твердотельным накопителем и жестким диском."}
{"Text":"Тонкости использования селекторов аттрибутов в CSS / Хабр                \n\n02-03-2010\nCSS может связываться с HTML элементами используя любые из его атрибутов. Вы наверняка знаете о классах и ID. Проверим это в HTML: < h2 id =\"first-title\" class =\"magical\" rel =\"friend\" > David Walsh </ h2 > Этот один элемент имеет три аттрибута: ID, class и rel. Для выбора элемента в CSS вы можете использовать селектор ID (#first-title) и селектор class (.magical). Но знаете ли вы, что можно использовать для выбора атрибут rel? Это так называемый селектор атрибута: h2[rel=friend] { /* woohoo! */ } Существует множество селекторов атрибутов, мы не будем рассматривать все возможные варианты, рассмотрим наиболее полезные сценарии, встречающиеся в реальном мире. [rel = external] — Точное совпадение значения атрибута В примере выше, мы использовали атрибут со значением «friend» у элемента h2. CSS селектор, который мы написали, нацелен на элемент h2, потому что его атрибут rel имеет значение «friend». Другими словами, знак равенства означает точное соответствие. Рассмотрим другие примеры. < h1 rel =\"external\" > Attribute Equals </ h1 > h1[rel=external] { color: red; } Более реальный пример из жизни — это стилизация списка блогов. Например у вас есть список ссылок на сайты друзей: < a href =\"http://perishablepress.com\" > Jeff Starr </ a > < a href =\"http://davidwalsh.name\" > David Walsh </ a > < a href =\"http://accidentalninja.net/\" > Richard Felix </ a > Вы хотите сделать различные стили для каждой из ссылок. Традиционный способ — задать каждой ссылке имя класса, но это требует дополнительной разметки, что не всегда хорошо (для семантики и прочего). Другой способ — это использование :nth-child, но для этого потребуется неизменный порядок списка. В данном случае идеальным решением будет использование селекторов атрибутов… Ссылки уже имеют уникальные атрибуты. a[href=http://perishablepress.com] { color: red; } Я думаю, что чаще всего селекторы атрибутов используют для элементов input. Это text, button, checkbox, file, hidden, image, password, radio, reset и submit. Все они являются элементом <input/> и все они очень разные. Так что делать, что то вроди input {padding: 10px;}, почти всегда плохая идея. Поэтому очень часто можно увидеть, нечто похожее на это: input[type=text] { padding: 3px; } input[type=radio] { float: left; } Это единственный способ получить различные типы инпутов без добавления дополнительной разметки. [rel *= external] — Атрибут содержит некоторое значение в любом месте Именно здесь становится более интересно. Знаку равенства в селекторе атрибута могут предшествовать другие символы изменяющие значение. Например, \"*=\" означает нахождение искомого значения в любом месте значения атрибута. Посмотрим на пример: < h1 rel =\"xxxexternalxxx\" > Attribute Contains </ h1 > h1[rel*=external] { color: red; } Помните, что классы и ID тоже являются атрибутами, и могут быть использованы селектором атрибута. Допустим вы пишете CSS для сайта в котором вы не можете контролировать разметку и разработчики сделали три дива: < div id =\"post_1\" ></ div > < div id =\"post_two\" ></ div > < div id =\"third_post\" ></ div > Вы можете выбрать их все: div[id*=post] { color: red; } [rel ^= external] — Атрибут начинается с определенного значения < h1 rel =\"external-link yep\" > Attribute Begins </ h1 > h1[rel^=external] { color: red; } Реальным примером использования может быть случай, когда вам нужно, что бы любая ссылка на сайт друга отличалась от других ссылок. Не важно это ссылка на главную страницу или внутреннюю, стиль должен быть одним. a[href^=http://perishablepress.com] { color: red; } Это будет соответствовать ссылке на главную и второстепенные страницы. [rel $= external] — Атрибут оканчивается определенным значением Мы можем выбирать основываясь на начальном значении атрибута, почему бы не выбирать с конечного? < h1 rel =\"friend external\" > Attribute Ends </ h1 > h1[rel$=external] { color: red; } Честно говоря, я изо всех сил пытаюсь найти реальный пример использования этого. К примеру вы можете найти ссылки имеющие в конце определенные символы. a[href$=#], a[href$=?] { color: red; } [rel ~= external] — Атрибут содержит значение в списке разделенном пробелами Вы наверняка знаете, что к элементу можно применять несколько классов. Если вы это сделаете, вы можете использовать .class-name в CSS для связи. В селекторе атрибута не все так просто. Если ваш атрибут имеет несколько значений (например список разделенный пробелами) вам прийдется использовать \"~=\". < h1 rel =\"friend external sandwich\" > Attribute Space Separated </ h1 > h1[rel~=external] { color: red; } Вы можете подумать, зачем это использовать, когда \"*=\" найдет то же самое и будет более гибким? Действительно, более универсально, но может быть слишком универсальным. Этот селектор требует места вокруг значения, когда *= нет. Так что если у вас есть два элемента, один с атрибутом rel=\"home friend-link\", другой rel=\"home friend link\". Вам понадобится селектор разделенный пробелами для связи с вторым элементом. [rel |= external] — Атрибут содержит значение в списке разделенном тире Список разделенный тире очень похож на список разделенный пробелами, и его так же используют для более строгого соблюдения правил, чем при использовании *=. < h1 rel =\"friend-external-sandwich\" > Attribute Dash Separated </ h1 > h1[rel|=external] { color: red; } [title = one][rel ^= external] — Совпадение нескольких атрибутов Вы можете использовать несколько селекторов атрибутов в одном селекторе, который требуют совпадения от всех условий. < h1 rel =\"handsome\" title =\"Important note\" > Multiple Attributes </ h1 > h1[rel=handsome][title^=Important] { color: red; } Поддержка браузерами Каждый приведенный выше пример работает во всех современных браузерах: Safari, Chrome, Firefox, Opera и IE. Internet Explorer обладает превосходной поддержкой всего этого в 7 версии и нулевой поддержкой в 6 версии. Что бы протестировать в браузере — откройте тестовую страницу. Если строка/селектор красная — значит селектор работает.\n","metric":0.67529296880000000414,"metric_input":"Текст статьи:\nТонкости использования селекторов аттрибутов в CSS / Хабр                \n\n02-03-2010\nCSS может связываться с HTML элементами используя любые из его атрибутов. Вы наверняка знаете о классах и ID. Проверим это в HTML: < h2 id =\"first-title\" class =\"magical\" rel =\"friend\" > David Walsh </ h2 > Этот один элемент имеет три аттрибута: ID, class и rel. Для выбора элемента в CSS вы можете использовать селектор ID (#first-title) и селектор class (.magical). Но знаете ли вы, что можно использовать для выбора атрибут rel? Это так называемый селектор атрибута: h2[rel=friend] { /* woohoo! */ } Существует множество селекторов атрибутов, мы не будем рассматривать все возможные варианты, рассмотрим наиболее полезные сценарии, встречающиеся в реальном мире. [rel = external] — Точное совпадение значения атрибута В примере выше, мы использовали атрибут со значением «friend» у элемента h2. CSS селектор, который мы написали, нацелен на элемент h2, потому что его атрибут rel имеет значение «friend». Другими словами, знак равенства означает точное соответствие. Рассмотрим другие примеры. < h1 rel =\"external\" > Attribute Equals </ h1 > h1[rel=external] { color: red; } Более реальный пример из жизни — это стилизация списка блогов. Например у вас есть список ссылок на сайты друзей: < a href =\"http://perishablepress.com\" > Jeff Starr </ a > < a href =\"http://davidwalsh.name\" > David Walsh </ a > < a href =\"http://accidentalninja.net/\" > Richard Felix </ a > Вы хотите сделать различные стили для каждой из ссылок. Традиционный способ — задать каждой ссылке имя класса, но это требует дополнительной разметки, что не всегда хорошо (для семантики и прочего). Другой способ — это использование :nth-child, но для этого потребуется неизменный порядок списка. В данном случае идеальным решением будет использование селекторов атрибутов… Ссылки уже имеют уникальные атрибуты. a[href=http://perishablepress.com] { color: red; } Я думаю, что чаще всего селекторы атрибутов используют для элементов input. Это text, button, checkbox, file, hidden, image, password, radio, reset и submit. Все они являются элементом <input/> и все они очень разные. Так что делать, что то вроди input {padding: 10px;}, почти всегда плохая идея. Поэтому очень часто можно увидеть, нечто похожее на это: input[type=text] { padding: 3px; } input[type=radio] { float: left; } Это единственный способ получить различные типы инпутов без добавления дополнительной разметки. [rel *= external] — Атрибут содержит некоторое значение в любом месте Именно здесь становится более интересно. Знаку равенства в селекторе атрибута могут предшествовать другие символы изменяющие значение. Например, \"*=\" означает нахождение искомого значения в любом месте значения атрибута. Посмотрим на пример: < h1 rel =\"xxxexternalxxx\" > Attribute Contains </ h1 > h1[rel*=external] { color: red; } Помните, что классы и ID тоже являются атрибутами, и могут быть использованы селектором атрибута. Допустим вы пишете CSS для сайта в котором вы не можете контролировать разметку и разработчики сделали три дива: < div id =\"post_1\" ></ div > < div id =\"post_two\" ></ div > < div id =\"third_post\" ></ div > Вы можете выбрать их все: div[id*=post] { color: red; } [rel ^= external] — Атрибут начинается с определенного значения < h1 rel =\"external-link yep\" > Attribute Begins </ h1 > h1[rel^=external] { color: red; } Реальным примером использования может быть случай, когда вам нужно, что бы любая ссылка на сайт друга отличалась от других ссылок. Не важно это ссылка на главную страницу или внутреннюю, стиль должен быть одним. a[href^=http://perishablepress.com] { color: red; } Это будет соответствовать ссылке на главную и второстепенные страницы. [rel $= external] — Атрибут оканчивается определенным значением Мы можем выбирать основываясь на начальном значении атрибута, почему бы не выбирать с конечного? < h1 rel =\"friend external\" > Attribute Ends </ h1 > h1[rel$=external] { color: red; } Честно говоря, я изо всех сил пытаюсь найти реальный пример использования этого. К примеру вы можете найти ссылки имеющие в конце определенные символы. a[href$=#], a[href$=?] { color: red; } [rel ~= external] — Атрибут содержит значение в списке разделенном пробелами Вы наверняка знаете, что к элементу можно применять несколько классов. Если вы это сделаете, вы можете использовать .class-name в CSS для связи. В селекторе атрибута не все так просто. Если ваш атрибут имеет несколько значений (например список разделенный пробелами) вам прийдется использовать \"~=\". < h1 rel =\"friend external sandwich\" > Attribute Space Separated </ h1 > h1[rel~=external] { color: red; } Вы можете подумать, зачем это использовать, когда \"*=\" найдет то же самое и будет более гибким? Действительно, более универсально, но может быть слишком универсальным. Этот селектор требует места вокруг значения, когда *= нет. Так что если у вас есть два элемента, один с атрибутом rel=\"home friend-link\", другой rel=\"home friend link\". Вам понадобится селектор разделенный пробелами для связи с вторым элементом. [rel |= external] — Атрибут содержит значение в списке разделенном тире Список разделенный тире очень похож на список разделенный пробелами, и его так же используют для более строгого соблюдения правил, чем при использовании *=. < h1 rel =\"friend-external-sandwich\" > Attribute Dash Separated </ h1 > h1[rel|=external] { color: red; } [title = one][rel ^= external] — Совпадение нескольких атрибутов Вы можете использовать несколько селекторов атрибутов в одном селекторе, который требуют совпадения от всех условий. < h1 rel =\"handsome\" title =\"Important note\" > Multiple Attributes </ h1 > h1[rel=handsome][title^=Important] { color: red; } Поддержка браузерами Каждый приведенный выше пример работает во всех современных браузерах: Safari, Chrome, Firefox, Opera и IE. Internet Explorer обладает превосходной поддержкой всего этого в 7 версии и нулевой поддержкой в 6 версии. Что бы протестировать в браузере — откройте тестовую страницу. Если строка/селектор красная — значит селектор работает.\n\n\nКраткое содержание:\nВ статье рассматриваются различные селекторы атрибутов в CSS, которые позволяют выбирать элементы по их атрибутам. Описываются наиболее полезные сценарии использования селекторов атрибутов, такие как выбор элементов по точному значению атрибута, атрибуту, начинающемуся или заканчивающемуся определенным значением, а также по списку значений, разделенных пробелами или дефисами. Приводится пример использования нескольких селекторов атрибутов в одном селекторе. Также указывается, что все приведенные примеры селекторов работают во всех современных браузерах.","summary":"В статье рассматриваются различные селекторы атрибутов в CSS, которые позволяют выбирать элементы по их атрибутам. Описываются наиболее полезные сценарии использования селекторов атрибутов, такие как выбор элементов по точному значению атрибута, атрибуту, начинающемуся или заканчивающемуся определенным значением, а также по списку значений, разделенных пробелами или дефисами. Приводится пример использования нескольких селекторов атрибутов в одном селекторе. Также указывается, что все приведенные примеры селекторов работают во всех современных браузерах."}
{"Text":"Не пытайтесь приспособить свой код к будущему / Хабр                                                          \n\nНе имеет значения, что, по вашему мнению, может случиться потом.\n\nРазработчики любят приспосабливать свои планы под будущее. Им хочется быть уверенными, что они действуют на опережение потребностей, и их код это отражает. Но есть плохие новости: с предсказанием будущего дела у людей обстоят неважно. Это один из ключевых уроков, которые нужно усвоить: код, который пишется на упреждение, часто оборачивается потраченными зря стараниями.\n\nВ эту ловушку попадаются и опытные программисты, и новички. Они пытаются предугадать, что произойдет дальше, и внедряют соответствующие изменения в текущий дизайн. На практике это означает, что они добавляют пару лишних полей в базу данных, проводят рефакторинг логики с прицелом на большую производительность или создают ресурсы, которые можно будет использовать повторно, из расчета, что запросов в будущем станет больше.\n\nНо тут вот какое дело. Эти дополнительные поля, изменения в логике и ресурсы для повторного использования достаются им не бесплатно. Они стоят времени и сил. Если постоянно дорабатывать дизайн, чтобы предвосхитить будущие потребности, выпуск первой версии займет больше времени. И, к сожалению, это время, скорее всего, уйдет впустую.\n\nЗдесь и сейчас вам может казаться, что вы знаете, что потребуется бизнесу в будущем. Уверяю вас, на самом деле вы этого не знаете. Если повезет, потребности можно угадать, ну, может, в одном случае из десяти. В остальных девяти вы будете понапрасну тратить силы и усложнять систему в попытках приспособить под будущее то, что либо претерпит изменения, либо вообще не пригодится.\n\nЭто сложно усвоить. Даже среди сениоров многие так и не прониклись этой мыслью. Нас снова и снова затягивает в эту ловушку: мы пишем код в предвкушении какого-то грядущего сценария – воображаемого сценария, который никогда не сложится в реальности.\n\nВы реализуете функциональность, которая великолепно подготовлена к будущему, или закладываете фундамент для предстоящей функциональности, оставляя конечные точки не прописанными. Вроде бы всё движется в предугаданном вами направлении. Вы попали в точку и станете героем, который предсказал, что будет необходимо в дальнейшем, и ловко всё это обеспечил.\n\nИ тут ни с того ни с сего у компании меняются приоритеты.\n\nНа общем собрании директор объявляет новую масштабную инициативу. Продукты, которые, как вы предполагали, будут использовать ваш код, сворачиваются.\n\nИли, может быть, продуктовой команде приходит обратная связь от клиента: та функциональность, над которой вы работаете, ему на самом деле не нужна, с этими задачами справятся и внутренние инструменты.\n\nВозможно, отдел продаж выясняет, что возможности, которые вы внедряете, пусть и полезны, но не слишком хорошо поддаются монетизации. Потребители за это платить не станут, а значит, нужно сдвинуть функциональность вниз в списке приоритетов для разработки.\n\nДругая команда тоже работает над чем-то подобным. Когда ей становится известно, что вы пишете собственную версию, поступает предложение слить наборы функций. Вам придется поддерживать унаследованные сценарии использования и хосты исходя из требований, которые будут отличаться от тех, что вы запланировали для себя.\n\nНаконец, предположим, с вашим проектом всё в полном порядке, но случилось что-то экстренное по другим задачам. Они перетягивают на себя внимание команды на несколько месяцев, и после этого вам уже не удается снова войти в ритм и привести свой продукт в законченный вид – он так и остается в недоделанном состоянии.\n\nВариантов много, но итог один: тому, что вы так красиво напроектировали и прописали на будущее, не суждено увидеть свет.\n\nВашим девизом должно стать YAGNI (You aren’t gonna need it – Вам это не понадобится). Забегать слишком далеко в будущее опасно и контрпродуктивно. Существует большая вероятность того, что этот код вам не пригодится. В итоге он обернется техническим долгом, кое-как поддерживаемым хламом, с которым вашей команде придется какое-то время жить. Понимаю, что соблазн велик, но пожалуйста, не создавайте то, чего никто не просит. Сделайте то, что прописано в требованиях, и остановитесь на этом.\n\nЧто делать вместо этого? Позвольте я еще разок залезу на табуретку:\n\n> Напишите наиболее простую версию кода, которая будет удовлетворять требованиям.\n\nНебольшие порции изменений, быстрые релизы – вот самый лучший путь.\n\nЕсли вы поймаете себя на том, что пытаетесь подогнать что-то под будущее, задайте себе вопрос: «Что здесь можно упростить?». Если мысль собьется на «в будущем нам, вероятно, пригодится то и это», сразу напомните себе: что вам точно понадобится в будущем, так это кодовая база, с которой просто работать.\n\nЛучшее, что вы можете сделать – это писать простые вещи. Вам-из-будущего совершенно ни к чему перегруженное псевдорешение, написанное человеком, который еще не в курсе требований. А если у вас нет машины времени, вы никак не можете быть в курсе требований. Так что пишите исходя из тех, которые вам уже известны. И достаточно.\n\nСоздание самой простой версии кода под заданные требования – задача сложная, хотя с виду она обманчиво проста. Для этого нужно аккуратно смоделировать проблему, с которой имеешь дело, и сделать каждый компонент решения легкодоступным для понимания. Изящный код выглядит самоочевидным, но требует тщательного продумывания.\n\nВместо того чтобы писать код, приспособленный для будущего, лучше писать код, простой для расширения. Это предполагает простые функции, четко определенные классы и всесторонне тестирование. Когда в будущем вы захотите добавить какую-то функциональность, должно быть ясно, куда и как именно ее добавлять.\n\nРазгадка кроется в простоте. Предсказать будущее невозможно, поэтому не стоит и пытаться. Вместо этого постарайтесь удовлетворить текущие требования наилучшим образом и с предельной простотой.","metric":0.69482421880000000414,"metric_input":"Текст статьи:\nНе пытайтесь приспособить свой код к будущему / Хабр                                                          \n\nНе имеет значения, что, по вашему мнению, может случиться потом.\n\nРазработчики любят приспосабливать свои планы под будущее. Им хочется быть уверенными, что они действуют на опережение потребностей, и их код это отражает. Но есть плохие новости: с предсказанием будущего дела у людей обстоят неважно. Это один из ключевых уроков, которые нужно усвоить: код, который пишется на упреждение, часто оборачивается потраченными зря стараниями.\n\nВ эту ловушку попадаются и опытные программисты, и новички. Они пытаются предугадать, что произойдет дальше, и внедряют соответствующие изменения в текущий дизайн. На практике это означает, что они добавляют пару лишних полей в базу данных, проводят рефакторинг логики с прицелом на большую производительность или создают ресурсы, которые можно будет использовать повторно, из расчета, что запросов в будущем станет больше.\n\nНо тут вот какое дело. Эти дополнительные поля, изменения в логике и ресурсы для повторного использования достаются им не бесплатно. Они стоят времени и сил. Если постоянно дорабатывать дизайн, чтобы предвосхитить будущие потребности, выпуск первой версии займет больше времени. И, к сожалению, это время, скорее всего, уйдет впустую.\n\nЗдесь и сейчас вам может казаться, что вы знаете, что потребуется бизнесу в будущем. Уверяю вас, на самом деле вы этого не знаете. Если повезет, потребности можно угадать, ну, может, в одном случае из десяти. В остальных девяти вы будете понапрасну тратить силы и усложнять систему в попытках приспособить под будущее то, что либо претерпит изменения, либо вообще не пригодится.\n\nЭто сложно усвоить. Даже среди сениоров многие так и не прониклись этой мыслью. Нас снова и снова затягивает в эту ловушку: мы пишем код в предвкушении какого-то грядущего сценария – воображаемого сценария, который никогда не сложится в реальности.\n\nВы реализуете функциональность, которая великолепно подготовлена к будущему, или закладываете фундамент для предстоящей функциональности, оставляя конечные точки не прописанными. Вроде бы всё движется в предугаданном вами направлении. Вы попали в точку и станете героем, который предсказал, что будет необходимо в дальнейшем, и ловко всё это обеспечил.\n\nИ тут ни с того ни с сего у компании меняются приоритеты.\n\nНа общем собрании директор объявляет новую масштабную инициативу. Продукты, которые, как вы предполагали, будут использовать ваш код, сворачиваются.\n\nИли, может быть, продуктовой команде приходит обратная связь от клиента: та функциональность, над которой вы работаете, ему на самом деле не нужна, с этими задачами справятся и внутренние инструменты.\n\nВозможно, отдел продаж выясняет, что возможности, которые вы внедряете, пусть и полезны, но не слишком хорошо поддаются монетизации. Потребители за это платить не станут, а значит, нужно сдвинуть функциональность вниз в списке приоритетов для разработки.\n\nДругая команда тоже работает над чем-то подобным. Когда ей становится известно, что вы пишете собственную версию, поступает предложение слить наборы функций. Вам придется поддерживать унаследованные сценарии использования и хосты исходя из требований, которые будут отличаться от тех, что вы запланировали для себя.\n\nНаконец, предположим, с вашим проектом всё в полном порядке, но случилось что-то экстренное по другим задачам. Они перетягивают на себя внимание команды на несколько месяцев, и после этого вам уже не удается снова войти в ритм и привести свой продукт в законченный вид – он так и остается в недоделанном состоянии.\n\nВариантов много, но итог один: тому, что вы так красиво напроектировали и прописали на будущее, не суждено увидеть свет.\n\nВашим девизом должно стать YAGNI (You aren’t gonna need it – Вам это не понадобится). Забегать слишком далеко в будущее опасно и контрпродуктивно. Существует большая вероятность того, что этот код вам не пригодится. В итоге он обернется техническим долгом, кое-как поддерживаемым хламом, с которым вашей команде придется какое-то время жить. Понимаю, что соблазн велик, но пожалуйста, не создавайте то, чего никто не просит. Сделайте то, что прописано в требованиях, и остановитесь на этом.\n\nЧто делать вместо этого? Позвольте я еще разок залезу на табуретку:\n\n> Напишите наиболее простую версию кода, которая будет удовлетворять требованиям.\n\nНебольшие порции изменений, быстрые релизы – вот самый лучший путь.\n\nЕсли вы поймаете себя на том, что пытаетесь подогнать что-то под будущее, задайте себе вопрос: «Что здесь можно упростить?». Если мысль собьется на «в будущем нам, вероятно, пригодится то и это», сразу напомните себе: что вам точно понадобится в будущем, так это кодовая база, с которой просто работать.\n\nЛучшее, что вы можете сделать – это писать простые вещи. Вам-из-будущего совершенно ни к чему перегруженное псевдорешение, написанное человеком, который еще не в курсе требований. А если у вас нет машины времени, вы никак не можете быть в курсе требований. Так что пишите исходя из тех, которые вам уже известны. И достаточно.\n\nСоздание самой простой версии кода под заданные требования – задача сложная, хотя с виду она обманчиво проста. Для этого нужно аккуратно смоделировать проблему, с которой имеешь дело, и сделать каждый компонент решения легкодоступным для понимания. Изящный код выглядит самоочевидным, но требует тщательного продумывания.\n\nВместо того чтобы писать код, приспособленный для будущего, лучше писать код, простой для расширения. Это предполагает простые функции, четко определенные классы и всесторонне тестирование. Когда в будущем вы захотите добавить какую-то функциональность, должно быть ясно, куда и как именно ее добавлять.\n\nРазгадка кроется в простоте. Предсказать будущее невозможно, поэтому не стоит и пытаться. Вместо этого постарайтесь удовлетворить текущие требования наилучшим образом и с предельной простотой.\n\nКраткое содержание:\nСтатья обсуждает проблему попыток разработчиков приспособить свой код к будущим потребностям, что может привести к напрасной трате времени и сил. Автор утверждает, что невозможно точно предсказать будущее, поэтому лучше сосредоточиться на текущем функционале и простоте кода. Вместо этого рекомендуется писать код, который легко расширяется и добавляется к в будущем.","summary":"Статья обсуждает проблему попыток разработчиков приспособить свой код к будущим потребностям, что может привести к напрасной трате времени и сил. Автор утверждает, что невозможно точно предсказать будущее, поэтому лучше сосредоточиться на текущем функционале и простоте кода. Вместо этого рекомендуется писать код, который легко расширяется и добавляется к в будущем."}
{"Text":"Чистый код для TypeScript — Часть 1 / Habr                        \n\n17-01-2020\nНаткнувшись на материал по принципам чистый код для TypeScript и прочитав его решил взяться за его перевод. Здесь я хочу поделиться с вами некоторыми выдержками из этого перевода, так как некоторые моменты чистого кода для TypeScript повторяют такие же принципы для JavaScript, я их здесь описывать не буду, если будет интересно перевод для JS уже публиковался на хабре(@BoryaMogila) или же можете ознакомится с ними в первоисточнике. Для начала давайте же разберемся что такое, эти принципы чистого кода. Но дать четкого определения чистого кода к сожалению вряд ли получится. Отчасти это все зависит от людей, так например приходишь в музей рассматриваешь картину и думаешь что за уродство, но тут же подходит другой человек и говорит, какое великолепие. Да у нас есть какие определенные, общие черты мира, где мы можем сказать, что то или иное красиво, но дать всему этому определение мы точно не сможем. Так и здесь это всего лишь какие то небольшие критерии этой красоты, соблюдение которых выбирает сам, так как. это не те правила которые высечены в граните. Это просто рекомендации. Переменные Используйте enum для документирования Enam'ы могут помочь документированию вашего кода. Например когда мы обеспокоены тем, что наши переменные отличаются от значений. Плохо: const GENRE = { ROMANTIC: 'romantic', DRAMA: 'drama', COMEDY: 'comedy', DOCUMENTARY: 'documentary', } projector.configureFilm(GENRE.COMEDY); class Projector { // delactation of Projector configureFilm(genre) { switch (genre) { case GENRE.ROMANTIC: // some logic to be executed } } } Хорошо: enum GENRE { ROMANTIC, DRAMA, COMEDY, DOCUMENTARY, } projector.configureFilm(GENRE.COMEDY); class Projector { // delactation of Projector configureFilm(genre) { switch (genre) { case GENRE.ROMANTIC: // some logic to be executed } } } Функции Избегайте проверки типов TypeScript является надмножеством синтаксиса JavaScript и добавляют дополнительные статические проверки типов для языка. Всегда предпочитайте указывать типы переменных, параметров и возвращаемых значений, чтобы использовать всю мощь TypeScript. Это делает будущий рефакторинг более легким. Плохо: function travelToTexas(vehicle: Bicycle | Car) { if (vehicle instanceof Bicycle) { vehicle.pedal(currentLocation, new Location('texas')); } else if (vehicle instanceof Car) { vehicle.drive(currentLocation, new Location('texas')); } } Хорошо: type Vehicle = Bicycle | Car; function travelToTexas(vehicle: Vehicle) { vehicle.move(currentLocation, new Location('texas')); } Используйте итераторы и генераторы Используйте генераторы и итераторы при работе с коллекциями данных, которые используются как поток. Есть несколько причин для этого: отделяет вызываемый объект от реализации генератора в том смысле, что вызываемый объект решает сколько элементов иметь для доступа ленивое выполнение, элементы передаются по требованию встроенная поддержка итерации элементов с использованием синтаксиса for-of итераторы позволяют реализовать оптимизированные паттерны итераторов Плохо: function fibonacci(n: number): number[] { if (n === 1) return [0]; if (n === 2) return [0, 1]; const items: number[] = [0, 1]; while (items.length < n) { items.push(items[items.length - 2] + items[items.length - 1]); } return items; } function print(n: number) { fibonacci(n).forEach(fib => console.log(fib)); } // Print first 10 Fibonacci numbers. print(10); Хорошо: // Generates an infinite stream of Fibonacci numbers. // The generator doesn't keep the array of all numbers. function* fibonacci(): IterableIterator<number> { let [a, b] = [0, 1]; while (true) { yield a; [a, b] = [b, a + b]; } } function print(n: number) { let i = 0; for (const fib of fibonacci()) { if (i++ === n) break; console.log(fib); } } // Print first 10 Fibonacci numbers. print(10); Существуют библиотеки, которые позволяют работать с итераторами так же, как и с собственными массивами, путем цепочки методов, таких как map, slice, forEach и др. Смотрите itiriri пример продвинутой манипуляции с итераторами (или itiriri-async для манипуляции с асинхронными итераторами). import itiriri from 'itiriri'; function* fibonacci(): IterableIterator<number> { let [a, b] = [0, 1]; while (true) { yield a; [a, b] = [b, a + b]; } } itiriri(fibonacci()) .take(10) .forEach(fib => console.log(fib)); Объекты и структуры данных Используйте геттеры и сеттеры TypeScript поддерживает синтаксис геттеров и сеттеров. Использовать геттеры и сеттеры для доступа к данным объекта гораздо лучше, чем напрямую обращаться к его свойствам. \"Почему?\" спросите вы. Вот список причин: Если вы хотите реализовать больше, чем просто доступ к свойству, вам нужно поменять реализацию в одном месте, а не по всему коду Валидацию легко реализовать на уровне реализации set Инкапсуляция внутреннего состояния Легко добавить логирование и обработку ошибок на уровне геттеров и сеттеров Вы можете лениво подгружать свойства вашего объекта, например, с сервера Плохо: type BankAccount = { balance: number; // ... } const value = 100; const account: BankAccount = { balance: 0, // ... }; if (value < 0) { throw new Error('Cannot set negative balance.'); } account.balance = value; Хорошо: class BankAccount { private accountBalance: number = 0; get balance(): number { return this.accountBalance; } set balance(value: number) { if (value < 0) { throw new Error('Cannot set negative balance.'); } this.accountBalance = value; } // ... } // Теперь `BankAccount` инкапсулирует логику проверки. // Если однажды спецификации изменятся, и нам понадобится дополнительное правило проверки, // нам придется изменить только реализацию `сеттера`, // оставив весь зависимый код без изменений. const account = new BankAccount(); account.balance = 100; Создавайте объекты с приватными/защищенными полями TypeScript поддерживает public (по умолчанию), protected и private средства доступа к свойствам класса. Плохо: class Circle { radius: number; constructor(radius: number) { this.radius = radius; } perimeter() { return 2 * Math.PI * this.radius; } surface() { return Math.PI * this.radius * this.radius; } } Хорошо: class Circle { constructor(private readonly radius: number) { } perimeter() { return 2 * Math.PI * this.radius; } surface() { return Math.PI * this.radius * this.radius; } } Уважаемые читатели, а какими принципами вы пользуетесь при использовании TypeScript? Продолжение: Вторая часть Третья часть Полный перевод\n","metric":0.3662109375,"metric_input":"Текст статьи:\nЧистый код для TypeScript — Часть 1 / Habr                        \n\n17-01-2020\nНаткнувшись на материал по принципам чистый код для TypeScript и прочитав его решил взяться за его перевод. Здесь я хочу поделиться с вами некоторыми выдержками из этого перевода, так как некоторые моменты чистого кода для TypeScript повторяют такие же принципы для JavaScript, я их здесь описывать не буду, если будет интересно перевод для JS уже публиковался на хабре(@BoryaMogila) или же можете ознакомится с ними в первоисточнике. Для начала давайте же разберемся что такое, эти принципы чистого кода. Но дать четкого определения чистого кода к сожалению вряд ли получится. Отчасти это все зависит от людей, так например приходишь в музей рассматриваешь картину и думаешь что за уродство, но тут же подходит другой человек и говорит, какое великолепие. Да у нас есть какие определенные, общие черты мира, где мы можем сказать, что то или иное красиво, но дать всему этому определение мы точно не сможем. Так и здесь это всего лишь какие то небольшие критерии этой красоты, соблюдение которых выбирает сам, так как. это не те правила которые высечены в граните. Это просто рекомендации. Переменные Используйте enum для документирования Enam'ы могут помочь документированию вашего кода. Например когда мы обеспокоены тем, что наши переменные отличаются от значений. Плохо: const GENRE = { ROMANTIC: 'romantic', DRAMA: 'drama', COMEDY: 'comedy', DOCUMENTARY: 'documentary', } projector.configureFilm(GENRE.COMEDY); class Projector { // delactation of Projector configureFilm(genre) { switch (genre) { case GENRE.ROMANTIC: // some logic to be executed } } } Хорошо: enum GENRE { ROMANTIC, DRAMA, COMEDY, DOCUMENTARY, } projector.configureFilm(GENRE.COMEDY); class Projector { // delactation of Projector configureFilm(genre) { switch (genre) { case GENRE.ROMANTIC: // some logic to be executed } } } Функции Избегайте проверки типов TypeScript является надмножеством синтаксиса JavaScript и добавляют дополнительные статические проверки типов для языка. Всегда предпочитайте указывать типы переменных, параметров и возвращаемых значений, чтобы использовать всю мощь TypeScript. Это делает будущий рефакторинг более легким. Плохо: function travelToTexas(vehicle: Bicycle | Car) { if (vehicle instanceof Bicycle) { vehicle.pedal(currentLocation, new Location('texas')); } else if (vehicle instanceof Car) { vehicle.drive(currentLocation, new Location('texas')); } } Хорошо: type Vehicle = Bicycle | Car; function travelToTexas(vehicle: Vehicle) { vehicle.move(currentLocation, new Location('texas')); } Используйте итераторы и генераторы Используйте генераторы и итераторы при работе с коллекциями данных, которые используются как поток. Есть несколько причин для этого: отделяет вызываемый объект от реализации генератора в том смысле, что вызываемый объект решает сколько элементов иметь для доступа ленивое выполнение, элементы передаются по требованию встроенная поддержка итерации элементов с использованием синтаксиса for-of итераторы позволяют реализовать оптимизированные паттерны итераторов Плохо: function fibonacci(n: number): number[] { if (n === 1) return [0]; if (n === 2) return [0, 1]; const items: number[] = [0, 1]; while (items.length < n) { items.push(items[items.length - 2] + items[items.length - 1]); } return items; } function print(n: number) { fibonacci(n).forEach(fib => console.log(fib)); } // Print first 10 Fibonacci numbers. print(10); Хорошо: // Generates an infinite stream of Fibonacci numbers. // The generator doesn't keep the array of all numbers. function* fibonacci(): IterableIterator<number> { let [a, b] = [0, 1]; while (true) { yield a; [a, b] = [b, a + b]; } } function print(n: number) { let i = 0; for (const fib of fibonacci()) { if (i++ === n) break; console.log(fib); } } // Print first 10 Fibonacci numbers. print(10); Существуют библиотеки, которые позволяют работать с итераторами так же, как и с собственными массивами, путем цепочки методов, таких как map, slice, forEach и др. Смотрите itiriri пример продвинутой манипуляции с итераторами (или itiriri-async для манипуляции с асинхронными итераторами). import itiriri from 'itiriri'; function* fibonacci(): IterableIterator<number> { let [a, b] = [0, 1]; while (true) { yield a; [a, b] = [b, a + b]; } } itiriri(fibonacci()) .take(10) .forEach(fib => console.log(fib)); Объекты и структуры данных Используйте геттеры и сеттеры TypeScript поддерживает синтаксис геттеров и сеттеров. Использовать геттеры и сеттеры для доступа к данным объекта гораздо лучше, чем напрямую обращаться к его свойствам. \"Почему?\" спросите вы. Вот список причин: Если вы хотите реализовать больше, чем просто доступ к свойству, вам нужно поменять реализацию в одном месте, а не по всему коду Валидацию легко реализовать на уровне реализации set Инкапсуляция внутреннего состояния Легко добавить логирование и обработку ошибок на уровне геттеров и сеттеров Вы можете лениво подгружать свойства вашего объекта, например, с сервера Плохо: type BankAccount = { balance: number; // ... } const value = 100; const account: BankAccount = { balance: 0, // ... }; if (value < 0) { throw new Error('Cannot set negative balance.'); } account.balance = value; Хорошо: class BankAccount { private accountBalance: number = 0; get balance(): number { return this.accountBalance; } set balance(value: number) { if (value < 0) { throw new Error('Cannot set negative balance.'); } this.accountBalance = value; } // ... } // Теперь `BankAccount` инкапсулирует логику проверки. // Если однажды спецификации изменятся, и нам понадобится дополнительное правило проверки, // нам придется изменить только реализацию `сеттера`, // оставив весь зависимый код без изменений. const account = new BankAccount(); account.balance = 100; Создавайте объекты с приватными/защищенными полями TypeScript поддерживает public (по умолчанию), protected и private средства доступа к свойствам класса. Плохо: class Circle { radius: number; constructor(radius: number) { this.radius = radius; } perimeter() { return 2 * Math.PI * this.radius; } surface() { return Math.PI * this.radius * this.radius; } } Хорошо: class Circle { constructor(private readonly radius: number) { } perimeter() { return 2 * Math.PI * this.radius; } surface() { return Math.PI * this.radius * this.radius; } } Уважаемые читатели, а какими принципами вы пользуетесь при использовании TypeScript? Продолжение: Вторая часть Третья часть Полный перевод\n\n\nКраткое содержание:\nЧистый код для TypeScript — Часть 1 / Habr 17-01-2020\n\nНаткнувшись на материал по принципам чистый код для TypeScript и прочитав его решил взяться за его перевод. Для начала давайте же разберемся что такое, эти принципы чистого кода. Переменные Используйте enum для документирования Функции Избегайте проверки типов Функции избегайте проверки типов Объекты и структуры данных Используйте геттеры и сеттеры Создавайте объекты с приватными/защищенными полями Уважаемые читатели, а какими принципами вы пользуетесь при использовании TypeScript? Продолжение: Вторая часть Третья часть Полный перевод","summary":"Чистый код для TypeScript — Часть 1 / Habr 17-01-2020\n\nНаткнувшись на материал по принципам чистый код для TypeScript и прочитав его решил взяться за его перевод. Для начала давайте же разберемся что такое, эти принципы чистого кода. Переменные Используйте enum для документирования Функции Избегайте проверки типов Функции избегайте проверки типов Объекты и структуры данных Используйте геттеры и сеттеры Создавайте объекты с приватными/защищенными полями Уважаемые читатели, а какими принципами вы пользуетесь при использовании TypeScript? Продолжение: Вторая часть Третья часть Полный перевод"}
{"Text":"Как использовать психологические принципы для увеличения конверсии / Habr           \n\n06-11-2014\nОптимизация конверсии — это увеличение вероятности того, что пользователь выполнит какое-либо целевое действие на вашей странице. Вашему пользователю нужно принять решение, и вы хотите повлиять на этот процесс. Понимание психологии, которая управляет поведением человека, может помочь нам в этом. Боязнь потери Боязнь потери — это мощная консервативная сила, которая заставляет людей больше стремиться сохранить что-либо, чем рисковать ради достижения чего-нибудь большего. Исследование показало, что люди получают больше удовольствия от обхода потерь, чем от получения прибыли (такой же ценности). Представьте, например, что вы играете в орла и решку на деньги, если бросить монету и выпадет решка, вы потеряете 10 долларов. Сколько бы вы хотели получить, если бы выпал орел? Большинство людей отвечает 20 долларов. То есть они хотят получить потенциальное вознаграждение в два раза больше потенциальной потери. Другими словами, мы хотим иметь много денег в карманах, ничего при этом не теряя. Бесплатные Пробные Версии Боязнь потери является одной из причин успешной работы стратегии пробных версий, которые очень эффективно работают для SAAS. Когда мы получаем бесплатно что-нибудь нужное, мы со временем привыкаем и уже не хотим расставаться с этим, поэтому готовы заплатить деньги, сохраняя положение вещей. Проще говоря, мы переоцениваем потери и стараемся избегать излишних расходов и высоких цен. Booking.com извлекает из этого выгоду с помощью пунктов «Заблокировать цену» перед тем, как цены «могут» подскочить. Самоэффективность Самоэффективность — это сила веры в собственную компетентность — как хорошо, на ваш взгляд, вы справитесь с задачей. Самоэффективность может напрямую влиять на вашу мотивацию и ваше стремление к выполнению задачи. Приведу пример, самоэффективность напрямую влияет на то, как вы соблюдаете диету или режиму тренировок. Если вы не верите в свои возможности, то высоки шансы, что вы все забросите. Таким образом низкая самоэффективность может вынудить людей поверить что задача сложнее, чем она есть на самом деле и это может воспрепятствовать даже началу выполнения задачи. Награда за успех Само-эффективность растет вместе с успехами, мы можем дать пользователю видимость этого успеха, периодически награждая его за верность выполненного действия, тем самым увеличивая вероятность того, что он пройдёт дальше и выполнит задачу. Наградой может быть всё, что угодно, даже отмечающая правильность выполненного действия зелёная галочка. Награда за успех по выполнению задачи в процессе укрепляет веру в то, что вы можете выполнить задачу В тесте, опубликованном Люком Вроблевски, встроенная проверка формы на веб-странице показала 22-процентное увеличение показателей успеха и 31-процентне увеличение удовлетворенности пользователей. Пусть вещи выглядят проще Люди с низкой самоэффективностью могут не справиться с выполнением задания, поэтому разбитие процесса на несколько частей может все сделать визуально проще. По мере работы над процессом, вознаграждайте их за каждый шаг, чтобы поддерживать самоэффективность по мере продвижения. Демонстрируйте успех остальных Самоэффективность также может увеличиваться за счет активности других пользователей. Если мы видим, как другие успешно выполняют задание, мы тоже начинаем верить в то, что можем справиться с заданием. Можно использовать это, показывая существующих клиентов, которые уже сделали покупки: Или количество уже подписавшихся людей: Теория двойного процесса Также огромную пользу мы можем получить зная, каким образом наш мозг обрабатывает информацию в процессе принятия решения. Одним из наиболее популярных учений о мысли, является «теория двойного процесса», гласящая о том, что наш мозг формирует мысли с помощью двух отдельных процессов. Один из процессов осознанный, а другой подсознательный. В своей книге «Мышление: быстрое и медленное» Дэниел Канеман описывает две системы следующим образом: Система 1: Быстрое, автоматическое, частое, эмоциональное, стереотипное, подсознательное Система 2: Медленное, требующее усилий, нечастое, логическое, расчетливое, осознанное Обе системы постоянно задействованы и какая-то из них каждый раз принимает за нас решения. Проще говоря, мы либо принимаем поспешные решения либо перевариваем мысли очень щепетильно. “Системы 1 и 2 всегда активны, когда мы бодрствуем. Система 1 запускается автоматически, а Система 2 в стандартном режиме задействована лишь частично. Если у Системы 1 возникают проблемы, подключается Система 2, предоставляя более подробную информацию для решения проблемы в текущий момент. Таким образом, опытный водитель может спокойно управлять большинством аспектов вождения без размышлений, используя Систему 1. Такие быстрые решения, как переключение передачи или выбор курса могут быть приняты автоматически, даже если разговаривать с пассажиром или слушать музыку. Когда же он подьезжает к сложному перекрестку включается система 2, так как в этот момент требуется большая сосредоточенность и рациональность рассуждения. Оптимизирование для Системы 1 По словам профессора Гарвардского университета Джеральда Залтмана, вплоть до 95% покупок мы делаем с использованием Системы 1. И мы можем эффективно использовать Систему 1 главное не вынуждать пользователя думать. Визуальное целенаправление Используйте визуальные подсказки, чтобы в буквальном смысле направлять внимание туда, куда нужно. Наше внимание очень легко поддается влиянию, так что даже незаметные сигналы могут быть результативными. Optimizely это делает «привлекая» взгляд к побуждению к действию: Evernote делает точно так же, даже с большей интенсивностью, создавая силуэт курсора возле CTA. В Wheelofpersuasion опубликованы результаты использования визуального целенаправления на страницу с отчетной формой для клиента их отеля Ван Дер Валка. Вы можете видеть ниже, что обычное дополнение к курсору привело к 57-процентному повышения конверсии! Мы конечно не должны придавать особое значение визуальным указателям. Но это действующий метод привлечения внимания пользователей. Общение с помощью изображений Поскольку Система 1 изначально эмоциональна, мы можем управлять эмоциями с помощью образов реальных людей (НЕ стоковых фотографий!), и это тесно связано с выражением лица. Отметьте, что на этом примере взгляд сперва притягивается к лицу женщины, и только затем мы обращаем внимание на форму для регистрации – это еще один пример визуального указания. На Kissmetrics опубликованы исследования привлечения взгляда и того, чему мы можем научиться из этого. Поскольку мы подсознательно сначала реагируем на лицо человека, то с помощью правильно подобранной физиономии мы можем помочь посетителю нашего сайта сформировать положительное ощущение от нашего продукта. Предлагайте быстрое вознаграждение Система 1 принимает быстрые, автоматические решения, благодаря чему немедленное вознаграждения может сыграть на руку за счет эфекта когнитивной предвзятости. Исследование, опубликованное на ContentVerve, показывает 31-процентный прирост посещения страницы оплаты за счет добавления к клавиши информации о поощрении Психология Ваших Клиентов Поиск психологических принципов при создании идей для тестирования не исключает потребность в самом тестировании. Мы пытаемся помочь пользователю принять решение, воздействуя на определенные точки в его сознании – но не все покупатели одинаковы. К тому же, не кто не отменял воздействие простой логики и грамотного описания вашего продукта. В любом случае, лучше понимая способ принятия решений нашими клиентами, мы имеем больше шансов убедить их принять нужное нам решение. Переведено сотрудниками проекта Multi-landing. Будем рады ответить на все ваши вопросы.\n","metric":0.59814453130000000414,"metric_input":"Текст статьи:\nКак использовать психологические принципы для увеличения конверсии / Habr           \n\n06-11-2014\nОптимизация конверсии — это увеличение вероятности того, что пользователь выполнит какое-либо целевое действие на вашей странице. Вашему пользователю нужно принять решение, и вы хотите повлиять на этот процесс. Понимание психологии, которая управляет поведением человека, может помочь нам в этом. Боязнь потери Боязнь потери — это мощная консервативная сила, которая заставляет людей больше стремиться сохранить что-либо, чем рисковать ради достижения чего-нибудь большего. Исследование показало, что люди получают больше удовольствия от обхода потерь, чем от получения прибыли (такой же ценности). Представьте, например, что вы играете в орла и решку на деньги, если бросить монету и выпадет решка, вы потеряете 10 долларов. Сколько бы вы хотели получить, если бы выпал орел? Большинство людей отвечает 20 долларов. То есть они хотят получить потенциальное вознаграждение в два раза больше потенциальной потери. Другими словами, мы хотим иметь много денег в карманах, ничего при этом не теряя. Бесплатные Пробные Версии Боязнь потери является одной из причин успешной работы стратегии пробных версий, которые очень эффективно работают для SAAS. Когда мы получаем бесплатно что-нибудь нужное, мы со временем привыкаем и уже не хотим расставаться с этим, поэтому готовы заплатить деньги, сохраняя положение вещей. Проще говоря, мы переоцениваем потери и стараемся избегать излишних расходов и высоких цен. Booking.com извлекает из этого выгоду с помощью пунктов «Заблокировать цену» перед тем, как цены «могут» подскочить. Самоэффективность Самоэффективность — это сила веры в собственную компетентность — как хорошо, на ваш взгляд, вы справитесь с задачей. Самоэффективность может напрямую влиять на вашу мотивацию и ваше стремление к выполнению задачи. Приведу пример, самоэффективность напрямую влияет на то, как вы соблюдаете диету или режиму тренировок. Если вы не верите в свои возможности, то высоки шансы, что вы все забросите. Таким образом низкая самоэффективность может вынудить людей поверить что задача сложнее, чем она есть на самом деле и это может воспрепятствовать даже началу выполнения задачи. Награда за успех Само-эффективность растет вместе с успехами, мы можем дать пользователю видимость этого успеха, периодически награждая его за верность выполненного действия, тем самым увеличивая вероятность того, что он пройдёт дальше и выполнит задачу. Наградой может быть всё, что угодно, даже отмечающая правильность выполненного действия зелёная галочка. Награда за успех по выполнению задачи в процессе укрепляет веру в то, что вы можете выполнить задачу В тесте, опубликованном Люком Вроблевски, встроенная проверка формы на веб-странице показала 22-процентное увеличение показателей успеха и 31-процентне увеличение удовлетворенности пользователей. Пусть вещи выглядят проще Люди с низкой самоэффективностью могут не справиться с выполнением задания, поэтому разбитие процесса на несколько частей может все сделать визуально проще. По мере работы над процессом, вознаграждайте их за каждый шаг, чтобы поддерживать самоэффективность по мере продвижения. Демонстрируйте успех остальных Самоэффективность также может увеличиваться за счет активности других пользователей. Если мы видим, как другие успешно выполняют задание, мы тоже начинаем верить в то, что можем справиться с заданием. Можно использовать это, показывая существующих клиентов, которые уже сделали покупки: Или количество уже подписавшихся людей: Теория двойного процесса Также огромную пользу мы можем получить зная, каким образом наш мозг обрабатывает информацию в процессе принятия решения. Одним из наиболее популярных учений о мысли, является «теория двойного процесса», гласящая о том, что наш мозг формирует мысли с помощью двух отдельных процессов. Один из процессов осознанный, а другой подсознательный. В своей книге «Мышление: быстрое и медленное» Дэниел Канеман описывает две системы следующим образом: Система 1: Быстрое, автоматическое, частое, эмоциональное, стереотипное, подсознательное Система 2: Медленное, требующее усилий, нечастое, логическое, расчетливое, осознанное Обе системы постоянно задействованы и какая-то из них каждый раз принимает за нас решения. Проще говоря, мы либо принимаем поспешные решения либо перевариваем мысли очень щепетильно. “Системы 1 и 2 всегда активны, когда мы бодрствуем. Система 1 запускается автоматически, а Система 2 в стандартном режиме задействована лишь частично. Если у Системы 1 возникают проблемы, подключается Система 2, предоставляя более подробную информацию для решения проблемы в текущий момент. Таким образом, опытный водитель может спокойно управлять большинством аспектов вождения без размышлений, используя Систему 1. Такие быстрые решения, как переключение передачи или выбор курса могут быть приняты автоматически, даже если разговаривать с пассажиром или слушать музыку. Когда же он подьезжает к сложному перекрестку включается система 2, так как в этот момент требуется большая сосредоточенность и рациональность рассуждения. Оптимизирование для Системы 1 По словам профессора Гарвардского университета Джеральда Залтмана, вплоть до 95% покупок мы делаем с использованием Системы 1. И мы можем эффективно использовать Систему 1 главное не вынуждать пользователя думать. Визуальное целенаправление Используйте визуальные подсказки, чтобы в буквальном смысле направлять внимание туда, куда нужно. Наше внимание очень легко поддается влиянию, так что даже незаметные сигналы могут быть результативными. Optimizely это делает «привлекая» взгляд к побуждению к действию: Evernote делает точно так же, даже с большей интенсивностью, создавая силуэт курсора возле CTA. В Wheelofpersuasion опубликованы результаты использования визуального целенаправления на страницу с отчетной формой для клиента их отеля Ван Дер Валка. Вы можете видеть ниже, что обычное дополнение к курсору привело к 57-процентному повышения конверсии! Мы конечно не должны придавать особое значение визуальным указателям. Но это действующий метод привлечения внимания пользователей. Общение с помощью изображений Поскольку Система 1 изначально эмоциональна, мы можем управлять эмоциями с помощью образов реальных людей (НЕ стоковых фотографий!), и это тесно связано с выражением лица. Отметьте, что на этом примере взгляд сперва притягивается к лицу женщины, и только затем мы обращаем внимание на форму для регистрации – это еще один пример визуального указания. На Kissmetrics опубликованы исследования привлечения взгляда и того, чему мы можем научиться из этого. Поскольку мы подсознательно сначала реагируем на лицо человека, то с помощью правильно подобранной физиономии мы можем помочь посетителю нашего сайта сформировать положительное ощущение от нашего продукта. Предлагайте быстрое вознаграждение Система 1 принимает быстрые, автоматические решения, благодаря чему немедленное вознаграждения может сыграть на руку за счет эфекта когнитивной предвзятости. Исследование, опубликованное на ContentVerve, показывает 31-процентный прирост посещения страницы оплаты за счет добавления к клавиши информации о поощрении Психология Ваших Клиентов Поиск психологических принципов при создании идей для тестирования не исключает потребность в самом тестировании. Мы пытаемся помочь пользователю принять решение, воздействуя на определенные точки в его сознании – но не все покупатели одинаковы. К тому же, не кто не отменял воздействие простой логики и грамотного описания вашего продукта. В любом случае, лучше понимая способ принятия решений нашими клиентами, мы имеем больше шансов убедить их принять нужное нам решение. Переведено сотрудниками проекта Multi-landing. Будем рады ответить на все ваши вопросы.\n\n\nКраткое содержание:\nСтатья обсуждает, как использовать психологические принципы для увеличения конверсии на веб-страницах. Ключевые моменты:\n\n1. **Боязнь потери:** Люди стремятся сохранить то, что имеют, больше, чем рисковать ради большего. Это может быть причиной успеха стратегии пробных версий.\n\n2. **Самоэффективность:** Вера в свою способность выполнить задачу. Это влияет на мотивацию и стремление к действию.\n\n3. **Награда за успех:** Периодическое вознаграждение за выполнение задачи укрепляет веру в свои силы и увеличивает вероятность успеха.\n\n4. **Визуальное упрощение:** Разбиение процесса на части может сделать его визуально проще, что повышает самоэффективность.\n\n5. **Демонстрация успеха:** Наблюдение за успехом других людей укрепляет веру в собственные возможности.\n\n6. **Теория двойного процесса:** Наш мозг обрабатывает информацию с помощью двух процессов: осознанного и подсознательного.\n\n7. **Визуальное направление:** Использование визуальных подсказок для направления внимания пользователя.\n\n8. **Образы и эмоции:** Управление эмоциями с помощью образов реальных людей.\n\n9. **Быстрое вознаграждение:** Использование Системы 1 (автоматического мышления) для быстрого вознаграждения.\n\n10. **Разнообразие подходов:** Понимание психологических принципов не исключает необходимости тестирования и логики.","summary":"Статья обсуждает, как использовать психологические принципы для увеличения конверсии на веб-страницах. Ключевые моменты:\n\n1. **Боязнь потери:** Люди стремятся сохранить то, что имеют, больше, чем рисковать ради большего. Это может быть причиной успеха стратегии пробных версий.\n\n2. **Самоэффективность:** Вера в свою способность выполнить задачу. Это влияет на мотивацию и стремление к действию.\n\n3. **Награда за успех:** Периодическое вознаграждение за выполнение задачи укрепляет веру в свои силы и увеличивает вероятность успеха.\n\n4. **Визуальное упрощение:** Разбиение процесса на части может сделать его визуально проще, что повышает самоэффективность.\n\n5. **Демонстрация успеха:** Наблюдение за успехом других людей укрепляет веру в собственные возможности.\n\n6. **Теория двойного процесса:** Наш мозг обрабатывает информацию с помощью двух процессов: осознанного и подсознательного.\n\n7. **Визуальное направление:** Использование визуальных подсказок для направления внимания пользователя.\n\n8. **Образы и эмоции:** Управление эмоциями с помощью образов реальных людей.\n\n9. **Быстрое вознаграждение:** Использование Системы 1 (автоматического мышления) для быстрого вознаграждения.\n\n10. **Разнообразие подходов:** Понимание психологических принципов не исключает необходимости тестирования и логики."}
{"Text":"Советы для тех, кто планирует заняться локализацией своего проекта / Habr             \n\n26-07-2017\nВопрос локализации является краеугольным камнем для множества команд разработчиков по всему миру. Особенно остро этот вопрос стоит, когда рынок продукта однозначно не определен и команда до конца не знает своего потребителя. Будем откровенны: если ваш продукт ориентирован на широкую аудиторию, то английского языка явно недостаточно. Конечно, существуют узкоспециализированные проекты и сервисы вроде нашего, когда знание главного международного языка не прихоть – необходимость, однако ниша подобных разработок крайне узка. И вот, тысячи команд по всему миру рано или поздно упираются в потолок одного-двух языков: один английский, а второй – родной для команды (если она не англоговорящая). Дальше начинаются споры, ссоры, попытки локализации и последующие пляски с бубном. В этой публикации мы собрали ряд популярных советов и рекомендаций как от частных разработчиков, так и от матерых команд уровня Mozilla, в которых более опытные товарищи делятся со своими коллегами опытом локализации проектов. Семь раз отмерь Одной из самых раздражающих ошибок в ходе локализации является «деревянная» верстка проекта, когда поля и кнопки не адаптируются под размер текста. Этим страдают все: от веб, до гейм-разработчиков. Чем серьезнее и сложнее ваш проект в плане кодовой части и чем сложнее добраться до размеров тех или иных элементов так, чтобы не поехала вся страница вместе с дизайном, тем внимательнее стоит относиться к размерности элементов. Многие опытные разработчики рекомендуют всегда оставлять минимум 30% запас (в каждую сторону) на каждый локализуемый элемент интерфейса: в зависимости от языковой группы размерность надписей может видоизменяться как в большую (языки романской группы), так и в меньшую (азиатские языки) сторону. Думаю, каждый хотя бы раз сталкивался с тем, что какая-то надпись банально не влезает в отведенное ей пространство. Как можно понять из текста выше, немалая роль в успешном проведении локализации лежит на дизайнерах. По мнению Standish Group лишь 20% фич и функций проекта обеспечивают его конкурентоспособность, но вот однозначно и объективно выявить эти самые ключевые «точки» самим разработчикам удается не всегда. Поэтому уделять внимание приходится всему и вся, ведь ошибка на критичном для потребителя, но не слишком приоритетном по мнению разработчиков участке проекта может сыграть решающую роль. Также необходимо понимать, что просто перевести весь проект на нужный язык — это не значит локализовать его. Разные языковые группы и регионы определяют разные паттерны поведения пользователей и то, что работает для англоязычной или русскоязычной аудитории, не будет работать в Азии и наоборот. Речь идет не только о содержании, но и о дизайнерских решениях, денежных единицах, формате дат и т.п. Важно не только снаружи, но и внутри Все сказанное выше, по сути — лирика для дизайнеров и проект-менеджеров. Команда же Mozilla, которая имеет весьма солидный опыт в локализации продуктов, дает более развернутые советы для разработчиков. Например, Mozilla Developer Network в своем блоге рекомендует в ходе разработки обращать внимание на мелочи и в общем быть опрятными. Так, по их мнению крайне важно выбирать адекватные имена для меток (ключей) в ходе локализации и мы, исходя из опыта наших клиентов, согласны с этим утверждением. Метки, вне зависимости от степени важности и роли, «всегда должны описывать строку и ее роль в интерфейсе». Если строка привязывается к какой-то особенной функции или всплывающему окну, то метка должна отражать и эту информацию. Еще один аспект, на который обращают внимание разработчики Mozilla — это то, что локализаторы часто работают без контекста, в отрыве от самого проекта. Для облегчения их участи следует оставлять пометки, которые бы давали им ориентир, в какую сторону двигаться. При этом все комментарии, опять таки, должны быть выполнены в едином формате и иметь однозначное содержание. Кстати, об однозначности мы упомянем позже. Фактически, организация локализации — это тест на внимательность и опрятность: если команда сможет его успешно пройти, то и локализация не доставит существенных проблем. Сложность данного квеста нелинейно возрастает с увеличением числа языков, и если для одностороннего перевода советы команды Mozilla могут показаться надуманными, то когда вам нужно перевести и адаптировать проект под 10-15 языков, они внезапно обретают смысл уровня библейских откровений. Как это не удивительно, но успешность процесса локализации напрямую зависит и от стиля самой разработки. Если в вашем проекте есть временные строки и блоки, которые, в перспективе (обозримой или не очень) подвергнутся изменениями, то от их локализации стоит воздержаться. Конечно, все мы знаем, что нет ничего более постоянного, чем временное, однако это правило срабатывает не всегда. Тут уж стоит руководствоваться законом Мерфи и ожидать всего (то есть худшего). Переводчики — наше все Для того, чтобы грамотно и адекватно организовать работу, команды локализаторов вышеописанного все еще недостаточно. И неважно, используете вы специализированный сервис по локализации продуктов вроде нашего или проводите эту процедуру по-старинке, вручную. Одна из самых коварных угроз на этапе локализаций поджидает в самом процессе коммуникации. Да, все верно: самые нелепые и непредсказуемые ошибки возникают тогда, когда люди до конца друг друга не понимают. И дело тут не в тупости разработчиков или непосредственных локализаторов, а в коммуникативном инструментарии. Очень часто локализацией занимаются так называемые «native speakers», или, если выражаться по-русски — носители языка. Коммуникация же происходит либо на языке первоисточника ресурса (если это прямая локализация с какого-то языка, отличного от английского, на какой-то третий язык), либо при помощи того же английского. Многие разработчики забывают, что привычная им англоязычная лексика отличается от общепринятой. Таким образом, общаясь с переводчиком, который не слишком плотно знаком с теми или иными тонкостями профессиональной лексики, могут возникнуть смысловые коллизии. Банальный пример — bus, bookmark или просто слова, которые в английском имеют несколько значений исходя из контекста (fire, right, set, date и так далее, список крайне длинный). Всегда необходимо помнить, что как минимум один из участников процесса локализации (если вы привлекаете к работе носителей), говорит и пишет на неродном для себя языке. Конечно, есть специалисты, чьи способности позволяют им общаться на уровне носителей сразу на нескольких языках, но это случается крайне редко и не по карману большинству команд. Будьте терпимее к локализаторам и помните, что от того, насколько четко и однозначно вы изъясняетесь с ними, зависит, насколько качественнее будет выполнена работа. Если вы планируете в ближайшее время локализовать какой-то из своих проектов, то рекомендуем воспользоваться нашим сервисом Lokalise. За три года разработки сервиса мы съели на локализации и переводах не одну стаю собак.\n","metric":0.6201171875,"metric_input":"Текст статьи:\nСоветы для тех, кто планирует заняться локализацией своего проекта / Habr             \n\n26-07-2017\nВопрос локализации является краеугольным камнем для множества команд разработчиков по всему миру. Особенно остро этот вопрос стоит, когда рынок продукта однозначно не определен и команда до конца не знает своего потребителя. Будем откровенны: если ваш продукт ориентирован на широкую аудиторию, то английского языка явно недостаточно. Конечно, существуют узкоспециализированные проекты и сервисы вроде нашего, когда знание главного международного языка не прихоть – необходимость, однако ниша подобных разработок крайне узка. И вот, тысячи команд по всему миру рано или поздно упираются в потолок одного-двух языков: один английский, а второй – родной для команды (если она не англоговорящая). Дальше начинаются споры, ссоры, попытки локализации и последующие пляски с бубном. В этой публикации мы собрали ряд популярных советов и рекомендаций как от частных разработчиков, так и от матерых команд уровня Mozilla, в которых более опытные товарищи делятся со своими коллегами опытом локализации проектов. Семь раз отмерь Одной из самых раздражающих ошибок в ходе локализации является «деревянная» верстка проекта, когда поля и кнопки не адаптируются под размер текста. Этим страдают все: от веб, до гейм-разработчиков. Чем серьезнее и сложнее ваш проект в плане кодовой части и чем сложнее добраться до размеров тех или иных элементов так, чтобы не поехала вся страница вместе с дизайном, тем внимательнее стоит относиться к размерности элементов. Многие опытные разработчики рекомендуют всегда оставлять минимум 30% запас (в каждую сторону) на каждый локализуемый элемент интерфейса: в зависимости от языковой группы размерность надписей может видоизменяться как в большую (языки романской группы), так и в меньшую (азиатские языки) сторону. Думаю, каждый хотя бы раз сталкивался с тем, что какая-то надпись банально не влезает в отведенное ей пространство. Как можно понять из текста выше, немалая роль в успешном проведении локализации лежит на дизайнерах. По мнению Standish Group лишь 20% фич и функций проекта обеспечивают его конкурентоспособность, но вот однозначно и объективно выявить эти самые ключевые «точки» самим разработчикам удается не всегда. Поэтому уделять внимание приходится всему и вся, ведь ошибка на критичном для потребителя, но не слишком приоритетном по мнению разработчиков участке проекта может сыграть решающую роль. Также необходимо понимать, что просто перевести весь проект на нужный язык — это не значит локализовать его. Разные языковые группы и регионы определяют разные паттерны поведения пользователей и то, что работает для англоязычной или русскоязычной аудитории, не будет работать в Азии и наоборот. Речь идет не только о содержании, но и о дизайнерских решениях, денежных единицах, формате дат и т.п. Важно не только снаружи, но и внутри Все сказанное выше, по сути — лирика для дизайнеров и проект-менеджеров. Команда же Mozilla, которая имеет весьма солидный опыт в локализации продуктов, дает более развернутые советы для разработчиков. Например, Mozilla Developer Network в своем блоге рекомендует в ходе разработки обращать внимание на мелочи и в общем быть опрятными. Так, по их мнению крайне важно выбирать адекватные имена для меток (ключей) в ходе локализации и мы, исходя из опыта наших клиентов, согласны с этим утверждением. Метки, вне зависимости от степени важности и роли, «всегда должны описывать строку и ее роль в интерфейсе». Если строка привязывается к какой-то особенной функции или всплывающему окну, то метка должна отражать и эту информацию. Еще один аспект, на который обращают внимание разработчики Mozilla — это то, что локализаторы часто работают без контекста, в отрыве от самого проекта. Для облегчения их участи следует оставлять пометки, которые бы давали им ориентир, в какую сторону двигаться. При этом все комментарии, опять таки, должны быть выполнены в едином формате и иметь однозначное содержание. Кстати, об однозначности мы упомянем позже. Фактически, организация локализации — это тест на внимательность и опрятность: если команда сможет его успешно пройти, то и локализация не доставит существенных проблем. Сложность данного квеста нелинейно возрастает с увеличением числа языков, и если для одностороннего перевода советы команды Mozilla могут показаться надуманными, то когда вам нужно перевести и адаптировать проект под 10-15 языков, они внезапно обретают смысл уровня библейских откровений. Как это не удивительно, но успешность процесса локализации напрямую зависит и от стиля самой разработки. Если в вашем проекте есть временные строки и блоки, которые, в перспективе (обозримой или не очень) подвергнутся изменениями, то от их локализации стоит воздержаться. Конечно, все мы знаем, что нет ничего более постоянного, чем временное, однако это правило срабатывает не всегда. Тут уж стоит руководствоваться законом Мерфи и ожидать всего (то есть худшего). Переводчики — наше все Для того, чтобы грамотно и адекватно организовать работу, команды локализаторов вышеописанного все еще недостаточно. И неважно, используете вы специализированный сервис по локализации продуктов вроде нашего или проводите эту процедуру по-старинке, вручную. Одна из самых коварных угроз на этапе локализаций поджидает в самом процессе коммуникации. Да, все верно: самые нелепые и непредсказуемые ошибки возникают тогда, когда люди до конца друг друга не понимают. И дело тут не в тупости разработчиков или непосредственных локализаторов, а в коммуникативном инструментарии. Очень часто локализацией занимаются так называемые «native speakers», или, если выражаться по-русски — носители языка. Коммуникация же происходит либо на языке первоисточника ресурса (если это прямая локализация с какого-то языка, отличного от английского, на какой-то третий язык), либо при помощи того же английского. Многие разработчики забывают, что привычная им англоязычная лексика отличается от общепринятой. Таким образом, общаясь с переводчиком, который не слишком плотно знаком с теми или иными тонкостями профессиональной лексики, могут возникнуть смысловые коллизии. Банальный пример — bus, bookmark или просто слова, которые в английском имеют несколько значений исходя из контекста (fire, right, set, date и так далее, список крайне длинный). Всегда необходимо помнить, что как минимум один из участников процесса локализации (если вы привлекаете к работе носителей), говорит и пишет на неродном для себя языке. Конечно, есть специалисты, чьи способности позволяют им общаться на уровне носителей сразу на нескольких языках, но это случается крайне редко и не по карману большинству команд. Будьте терпимее к локализаторам и помните, что от того, насколько четко и однозначно вы изъясняетесь с ними, зависит, насколько качественнее будет выполнена работа. Если вы планируете в ближайшее время локализовать какой-то из своих проектов, то рекомендуем воспользоваться нашим сервисом Lokalise. За три года разработки сервиса мы съели на локализации и переводах не одну стаю собак.\n\n\nКраткое содержание:\nВ статье рассматриваются важные аспекты процесса локализации проектов, включая верстку, дизайн, выбор ключевых функций и разработку коммуникационных стратегий. Авторы подчеркивают, что успешная локализация требует внимательности к деталям, четкости коммуникации и учета культурных различий. Они также рекомендуют использовать специализированные сервисы для локализации, такие как Lokalise, для обеспечения качественного и эффективного процесса.","summary":"В статье рассматриваются важные аспекты процесса локализации проектов, включая верстку, дизайн, выбор ключевых функций и разработку коммуникационных стратегий. Авторы подчеркивают, что успешная локализация требует внимательности к деталям, четкости коммуникации и учета культурных различий. Они также рекомендуют использовать специализированные сервисы для локализации, такие как Lokalise, для обеспечения качественного и эффективного процесса."}
{"Text":"Подборка: Более 70 источников по машинному обучению для начинающих / Habr            \n\n03-02-2016\nИндикатор кулачкового аналогового компьютера / Wiki В нашем блоге мы уже рассказывали о разработке системы квантовой связи и о том, как из простых студентов готовят продвинутых программистов. Сегодня мы решили вернуться к теме машинного обучения и привести адаптированную (источник) подборку полезных материалов. Это список предназначен для тех, кто только начинает изучать тему машинного обучения, например, с использованием Python (если вы хотите начать учить Python, вам в помощь эта статья). Машинное обучение – это лишь одна из математических дисциплин, связанных с понятием «данные». Чтобы разобраться в том, что такое аналитика данных, анализ данных, наука о данных, машинное обучение и большие данные, прочитайте этот материал. Вот инструменты, которые вам понадобятся: Python (Python 3 – наилучший вариант), IPython и Jupyter Notebook (IPython Notebook), Пакеты для расчетов: NumPy, Pandas, Scikit-Learn, Matplotlib. Вы можете установить Python 3 и все необходимые пакеты в несколько кликов с помощью сборки Anaconda Python. Anaconda – это достаточно популярный дистрибутив среди людей, занимающихся машинным обучением. Не страшно, если у вас установлен Python 2.7. Переходить на Python 3 нет необходимости. Вместо Anaconda вы можете воспользоваться pip или virtualenv. Не можете определиться? Прочитайте этот материал. Для начала познакомьтесь с IPython Notebook (это займет 5-10 минут). Еще можете посмотреть это видео. Далее рассмотрите небольшой пример (это займет 10 минут) классификации цифр с использованием библиотеки scikit-learn. Наглядное введение в теорию машинного обучения Давайте побольше узнаем о машинном обучении: об идеях и особенностях. Прочитайте статью Стефани Йи (Stephanie Yee) и Тони Чу (Tony Chu) «Наглядное введение в машинное обучение. Часть 1». Прочитайте статью профессора Педро Домингоса (Pedro Domingos). Во время чтения не торопитесь, делайте заметки. В статье можно выделить два основных момента: Одних только данных недостаточно. Домингос писал: «…нет ничего удивительного в том, что для обучения нужны знания. Машинное обучение не может получить что-то из ничего, но может получить большее из меньшего. Обучение похоже на сельское хозяйство, где большую часть работы делает природа. Фермеры дают семенам питательные вещества, чтобы вырастить урожай. Так и здесь: чтобы создать программу, нужно совместить знания и данные». Большое количество данных лучше детально продуманного алгоритма. Не пытайтесь изобретать велосипед и усложнять решения: выбирайте кратчайший путь, ведущий к цели. Домингос говорит: «Как правило, «глупый» алгоритм с большим количеством данных превосходит «умный» алгоритм с небольшим количеством данных. В машинном обучении главную роль всегда играют данные». Итак, знания и данные имеют решающее значение. Это означает, что усложнять алгоритмы нужно только тогда, когда у вас действительно нет выбора. Схема составлена на основе слайда из лекции Алекса Пинто (Alex Pinto) «Математика на страже безопасности: руководство по мониторингу с применением машинного обучения». Обучайтесь на примерах Выберете и рассмотрите один или два примера из представленных ниже. Распознавание лиц на фотографиях из базы данных сайта Labeled Faces in the Wild. Машинное обучение на основе данных о катастрофе Титаника. Здесь демонстрируются методы преобразования данных и их анализа, а также техники визуализации. Есть примеры методов машинного обучения с учителем. Прогнозирование итогов выборов: использование модели Нейта Сильвера (Nate Silver) для составления прогноза итогов выборов президента США в 2012 году, опубликованных The New York Times. Вот еще руководства и обзоры: Введение в машинное обучение с sklearn Обучение с помощью scikit-learn Другие источники, в которых можно найти блокноты IPython: Галерея интересных блокнотов IPython: статистика, машинное обучение и наука о данных. Большая галерея Фабиана Педрегозы (Fabian Pedregosa). Курсы по машинному обучению Будет полезно, если вы начнете работать над каким-нибудь небольшим самостоятельным проектом – так у вас будет возможность применить полученные знания на практике. Можете воспользоваться одним из этих наборов данных. Еще часто рекомендуют книгу «The Elements of Statistical Learning», но она, как правило, выступает в роли справочника. Книга бесплатная, поэтому скачайте ее или добавьте в закладки браузера. Еще есть вот эти онлайн-курсы: Курс «Машинное обучение» профессора Педро Домингоса из Вашингтонского университета. Практикум по науке о данных. Наука о данных. Видео «Введение в машинное обучение с scikit-learn» от Кевина Маркхэма (Kevin Markham). После просмотра видеоматериала, вы можете пройти интерактивный курс по науке о данных (есть его более ранние версии: 7, 5, 4, 3). Гарвардский курс CS109 – наука о данных. Продвинутый курс статистических вычислений (курс BIOS8366 университета Вандербильта). Отзывы о курсах и различные обсуждения: Ознакомьтесь с ответом Джека Голдинга (Jack Golding) на Quora. Там вы найдете ссылку на специализацию «Data Science» на Coursera – если вам не нужен сертификат, то можете пройти все 9 курсов бесплатно. Другое обсуждение на Quora: как стать специалистом по обработке и анализу данных? Большой перечень ресурсов по науке о данных от сайта Data Science Weekly, а также список открытых онлайн-курсов. Изучаем Pandas Чтобы работать с Python, вам необходимо познакомиться с пакетом Pandas. Вот список материалов, которые в этом помогут: Основное: знакомство с Pandas, Руководство: несколько вещей в Pandas, которые я бы хотел знать раньше (блокноты IPython), Полезные фрагменты кода Pandas, Вам также стоит уделить внимание этим ресурсам: Cookbook: репозиторий с примерами, Структуры данных: раздел DataFrame, Изменение формы данных путем транспонирования DataFrame, Вычислительные средства: моменты и функции, SE: о ковариации простыми словами, Groupby: как выполнить split-apply-combine на некотором подмножестве столбцов, Визуализация DataFrame. Еще больше материалов и статей Доступная книга Джона Формана (John Foreman) «Data Smart», Курс по науке о данных с блокнотами IPython, Статья: основные трудности раздела науки о данных (прочитайте статью и комментарий Джозефа Маккарти (Joseph McCarthy)), IPython: ключевые навыки специалистов по работе с данными. Вопросы, ответы, чаты На данный момент лучшим местом для поиска ответов на свои вопросы является раздел о машинном обучении на stackexchange.com. Также есть сабреддит: /r/machinelearning. Присоединяйтесь к каналу по scikit-learn на Gitter! Еще стоит обратить внимание на обсуждения на Quora и большой перечень материалов по науке о данных от сайта Data Science Weekly. Другие вещи, которые полезно знать Наука о данных: статья Джона Формана (John Foreman), специалиста по обработке и анализу данных в MailChimp. Статья: одиннадцать факторов, ведущих к переобучению, и как их избежать. Достойная статья: «Машинное обучение: накладные расходы, которые влечет за собой технический долг» («Machine Learning: The High-Interest Credit Card of Technical Debt»). Цель данной статьи: определить специфические факторы риска при машинном обучении и создать шаблоны, с помощью которых можно их избежать. Джон Форман: «Опасный мир машинного обучения». Kdnuggets: «Издержки систем машинного обучения». Вам нужна практика. Пользователь с ником Olympus на Hacker News отметил, что для этого необходимо участвовать в конкурсах и соревнованиях. Kaggle и ChaLearn – это платформы для исследователей, где можно попробовать свои силы, участвуя в различных состязаниях. Здесь вы найдете примеры кода для конкурса Kaggle. Еще вариант: HackerRank. Послушайте и почитайте, что победители конкурсов Kaggle говорят о предложенных ими решениях. Например, почитайте блог «No Free Hunch». Конкурсы или состязания лишь один из способов попрактиковаться. Вы можете начать проводить исследования: Начните с вопроса. «Самая важная вещь в науке о данных – это вопрос», – говорит доктор Джефф Лик (Dr. Jeff T. Leek). Начните с вопроса, затем найдите реальные данные и проанализируйте их. Огласите результаты и обратитесь за экспертной оценкой. Устраните найденные проблемы. Поделитесь своими открытиями. Подробнее о научном методе вы можете узнать здесь и здесь. Вот еще парочка руководств по машинному обучению: Машинное обучение для разработчиков: машинное обучение и библиотека Smile для Java и Scala. Материалы для изучения машинного обучения от Джека Симпсона (Jack Simpson). Гидеон Вульфсон: как обучить машину. Доктор Рэндал Олсон (Randal Olson): примеры машинного обучения, упражнения и руководство. Богатый раздел с дополнительной литературой.\n","metric":0.48046875,"metric_input":"Текст статьи:\nПодборка: Более 70 источников по машинному обучению для начинающих / Habr            \n\n03-02-2016\nИндикатор кулачкового аналогового компьютера / Wiki В нашем блоге мы уже рассказывали о разработке системы квантовой связи и о том, как из простых студентов готовят продвинутых программистов. Сегодня мы решили вернуться к теме машинного обучения и привести адаптированную (источник) подборку полезных материалов. Это список предназначен для тех, кто только начинает изучать тему машинного обучения, например, с использованием Python (если вы хотите начать учить Python, вам в помощь эта статья). Машинное обучение – это лишь одна из математических дисциплин, связанных с понятием «данные». Чтобы разобраться в том, что такое аналитика данных, анализ данных, наука о данных, машинное обучение и большие данные, прочитайте этот материал. Вот инструменты, которые вам понадобятся: Python (Python 3 – наилучший вариант), IPython и Jupyter Notebook (IPython Notebook), Пакеты для расчетов: NumPy, Pandas, Scikit-Learn, Matplotlib. Вы можете установить Python 3 и все необходимые пакеты в несколько кликов с помощью сборки Anaconda Python. Anaconda – это достаточно популярный дистрибутив среди людей, занимающихся машинным обучением. Не страшно, если у вас установлен Python 2.7. Переходить на Python 3 нет необходимости. Вместо Anaconda вы можете воспользоваться pip или virtualenv. Не можете определиться? Прочитайте этот материал. Для начала познакомьтесь с IPython Notebook (это займет 5-10 минут). Еще можете посмотреть это видео. Далее рассмотрите небольшой пример (это займет 10 минут) классификации цифр с использованием библиотеки scikit-learn. Наглядное введение в теорию машинного обучения Давайте побольше узнаем о машинном обучении: об идеях и особенностях. Прочитайте статью Стефани Йи (Stephanie Yee) и Тони Чу (Tony Chu) «Наглядное введение в машинное обучение. Часть 1». Прочитайте статью профессора Педро Домингоса (Pedro Domingos). Во время чтения не торопитесь, делайте заметки. В статье можно выделить два основных момента: Одних только данных недостаточно. Домингос писал: «…нет ничего удивительного в том, что для обучения нужны знания. Машинное обучение не может получить что-то из ничего, но может получить большее из меньшего. Обучение похоже на сельское хозяйство, где большую часть работы делает природа. Фермеры дают семенам питательные вещества, чтобы вырастить урожай. Так и здесь: чтобы создать программу, нужно совместить знания и данные». Большое количество данных лучше детально продуманного алгоритма. Не пытайтесь изобретать велосипед и усложнять решения: выбирайте кратчайший путь, ведущий к цели. Домингос говорит: «Как правило, «глупый» алгоритм с большим количеством данных превосходит «умный» алгоритм с небольшим количеством данных. В машинном обучении главную роль всегда играют данные». Итак, знания и данные имеют решающее значение. Это означает, что усложнять алгоритмы нужно только тогда, когда у вас действительно нет выбора. Схема составлена на основе слайда из лекции Алекса Пинто (Alex Pinto) «Математика на страже безопасности: руководство по мониторингу с применением машинного обучения». Обучайтесь на примерах Выберете и рассмотрите один или два примера из представленных ниже. Распознавание лиц на фотографиях из базы данных сайта Labeled Faces in the Wild. Машинное обучение на основе данных о катастрофе Титаника. Здесь демонстрируются методы преобразования данных и их анализа, а также техники визуализации. Есть примеры методов машинного обучения с учителем. Прогнозирование итогов выборов: использование модели Нейта Сильвера (Nate Silver) для составления прогноза итогов выборов президента США в 2012 году, опубликованных The New York Times. Вот еще руководства и обзоры: Введение в машинное обучение с sklearn Обучение с помощью scikit-learn Другие источники, в которых можно найти блокноты IPython: Галерея интересных блокнотов IPython: статистика, машинное обучение и наука о данных. Большая галерея Фабиана Педрегозы (Fabian Pedregosa). Курсы по машинному обучению Будет полезно, если вы начнете работать над каким-нибудь небольшим самостоятельным проектом – так у вас будет возможность применить полученные знания на практике. Можете воспользоваться одним из этих наборов данных. Еще часто рекомендуют книгу «The Elements of Statistical Learning», но она, как правило, выступает в роли справочника. Книга бесплатная, поэтому скачайте ее или добавьте в закладки браузера. Еще есть вот эти онлайн-курсы: Курс «Машинное обучение» профессора Педро Домингоса из Вашингтонского университета. Практикум по науке о данных. Наука о данных. Видео «Введение в машинное обучение с scikit-learn» от Кевина Маркхэма (Kevin Markham). После просмотра видеоматериала, вы можете пройти интерактивный курс по науке о данных (есть его более ранние версии: 7, 5, 4, 3). Гарвардский курс CS109 – наука о данных. Продвинутый курс статистических вычислений (курс BIOS8366 университета Вандербильта). Отзывы о курсах и различные обсуждения: Ознакомьтесь с ответом Джека Голдинга (Jack Golding) на Quora. Там вы найдете ссылку на специализацию «Data Science» на Coursera – если вам не нужен сертификат, то можете пройти все 9 курсов бесплатно. Другое обсуждение на Quora: как стать специалистом по обработке и анализу данных? Большой перечень ресурсов по науке о данных от сайта Data Science Weekly, а также список открытых онлайн-курсов. Изучаем Pandas Чтобы работать с Python, вам необходимо познакомиться с пакетом Pandas. Вот список материалов, которые в этом помогут: Основное: знакомство с Pandas, Руководство: несколько вещей в Pandas, которые я бы хотел знать раньше (блокноты IPython), Полезные фрагменты кода Pandas, Вам также стоит уделить внимание этим ресурсам: Cookbook: репозиторий с примерами, Структуры данных: раздел DataFrame, Изменение формы данных путем транспонирования DataFrame, Вычислительные средства: моменты и функции, SE: о ковариации простыми словами, Groupby: как выполнить split-apply-combine на некотором подмножестве столбцов, Визуализация DataFrame. Еще больше материалов и статей Доступная книга Джона Формана (John Foreman) «Data Smart», Курс по науке о данных с блокнотами IPython, Статья: основные трудности раздела науки о данных (прочитайте статью и комментарий Джозефа Маккарти (Joseph McCarthy)), IPython: ключевые навыки специалистов по работе с данными. Вопросы, ответы, чаты На данный момент лучшим местом для поиска ответов на свои вопросы является раздел о машинном обучении на stackexchange.com. Также есть сабреддит: /r/machinelearning. Присоединяйтесь к каналу по scikit-learn на Gitter! Еще стоит обратить внимание на обсуждения на Quora и большой перечень материалов по науке о данных от сайта Data Science Weekly. Другие вещи, которые полезно знать Наука о данных: статья Джона Формана (John Foreman), специалиста по обработке и анализу данных в MailChimp. Статья: одиннадцать факторов, ведущих к переобучению, и как их избежать. Достойная статья: «Машинное обучение: накладные расходы, которые влечет за собой технический долг» («Machine Learning: The High-Interest Credit Card of Technical Debt»). Цель данной статьи: определить специфические факторы риска при машинном обучении и создать шаблоны, с помощью которых можно их избежать. Джон Форман: «Опасный мир машинного обучения». Kdnuggets: «Издержки систем машинного обучения». Вам нужна практика. Пользователь с ником Olympus на Hacker News отметил, что для этого необходимо участвовать в конкурсах и соревнованиях. Kaggle и ChaLearn – это платформы для исследователей, где можно попробовать свои силы, участвуя в различных состязаниях. Здесь вы найдете примеры кода для конкурса Kaggle. Еще вариант: HackerRank. Послушайте и почитайте, что победители конкурсов Kaggle говорят о предложенных ими решениях. Например, почитайте блог «No Free Hunch». Конкурсы или состязания лишь один из способов попрактиковаться. Вы можете начать проводить исследования: Начните с вопроса. «Самая важная вещь в науке о данных – это вопрос», – говорит доктор Джефф Лик (Dr. Jeff T. Leek). Начните с вопроса, затем найдите реальные данные и проанализируйте их. Огласите результаты и обратитесь за экспертной оценкой. Устраните найденные проблемы. Поделитесь своими открытиями. Подробнее о научном методе вы можете узнать здесь и здесь. Вот еще парочка руководств по машинному обучению: Машинное обучение для разработчиков: машинное обучение и библиотека Smile для Java и Scala. Материалы для изучения машинного обучения от Джека Симпсона (Jack Simpson). Гидеон Вульфсон: как обучить машину. Доктор Рэндал Олсон (Randal Olson): примеры машинного обучения, упражнения и руководство. Богатый раздел с дополнительной литературой.\n\n\nКраткое содержание:\nВ данной статье приводится подборка материалов для начинающих изучать машинное обучение. В подборку включены ссылки на Python, IPython и Jupyter Notebook, а также пакеты для расчетов, такие как NumPy, Pandas, Scikit-Learn и Matplotlib. Также представлены ссылки на статьи и видеоматериалы, которые помогут разобраться в теории машинного обучения и его применении.","summary":"В данной статье приводится подборка материалов для начинающих изучать машинное обучение. В подборку включены ссылки на Python, IPython и Jupyter Notebook, а также пакеты для расчетов, такие как NumPy, Pandas, Scikit-Learn и Matplotlib. Также представлены ссылки на статьи и видеоматериалы, которые помогут разобраться в теории машинного обучения и его применении."}
{"Text":"PHPUnit и его Database Extension. Беглый взгляд / Хабр                 \n\n09-06-2009\nПространное и многословное вступление Уже чуть более года в проекте где я работаю ходят разговоры о модульном тестировании. Помимо разговоров неоднократно делались попытки превратить эти разговоры в жизнь. Все попытки на данный момент закончились тем, что ни один ранее написанный модульный тест не прогоняется в процессе разработки. Все они лежат мертвым кодом в недрах нашей системы. Представили такой себе постсоветский индастриэл? Торчащие колонны из земли, ржавая арматура на фоне хмурого неба:) Некоторые тесты не используются ввиду того что тестируемый функционал так и не вышел в продакшн и на годы осел в файлах и директориях, а некоторые просто всех утомили тем хаосом который там был сотворен. Вся та произведенная нами на свет энтропия стала результатом нашей некомпетентности в модульном тестировании и не системного подхода к их внедрению. В наших тестах нарушены практически все принципы модульного тестирования. Начиная с того что они зависят один от другого заканчивая тем что в них можно увидеть разбухшие от подготовительной работы тестовые методы пестрящие sql-синтаксисом и многое другое… Плюс ко всему выше названному, почти все предыдущие попытки внедрения модульного тестирования были в большей мере самодеятельностью не особо санкционированной заказчиком, потому как только происходил затык тесты бросались и функционал начинал писаться в отрыве от тестов, и стоит ли говорить о том что к дописыванию тестов возвращаться не собирался никто. Плавный переход к практической стороне вопроса Это все лирика. Теперь пойдет больше по делу. Как можно понять проект написан на php, не маленький и живет уже по меньшей мере больше года(а в действительности 4 года). Настали времена когда даже самые упрямые и «технически грамотные» заказчики стали понимать необходимость в качестве кода на котором так настаивают сочувствующие PM-ы. Сверху пришла «указивка» заняться модульным тестированием по настоящему. Даже задачи в жиру положили соответствующие. Итак основываясь на нашем опыте прежних неудач мы выделили ряд проблем: Проблема фикстур. Не все что берется из базы можно заменить моками. Связанно это с некоторой безсистемностью работы с базой. У нас нет ORM и все функции последнего между делом выполняет модель. Скорость прогона тестов. Последняя попытка внедрения захлебнулась отчасти из-за этого, а отчасти и … Поддерживаемость. … из-за неподдерживаемости причиной которой стала проблема из п.1 решая которую мы оснастили наши тесты многострочными инсертами и апдейтами. Мы обратились к компетентному в этом вопросе коллеге — Java-разработчику. Он то нам и поведал о таких инструментах как JUnit и DbUnit. Он же нам и рассказал о процессе который они используют с участием этого инструментария: во время разработки они прогоняют тесты на in memory базах данных, решая таким образом задачу скорости работы тестов увеличивая вероятность их более частого запуска, перед коммитами они запускают те же тесты на реальной базе и потом еще добивают баги continious integration системой. xUnit фреймворк(SimpleTest) мы использовали и ранее, а вот о DbUnit мы слышали впервые. Собственно о нем и пойдет речь далее. Для php есть реализация DbUnit в виде расширения к PHPUnit. Смысл его использования в том что все фикстуры готовятся в виде xml-файлов и заливаются в тестовую базу перед запуском каждого тестового метода. Это по моему мнению решает задачу поддерживаемости фикстур, потому как все же xml более читаемый чем sql-дампы. Практический пример Предположим есть у нас таблица person CREATE TABLE person ( id INTEGER PRIMARY KEY AUTOINCREMENT , name VARCHAR ( 255 ) , surname VARCHAR ( 255 ) ) ; Для того что бы что-то в ней тестировать нужно что бы она была наполнена какими-то данными. В Database Extension PHPUnit наполнение описывается в xml формате, по одному элементу на каждую строку в таблице. Имя элемента должно соответствовать имени таблицы, а имена атрибутов — именам полей таблицы. Создадим такой себе persons.xml с фикстурой для этой таблицы: <?xml version = \"1.0\" encoding = \"UTF-8\" ?> <dataset > <person id = \"1\" name = \"Nalabal\" surname = \"Nadjava\" /> </dataset > Теперь внимание тест кейс: require_once 'PHPUnit/Extensions/Database/TestCase.php' ; require_once 'PHPUnit/Framework.php' ; class PersonTest extends PHPUnit_Extensions_Database_TestCase { public function __construct ( ) { $this -> connection = new PDO ( 'sqlite::memory:' ) ; $this -> connection -> query ( \" CREATE TABLE person ( id INTEGER PRIMARY KEY AUTOINCREMENT, name VARCHAR(255), surname VARCHAR(255) ); \" ) ; } protected function getConnection ( ) { return $this -> createDefaultDBConnection ( $this -> connection , 'sqlite' ) ; } protected function getDataSet ( ) { return $this -> createFlatXMLDataSet ( dirname ( __FILE__ ) . '/persons.xml' ) ; } } Очевидно — тест ничего не тестирует, но что то там мы уже понакодили: Он унаследован от PHPUnit_Extensions_Database_TestCase в котором уже реализованы некоторые методы, в том числе setUp (очищает базу и заполняет ее вновь) и tearDown — который делает тоже самое. В доке по PHPUnit более детально расписано, то как эти методы взаимодействуют с тестовыми методами. В конструкторе для простоты я создаю вышеуказанную табличку и в inmemory базе данных. После я переопределяю два абстрактных метода Database_TestCase. Первый возвращает объект соединения с базой, а второй объект созданный из нашей xml-фикстуры. Эти методы используются в определенных в Database_TestCase setUp и tearDown методах. Теперь проверим, добавив новый метод public function testPerson ( ) { $sql = \"SELECT * FROM person\" ; $statement = $this -> getConnection ( ) -> getConnection ( ) -> query ( $sql ) ; $result = $statement -> fetchAll ( ) ; $this -> assertEquals ( 1 , sizeof ( $result ) ) ; $this -> assertEquals ( 'Nalabal' , $result [ 0 0 ] [ 'name' ] ) ; } Результат: OK (1 test, 2 assertions) В методе выбираем все записи из таблицы persons и проверяем выборку на соответствие нашей фикстуре Мне в конкретном случае понадобилось добавлять некоторые данные во время выполнения конкретного тестового метода, не найдя ничего в доке я поковырялся слегонь в коде и пришел к следующему решению: Добавляем новую фикстуру additionalPersons.xml <?xml version = \"1.0\" encoding = \"UTF-8\" ?> <dataset > <person id = \"2\" name = \"Nageneril\" surname = \"Mudrapragram\" /> <person id = \"3\" name = \"Strudomprassal\" surname = \"Vashapragram\" /> </dataset > И пишем такой тестовый метод: public function testAdditionalPerson ( ) { $insertOperation = PHPUnit_Extensions_Database_Operation_Factory :: INSERT ( ) ; $insertOperation -> execute ( $this -> getConnection ( ) , $this -> createFlatXMLDataSet ( dirname ( __FILE__ ) . '/additionalPersons.xml' ) ) ; $sql = \"SELECT * FROM person\" ; $statement = $this -> getConnection ( ) -> getConnection ( ) -> query ( $sql ) ; $result = $statement -> fetchAll ( ) ; $this -> assertEquals ( 3 , sizeof ( $result ) ) ; $this -> assertEquals ( 'Nalabal' , $result [ 0 0 ] [ 'name' ] ) ; $this -> assertEquals ( 'Nageneril' , $result [ 1 ] [ 'name' ] ) ; $this -> assertEquals ( 'Strudomprassal' , $result [ 2 ] [ 'name' ] ) ; } В 3й строке создаем с помощью Operation_Factory (вопрос с Операциями в официальной доке пока еще не описан) объект вставки который реализовывает интерфейс PHPUnit_Extensions_Database_Operation_IDatabaseOperation у которого есть только один метод имеющий следующую сигнатуру: public function execute ( PHPUnit_Extensions_Database_DB_IDatabaseConnection $connection , PHPUnit_Extensions_Database_DataSet_IDataSet $dataSet ) ; Его то мы и вызываем в 4й строке предыдущего листинга, а он в свою очередь дополняет нашу первоначальную фикстуру из persons.xml данными из additionalPersons.xml Результатом запуска будет: OK (2 tests, 6 assertions) Заключение Вероятно что в жизни все не так сложно как в этом примере. Потому прошу не поддавать искусственность описываемого критике. По мере развития ситуации мне хотелось бы посвящать Вас во все допустимые детали как возможность запомнить все раскопанное и неописанное в документации. Конечно же, только при условии, что это кому-нибудь нужно(это я в скором времени пойму).\n","metric":0.44091796880000000414,"metric_input":"Текст статьи:\nPHPUnit и его Database Extension. Беглый взгляд / Хабр                 \n\n09-06-2009\nПространное и многословное вступление Уже чуть более года в проекте где я работаю ходят разговоры о модульном тестировании. Помимо разговоров неоднократно делались попытки превратить эти разговоры в жизнь. Все попытки на данный момент закончились тем, что ни один ранее написанный модульный тест не прогоняется в процессе разработки. Все они лежат мертвым кодом в недрах нашей системы. Представили такой себе постсоветский индастриэл? Торчащие колонны из земли, ржавая арматура на фоне хмурого неба:) Некоторые тесты не используются ввиду того что тестируемый функционал так и не вышел в продакшн и на годы осел в файлах и директориях, а некоторые просто всех утомили тем хаосом который там был сотворен. Вся та произведенная нами на свет энтропия стала результатом нашей некомпетентности в модульном тестировании и не системного подхода к их внедрению. В наших тестах нарушены практически все принципы модульного тестирования. Начиная с того что они зависят один от другого заканчивая тем что в них можно увидеть разбухшие от подготовительной работы тестовые методы пестрящие sql-синтаксисом и многое другое… Плюс ко всему выше названному, почти все предыдущие попытки внедрения модульного тестирования были в большей мере самодеятельностью не особо санкционированной заказчиком, потому как только происходил затык тесты бросались и функционал начинал писаться в отрыве от тестов, и стоит ли говорить о том что к дописыванию тестов возвращаться не собирался никто. Плавный переход к практической стороне вопроса Это все лирика. Теперь пойдет больше по делу. Как можно понять проект написан на php, не маленький и живет уже по меньшей мере больше года(а в действительности 4 года). Настали времена когда даже самые упрямые и «технически грамотные» заказчики стали понимать необходимость в качестве кода на котором так настаивают сочувствующие PM-ы. Сверху пришла «указивка» заняться модульным тестированием по настоящему. Даже задачи в жиру положили соответствующие. Итак основываясь на нашем опыте прежних неудач мы выделили ряд проблем: Проблема фикстур. Не все что берется из базы можно заменить моками. Связанно это с некоторой безсистемностью работы с базой. У нас нет ORM и все функции последнего между делом выполняет модель. Скорость прогона тестов. Последняя попытка внедрения захлебнулась отчасти из-за этого, а отчасти и … Поддерживаемость. … из-за неподдерживаемости причиной которой стала проблема из п.1 решая которую мы оснастили наши тесты многострочными инсертами и апдейтами. Мы обратились к компетентному в этом вопросе коллеге — Java-разработчику. Он то нам и поведал о таких инструментах как JUnit и DbUnit. Он же нам и рассказал о процессе который они используют с участием этого инструментария: во время разработки они прогоняют тесты на in memory базах данных, решая таким образом задачу скорости работы тестов увеличивая вероятность их более частого запуска, перед коммитами они запускают те же тесты на реальной базе и потом еще добивают баги continious integration системой. xUnit фреймворк(SimpleTest) мы использовали и ранее, а вот о DbUnit мы слышали впервые. Собственно о нем и пойдет речь далее. Для php есть реализация DbUnit в виде расширения к PHPUnit. Смысл его использования в том что все фикстуры готовятся в виде xml-файлов и заливаются в тестовую базу перед запуском каждого тестового метода. Это по моему мнению решает задачу поддерживаемости фикстур, потому как все же xml более читаемый чем sql-дампы. Практический пример Предположим есть у нас таблица person CREATE TABLE person ( id INTEGER PRIMARY KEY AUTOINCREMENT , name VARCHAR ( 255 ) , surname VARCHAR ( 255 ) ) ; Для того что бы что-то в ней тестировать нужно что бы она была наполнена какими-то данными. В Database Extension PHPUnit наполнение описывается в xml формате, по одному элементу на каждую строку в таблице. Имя элемента должно соответствовать имени таблицы, а имена атрибутов — именам полей таблицы. Создадим такой себе persons.xml с фикстурой для этой таблицы: <?xml version = \"1.0\" encoding = \"UTF-8\" ?> <dataset > <person id = \"1\" name = \"Nalabal\" surname = \"Nadjava\" /> </dataset > Теперь внимание тест кейс: require_once 'PHPUnit/Extensions/Database/TestCase.php' ; require_once 'PHPUnit/Framework.php' ; class PersonTest extends PHPUnit_Extensions_Database_TestCase { public function __construct ( ) { $this -> connection = new PDO ( 'sqlite::memory:' ) ; $this -> connection -> query ( \" CREATE TABLE person ( id INTEGER PRIMARY KEY AUTOINCREMENT, name VARCHAR(255), surname VARCHAR(255) ); \" ) ; } protected function getConnection ( ) { return $this -> createDefaultDBConnection ( $this -> connection , 'sqlite' ) ; } protected function getDataSet ( ) { return $this -> createFlatXMLDataSet ( dirname ( __FILE__ ) . '/persons.xml' ) ; } } Очевидно — тест ничего не тестирует, но что то там мы уже понакодили: Он унаследован от PHPUnit_Extensions_Database_TestCase в котором уже реализованы некоторые методы, в том числе setUp (очищает базу и заполняет ее вновь) и tearDown — который делает тоже самое. В доке по PHPUnit более детально расписано, то как эти методы взаимодействуют с тестовыми методами. В конструкторе для простоты я создаю вышеуказанную табличку и в inmemory базе данных. После я переопределяю два абстрактных метода Database_TestCase. Первый возвращает объект соединения с базой, а второй объект созданный из нашей xml-фикстуры. Эти методы используются в определенных в Database_TestCase setUp и tearDown методах. Теперь проверим, добавив новый метод public function testPerson ( ) { $sql = \"SELECT * FROM person\" ; $statement = $this -> getConnection ( ) -> getConnection ( ) -> query ( $sql ) ; $result = $statement -> fetchAll ( ) ; $this -> assertEquals ( 1 , sizeof ( $result ) ) ; $this -> assertEquals ( 'Nalabal' , $result [ 0 0 ] [ 'name' ] ) ; } Результат: OK (1 test, 2 assertions) В методе выбираем все записи из таблицы persons и проверяем выборку на соответствие нашей фикстуре Мне в конкретном случае понадобилось добавлять некоторые данные во время выполнения конкретного тестового метода, не найдя ничего в доке я поковырялся слегонь в коде и пришел к следующему решению: Добавляем новую фикстуру additionalPersons.xml <?xml version = \"1.0\" encoding = \"UTF-8\" ?> <dataset > <person id = \"2\" name = \"Nageneril\" surname = \"Mudrapragram\" /> <person id = \"3\" name = \"Strudomprassal\" surname = \"Vashapragram\" /> </dataset > И пишем такой тестовый метод: public function testAdditionalPerson ( ) { $insertOperation = PHPUnit_Extensions_Database_Operation_Factory :: INSERT ( ) ; $insertOperation -> execute ( $this -> getConnection ( ) , $this -> createFlatXMLDataSet ( dirname ( __FILE__ ) . '/additionalPersons.xml' ) ) ; $sql = \"SELECT * FROM person\" ; $statement = $this -> getConnection ( ) -> getConnection ( ) -> query ( $sql ) ; $result = $statement -> fetchAll ( ) ; $this -> assertEquals ( 3 , sizeof ( $result ) ) ; $this -> assertEquals ( 'Nalabal' , $result [ 0 0 ] [ 'name' ] ) ; $this -> assertEquals ( 'Nageneril' , $result [ 1 ] [ 'name' ] ) ; $this -> assertEquals ( 'Strudomprassal' , $result [ 2 ] [ 'name' ] ) ; } В 3й строке создаем с помощью Operation_Factory (вопрос с Операциями в официальной доке пока еще не описан) объект вставки который реализовывает интерфейс PHPUnit_Extensions_Database_Operation_IDatabaseOperation у которого есть только один метод имеющий следующую сигнатуру: public function execute ( PHPUnit_Extensions_Database_DB_IDatabaseConnection $connection , PHPUnit_Extensions_Database_DataSet_IDataSet $dataSet ) ; Его то мы и вызываем в 4й строке предыдущего листинга, а он в свою очередь дополняет нашу первоначальную фикстуру из persons.xml данными из additionalPersons.xml Результатом запуска будет: OK (2 tests, 6 assertions) Заключение Вероятно что в жизни все не так сложно как в этом примере. Потому прошу не поддавать искусственность описываемого критике. По мере развития ситуации мне хотелось бы посвящать Вас во все допустимые детали как возможность запомнить все раскопанное и неописанное в документации. Конечно же, только при условии, что это кому-нибудь нужно(это я в скором времени пойму).\n\n\nКраткое содержание:\nВ статье рассматривается использование PHPUnit и его Database Extension для модульного тестирования в проекте, где ранее не проводилось модульное тестирование. Авторы статьи описывают проблемы, с которыми они столкнулись, такие как проблемы с фикстурами, скорость прогона тестов и поддерживаемость фикстур. Они также описывают использование DbUnit в качестве инструмента для подготовки фикстур в формате XML и их загрузки в тестовую базу данных. В статье приводится практический пример использования PHPUnit и DbUnit для тестирования данных в таблице \"person\". Авторы также описывают, как можно добавить дополнительные фикстуры и протестировать их с помощью PHPUnit. В заключение, авторы отмечают, что использование PHPUnit и DbUnit может помочь улучшить модульное тестирование в проекте и обеспечить более высокое качество кода.","summary":"В статье рассматривается использование PHPUnit и его Database Extension для модульного тестирования в проекте, где ранее не проводилось модульное тестирование. Авторы статьи описывают проблемы, с которыми они столкнулись, такие как проблемы с фикстурами, скорость прогона тестов и поддерживаемость фикстур. Они также описывают использование DbUnit в качестве инструмента для подготовки фикстур в формате XML и их загрузки в тестовую базу данных. В статье приводится практический пример использования PHPUnit и DbUnit для тестирования данных в таблице \"person\". Авторы также описывают, как можно добавить дополнительные фикстуры и протестировать их с помощью PHPUnit. В заключение, авторы отмечают, что использование PHPUnit и DbUnit может помочь улучшить модульное тестирование в проекте и обеспечить более высокое качество кода."}
{"Text":"Найдено ещё 10 землеподобных планет в обитаемой зоне / Habr            \n\n20-06-2017\nНаучная группа космического телескопа «Кеплер» выпустила восьмую и окончательную версию каталога планет-кандидатов, в котором список увеличился на 219 планет и достиг 4034 планет-кандидатов. Десять из них — земного размера и находятся в зоне обитаемости. То есть в диапазоне расстояний до звезды, которое допускает существование воды в жидкой фазе на твёрдой поверхности планеты. На этом обработка данных с первого этапа миссии «Кеплера» закончена. Из 4034 планет-кандидатов 2335 подтверждены как экзопланеты, а примерно 50 детектированы как землеподобные планеты в обитаемой зоне. Из них для более 30 уже получено подтверждение. Восьмой каталог содержит самые полные данные, собранные за первые четыре года работы телескопа «Кеплер». Это последний каталог, куда включена информация из этого участка неба в созвездии Лебедя. Для финального каталога все данные, собранные на первом этапе миссии, были заново обработаны наиболее продвинутыми аналитическими методами, полностью автоматически. Чтобы убедиться в том, что никакие планеты не упущены из анализа, в систему добавили сигнал от поддельных планет — и проверили, как система его выявляет. Телескоп наблюдал ограниченный участок звёздного неба. Но собранная информация имеет важнейшее научное значение. Она позволяет оценить количество планет землеподобного типа в галактике — планет, на которых может существовать вода, может возникнуть своя жизнь и которые пригодны для жизни людей. Кроме того, данные «Кеплера» дают основания полагать, что малые планеты в космосе разбиваются на два чётких типа. Примерно половина из них — это каменистые планеты, похожие на Землю. А вторая половина — газообразные планеты меньше Нептуна, вряд ли способные поддерживать жизнь. Между этими двумя группами находится очень мало планет. К такому выводу пришла одна группа учёных, которая опубликовала научную работу, основываясь на данных с «Кеплера». Используя мощный телескоп в обсерватории Кека, они измерили с высокой точностью радиус 2000 «кеплеровских» планет и 1300 звёзд из его поля зрения. Судя по всему, природа склонна делать каменистыми планеты размером до 175% земного. Учёные пока не могут понять, почему примерно половина из планет берут в себя небольшое количество водорода и гелия, что драматически раздувает их размеры и приближает их к размерам Нептуна. Но всё равно планеты земного размера в галактике встречаются очень часто. Десять новых планет-кандидатов в обитаемой зоне из кеплеровского каталога показаны на иллюстрации. Все они имеют размер менее 200% от земного. Консервативная обитаемая зона изображена светло-зелёным полем, оптимистичная область добавлена тёмно-зелёным. Почти все планеты находятся на орбите звёзд с температурой, близкой к Солнцу (около 5800 K). Эти открытия имеют важное значение в наших дальнейших поисках внеземной жизни. Телескоп войдёт в историю — он впервые показал планеты, такие как Земля, примерно такого же размера и на такой же орбите. Такие данные отсутствуют в результатах работы любого другого телескопа. «Кеплер» единственный способен находить планеты на относительно далёкой орбите от родной звезды, с сидерическими периодами более 100 дней. Каталог «Кеплера» станет фундаментом для дальнейшего изучения, насколько распространёнными являются экзопланеты в галактике, а также на какие типы они разбиваются. «Понимание частоты их встречаемости в галактике поможет в проектировании будущих миссий НАСА по прямой съёмке другой Земли», — сказал Марио Перес (Mario Perez), учёный программы «Кеплер» из отдела астрофизики Управления научными миссиями НАСА. На иллюстрации показано поле зрение телескопа «Кеплер» Каждый прямоугольник соответствует конкретному региону, который обрабатывается каждым ПЗС-элементом фотометра «Кеплер». На телескопе установлено 42 ПЗС-элемента размером 5×2,5 см, разрешением 2200×1024 пикселей, попарно. Каждая пара образует квадрат, а общее разрешение матриц составляет 95 мегапикселей. Данные с матрицы снимаются каждые 6 секунд, достигают предела насыщения и суммируются в бортовом компьютере в течение 30 секунд для каждого пикселя. Полоса пропускания приёмника составляет 430-890 нм. Для наблюдения доступны звёзды до 16-й звёздной величины Астрономический спутник с космическим телескопом «Кеплер» — первый в истории аппарат, созданный для поиска экзопланет. Обсерватория могла одновременно наблюдать более чем 100 тыс. звёзд. Запуск аппарата состоялся 6 марта 2009 года, 12 мая 2013 года «Кеплер» вышел из строя, а 11 апреля 2016 года контроль над телескопом был восстановлен. Во второй части миссии после восстановления контроля «Кеплер» продолжит наблюдения за другими участками неба, поиск интересных астрономических объектов и более подробное исследование интересных систем, таких как TRAPPIST-1 с семью землеподобными планетами. Телескоп «Кеплер» ищет планеты методом транзитной фотометрии, основываясь на наблюдениях за прохождением планеты на фоне звезды. Такой метод позволяет определить размеры планеты, а в сочетании с методом Доплера — плотность планеты. Он также даёт информацию о наличии и составе атмосферы. Подробную информацию по каждой планете можно изучить на сайте NASA Exoplanet Archive. В этом хранилище данных собрана информация не только с «Кеплера», но и с других инструментов. Например, ниже статистика по всем экзопланетам, найденным на сегодняшний день. «Нам нравится думать, что в этом исследовании планеты классифицируются таким же образом, каким биологи идентифицируют новые виды животных, — говорит Бенджамин Фултон (Benjamin Fulton), кандидат в докторантуру Гавайского университета в Маноа и ведущий автор научной работы о классификации «кеплеровских» планет. — Нахождение двух отдельных групп экзопланет как открытие, что млекопитающие и ящеры составляют две ветви эволюции».\n","metric":0.60009765630000000414,"metric_input":"Текст статьи:\nНайдено ещё 10 землеподобных планет в обитаемой зоне / Habr            \n\n20-06-2017\nНаучная группа космического телескопа «Кеплер» выпустила восьмую и окончательную версию каталога планет-кандидатов, в котором список увеличился на 219 планет и достиг 4034 планет-кандидатов. Десять из них — земного размера и находятся в зоне обитаемости. То есть в диапазоне расстояний до звезды, которое допускает существование воды в жидкой фазе на твёрдой поверхности планеты. На этом обработка данных с первого этапа миссии «Кеплера» закончена. Из 4034 планет-кандидатов 2335 подтверждены как экзопланеты, а примерно 50 детектированы как землеподобные планеты в обитаемой зоне. Из них для более 30 уже получено подтверждение. Восьмой каталог содержит самые полные данные, собранные за первые четыре года работы телескопа «Кеплер». Это последний каталог, куда включена информация из этого участка неба в созвездии Лебедя. Для финального каталога все данные, собранные на первом этапе миссии, были заново обработаны наиболее продвинутыми аналитическими методами, полностью автоматически. Чтобы убедиться в том, что никакие планеты не упущены из анализа, в систему добавили сигнал от поддельных планет — и проверили, как система его выявляет. Телескоп наблюдал ограниченный участок звёздного неба. Но собранная информация имеет важнейшее научное значение. Она позволяет оценить количество планет землеподобного типа в галактике — планет, на которых может существовать вода, может возникнуть своя жизнь и которые пригодны для жизни людей. Кроме того, данные «Кеплера» дают основания полагать, что малые планеты в космосе разбиваются на два чётких типа. Примерно половина из них — это каменистые планеты, похожие на Землю. А вторая половина — газообразные планеты меньше Нептуна, вряд ли способные поддерживать жизнь. Между этими двумя группами находится очень мало планет. К такому выводу пришла одна группа учёных, которая опубликовала научную работу, основываясь на данных с «Кеплера». Используя мощный телескоп в обсерватории Кека, они измерили с высокой точностью радиус 2000 «кеплеровских» планет и 1300 звёзд из его поля зрения. Судя по всему, природа склонна делать каменистыми планеты размером до 175% земного. Учёные пока не могут понять, почему примерно половина из планет берут в себя небольшое количество водорода и гелия, что драматически раздувает их размеры и приближает их к размерам Нептуна. Но всё равно планеты земного размера в галактике встречаются очень часто. Десять новых планет-кандидатов в обитаемой зоне из кеплеровского каталога показаны на иллюстрации. Все они имеют размер менее 200% от земного. Консервативная обитаемая зона изображена светло-зелёным полем, оптимистичная область добавлена тёмно-зелёным. Почти все планеты находятся на орбите звёзд с температурой, близкой к Солнцу (около 5800 K). Эти открытия имеют важное значение в наших дальнейших поисках внеземной жизни. Телескоп войдёт в историю — он впервые показал планеты, такие как Земля, примерно такого же размера и на такой же орбите. Такие данные отсутствуют в результатах работы любого другого телескопа. «Кеплер» единственный способен находить планеты на относительно далёкой орбите от родной звезды, с сидерическими периодами более 100 дней. Каталог «Кеплера» станет фундаментом для дальнейшего изучения, насколько распространёнными являются экзопланеты в галактике, а также на какие типы они разбиваются. «Понимание частоты их встречаемости в галактике поможет в проектировании будущих миссий НАСА по прямой съёмке другой Земли», — сказал Марио Перес (Mario Perez), учёный программы «Кеплер» из отдела астрофизики Управления научными миссиями НАСА. На иллюстрации показано поле зрение телескопа «Кеплер» Каждый прямоугольник соответствует конкретному региону, который обрабатывается каждым ПЗС-элементом фотометра «Кеплер». На телескопе установлено 42 ПЗС-элемента размером 5×2,5 см, разрешением 2200×1024 пикселей, попарно. Каждая пара образует квадрат, а общее разрешение матриц составляет 95 мегапикселей. Данные с матрицы снимаются каждые 6 секунд, достигают предела насыщения и суммируются в бортовом компьютере в течение 30 секунд для каждого пикселя. Полоса пропускания приёмника составляет 430-890 нм. Для наблюдения доступны звёзды до 16-й звёздной величины Астрономический спутник с космическим телескопом «Кеплер» — первый в истории аппарат, созданный для поиска экзопланет. Обсерватория могла одновременно наблюдать более чем 100 тыс. звёзд. Запуск аппарата состоялся 6 марта 2009 года, 12 мая 2013 года «Кеплер» вышел из строя, а 11 апреля 2016 года контроль над телескопом был восстановлен. Во второй части миссии после восстановления контроля «Кеплер» продолжит наблюдения за другими участками неба, поиск интересных астрономических объектов и более подробное исследование интересных систем, таких как TRAPPIST-1 с семью землеподобными планетами. Телескоп «Кеплер» ищет планеты методом транзитной фотометрии, основываясь на наблюдениях за прохождением планеты на фоне звезды. Такой метод позволяет определить размеры планеты, а в сочетании с методом Доплера — плотность планеты. Он также даёт информацию о наличии и составе атмосферы. Подробную информацию по каждой планете можно изучить на сайте NASA Exoplanet Archive. В этом хранилище данных собрана информация не только с «Кеплера», но и с других инструментов. Например, ниже статистика по всем экзопланетам, найденным на сегодняшний день. «Нам нравится думать, что в этом исследовании планеты классифицируются таким же образом, каким биологи идентифицируют новые виды животных, — говорит Бенджамин Фултон (Benjamin Fulton), кандидат в докторантуру Гавайского университета в Маноа и ведущий автор научной работы о классификации «кеплеровских» планет. — Нахождение двух отдельных групп экзопланет как открытие, что млекопитающие и ящеры составляют две ветви эволюции».\n\n\nКраткое содержание:\nНаучная группа космического телескопа «Кеплер» выпустила восьмой каталог планет-кандидатов, в котором список увеличился на 219 планет. Десять из них — землеподобные планеты в обитаемой зоне. Это последний каталог, куда включена информация из этого участка неба в созвездии Лебедя.\n\nДанные «Кеплера» позволяют оценить количество планет землеподобного типа в галактике. Они также дают основания полагать, что малые планеты в космосе разбиваются на два чётких типа: каменистые планеты, похожие на Землю, и газообразные планеты меньше Нептуна.\n\n«Кеплер» ищет планеты методом транзитной фотометрии, основываясь на наблюдениях за прохождением планеты на фоне звезды. Это позволяет определить размеры планеты, а в сочетании с методом Доплера — плотность планеты. Он также даёт информацию о наличии и составе атмосферы.","summary":"Научная группа космического телескопа «Кеплер» выпустила восьмой каталог планет-кандидатов, в котором список увеличился на 219 планет. Десять из них — землеподобные планеты в обитаемой зоне. Это последний каталог, куда включена информация из этого участка неба в созвездии Лебедя.\n\nДанные «Кеплера» позволяют оценить количество планет землеподобного типа в галактике. Они также дают основания полагать, что малые планеты в космосе разбиваются на два чётких типа: каменистые планеты, похожие на Землю, и газообразные планеты меньше Нептуна.\n\n«Кеплер» ищет планеты методом транзитной фотометрии, основываясь на наблюдениях за прохождением планеты на фоне звезды. Это позволяет определить размеры планеты, а в сочетании с методом Доплера — плотность планеты. Он также даёт информацию о наличии и составе атмосферы."}
{"Text":"Intel Power Monitoring Tool — на страже энергоэффективности / Habr               \n\n02-08-2013\nМы продолжаем разговор об энергоэффективности мобильных устройств и приложениях Intel под Android. Сегодняшний пост касается обеих тем одновременно, ведь речь пойдет о программе Intel Power Monitoring Tool (Intel PMT) — средстве, позволяющем осуществлять аудит потребляемых ресурсов как системой в целом, так и отдельным приложением. Intel PMT может быть интересен любопытным пользователям Android, желающим все знать о своей системе, но адресована прежде всего разработчикам и служит средством диагностики проблем, не видимых невооруженным глазом. Итак, Intel Power Monitoring Tool является инструментом профилирования данных системы Android, связанных с энергопотреблением и производительностью. С заданным пользователем интервалом приложение записывает действующие значения четырех десятков основных показателей ОС (количество десятков сильно зависит от числа ядер процессора, поскольку именно он находится в центре внимания утилиты). Среди этих значений вы найдете параметры: Батареи — состояние, напряжение, ток, потребляемую/получаемую мощность, оставшуюся емкость; ЦПУ — частоту и утилизацию каждого ядра, CPU P state residency и CPU C state residency; Дисплея — состояние, яркость подсветки Интерфейсов Wi-Fi и 3G — скорость передачи в пакетах и битах Внешних и внутренних устройств хранения — скорость записи и чтения Интерфейс приложения суров и не перегружен каким-либо дизайном. Элементы управления исчерпываются буквально несколькими кнопками, реализующими самое необходимое. Можно стартовать процесс записи (он идет в фоне) и остановить его; вывести результаты на дисплей или строку состояния системы; отправить лог программы по почте. Наконец, отдельной кнопокой инициируется мониторинг конкретного приложения, имя которого, в лучших традициях консольных утилит, вводится вручную. Вывод результатов оформлен еще проще: никаких графических представлений данных, только текстовый формат, только хардкор! Возможно, такой подход и оправдан, ведь профессиональным разработчикам нужна не цветомузыка, а поток цифровых данных, который далее можно обработать в базе данных или табличном редакторе. С другой стороны, с непривычки такой аскетизм выглядит чрезмерным — можно было включить визуализацию хотя бы в виде опции. Программа пишет свой лог в текстовом формате, его можно получить через подключение к ПК или с помощью электронной почты — соответствующая кнопка есть в интерфейсе. Каждая строчка лога имеет такой вид: 07:40:21 Status Charging BP 406.30 V 3833 I 106 C 692 CPU0 900 27.00% 17.00% 10.00% CPU1 900 42.00% 39.00% 3.00% CpuPStateResidency 0.00% 1.01% 13.13% 85.86% 0.00% CpuCStateResidency 0.01% 3.44% 41.49% 11.09% 43.98% 0.00% 0.00% On 1 Brightness 15.00% WifiTxPackets 0 WifiRxPackets 0 WifiTxBytes 0 WifiRxBytes 0 ModemTxPackets 0 ModemRxPackets 0 ModemTxBytes 0 ModemRxBytes 0 DiskReadBandwidth 0.00 DiskWriteBandwidth 0.00 EmmcReadBandwidth 0.00 EmmcWriteBandwidth 6.00 SdCardReadBandwidth 0.00 SdCardWriteBandwidth 0.00 youtube PID 3858 AppState 83 AppNumOfThreads 48 AppCpuUserUtil 21.00 AppCpuSystemUtil 2.00 AppCpuUtil 23.00 Что означает каждое поле, можно выяснить из таблицы. Последняя секция лога относится к контролируемому приложению, в данном примере им является клиент YouTube. Каким образом Intel Power Monitoring Tool может быть использован для нужд разработчиков? Приведем с сокращениями пример из статьи, посвященной инструменту, с сайта Intel. Предположим, Марк — разработчик приложений для мобильных устройств Android. Он разработал приложение для чтения книг под названием EReader. Ему хотелось бы выяснить, нет ли в его приложении серьезных проблем с энергопотреблением и производительностью. Для этого он решил использовать инструмент Power Monitoring Tool. Изучая полученные данные, он обращает внимание на то, что общая загрузка процессора и загрузка процессора приложением почти достигает 100 % без каких-либо действий пользователя. Он решает проверить данные профилирования, чтобы выяснить, является ли это проблемой. Он извлекает данные Power Monitoring Tool и строит графики загрузки процессора пользовательским приложением и системой, как показано на следующем рисунке. На этом рисунке загрузка процессора пользовательским приложением достигает 50 %, что означает, что приложение полностью использует одно ядро для выполнения кода в пользовательском пространстве. Благодаря полученным от инструмента Power Monitoring Tool данным о загрузке процессора приложением Марк определяет, что максимальная загрузка процессора приходится на момент открытия книги. Марк догадывается, что это может быть вызвано фрагментом кода, открывающим электронную книгу. Он проверяет этот фрагмент кода и находит в нем ошибку в виде бесконечного цикла. Несмотря на некоторую деревянность интерфейса, Intel Power Monitoring Tool может оказаться удобным средством для тех разработчиков, кто скрупулезно следит за энергоэффективностью своих приложений. К сожалению, приложение уже довольно давно (более года) не обновлялось, что приводит к некоторой несовместимости с новым «железом» (о чем программа честно сообщает при запуске). Правда, в случае нашего тестового смартфона Orange AZ210A несовместимость проявила себя только в отсутствии показаний с Wi-Fi интерфейса. Приложение Intel PMT работает под управлением ОС Android 2.3 и выше на платформе Intel Atom.\n","metric":0.56982421880000000414,"metric_input":"Текст статьи:\nIntel Power Monitoring Tool — на страже энергоэффективности / Habr               \n\n02-08-2013\nМы продолжаем разговор об энергоэффективности мобильных устройств и приложениях Intel под Android. Сегодняшний пост касается обеих тем одновременно, ведь речь пойдет о программе Intel Power Monitoring Tool (Intel PMT) — средстве, позволяющем осуществлять аудит потребляемых ресурсов как системой в целом, так и отдельным приложением. Intel PMT может быть интересен любопытным пользователям Android, желающим все знать о своей системе, но адресована прежде всего разработчикам и служит средством диагностики проблем, не видимых невооруженным глазом. Итак, Intel Power Monitoring Tool является инструментом профилирования данных системы Android, связанных с энергопотреблением и производительностью. С заданным пользователем интервалом приложение записывает действующие значения четырех десятков основных показателей ОС (количество десятков сильно зависит от числа ядер процессора, поскольку именно он находится в центре внимания утилиты). Среди этих значений вы найдете параметры: Батареи — состояние, напряжение, ток, потребляемую/получаемую мощность, оставшуюся емкость; ЦПУ — частоту и утилизацию каждого ядра, CPU P state residency и CPU C state residency; Дисплея — состояние, яркость подсветки Интерфейсов Wi-Fi и 3G — скорость передачи в пакетах и битах Внешних и внутренних устройств хранения — скорость записи и чтения Интерфейс приложения суров и не перегружен каким-либо дизайном. Элементы управления исчерпываются буквально несколькими кнопками, реализующими самое необходимое. Можно стартовать процесс записи (он идет в фоне) и остановить его; вывести результаты на дисплей или строку состояния системы; отправить лог программы по почте. Наконец, отдельной кнопокой инициируется мониторинг конкретного приложения, имя которого, в лучших традициях консольных утилит, вводится вручную. Вывод результатов оформлен еще проще: никаких графических представлений данных, только текстовый формат, только хардкор! Возможно, такой подход и оправдан, ведь профессиональным разработчикам нужна не цветомузыка, а поток цифровых данных, который далее можно обработать в базе данных или табличном редакторе. С другой стороны, с непривычки такой аскетизм выглядит чрезмерным — можно было включить визуализацию хотя бы в виде опции. Программа пишет свой лог в текстовом формате, его можно получить через подключение к ПК или с помощью электронной почты — соответствующая кнопка есть в интерфейсе. Каждая строчка лога имеет такой вид: 07:40:21 Status Charging BP 406.30 V 3833 I 106 C 692 CPU0 900 27.00% 17.00% 10.00% CPU1 900 42.00% 39.00% 3.00% CpuPStateResidency 0.00% 1.01% 13.13% 85.86% 0.00% CpuCStateResidency 0.01% 3.44% 41.49% 11.09% 43.98% 0.00% 0.00% On 1 Brightness 15.00% WifiTxPackets 0 WifiRxPackets 0 WifiTxBytes 0 WifiRxBytes 0 ModemTxPackets 0 ModemRxPackets 0 ModemTxBytes 0 ModemRxBytes 0 DiskReadBandwidth 0.00 DiskWriteBandwidth 0.00 EmmcReadBandwidth 0.00 EmmcWriteBandwidth 6.00 SdCardReadBandwidth 0.00 SdCardWriteBandwidth 0.00 youtube PID 3858 AppState 83 AppNumOfThreads 48 AppCpuUserUtil 21.00 AppCpuSystemUtil 2.00 AppCpuUtil 23.00 Что означает каждое поле, можно выяснить из таблицы. Последняя секция лога относится к контролируемому приложению, в данном примере им является клиент YouTube. Каким образом Intel Power Monitoring Tool может быть использован для нужд разработчиков? Приведем с сокращениями пример из статьи, посвященной инструменту, с сайта Intel. Предположим, Марк — разработчик приложений для мобильных устройств Android. Он разработал приложение для чтения книг под названием EReader. Ему хотелось бы выяснить, нет ли в его приложении серьезных проблем с энергопотреблением и производительностью. Для этого он решил использовать инструмент Power Monitoring Tool. Изучая полученные данные, он обращает внимание на то, что общая загрузка процессора и загрузка процессора приложением почти достигает 100 % без каких-либо действий пользователя. Он решает проверить данные профилирования, чтобы выяснить, является ли это проблемой. Он извлекает данные Power Monitoring Tool и строит графики загрузки процессора пользовательским приложением и системой, как показано на следующем рисунке. На этом рисунке загрузка процессора пользовательским приложением достигает 50 %, что означает, что приложение полностью использует одно ядро для выполнения кода в пользовательском пространстве. Благодаря полученным от инструмента Power Monitoring Tool данным о загрузке процессора приложением Марк определяет, что максимальная загрузка процессора приходится на момент открытия книги. Марк догадывается, что это может быть вызвано фрагментом кода, открывающим электронную книгу. Он проверяет этот фрагмент кода и находит в нем ошибку в виде бесконечного цикла. Несмотря на некоторую деревянность интерфейса, Intel Power Monitoring Tool может оказаться удобным средством для тех разработчиков, кто скрупулезно следит за энергоэффективностью своих приложений. К сожалению, приложение уже довольно давно (более года) не обновлялось, что приводит к некоторой несовместимости с новым «железом» (о чем программа честно сообщает при запуске). Правда, в случае нашего тестового смартфона Orange AZ210A несовместимость проявила себя только в отсутствии показаний с Wi-Fi интерфейса. Приложение Intel PMT работает под управлением ОС Android 2.3 и выше на платформе Intel Atom.\n\n\nКраткое содержание:\nСтатья рассказывает о программе Intel Power Monitoring Tool, которая позволяет проводить аудит энергопотребления и производительности системы Android. Программа записывает данные о состоянии батареи, процессора, дисплея и других компонентов, а также скорость работы внешних устройств. Результаты выводятся в текстовом формате, что может быть полезно для разработчиков. Однако, программа не совместима с новым «железом», и уже давно не обновлялась.","summary":"Статья рассказывает о программе Intel Power Monitoring Tool, которая позволяет проводить аудит энергопотребления и производительности системы Android. Программа записывает данные о состоянии батареи, процессора, дисплея и других компонентов, а также скорость работы внешних устройств. Результаты выводятся в текстовом формате, что может быть полезно для разработчиков. Однако, программа не совместима с новым «железом», и уже давно не обновлялась."}
{"Text":"Виртуализация на oVirt в крупнейшем ритейлере, или Любовь придумали, чтобы денег не платить / Habr              \n\n01-11-2022\nИсточник: «Инфосистемы Джет» Это было в 2016 году. У одного крупнейшего в России ритейлера стояла глобальная задача перейти на новую ERP-систему. Поскольку основные усилия были сосредоточены на перестроении бизнес-процессов, заказчик решил отдать задачи по ИТ на аутсорс и обратился к нам. Мы предложили ему комплексную услугу по аутсорсингу и аренде инфраструктуры, куда входили СХД, СРК, СУБД, виртуализация, физические серверы, сетевая инфраструктура. После того, как завершился начальный этап внедрения продуктива, выявились ряд проблем, которые потенциально могли влиять на производительность и отказоустойчивость созданной системы. И таким «узким местом» стала виртуализация. О том, почему мы решили уйти от enterprise-решения на Open Source, а также почему этот опыт в текущих реалиях может быть полезным, мы и собираемся рассказать в сегодняшнем посте. Великий и ужасный Open Source Заказчик полностью доверился нашей экспертизе в части подбора оптимальных для него по стоимости и производительности инфраструктуры и софта. Флагманом корпоративного сегмента виртуализации на тот момент был VMware — удовольствие не из дешевых, но и продукты с открытым исходным кодом изначально мы не рассматривали, поскольку отдавали себе отчет в том, что не готовы взять на себя риски, оставшись с проблемой один на один, без участия вендора. Поддержка профессионального коммьюнити казалась чем-то далеким и ненадежным. В итоге мы сделали выбор в пользу Oracle VM, у которого техподдержка стоила адекватных денег, вендор позиционировал его как enterprise-продукт, и по функционалу он не уступал проприетарным решениям. Постепенно функциональная нагрузка на системы ритейлера стала расти, также поменялись требования к производительности. Выяснилось, что первоначальный расчетный сайзинг от вендора приложений уже не соответствовал текущей нагрузке. Стало не хватать производительности сетевых драйверов виртуальных машин. Реальная производительность была сильно ниже заявленной. Вендор проблему подтвердил, но ее решение мы ждали в течение года. За это время мы успели начать проекты по модернизации существующей инфраструктуры. Но тут на нас обрушились новые проблемы, гораздо более критичные, поскольку они касались отказоустойчивости и стабильной работы кластера виртуализации. Например, когда ВМ мигрировала на новый хост, она могла внезапно перезагрузиться, не смигрировать без объяснения причины, что сильно тормозило процесс переезда на новое оборудование. Другой важной проблемой стали периодические перезагрузки хостов виртуализации. В части случаев причиной могла стать неудачная миграция виртуальной машины на новый сервер, причем перезагружался именно новый сервер со всеми ВМ на нем. Проблемы, как обычно, были вендору озвучены, и снова мы не смогли получить оперативной поддержки. И мы, и заказчик уже отчаялись бороться с «ветряными мельницами», поэтому нам пришлось рассматривать для этого проекта альтернативное решение. Чтобы сохранить требуемые SLA, мы предложили удачно показавшее себя в тестировании решение oVirt, которое выигрывало в надежности и производительности. Эта Open Source платформа на тот момент уже достигла технологической зрелости, и в ней были устранены основные баги, в частности, связанные с работой Windows. Сроки проекта по модернизации поджимали, а решение проблемы от Oracle VM так и не поступило. Решили совместить миграцию со сменой платформы виртуализации, чтобы наверстать упущенное время. С заказчиком удалось быстро согласовать ряд критически важных параметров времени простоя, и миграция прошла за рекордно короткие сроки – всего за две недели. Риск — дело благородное Характерно, что с тех пор, как мы перенесли все на oVirt, никаких особых проблем в технической части не было. Тем не менее, человеческий фактор еще никто не отменял, и заказчик еще в течение нескольких лет с трепетом и дрожью вспоминал виртуализацию как «шкатулку проклятья», от которой можно ожидать всё что угодно. Резюмируя этот кейс, хотим отметить, что заказчик оказался доволен виртуализацией на Open Source, более того, если ранее он располагал свои системы на общей ферме вместе с другими нашими клиентами, то теперь у него две собственные фермы для продуктивных систем, тестов и разработки под управлением oVirt. И системы продолжают уверенно расти и развиваться и по сей день. Безусловно, такое решение является отказоустойчивым и надежным. К сожалению, в платных продуктах маркетинговые обещания могут не соответствовать действительности, поэтому при выборе решения надо, в первую очередь, основываться на всестороннем тестировании. При этом условии, вполне можно рассматривать Open Source как достойную альтернативу проприетарным решениям. Зачастую, можно получить требуемый функционал за меньшую стоимость. В 2020 году мы провели модернизацию систем ритейлера, в ходе которой обновили гипервизор и его расширение до актуальной версии. При этом перейти на какой-то другой продукт желания у заказчика не возникло. На нашей практике этот пример — уникальный. На российском рынке достаточно компаний, готовых развернуть виртуализацию на Open Source. Однако разместить на oVirt крупную ERP-систему? С таким опытом мы не сталкивались. Мы считаем, что можно смело располагать продуктивные системы любого объема и тесты на таком решении — это не повлечет особых рисков, но позволит существенно сэкономить бюджет. Разворачивали ли вы продуктивные системы на Open Source продуктах? С какими трудностями вам пришлось при этом столкнуться, и, главное, как их удалось обойти? Jet Service Team\n","metric":0.57568359380000000414,"metric_input":"Текст статьи:\nВиртуализация на oVirt в крупнейшем ритейлере, или Любовь придумали, чтобы денег не платить / Habr              \n\n01-11-2022\nИсточник: «Инфосистемы Джет» Это было в 2016 году. У одного крупнейшего в России ритейлера стояла глобальная задача перейти на новую ERP-систему. Поскольку основные усилия были сосредоточены на перестроении бизнес-процессов, заказчик решил отдать задачи по ИТ на аутсорс и обратился к нам. Мы предложили ему комплексную услугу по аутсорсингу и аренде инфраструктуры, куда входили СХД, СРК, СУБД, виртуализация, физические серверы, сетевая инфраструктура. После того, как завершился начальный этап внедрения продуктива, выявились ряд проблем, которые потенциально могли влиять на производительность и отказоустойчивость созданной системы. И таким «узким местом» стала виртуализация. О том, почему мы решили уйти от enterprise-решения на Open Source, а также почему этот опыт в текущих реалиях может быть полезным, мы и собираемся рассказать в сегодняшнем посте. Великий и ужасный Open Source Заказчик полностью доверился нашей экспертизе в части подбора оптимальных для него по стоимости и производительности инфраструктуры и софта. Флагманом корпоративного сегмента виртуализации на тот момент был VMware — удовольствие не из дешевых, но и продукты с открытым исходным кодом изначально мы не рассматривали, поскольку отдавали себе отчет в том, что не готовы взять на себя риски, оставшись с проблемой один на один, без участия вендора. Поддержка профессионального коммьюнити казалась чем-то далеким и ненадежным. В итоге мы сделали выбор в пользу Oracle VM, у которого техподдержка стоила адекватных денег, вендор позиционировал его как enterprise-продукт, и по функционалу он не уступал проприетарным решениям. Постепенно функциональная нагрузка на системы ритейлера стала расти, также поменялись требования к производительности. Выяснилось, что первоначальный расчетный сайзинг от вендора приложений уже не соответствовал текущей нагрузке. Стало не хватать производительности сетевых драйверов виртуальных машин. Реальная производительность была сильно ниже заявленной. Вендор проблему подтвердил, но ее решение мы ждали в течение года. За это время мы успели начать проекты по модернизации существующей инфраструктуры. Но тут на нас обрушились новые проблемы, гораздо более критичные, поскольку они касались отказоустойчивости и стабильной работы кластера виртуализации. Например, когда ВМ мигрировала на новый хост, она могла внезапно перезагрузиться, не смигрировать без объяснения причины, что сильно тормозило процесс переезда на новое оборудование. Другой важной проблемой стали периодические перезагрузки хостов виртуализации. В части случаев причиной могла стать неудачная миграция виртуальной машины на новый сервер, причем перезагружался именно новый сервер со всеми ВМ на нем. Проблемы, как обычно, были вендору озвучены, и снова мы не смогли получить оперативной поддержки. И мы, и заказчик уже отчаялись бороться с «ветряными мельницами», поэтому нам пришлось рассматривать для этого проекта альтернативное решение. Чтобы сохранить требуемые SLA, мы предложили удачно показавшее себя в тестировании решение oVirt, которое выигрывало в надежности и производительности. Эта Open Source платформа на тот момент уже достигла технологической зрелости, и в ней были устранены основные баги, в частности, связанные с работой Windows. Сроки проекта по модернизации поджимали, а решение проблемы от Oracle VM так и не поступило. Решили совместить миграцию со сменой платформы виртуализации, чтобы наверстать упущенное время. С заказчиком удалось быстро согласовать ряд критически важных параметров времени простоя, и миграция прошла за рекордно короткие сроки – всего за две недели. Риск — дело благородное Характерно, что с тех пор, как мы перенесли все на oVirt, никаких особых проблем в технической части не было. Тем не менее, человеческий фактор еще никто не отменял, и заказчик еще в течение нескольких лет с трепетом и дрожью вспоминал виртуализацию как «шкатулку проклятья», от которой можно ожидать всё что угодно. Резюмируя этот кейс, хотим отметить, что заказчик оказался доволен виртуализацией на Open Source, более того, если ранее он располагал свои системы на общей ферме вместе с другими нашими клиентами, то теперь у него две собственные фермы для продуктивных систем, тестов и разработки под управлением oVirt. И системы продолжают уверенно расти и развиваться и по сей день. Безусловно, такое решение является отказоустойчивым и надежным. К сожалению, в платных продуктах маркетинговые обещания могут не соответствовать действительности, поэтому при выборе решения надо, в первую очередь, основываться на всестороннем тестировании. При этом условии, вполне можно рассматривать Open Source как достойную альтернативу проприетарным решениям. Зачастую, можно получить требуемый функционал за меньшую стоимость. В 2020 году мы провели модернизацию систем ритейлера, в ходе которой обновили гипервизор и его расширение до актуальной версии. При этом перейти на какой-то другой продукт желания у заказчика не возникло. На нашей практике этот пример — уникальный. На российском рынке достаточно компаний, готовых развернуть виртуализацию на Open Source. Однако разместить на oVirt крупную ERP-систему? С таким опытом мы не сталкивались. Мы считаем, что можно смело располагать продуктивные системы любого объема и тесты на таком решении — это не повлечет особых рисков, но позволит существенно сэкономить бюджет. Разворачивали ли вы продуктивные системы на Open Source продуктах? С какими трудностями вам пришлось при этом столкнуться, и, главное, как их удалось обойти? Jet Service Team\n\n\nКраткое содержание:\nВ 2016 году крупный российский ритейлер столкнулся с проблемами в работе новой ERP-системы из-за использования виртуализации на базе VMware. В результате было принято решение перейти на Open Source решение - oVirt, которое показало надежность и производительность. \n\nВ ходе модернизации системы под управлением oVirt удалось сократить время простоя и избежать проблем с отказоустойчивостью. Заказчик остался доволен решением и продолжает использовать его для продуктивных систем, тестов и разработки.","summary":"В 2016 году крупный российский ритейлер столкнулся с проблемами в работе новой ERP-системы из-за использования виртуализации на базе VMware. В результате было принято решение перейти на Open Source решение - oVirt, которое показало надежность и производительность. \n\nВ ходе модернизации системы под управлением oVirt удалось сократить время простоя и избежать проблем с отказоустойчивостью. Заказчик остался доволен решением и продолжает использовать его для продуктивных систем, тестов и разработки."}
{"Text":"Microsoft и Adobe выпустили набор обновлений, декабрь 2013 / Habr           \n\n10-12-2013\nКомпания выпустила серию обновлений для своих продуктов, которые закрывают 24 уникальных уязвимостей (6 исправлений со статусом Critical и 5 со статусом Important). Детальный отчет о закрываемых уязвимостях, компонентах продуктов и их версий можно найти на соответствующей странице security bulletins. Это последний patch tuesday в этом году, в рамках него MS исправляет уязвимости для таких продуктов как Internet Explorer, Windows, Office, Exchange и др., причем на исправление продуктов Office нацелено сразу три обновления. Обновление MS13-096 закрывает эксплуатируемую in-the-wild уязвимость в Office CVE-2013-3906 (Win32/Exploit.CVE-2013-3906.A). Мы более подробно писали о ней здесь. Злоумышленники использовали специальным образом сформированный .doc файлы для установки в систему вредоносного кода через эксплойт. Уязвимость существует в коде обработки встроенного в документ специального изображения (TIFF codec). MS Word использует этот компонент для открытия встроенных в документ других объектов, в этом случае TIFF-изображения. Обновлению подлежат Windows Vista и Server 2008, а также Office 2003-2007-2010. Для применения обновления нужна перезагрузка. Обновление MS13-097 закрывает семь уязвимостей во всех версиях браузера Internet Explorer (6-11) для всех поддерживаемых ОС, от Windows XP SP3 до Windows 8.1. Большинство исправляемых уязвимостей относятся к типу Remote Code Execution (RCE) и могут использоваться атакующими для удаленного исполнения кода через специальным образом сформированную веб-страницу. Две уязвимости CVE-2013-5045 и CVE-2013-5046 могут быть использованы атакующими для повышения своих привилегий в системе (Elevation of Privelege). Critical. Exploit code likely. Обновление MS13-098 исправляет уязвимость CVE-2013-3900 типа Remote Code Execution для всех поддерживаемых версий ОС от Windows XP SP3 до Windows 8.1/RT 8.1. Исправлению подвергается функция WinVerifyTrust библиотеки Wintrust.dll. Злоумышленник может модифицировать подписанный цифровой подписью (формата Authenticode) файл для добавления туда вредоносного кода, причем в обход механизма проверки подписи (т. е. без нарушения ее целостности, если целостность проверяется Windows). Модифицируя таким образом один из системных файлов, злоумышленник может получить полный контроль над системой. Critical. Уязвимость эксплуатируется in-the-wild. Более подробно MS13-098: Update to enhance the security of Authenticode. Обновление MS13-099 также устраняет одну RCE уязвимость CVE-2013-5056 для всех поддерживаемых версий ОС. Исправлению подвергается компонент Microsoft Scripting Runtime Object Library, который представлен в системе библиотекой Scrrun.dll. Атакующий может скомпрометировать пользователя вредоносным кодом через специальным образом подготовленную веб-страницу (drive-by). Для эксплуатации используется техника use-after-free. Critical. Exploit code likely. Обновление MS13-100 исправляет RCE уязвимость в продукте Microsoft SharePoint Server. Атакующий может выполнить произвольный код на сервере через отправку специальным образом подготовленного содержимого. Critical. Exploit code likely. Обновление MS13-101 нацелено на устранение пяти уязвимостей в драйверах ОС. Три уязвимости присутствуют в драйвере режима ядра подсистемы Windows — Win32k.sys (CVE-2013-3899 memory-corruption / EoP. CVE-2013-3902 use-after-free / EoP, CVE-2013-5058 integer overflow / Denial of Service). С использованием этих уязвимостей злоумышленник может либо повысить свои привилегии в системе, либо спровоцировать ее зависание до перезагрузки. Еще одна уязвимость CVE-2013-3907 присутствует в драйвере Windows audio — portcls.sys. С ее помощью атакующий также может поднять свои привилегии в системе. Пятая уязвимость CVE-2013-3903 присутствует в ядре ОС и может использоваться для провоцирования зависания системы. Данному обновлению подлежат все версии ОС. Important. Обновление MS13-102 исправляет уязвимость CVE-2013-3878 типа Elevation of Privelege в компоненте Local Remote Procedure Call (LRPC) устаревших Windows XP SP3 и Windows Server 2003 SP2. Атакующий может спровоцировать ситуацию переполнения буфера на стеке у получателя специальным образом сформированного LPC сообщения. Important. Exploit code likely. Обновление MS13-103 исправляет одну EoP уязвимость в продукте ASP.NET SignalR. Important. Exploit code likely. Обновление MS13-104 исправляет одну уязвимость CVE-2013-5054 типа Information Disclosure в Office 2013-2013 RT. Ошибка в продукте присутствует при обработке специальным образом подготовленного запроса на открытие офисного файла, который располагается на вредоносной веб-странице. При этом атакующий может получить доступ к информации о маркере доступа текущего пользователя. Important. Exploit code unlikely. Обновление MS13-105 исправляет четыре уязвимости в продукте Exchange Server 2007-2010-2013, которые могут использоваться для удаленного исполнения кода через сообщение со специальным образом подготовленным вложением. Critical. Обновление MS13-106 устраняет недоработку типа «Security Feature Bypass» для Microsoft Office 2007-2010 (hxds.dll). Атакующий может обойти ограничения, накладываемые ASLR, при использовании специальным образом подготовленной веб-страницы в Internet Explorer. Таким образом эта уязвимость может быть использована совместно с иной RCE уязвимостью для исполнения удаленного кода в системе. В данном случае такая возможность дает коду эксплойта успешно обходить ограничения ASLR и заранее рассчитывать необходимый адрес нужного кода. Обновление доставляет новую версию библиотеки hxds.dll, которая скомпилирована с поддержкой ASLR (/DYNAMICBASE flag). Недоработка эксплуатируется in-the-wild. Более подробно MS13-106: Farewell to another ASLR bypass. 1 – Exploit code likely Вероятность эксплуатирования уязвимости очень высока, злоумышленники могут использовать эксплойт, например, для удаленного выполнения кода. 2 – Exploit code would be difficult to build Вероятность эксплуатирования средняя, поскольку злоумышленники вряд ли смогут добиться ситуации устойчивого эксплуатирования, а также в силу технической особенности уязвимости и сложности разработки эксплойта. 3 – Exploit code unlikely Вероятность эксплуатирования минимальная и злоумышленники вряд ли смогут разработать успешно работающий код и воспользоваться этой уязвимостью для проведения атаки. Мы рекомендуем нашим пользователям установить обновления как можно скорее и, если вы еще этого не сделали, включить автоматическую доставку обновлений с использованием Windows Update (по-умолчанию такая возможность включена). Компания Adobe также анонсировала выпуск обновлений для своих продуктов Adobe Flash Player и Shockwave Player. Обновления для Flash Player закрывают две Critical уязвимости CVE-2013-5331 и CVE-2013-5332, которые могут использоваться для несанкционированного исполнения кода. Компания сообщает о том, что CVE-2013-5331 уже эксплуатируется злоумышленниками, которые рассылают электронные письма с вложением файла .doc. Этот документ содержит специальным образом сформированный .swf файл, используемый для исполнения кода. Другая уязвимость CVE-2013-5332 использует технику use-after-free для исполнения своего кода. Версии Flash Player подлежащие исправлению до актуальных. Мы рекомендуем пользователям воспользоваться проверкой используемой вашим браузером версии Flash Player, для этого можно воспользоваться офиц. источником Adobe здесь или здесь. Заметьте, что такие браузеры как Google Chrome и Internet Explorer 10+ автоматически обновляются с выходом новой версии Flash Player. Вы можете получить информацию по обновлению Flash для вашего браузера по этой ссылке. be secure.\n","metric":0.56982421880000000414,"metric_input":"Текст статьи:\nMicrosoft и Adobe выпустили набор обновлений, декабрь 2013 / Habr           \n\n10-12-2013\nКомпания выпустила серию обновлений для своих продуктов, которые закрывают 24 уникальных уязвимостей (6 исправлений со статусом Critical и 5 со статусом Important). Детальный отчет о закрываемых уязвимостях, компонентах продуктов и их версий можно найти на соответствующей странице security bulletins. Это последний patch tuesday в этом году, в рамках него MS исправляет уязвимости для таких продуктов как Internet Explorer, Windows, Office, Exchange и др., причем на исправление продуктов Office нацелено сразу три обновления. Обновление MS13-096 закрывает эксплуатируемую in-the-wild уязвимость в Office CVE-2013-3906 (Win32/Exploit.CVE-2013-3906.A). Мы более подробно писали о ней здесь. Злоумышленники использовали специальным образом сформированный .doc файлы для установки в систему вредоносного кода через эксплойт. Уязвимость существует в коде обработки встроенного в документ специального изображения (TIFF codec). MS Word использует этот компонент для открытия встроенных в документ других объектов, в этом случае TIFF-изображения. Обновлению подлежат Windows Vista и Server 2008, а также Office 2003-2007-2010. Для применения обновления нужна перезагрузка. Обновление MS13-097 закрывает семь уязвимостей во всех версиях браузера Internet Explorer (6-11) для всех поддерживаемых ОС, от Windows XP SP3 до Windows 8.1. Большинство исправляемых уязвимостей относятся к типу Remote Code Execution (RCE) и могут использоваться атакующими для удаленного исполнения кода через специальным образом сформированную веб-страницу. Две уязвимости CVE-2013-5045 и CVE-2013-5046 могут быть использованы атакующими для повышения своих привилегий в системе (Elevation of Privelege). Critical. Exploit code likely. Обновление MS13-098 исправляет уязвимость CVE-2013-3900 типа Remote Code Execution для всех поддерживаемых версий ОС от Windows XP SP3 до Windows 8.1/RT 8.1. Исправлению подвергается функция WinVerifyTrust библиотеки Wintrust.dll. Злоумышленник может модифицировать подписанный цифровой подписью (формата Authenticode) файл для добавления туда вредоносного кода, причем в обход механизма проверки подписи (т. е. без нарушения ее целостности, если целостность проверяется Windows). Модифицируя таким образом один из системных файлов, злоумышленник может получить полный контроль над системой. Critical. Уязвимость эксплуатируется in-the-wild. Более подробно MS13-098: Update to enhance the security of Authenticode. Обновление MS13-099 также устраняет одну RCE уязвимость CVE-2013-5056 для всех поддерживаемых версий ОС. Исправлению подвергается компонент Microsoft Scripting Runtime Object Library, который представлен в системе библиотекой Scrrun.dll. Атакующий может скомпрометировать пользователя вредоносным кодом через специальным образом подготовленную веб-страницу (drive-by). Для эксплуатации используется техника use-after-free. Critical. Exploit code likely. Обновление MS13-100 исправляет RCE уязвимость в продукте Microsoft SharePoint Server. Атакующий может выполнить произвольный код на сервере через отправку специальным образом подготовленного содержимого. Critical. Exploit code likely. Обновление MS13-101 нацелено на устранение пяти уязвимостей в драйверах ОС. Три уязвимости присутствуют в драйвере режима ядра подсистемы Windows — Win32k.sys (CVE-2013-3899 memory-corruption / EoP. CVE-2013-3902 use-after-free / EoP, CVE-2013-5058 integer overflow / Denial of Service). С использованием этих уязвимостей злоумышленник может либо повысить свои привилегии в системе, либо спровоцировать ее зависание до перезагрузки. Еще одна уязвимость CVE-2013-3907 присутствует в драйвере Windows audio — portcls.sys. С ее помощью атакующий также может поднять свои привилегии в системе. Пятая уязвимость CVE-2013-3903 присутствует в ядре ОС и может использоваться для провоцирования зависания системы. Данному обновлению подлежат все версии ОС. Important. Обновление MS13-102 исправляет уязвимость CVE-2013-3878 типа Elevation of Privelege в компоненте Local Remote Procedure Call (LRPC) устаревших Windows XP SP3 и Windows Server 2003 SP2. Атакующий может спровоцировать ситуацию переполнения буфера на стеке у получателя специальным образом сформированного LPC сообщения. Important. Exploit code likely. Обновление MS13-103 исправляет одну EoP уязвимость в продукте ASP.NET SignalR. Important. Exploit code likely. Обновление MS13-104 исправляет одну уязвимость CVE-2013-5054 типа Information Disclosure в Office 2013-2013 RT. Ошибка в продукте присутствует при обработке специальным образом подготовленного запроса на открытие офисного файла, который располагается на вредоносной веб-странице. При этом атакующий может получить доступ к информации о маркере доступа текущего пользователя. Important. Exploit code unlikely. Обновление MS13-105 исправляет четыре уязвимости в продукте Exchange Server 2007-2010-2013, которые могут использоваться для удаленного исполнения кода через сообщение со специальным образом подготовленным вложением. Critical. Обновление MS13-106 устраняет недоработку типа «Security Feature Bypass» для Microsoft Office 2007-2010 (hxds.dll). Атакующий может обойти ограничения, накладываемые ASLR, при использовании специальным образом подготовленной веб-страницы в Internet Explorer. Таким образом эта уязвимость может быть использована совместно с иной RCE уязвимостью для исполнения удаленного кода в системе. В данном случае такая возможность дает коду эксплойта успешно обходить ограничения ASLR и заранее рассчитывать необходимый адрес нужного кода. Обновление доставляет новую версию библиотеки hxds.dll, которая скомпилирована с поддержкой ASLR (/DYNAMICBASE flag). Недоработка эксплуатируется in-the-wild. Более подробно MS13-106: Farewell to another ASLR bypass. 1 – Exploit code likely Вероятность эксплуатирования уязвимости очень высока, злоумышленники могут использовать эксплойт, например, для удаленного выполнения кода. 2 – Exploit code would be difficult to build Вероятность эксплуатирования средняя, поскольку злоумышленники вряд ли смогут добиться ситуации устойчивого эксплуатирования, а также в силу технической особенности уязвимости и сложности разработки эксплойта. 3 – Exploit code unlikely Вероятность эксплуатирования минимальная и злоумышленники вряд ли смогут разработать успешно работающий код и воспользоваться этой уязвимостью для проведения атаки. Мы рекомендуем нашим пользователям установить обновления как можно скорее и, если вы еще этого не сделали, включить автоматическую доставку обновлений с использованием Windows Update (по-умолчанию такая возможность включена). Компания Adobe также анонсировала выпуск обновлений для своих продуктов Adobe Flash Player и Shockwave Player. Обновления для Flash Player закрывают две Critical уязвимости CVE-2013-5331 и CVE-2013-5332, которые могут использоваться для несанкционированного исполнения кода. Компания сообщает о том, что CVE-2013-5331 уже эксплуатируется злоумышленниками, которые рассылают электронные письма с вложением файла .doc. Этот документ содержит специальным образом сформированный .swf файл, используемый для исполнения кода. Другая уязвимость CVE-2013-5332 использует технику use-after-free для исполнения своего кода. Версии Flash Player подлежащие исправлению до актуальных. Мы рекомендуем пользователям воспользоваться проверкой используемой вашим браузером версии Flash Player, для этого можно воспользоваться офиц. источником Adobe здесь или здесь. Заметьте, что такие браузеры как Google Chrome и Internet Explorer 10+ автоматически обновляются с выходом новой версии Flash Player. Вы можете получить информацию по обновлению Flash для вашего браузера по этой ссылке. be secure.\n\n\nКраткое содержание:\nMicrosoft и Adobe выпустили набор обновлений для своих продуктов, закрывая 24 уязвимости. Обновления включают исправления для Internet Explorer, Windows, Office, Exchange и других продуктов. Microsoft также исправила уязвимости в драйверах ОС и компонентах продуктов. Adobe выпустила обновления для Adobe Flash Player и Shockwave Player, закрывая две критические уязвимости.","summary":"Microsoft и Adobe выпустили набор обновлений для своих продуктов, закрывая 24 уязвимости. Обновления включают исправления для Internet Explorer, Windows, Office, Exchange и других продуктов. Microsoft также исправила уязвимости в драйверах ОС и компонентах продуктов. Adobe выпустила обновления для Adobe Flash Player и Shockwave Player, закрывая две критические уязвимости."}
{"Text":"Не только домашняя техника: Что еще есть в Аудиомании для меломанов, любителей и музыкантов / Хабр                 \n\n**Аудиомания**\nкак поставщик качественных аудиорешений предлагает не только домашнюю акустику и электронику, но и другие категории продуктов, связанных с музыкой. Не так давно к сегменту профессионального оборудования (у нас давно продаются товары как для фоновой подзвучки помещений, так и для профессиональных студий) мы добавили еще и музыкальные инструменты и аксессуары.\n\nЧтобы разобраться в том, чем отличается домашнее оборудование от\nпрофессионального\n, кому и когда оно нужно и что интересного у нас есть для музыкантов, мы задали несколько вопросов категорийному менеджеру Аудиомании по профессиональному оборудованию Наталье Богдановой.\n\n_Фото Steven Guzzardi  / CC_\n\n## Чем профессиональная акустика отличается от домашнего хайфая? ##\n\nНа некоторых рынках «профессиональный» — синоним объекта более дорогого и статусного. Однако в случае с акустическими системами и сопутствующим оборудованием основное различие между профессиональным и «пользовательским» сегментами действительно лежит в плоскости предназначения продукта: Hi-Fi используют дома для комфортного прослушивания — то есть для удовольствия, для отдыха.\n\nПрофессиональное же звуковое оборудование предназначено для работы в соответствующей сфере (концертные площадки, клубы, рестораны с банкетами, звуковые студии и т.д.). В данном случае приставка «про» —\n_это не синоним элитарности, а показатель назначения продукта_\n.\n\nЧастое отличие профессионального сегмента и Hi-Fi — это звуковая мощность. Профессиональное оборудование должно выдерживать большие и продолжительные нагрузки (по 10-15 часов бесперебойной работы), выдавать «ровное» звучание без искажений на высокой громкости (в таком формате музыку дома никто не слушает). Здесь используются большие динамики, мощные компоненты, и внешний вид системы зачастую отходит на второй план.\n\nВ домашнем оборудовании огромную роль играют нюансы, особенности звучания, а также качество отделки. Производители до мельчайших деталей продумывают каждую мелочь. С большой тщательностью подбираются комплектующие, немаловажную роль имеет дизайн акустики или электроники.\n\nПоскольку для Hi-Fi оборудования важно, чтобы звучание было сбалансированным и детальным даже на небольшой громкости, перед инженерами, которые проектируют такую акустику или усилитель, стоят совершенно другие задачи. В усилителе должны стоять малошумные конденсаторы и трансформаторы, сам корпус также должен быть не только надежным и иметь вентиляционные щели для охлаждения, но и приятным глазу.\n\nДинамики в таком оборудовании обычно меньше тех, которые используются для про-акустики (размеры динамиков и комплектующих в данном случае уменьшают не в ущерб звуку), — это позволяет сделать всю систему более компактной.\n\n## Можно ли использовать Hi-Fi акустику вместо профессиональной? ##\n\nОдин раз мы столкнулись с ситуацией, когда заказчик планировал серьезный ресторан и хотел, чтобы фоновая музыка, играющая для посетителей, была максимально комфортной и приятной на слух, а внешний вид акустики обязательно должен был соответствовать интерьеру (в цвете дерева венге). Так как ресторан был небольшой, эту идею удалось реализовать: вместо усилителя был использован мощный ресивер в особом режиме и 3 пары полочной акустики с настенными кронштейнами.\n\nТем не менее, Hi-Fi оборудование — не совсем то, что нужно для фонового звука в том же ресторане. Оно предназначено для небольших помещений (комнат от 15 до 40 метров в среднем). Площади же ресторанов и кафе имеют зачастую более 50 метров, и домашнее оборудование (без серьезных вмешательств в акустику и дизайн помещения) на такие пространства просто не рассчитано.\n\nОсновное заблуждение в этом случае проистекает из того, что профессиональная акустика, предназначенная именно для фоновой подзвучки помещений, стоит недорого — по сравнению с Hi-Fi комплектами. У покупателей же часто срабатывает принцип «дешево — значит подозрительно и плохо».\n\nПрофессиональные системы в данном случае действительно могут звучать не настолько интересно, детально и ярко, как Hi-Fi, но и работать они будут в большом и довольно шумном помещении — причем работать долго. В таких условиях, в неподготовленном помещении этого вполне достаточно.\n\n## Кому может понадобиться проф. оборудование — кроме профессиональных музыкантов или звукорежиссеров? ##\n\nКто-то (этот кейс мы уже озвучивали) берет профессиональную акустику для кафе, кто-то – в детский сад, в школу, кто-то – для выездных мероприятий и т.д. Обычно для таких целей популярностью пользуются активные комплекты акустики (2 сателлита + встроенный микшер). В связи с повсеместным открытием караоке-клубов в России очень востребованы радиомикрофоны и микшерные пульты со встроенными эффектами.\n\nЕсли говорить о\nмузыкальных инструментах\n(в Аудиомании с недавнего времени можно купить и их тоже) сейчас довольно много непрофессионалов интересуется цифровыми пианино (например, для детей, которые учатся в музыкальной школе, или просто «для души»). У\nних\nесть ряд преимуществ по сравнению с классическим инструментом: не занимают много места, их можно передвигать и перевозить в легковой машине, не надо заказывать настройщика каждый год и можно играть в наушниках, никому не мешая.\n\nОтдельную нишу занимает оборудование для записей подкастов —\nстудийные микрофоны\nи звуковые карты сейчас многие покупают именно для «домашней» звукозаписи. Кстати, микрофоны для iOS, портативные рекордеры, а также различные аксессуары (стойки, чехлы, кабели, кронштейны — то, без чего все равно не обходится практически ни один проект, связанный с записью звука — как профессиональной, так и любительской) — это абсолютно новые для Аудиомании товары.\n\nВ планах у нас — расширить сегмент\nоборудования для конференций\n(существуют целые системы, серьезные комплекты для больших аудиторий): это еще одно интересное направление, в котором проф. оборудование может быть интересно тем, кто далек от профессиональной звукозаписи. Трансляционное оборудование было представлено у нас и раньше — в основном брендом Apart. Сейчас добавились товары и от других производителей профессионального оборудования: QSC, JBL, Penton, Tannoy.\n\n## Теперь о музыкантах: что интересного, уникального или редкого есть у Аудиомании для этой аудитории? ##\n\nВ нашем ассортименте уже 3 года присутствует бренд\nTaurus\n— в России только мы представляем продукцию этого польского производителя. Басовые и гитарные усилители, кабинеты, комбоусилители и педали эффектов. Эндорсерами бренда в России являются музыканты из группы «Ария» — Виталий Дубинин и Сергей Попов.\n\nИскушенным музыкантам стоит обратить внимание на\nмикрофонные и гитарные кабели Analysis Plus\n(это наша эксклюзивная дистрибуция). Еще один интересный вариант профессиональных кабелей — немецкий бренд Schulz, который музыканты ценят за хорошее качество при сравнительно невысокой стоимости.\n\nТакже особое внимание стоит уделить уже легендарным немецким студийным мониторам\nADAM\n, которые тоже появились в нашем ассортименте недавно, и продукции\nQSC\n(это акустика, усилители со встроенными процессорами и цифровые микшерные пульты, которые привлекают много внимания уже за счет своей функциональности и простоты в управлении).\n\nНе секрет, что существуют бренды, которые делают оборудование как в сегменте домашнего оборудования, так и в про-направлении, например, DENON и Marantz: они сейчас также расширяют свой модельный ряд. У Marantz есть хорошие студийные микрофоны, появились новые портативные и стационарные рекордеры, студийные мониторы; у DENON стоит обратить внимание на профессиональные проигрыватели и\nрекордеры\n, а также студийные мониторы.\n\n## Что посоветуете подарить музыканту на Новый год? ##\n\nОчень сложно угодить музыканту, если он сам не озвучил свои пожелания — среди тех же гитаристов есть немало направлений. Чего-то универсального не существует, даже процессоры и педали эффектов подбираются по стилю музыки. Но в любом случае есть аксессуары, которые всегда пригодятся гитаристу — это новый\nчехол для гитары\n, ремень или\nпедалборд\nпод педали эффектов. Полезный и бюджетный подарок для барабанщика — комплект новых\nбарабанных палочек\n, так как они часто ломаются.\n\nБолее значимый и дорогой подарок гитаристу — качественный\nгитарный кабель\n. Для барабанщика, который играет только на репетиционных базах, подходящим подарком может стать электронная барабанная установка. А для человека, играющего на цифровом пианино или синтезаторе, может подойти подарок в виде хороших накладных или\nохватывающих наушников\n, которые позволят ему наслаждаться своей игрой или учиться, не теряя всех преимуществ встроенных динамиков инструмента.\n\nP.S. Дополнительное чтение по теме:\n\nКак построить домашний кинотеатр, и что можно узнать о производстве акустики\n\nЧто нужно знать о современных аудиосистемах\n\nКак сделать аудиотехнику элементом интерьера","metric":0.56396484380000000414,"metric_input":"Текст статьи:\nНе только домашняя техника: Что еще есть в Аудиомании для меломанов, любителей и музыкантов / Хабр                 \n\n**Аудиомания**\nкак поставщик качественных аудиорешений предлагает не только домашнюю акустику и электронику, но и другие категории продуктов, связанных с музыкой. Не так давно к сегменту профессионального оборудования (у нас давно продаются товары как для фоновой подзвучки помещений, так и для профессиональных студий) мы добавили еще и музыкальные инструменты и аксессуары.\n\nЧтобы разобраться в том, чем отличается домашнее оборудование от\nпрофессионального\n, кому и когда оно нужно и что интересного у нас есть для музыкантов, мы задали несколько вопросов категорийному менеджеру Аудиомании по профессиональному оборудованию Наталье Богдановой.\n\n_Фото Steven Guzzardi  / CC_\n\n## Чем профессиональная акустика отличается от домашнего хайфая? ##\n\nНа некоторых рынках «профессиональный» — синоним объекта более дорогого и статусного. Однако в случае с акустическими системами и сопутствующим оборудованием основное различие между профессиональным и «пользовательским» сегментами действительно лежит в плоскости предназначения продукта: Hi-Fi используют дома для комфортного прослушивания — то есть для удовольствия, для отдыха.\n\nПрофессиональное же звуковое оборудование предназначено для работы в соответствующей сфере (концертные площадки, клубы, рестораны с банкетами, звуковые студии и т.д.). В данном случае приставка «про» —\n_это не синоним элитарности, а показатель назначения продукта_\n.\n\nЧастое отличие профессионального сегмента и Hi-Fi — это звуковая мощность. Профессиональное оборудование должно выдерживать большие и продолжительные нагрузки (по 10-15 часов бесперебойной работы), выдавать «ровное» звучание без искажений на высокой громкости (в таком формате музыку дома никто не слушает). Здесь используются большие динамики, мощные компоненты, и внешний вид системы зачастую отходит на второй план.\n\nВ домашнем оборудовании огромную роль играют нюансы, особенности звучания, а также качество отделки. Производители до мельчайших деталей продумывают каждую мелочь. С большой тщательностью подбираются комплектующие, немаловажную роль имеет дизайн акустики или электроники.\n\nПоскольку для Hi-Fi оборудования важно, чтобы звучание было сбалансированным и детальным даже на небольшой громкости, перед инженерами, которые проектируют такую акустику или усилитель, стоят совершенно другие задачи. В усилителе должны стоять малошумные конденсаторы и трансформаторы, сам корпус также должен быть не только надежным и иметь вентиляционные щели для охлаждения, но и приятным глазу.\n\nДинамики в таком оборудовании обычно меньше тех, которые используются для про-акустики (размеры динамиков и комплектующих в данном случае уменьшают не в ущерб звуку), — это позволяет сделать всю систему более компактной.\n\n## Можно ли использовать Hi-Fi акустику вместо профессиональной? ##\n\nОдин раз мы столкнулись с ситуацией, когда заказчик планировал серьезный ресторан и хотел, чтобы фоновая музыка, играющая для посетителей, была максимально комфортной и приятной на слух, а внешний вид акустики обязательно должен был соответствовать интерьеру (в цвете дерева венге). Так как ресторан был небольшой, эту идею удалось реализовать: вместо усилителя был использован мощный ресивер в особом режиме и 3 пары полочной акустики с настенными кронштейнами.\n\nТем не менее, Hi-Fi оборудование — не совсем то, что нужно для фонового звука в том же ресторане. Оно предназначено для небольших помещений (комнат от 15 до 40 метров в среднем). Площади же ресторанов и кафе имеют зачастую более 50 метров, и домашнее оборудование (без серьезных вмешательств в акустику и дизайн помещения) на такие пространства просто не рассчитано.\n\nОсновное заблуждение в этом случае проистекает из того, что профессиональная акустика, предназначенная именно для фоновой подзвучки помещений, стоит недорого — по сравнению с Hi-Fi комплектами. У покупателей же часто срабатывает принцип «дешево — значит подозрительно и плохо».\n\nПрофессиональные системы в данном случае действительно могут звучать не настолько интересно, детально и ярко, как Hi-Fi, но и работать они будут в большом и довольно шумном помещении — причем работать долго. В таких условиях, в неподготовленном помещении этого вполне достаточно.\n\n## Кому может понадобиться проф. оборудование — кроме профессиональных музыкантов или звукорежиссеров? ##\n\nКто-то (этот кейс мы уже озвучивали) берет профессиональную акустику для кафе, кто-то – в детский сад, в школу, кто-то – для выездных мероприятий и т.д. Обычно для таких целей популярностью пользуются активные комплекты акустики (2 сателлита + встроенный микшер). В связи с повсеместным открытием караоке-клубов в России очень востребованы радиомикрофоны и микшерные пульты со встроенными эффектами.\n\nЕсли говорить о\nмузыкальных инструментах\n(в Аудиомании с недавнего времени можно купить и их тоже) сейчас довольно много непрофессионалов интересуется цифровыми пианино (например, для детей, которые учатся в музыкальной школе, или просто «для души»). У\nних\nесть ряд преимуществ по сравнению с классическим инструментом: не занимают много места, их можно передвигать и перевозить в легковой машине, не надо заказывать настройщика каждый год и можно играть в наушниках, никому не мешая.\n\nОтдельную нишу занимает оборудование для записей подкастов —\nстудийные микрофоны\nи звуковые карты сейчас многие покупают именно для «домашней» звукозаписи. Кстати, микрофоны для iOS, портативные рекордеры, а также различные аксессуары (стойки, чехлы, кабели, кронштейны — то, без чего все равно не обходится практически ни один проект, связанный с записью звука — как профессиональной, так и любительской) — это абсолютно новые для Аудиомании товары.\n\nВ планах у нас — расширить сегмент\nоборудования для конференций\n(существуют целые системы, серьезные комплекты для больших аудиторий): это еще одно интересное направление, в котором проф. оборудование может быть интересно тем, кто далек от профессиональной звукозаписи. Трансляционное оборудование было представлено у нас и раньше — в основном брендом Apart. Сейчас добавились товары и от других производителей профессионального оборудования: QSC, JBL, Penton, Tannoy.\n\n## Теперь о музыкантах: что интересного, уникального или редкого есть у Аудиомании для этой аудитории? ##\n\nВ нашем ассортименте уже 3 года присутствует бренд\nTaurus\n— в России только мы представляем продукцию этого польского производителя. Басовые и гитарные усилители, кабинеты, комбоусилители и педали эффектов. Эндорсерами бренда в России являются музыканты из группы «Ария» — Виталий Дубинин и Сергей Попов.\n\nИскушенным музыкантам стоит обратить внимание на\nмикрофонные и гитарные кабели Analysis Plus\n(это наша эксклюзивная дистрибуция). Еще один интересный вариант профессиональных кабелей — немецкий бренд Schulz, который музыканты ценят за хорошее качество при сравнительно невысокой стоимости.\n\nТакже особое внимание стоит уделить уже легендарным немецким студийным мониторам\nADAM\n, которые тоже появились в нашем ассортименте недавно, и продукции\nQSC\n(это акустика, усилители со встроенными процессорами и цифровые микшерные пульты, которые привлекают много внимания уже за счет своей функциональности и простоты в управлении).\n\nНе секрет, что существуют бренды, которые делают оборудование как в сегменте домашнего оборудования, так и в про-направлении, например, DENON и Marantz: они сейчас также расширяют свой модельный ряд. У Marantz есть хорошие студийные микрофоны, появились новые портативные и стационарные рекордеры, студийные мониторы; у DENON стоит обратить внимание на профессиональные проигрыватели и\nрекордеры\n, а также студийные мониторы.\n\n## Что посоветуете подарить музыканту на Новый год? ##\n\nОчень сложно угодить музыканту, если он сам не озвучил свои пожелания — среди тех же гитаристов есть немало направлений. Чего-то универсального не существует, даже процессоры и педали эффектов подбираются по стилю музыки. Но в любом случае есть аксессуары, которые всегда пригодятся гитаристу — это новый\nчехол для гитары\n, ремень или\nпедалборд\nпод педали эффектов. Полезный и бюджетный подарок для барабанщика — комплект новых\nбарабанных палочек\n, так как они часто ломаются.\n\nБолее значимый и дорогой подарок гитаристу — качественный\nгитарный кабель\n. Для барабанщика, который играет только на репетиционных базах, подходящим подарком может стать электронная барабанная установка. А для человека, играющего на цифровом пианино или синтезаторе, может подойти подарок в виде хороших накладных или\nохватывающих наушников\n, которые позволят ему наслаждаться своей игрой или учиться, не теряя всех преимуществ встроенных динамиков инструмента.\n\nP.S. Дополнительное чтение по теме:\n\nКак построить домашний кинотеатр, и что можно узнать о производстве акустики\n\nЧто нужно знать о современных аудиосистемах\n\nКак сделать аудиотехнику элементом интерьера\n\nКраткое содержание:\nАудиомания предлагает не только домашнюю акустику и электронику, но и другие категории продуктов, связанных с музыкой. В профессиональном оборудовании основное различие между Hi-Fi и про-сегментом заключается в предназначении продукта: Hi-Fi используется дома для комфортного прослушивания, а профессиональное звуковое оборудование предназначено для работы в соответствующей сфере.\n\nHi-Fi оборудование не подходит для фонового звука в больших помещениях, но может звучать не настолько интересно и ярко, как профессиональные системы. Профессиональные системы могут работать в больших и шумных помещениях и долго.\n\nКроме профессиональных музыкантов и звукорежиссеров, профессиональное оборудование может понадобиться для фоновой подзвучки помещений, детских садов, школ, караоке-клубов и других мероприятий.\n\nДля музыкантов Аудиомания предлагает басовые и гитарные усилители, кабинеты, комбоусилители и педали эффектов, микрофонные и гитарные кабели, студийные мониторы и цифровые микшерные пульты.\n\nВ качестве подарка музыканту можно рассмотреть новый чехол для гитары, ремень или педалборд, барабанные палочки, гитарный кабель, электронную барабанную установку или наушники.","summary":"Аудиомания предлагает не только домашнюю акустику и электронику, но и другие категории продуктов, связанных с музыкой. В профессиональном оборудовании основное различие между Hi-Fi и про-сегментом заключается в предназначении продукта: Hi-Fi используется дома для комфортного прослушивания, а профессиональное звуковое оборудование предназначено для работы в соответствующей сфере.\n\nHi-Fi оборудование не подходит для фонового звука в больших помещениях, но может звучать не настолько интересно и ярко, как профессиональные системы. Профессиональные системы могут работать в больших и шумных помещениях и долго.\n\nКроме профессиональных музыкантов и звукорежиссеров, профессиональное оборудование может понадобиться для фоновой подзвучки помещений, детских садов, школ, караоке-клубов и других мероприятий.\n\nДля музыкантов Аудиомания предлагает басовые и гитарные усилители, кабинеты, комбоусилители и педали эффектов, микрофонные и гитарные кабели, студийные мониторы и цифровые микшерные пульты.\n\nВ качестве подарка музыканту можно рассмотреть новый чехол для гитары, ремень или педалборд, барабанные палочки, гитарный кабель, электронную барабанную установку или наушники."}
{"Text":"Кешируем CRUD в IndexedDB / Habr             \n\n27-04-2021\nДопустим, у нас есть бекенд, который умеет хранить какие-то сущности. И у него есть апи для создания, чтения, изменения и удаления этих сущностей, сокращенно CRUD. Но апи на сервере, а пользователь забрался куда-то глубоко и половина запросов валится по таймауту. Не хотелось бы показывать бесконечный прелоадер и вообще блокировать действия пользователя. Offline first предполагает загрузку приложения из кеша, так может быть и данные брать оттуда? Предлагается хранить все данные в IndexedDB (допустим, что их не очень много), и по возможности синхронизировать с сервером. Возникает несколько проблем: Если Id сущности генерится на сервере, в базе, то как жить без Id, пока сервер недоступен? При синхронизации с сервером, как отличать сущности созданные на клиенте от удаленных на сервере другим пользователем? Как разрешать конфликты? Идентификация Идентификатор нужен, так что будем его создавать сами. Для этого прекрасно подходит GUID или `+new Date()` с некоторыми оговорками. Только когда придет ответ от сервера с настоящим Id, надо везде его заменить. Если на эту свежесозданную сущность уже ссылаются другие, то эти ссылки тоже надо поправить. Синхронизация Изобретать велосипед не будем, посмотрим на репликацию баз данных. Смотреть на нее можно бесконечно, как на пожар, но вкратце, один из вариантов выглядит так: помимо сохранения сущности в IndexedDB, будем писать лог изменений: [time, 'update', Id=3, Name='Иван'], [time, 'create', Name='Иван', Surname='Петров'], [time, 'delete', Id=3]... При получении с сервера свежих данных, мы можем сравнить их с существующими и вычислить аналогичный лог изменений на сервере. После этого объединить логи и отправить необходимые изменения на сервер, если интернет еще не пропал, и только после этого записывать изменения в IndexedDB. И не забыть обновить Id. Конфликты Конфликт - это не спор между двумя пользователями, чья точка зрения правильная, и поочередное исправление одной и той же записи до посинения. А вот ситуация, когда пользователи довольны и видят каждый свою версию - конфликт, а конкретно неконсистентность. Непрерывной консистентности в веб приложениях достичь несложно - при каждом изменении блокировать всех клиентов, кого это изменение касается, пока они все не подтвердят получение. Это никому не нравится, поэтому приходится идти на компромис: ладно, пусть иногда пользователи видят разное, но если все замрут и перестанут вносить изменения, то через некоторое время у всех будет одно и то же. Для этого придумали термин Eventual Consistency. Оказалось, что ее можно достичь незаметно для пользователя, но не так просто. Можно использовать Operational Transformations (OT) или Conflict-free Replicated Data Types (CRDT) но для них придется довольно радикально поменять формат обмена данных с сервером. Если это невозможно, то можно на коленке сделать CRDT на минималках: добавить в сущность поле UpdatedAt и записывать в него время последнего изменения. Это не избавит от всех конфликтов, но снизит их количество на порядок. Итак, при объединении двух логов группируем их по Id сущности и дальше работаем в каждой группе отдельно. Если в одном из логов есть операция удаления, то оставляем только ее. Пользователь, удаливший запись наверняка имел на это веские основания и не хотел бы, чтобы запись вдруг возродилась. Никто не любит зомби кроме зомби. Если в одном из логов есть операция создания сущности, то в другом логе должно быть пусто, ведь Id уникальный, ага. С изменениями немного сложнее - нужно посмотреть на время последнего изменения сущности в каждом из логов. Сравнить. И выбрать тот лог, в который изменение пришло позднее. Last write win. Проверим Eventual Consistency: если все пользователи перестанут вносить изменения и подключатся к интернету, у всех будут сущности последней версии. Отлично. function mergeLogs(left, right){ const ids = new Set([ ...left.map(x => x.id), ...right.map(x => x.id) ]); return [...ids].map(id => mergeIdLogs( left.filter(x => x.id == id), right.filter(x => x.id ==id) )).reduce((a,b) => ({ left: [...a.left, ...b.left], right: [...a.right, ...b.right] }), {left: [], right: []}); } function mergeIdLogs(left,right){ const isWin = log => log.some(x => ['create','delete'].includes(x.type)); const getMaxUpdate = log => Math.max(...log.map(x => +x.updatedAt)); if (isWin(left)) return {left: [], right: left}; if (isWin(right)) return {left: right, right: []}; if (getMaxUpdate(left) > getMaxUpdate(right)) return {left: [], right: left}; else return {left: right, right: []}; } Эпилог Реализации не будет, потому что в каждом конкретном случае есть свой дьявол в деталях, да и реализовывать тут по большому счету нечего - генерацию идентификатора и запись в indexedDB. Конечно, CRDT или OT будут лучше, но если нужно сделать быстро, а на бекенд не пускают, то сгодится и это поделие.\n","metric":0.62548828130000000414,"metric_input":"Текст статьи:\nКешируем CRUD в IndexedDB / Habr             \n\n27-04-2021\nДопустим, у нас есть бекенд, который умеет хранить какие-то сущности. И у него есть апи для создания, чтения, изменения и удаления этих сущностей, сокращенно CRUD. Но апи на сервере, а пользователь забрался куда-то глубоко и половина запросов валится по таймауту. Не хотелось бы показывать бесконечный прелоадер и вообще блокировать действия пользователя. Offline first предполагает загрузку приложения из кеша, так может быть и данные брать оттуда? Предлагается хранить все данные в IndexedDB (допустим, что их не очень много), и по возможности синхронизировать с сервером. Возникает несколько проблем: Если Id сущности генерится на сервере, в базе, то как жить без Id, пока сервер недоступен? При синхронизации с сервером, как отличать сущности созданные на клиенте от удаленных на сервере другим пользователем? Как разрешать конфликты? Идентификация Идентификатор нужен, так что будем его создавать сами. Для этого прекрасно подходит GUID или `+new Date()` с некоторыми оговорками. Только когда придет ответ от сервера с настоящим Id, надо везде его заменить. Если на эту свежесозданную сущность уже ссылаются другие, то эти ссылки тоже надо поправить. Синхронизация Изобретать велосипед не будем, посмотрим на репликацию баз данных. Смотреть на нее можно бесконечно, как на пожар, но вкратце, один из вариантов выглядит так: помимо сохранения сущности в IndexedDB, будем писать лог изменений: [time, 'update', Id=3, Name='Иван'], [time, 'create', Name='Иван', Surname='Петров'], [time, 'delete', Id=3]... При получении с сервера свежих данных, мы можем сравнить их с существующими и вычислить аналогичный лог изменений на сервере. После этого объединить логи и отправить необходимые изменения на сервер, если интернет еще не пропал, и только после этого записывать изменения в IndexedDB. И не забыть обновить Id. Конфликты Конфликт - это не спор между двумя пользователями, чья точка зрения правильная, и поочередное исправление одной и той же записи до посинения. А вот ситуация, когда пользователи довольны и видят каждый свою версию - конфликт, а конкретно неконсистентность. Непрерывной консистентности в веб приложениях достичь несложно - при каждом изменении блокировать всех клиентов, кого это изменение касается, пока они все не подтвердят получение. Это никому не нравится, поэтому приходится идти на компромис: ладно, пусть иногда пользователи видят разное, но если все замрут и перестанут вносить изменения, то через некоторое время у всех будет одно и то же. Для этого придумали термин Eventual Consistency. Оказалось, что ее можно достичь незаметно для пользователя, но не так просто. Можно использовать Operational Transformations (OT) или Conflict-free Replicated Data Types (CRDT) но для них придется довольно радикально поменять формат обмена данных с сервером. Если это невозможно, то можно на коленке сделать CRDT на минималках: добавить в сущность поле UpdatedAt и записывать в него время последнего изменения. Это не избавит от всех конфликтов, но снизит их количество на порядок. Итак, при объединении двух логов группируем их по Id сущности и дальше работаем в каждой группе отдельно. Если в одном из логов есть операция удаления, то оставляем только ее. Пользователь, удаливший запись наверняка имел на это веские основания и не хотел бы, чтобы запись вдруг возродилась. Никто не любит зомби кроме зомби. Если в одном из логов есть операция создания сущности, то в другом логе должно быть пусто, ведь Id уникальный, ага. С изменениями немного сложнее - нужно посмотреть на время последнего изменения сущности в каждом из логов. Сравнить. И выбрать тот лог, в который изменение пришло позднее. Last write win. Проверим Eventual Consistency: если все пользователи перестанут вносить изменения и подключатся к интернету, у всех будут сущности последней версии. Отлично. function mergeLogs(left, right){ const ids = new Set([ ...left.map(x => x.id), ...right.map(x => x.id) ]); return [...ids].map(id => mergeIdLogs( left.filter(x => x.id == id), right.filter(x => x.id ==id) )).reduce((a,b) => ({ left: [...a.left, ...b.left], right: [...a.right, ...b.right] }), {left: [], right: []}); } function mergeIdLogs(left,right){ const isWin = log => log.some(x => ['create','delete'].includes(x.type)); const getMaxUpdate = log => Math.max(...log.map(x => +x.updatedAt)); if (isWin(left)) return {left: [], right: left}; if (isWin(right)) return {left: right, right: []}; if (getMaxUpdate(left) > getMaxUpdate(right)) return {left: [], right: left}; else return {left: right, right: []}; } Эпилог Реализации не будет, потому что в каждом конкретном случае есть свой дьявол в деталях, да и реализовывать тут по большому счету нечего - генерацию идентификатора и запись в indexedDB. Конечно, CRDT или OT будут лучше, но если нужно сделать быстро, а на бекенд не пускают, то сгодится и это поделие.\n\n\nКраткое содержание:\nВ статье рассматривается проблема синхронизации данных между веб-приложением и сервером в режиме оффлайн. Предлагается хранить данные в IndexedDB, а при синхронизации с сервером записывать лог изменений. Для решения конфликтов предлагается использовать Eventual Consistency, группируя операции по идентификаторам сущностей и выбирая последнее изменение.","summary":"В статье рассматривается проблема синхронизации данных между веб-приложением и сервером в режиме оффлайн. Предлагается хранить данные в IndexedDB, а при синхронизации с сервером записывать лог изменений. Для решения конфликтов предлагается использовать Eventual Consistency, группируя операции по идентификаторам сущностей и выбирая последнее изменение."}
{"Text":"Двухфакторная аутентификация клиентов Cisco AnyConnect. FreeRadius и Google Authenticator / Habr                        \n\n19-11-2015\nВ предыдущей статье я рассматривал настройку двухфакторной аутентификации пользователей для подключения к корпоративным ресурсам через Cisco AnyConnect на базе Active Directory и Microsoft Azure Multi-Factor Authentication Server (MFAS). Сегодня предлагаю Вам рассмотреть вариант с использованием FreeRadius и Google Authenticator. Из плюсов данного варианта можно отметить только то, что это решение бесплатное. К минусам можно отнести: Интеграция с AD. Конечно можно интегрировать FreeRadius с AD через LDAP, но особенности архитектуры данного решения предусматривают хранение профилей пользователей локально на FreeRadius сервере. Только один тип аутентификации – Time-Based One Time Password (TOTP). Неочевидность необходимости использования второго фактора (пояснения в статье). Установка компонентов Для установки FreeRaduis я использовал Debian Jessie 8.0 на виртуальной машине. Ввиду того, что для авторизации будет использоваться TOTP, время на сервере должно быть правильным. Лучший способ синхронизировать время – установка NTP. sudo apt-get update sudo apt-get install ntp Далее устанавливаем FreeRadius и необходимые модули: sudo apt-get install build-essential libpam0g-dev freeradius libqrencode3 git Загружаем и устанавливаем Google Authenticator: cd ~ git clone https://code.google.com/p/google-authenticator/ cd google-authenticator/libpam/ make make install Также нам потребуется группа для пользователей, которых мы больше не хотим аутентифицировать: addgroup radius-off Настройка FreeRadius Так как FreeRadius должен иметь доступ к .google_authenticator токенам во всех пользовательских директориях, он должен иметь права root-а. Для предоставления прав редактируем файл /etc/freeradius/radusd.conf. Находим строки: user = freerad group = freerad И заменяем на: user = root group = root Здесь, и в дальнейшем, не забываем сохранять изменения. Далее редактируем /etc/freeradius/users. Мы должны добавить ранее созданную группу «radius-off» в секцию «Deny access for a group of users.» После строк: # Deny access for a group of users. # # Note that there is NO 'Fall-Through' attribute, so the user will not # be given any additional resources. # Добавляем: DEFAULT Group == \"radius-disabled\", Auth-Type := Reject Reply-Message = \"Your account has been disabled.\" DEFAULT Auth-Type := PAM Теперь редактируем /etc/freeradius/sites-enabled/default. Находим: # Pluggable Authentication Modules. # pam И раскомментируем строку с pam Редактируем /etc/pam.d/radiusd. Говорим FreeRadius аутентифицировать пользователей по принципу локальный unix пароль + Google Authenticator код. Здесь мы должны закомметнировать все строки которые начинаются с @: #@include common-auth #@include common-account #@include common-password #@include common-session И добавляем: auth requisite pam_google_authenticator.so forward_pass auth required pam_unix.so use_first_pass Теперь нам нужно добавить Radius клиента, что бы FreeRadius обрабатывал запросы от CISCO ASA. В файл /etc/freeradius/clients.conf добавляем: Client 192.168.110.6 { secret = Password shortname = ASA } Настройка пользователя Создаем пользователя: adduser mfatest Создаем для него google-authenticator: cd /home/mfatest/ su mfatest google-authenticator В ответ получаем QR code: После этого необходимо перезапустить сервис FreeRadius: sudo service freeradius restart На мобильном устройстве должно быть установлено приложение Google Authenticator (ссылки для iOS и Android). Сканируем QR code с помощью приложения Google Authenticator. В итоге учетная запись «mfatest» подвязывается к Вашему мобильному устройству. Протестируем аутентификацию: radtest <username> <unix_password><google_auth> localhost 18120 testing123 Где: <unix_password><google_auth> — это unix пароль + код из приложения в одну строку. В данном случае пароль Pass_123, код – 731923. localhost 18120 testing123– это параметры стандартного локального radius клиента. В итоге получается: radtest mfatest Pass_123731923 localhost 18120 testing123 Если все настроено верно, в ответ должны получить что-то вроде: Настройка CISCO ASA На ASA рекомендую настроить AnyConnect VPN gateway с локальной аутентификацией. Убедиться, что подключение работает, после чего приступить к настройке аутентификации через Radius. Настраиваем RADIUS: Переходим на Configuration / Remote Access VPN / AAA/Local Users / AAA Server Groups и создаем группу: Добавляем в группу сервер: Где Server Name or IP Address – это адрес нашего FreeRadius, Server Secret Key – ключ который мы настроили для клиента. Тестируем связку с RADIUS сервером: Где Password – это пароль пользователя + код из Google Authenticator При успешном тесте, на ранее настроенном «AnyConnect Connection Profiles» меняем аутентификацию с локальной на группу FreeRad: На этом настройки закончены. Чтобы убедится что все работает инициируем тестовое подключение. Вот здесь и появляется та неочевидность, о которой я говорил. FreeRadius не использует поэтапную аутентификацию, он не умеет (по крайней мере я не нашел) запрашивать код для Google Authentication отдельно. Поэтому для успешной аутентификации, как и в тестах, в строке Password мы вводим пароль+код. Для неискушенного пользователя это может стать преградой для комфортного использования сервиса. Данная схема универсальна и может быть реализована для любых сервисов, поддерживающих аутентификацию посредством протокола Radius. Остальные наши статьи http://servilon.ru/stati/\n","metric":0.53955078130000000414,"metric_input":"Текст статьи:\nДвухфакторная аутентификация клиентов Cisco AnyConnect. FreeRadius и Google Authenticator / Habr                        \n\n19-11-2015\nВ предыдущей статье я рассматривал настройку двухфакторной аутентификации пользователей для подключения к корпоративным ресурсам через Cisco AnyConnect на базе Active Directory и Microsoft Azure Multi-Factor Authentication Server (MFAS). Сегодня предлагаю Вам рассмотреть вариант с использованием FreeRadius и Google Authenticator. Из плюсов данного варианта можно отметить только то, что это решение бесплатное. К минусам можно отнести: Интеграция с AD. Конечно можно интегрировать FreeRadius с AD через LDAP, но особенности архитектуры данного решения предусматривают хранение профилей пользователей локально на FreeRadius сервере. Только один тип аутентификации – Time-Based One Time Password (TOTP). Неочевидность необходимости использования второго фактора (пояснения в статье). Установка компонентов Для установки FreeRaduis я использовал Debian Jessie 8.0 на виртуальной машине. Ввиду того, что для авторизации будет использоваться TOTP, время на сервере должно быть правильным. Лучший способ синхронизировать время – установка NTP. sudo apt-get update sudo apt-get install ntp Далее устанавливаем FreeRadius и необходимые модули: sudo apt-get install build-essential libpam0g-dev freeradius libqrencode3 git Загружаем и устанавливаем Google Authenticator: cd ~ git clone https://code.google.com/p/google-authenticator/ cd google-authenticator/libpam/ make make install Также нам потребуется группа для пользователей, которых мы больше не хотим аутентифицировать: addgroup radius-off Настройка FreeRadius Так как FreeRadius должен иметь доступ к .google_authenticator токенам во всех пользовательских директориях, он должен иметь права root-а. Для предоставления прав редактируем файл /etc/freeradius/radusd.conf. Находим строки: user = freerad group = freerad И заменяем на: user = root group = root Здесь, и в дальнейшем, не забываем сохранять изменения. Далее редактируем /etc/freeradius/users. Мы должны добавить ранее созданную группу «radius-off» в секцию «Deny access for a group of users.» После строк: # Deny access for a group of users. # # Note that there is NO 'Fall-Through' attribute, so the user will not # be given any additional resources. # Добавляем: DEFAULT Group == \"radius-disabled\", Auth-Type := Reject Reply-Message = \"Your account has been disabled.\" DEFAULT Auth-Type := PAM Теперь редактируем /etc/freeradius/sites-enabled/default. Находим: # Pluggable Authentication Modules. # pam И раскомментируем строку с pam Редактируем /etc/pam.d/radiusd. Говорим FreeRadius аутентифицировать пользователей по принципу локальный unix пароль + Google Authenticator код. Здесь мы должны закомметнировать все строки которые начинаются с @: #@include common-auth #@include common-account #@include common-password #@include common-session И добавляем: auth requisite pam_google_authenticator.so forward_pass auth required pam_unix.so use_first_pass Теперь нам нужно добавить Radius клиента, что бы FreeRadius обрабатывал запросы от CISCO ASA. В файл /etc/freeradius/clients.conf добавляем: Client 192.168.110.6 { secret = Password shortname = ASA } Настройка пользователя Создаем пользователя: adduser mfatest Создаем для него google-authenticator: cd /home/mfatest/ su mfatest google-authenticator В ответ получаем QR code: После этого необходимо перезапустить сервис FreeRadius: sudo service freeradius restart На мобильном устройстве должно быть установлено приложение Google Authenticator (ссылки для iOS и Android). Сканируем QR code с помощью приложения Google Authenticator. В итоге учетная запись «mfatest» подвязывается к Вашему мобильному устройству. Протестируем аутентификацию: radtest <username> <unix_password><google_auth> localhost 18120 testing123 Где: <unix_password><google_auth> — это unix пароль + код из приложения в одну строку. В данном случае пароль Pass_123, код – 731923. localhost 18120 testing123– это параметры стандартного локального radius клиента. В итоге получается: radtest mfatest Pass_123731923 localhost 18120 testing123 Если все настроено верно, в ответ должны получить что-то вроде: Настройка CISCO ASA На ASA рекомендую настроить AnyConnect VPN gateway с локальной аутентификацией. Убедиться, что подключение работает, после чего приступить к настройке аутентификации через Radius. Настраиваем RADIUS: Переходим на Configuration / Remote Access VPN / AAA/Local Users / AAA Server Groups и создаем группу: Добавляем в группу сервер: Где Server Name or IP Address – это адрес нашего FreeRadius, Server Secret Key – ключ который мы настроили для клиента. Тестируем связку с RADIUS сервером: Где Password – это пароль пользователя + код из Google Authenticator При успешном тесте, на ранее настроенном «AnyConnect Connection Profiles» меняем аутентификацию с локальной на группу FreeRad: На этом настройки закончены. Чтобы убедится что все работает инициируем тестовое подключение. Вот здесь и появляется та неочевидность, о которой я говорил. FreeRadius не использует поэтапную аутентификацию, он не умеет (по крайней мере я не нашел) запрашивать код для Google Authentication отдельно. Поэтому для успешной аутентификации, как и в тестах, в строке Password мы вводим пароль+код. Для неискушенного пользователя это может стать преградой для комфортного использования сервиса. Данная схема универсальна и может быть реализована для любых сервисов, поддерживающих аутентификацию посредством протокола Radius. Остальные наши статьи http://servilon.ru/stati/\n\n\nКраткое содержание:\nВ данной статье рассматривается настройка двухфакторной аутентификации пользователей для подключения к корпоративным ресурсам через Cisco AnyConnect на базе Active Directory и Microsoft Azure Multi-Factor Authentication Server (MFAS). Описывается вариант с использованием FreeRadius и Google Authenticator.","summary":"В данной статье рассматривается настройка двухфакторной аутентификации пользователей для подключения к корпоративным ресурсам через Cisco AnyConnect на базе Active Directory и Microsoft Azure Multi-Factor Authentication Server (MFAS). Описывается вариант с использованием FreeRadius и Google Authenticator."}
{"Text":"Uberмен: как водитель Uber зарабатывает $252 тыс в год / Хабр                 \n\nЧеловек, сидящий рядом со мной, даёт самые дельные советы по бизнесу, которые я слышал за последнее время. Правда, разговор не совсем соответствует месту, в котором мы находимся. Это не кафе и не офис. Мы находимся в автомобиле Uber, а он – мой водитель (\nUber\n— сервис для поиска, вызова и оплаты такси или частных водителей).\n\nЭто Гевин Эсколар, харизматичный филиппинец, смех которого имеет ещё более кричащий оттенок, чем его рубашка с оранжевыми и красными полосками. Мы едем, и я постепенно замечаю, что на панели болтаются бриллиантовые серьги, у него на запястьях блестят изумрудные браслеты. А из карманов сидений торчат глянцевые каталоги с множеством украшений и надписью: «Гевин Эсколар. Коллекция 2014»\n\nИ вдруг до меня дошло: я нахожусь не просто в машине – я в передвижном торговом павильоне! А он не просто водитель. И не просто бизнесмен. Он – Uberмен! Гевин использует сервис Uber для продвижения своего бизнеса. Мне захотелось узнать всю историю Гевина, и он с радостью ею поделился.\n\nЕго родина – Филиппины. Когда Гевин иммигрировал в Сан-Франциско, он поставил перед собой цель – создать свой ювелирный бизнес. Однако для начала он решил пойти работать водителем в Uber, чтобы хоть как-то свести концы с концами. «Мои пассажиры удивляли меня», – говорит Гевин. «Я думал, что они будут ехать молча или уткнутся в свои телефоны, но почему-то большинству пассажиров хотелось поговорить. Когда разговор заходил про мои украшения, то они просили у меня визитки. И тогда я подумал, что можно не останавливаться на визитках, а размещать ювелирные изделия и каталоги прямо в автомобиле!»\n\nТак Гевин превратил свою машину в торговый павильон.\n\n«Когда пассажиры замечали товар, я рассказывал про него. Но сам никогда не навязывался. А если они не обращали внимания, то, значит, это не моя целевая аудитория».\n\n«Произвести впечатление в течение 10 минут – мечта любого продавца. Потому что обычно времени гораздо меньше. Взять хотя бы ту же торговлю вразнос или online рекламу. И лучше всего – то, что в Uber мне платили стабильную зарплату, и я мог спокойно жить, когда продажа драгоценностей шла плохо»\n\nДля водителей Uber существует система рейтингов. У Гевина было 4.85/5.00 на Uber Black, 4.87/5.00 на UberX и 4.95/5.00 на Lyft. За последние 18 месяцев Гевин обслужил около 3,829 пассажиров. Среди них были и весьма обеспеченные деловые люди, например, редакторы Vogue, топ-менеджеры Силиконовой долины, включая известного инвестора\nШервина Пишвара\n.\n\n«Мы видели много удивительных водителей, но Гевин – один из лучших», – говорил Шервин. «Я увидел его драгоценности в машине и подумал, что это прекрасно и заслуживает хорошего поощрения с моей стороны»\n\nПока мы с Гевином ехали, я наблюдал, как он общался с пассажирами. Если пассажир всю дорогу молчал, то и Гевин не отвлекал его. Если же пассажиры начинали общение, то Гевин старался говорить немного, задавал им вопросы и пытался понять, нужно ли им что-нибудь.\n\nТак, в прошлом году Гевин зарабатывал на изготовлении драгоценностей в среднем \\$18,000 в месяц. Если добавить сюда заработок в Uber, то за год получается \\$252,000. Свою прибыль Гевин использовал для расширения бизнеса: купил ещё 3 машины и нанял 6 водителей.\n\n«Лучшие генералы всегда находятся со своими солдатами», – говорит он. Поэтому Гевин сам тоже возит пассажиров, как минимум, 8 часов в неделю. Это позволяет ему постоянно обновлять информацию о том, как изменилась ситуация на дорогах, по каким маршрутам быстрее всего ездить и в каких районах находится больше клиентов.\n\nРазвивая свой бизнес, Гевин думает не только о себе, но и о бедных иммигрантах из Филиппин, кем, в свое время, был и он сам. При приеме на работу водителей он отдает предпочтение филиппинцам. Многие из них не знакомы с современными технологиями, поэтому он также учит их пользоваться интернетом, смартфонами, сервисами Uber.\n\nОпыт Гевина будет полезен многим другим предпринимателям. Работая в Uber, он получил то, что хотят иметь многие: стабильную зарплату, возможность создать собственное дело, деловые контакты с бизнесменами, а также общение и обратную связь от своей целевой аудитории.\n\nКонечно, не всё так гладко, как может показаться. Ведь не все водители настолько тактичны, как Гевин. Не чувствуя настроения пассажира и не видя его заинтересованности, они будут предлагать свои товары чуть ли не насильно! Но в этом случае на помощь приходит система рейтингов. Недовольные пассажиры могут поставить им низкие оценки, защищая себя от назойливых и чрезмерно разговорчивых водителей.\n\nА может быть, Гевин не один такой? Может быть, его примеру последовал кто-то ещё? Чтобы узнать это, мы опросили 89 водителей Uber и Lyft. Оказалось, что 28 из них (31.5%) тоже имеют свой бизнес. Но только двое из них размещают свои рекламные материалы в своих автомобилях. Так что, эти люди позволяют себе просто сидеть за рулем, день за днем упуская отличную возможность для развития своего бизнеса.\n\nИстория Гевина ещё раз показывает, что путь, которым человек идет к своей мечте, может иметь самые неожиданные повороты. Главное – всегда быть готовым сделать шаг вперед и не смотреть на происходящие перемены как на новые проблемы, а видеть перед собой неограниченные возможности.","metric":0.56640625,"metric_input":"Текст статьи:\nUberмен: как водитель Uber зарабатывает $252 тыс в год / Хабр                 \n\nЧеловек, сидящий рядом со мной, даёт самые дельные советы по бизнесу, которые я слышал за последнее время. Правда, разговор не совсем соответствует месту, в котором мы находимся. Это не кафе и не офис. Мы находимся в автомобиле Uber, а он – мой водитель (\nUber\n— сервис для поиска, вызова и оплаты такси или частных водителей).\n\nЭто Гевин Эсколар, харизматичный филиппинец, смех которого имеет ещё более кричащий оттенок, чем его рубашка с оранжевыми и красными полосками. Мы едем, и я постепенно замечаю, что на панели болтаются бриллиантовые серьги, у него на запястьях блестят изумрудные браслеты. А из карманов сидений торчат глянцевые каталоги с множеством украшений и надписью: «Гевин Эсколар. Коллекция 2014»\n\nИ вдруг до меня дошло: я нахожусь не просто в машине – я в передвижном торговом павильоне! А он не просто водитель. И не просто бизнесмен. Он – Uberмен! Гевин использует сервис Uber для продвижения своего бизнеса. Мне захотелось узнать всю историю Гевина, и он с радостью ею поделился.\n\nЕго родина – Филиппины. Когда Гевин иммигрировал в Сан-Франциско, он поставил перед собой цель – создать свой ювелирный бизнес. Однако для начала он решил пойти работать водителем в Uber, чтобы хоть как-то свести концы с концами. «Мои пассажиры удивляли меня», – говорит Гевин. «Я думал, что они будут ехать молча или уткнутся в свои телефоны, но почему-то большинству пассажиров хотелось поговорить. Когда разговор заходил про мои украшения, то они просили у меня визитки. И тогда я подумал, что можно не останавливаться на визитках, а размещать ювелирные изделия и каталоги прямо в автомобиле!»\n\nТак Гевин превратил свою машину в торговый павильон.\n\n«Когда пассажиры замечали товар, я рассказывал про него. Но сам никогда не навязывался. А если они не обращали внимания, то, значит, это не моя целевая аудитория».\n\n«Произвести впечатление в течение 10 минут – мечта любого продавца. Потому что обычно времени гораздо меньше. Взять хотя бы ту же торговлю вразнос или online рекламу. И лучше всего – то, что в Uber мне платили стабильную зарплату, и я мог спокойно жить, когда продажа драгоценностей шла плохо»\n\nДля водителей Uber существует система рейтингов. У Гевина было 4.85/5.00 на Uber Black, 4.87/5.00 на UberX и 4.95/5.00 на Lyft. За последние 18 месяцев Гевин обслужил около 3,829 пассажиров. Среди них были и весьма обеспеченные деловые люди, например, редакторы Vogue, топ-менеджеры Силиконовой долины, включая известного инвестора\nШервина Пишвара\n.\n\n«Мы видели много удивительных водителей, но Гевин – один из лучших», – говорил Шервин. «Я увидел его драгоценности в машине и подумал, что это прекрасно и заслуживает хорошего поощрения с моей стороны»\n\nПока мы с Гевином ехали, я наблюдал, как он общался с пассажирами. Если пассажир всю дорогу молчал, то и Гевин не отвлекал его. Если же пассажиры начинали общение, то Гевин старался говорить немного, задавал им вопросы и пытался понять, нужно ли им что-нибудь.\n\nТак, в прошлом году Гевин зарабатывал на изготовлении драгоценностей в среднем \\$18,000 в месяц. Если добавить сюда заработок в Uber, то за год получается \\$252,000. Свою прибыль Гевин использовал для расширения бизнеса: купил ещё 3 машины и нанял 6 водителей.\n\n«Лучшие генералы всегда находятся со своими солдатами», – говорит он. Поэтому Гевин сам тоже возит пассажиров, как минимум, 8 часов в неделю. Это позволяет ему постоянно обновлять информацию о том, как изменилась ситуация на дорогах, по каким маршрутам быстрее всего ездить и в каких районах находится больше клиентов.\n\nРазвивая свой бизнес, Гевин думает не только о себе, но и о бедных иммигрантах из Филиппин, кем, в свое время, был и он сам. При приеме на работу водителей он отдает предпочтение филиппинцам. Многие из них не знакомы с современными технологиями, поэтому он также учит их пользоваться интернетом, смартфонами, сервисами Uber.\n\nОпыт Гевина будет полезен многим другим предпринимателям. Работая в Uber, он получил то, что хотят иметь многие: стабильную зарплату, возможность создать собственное дело, деловые контакты с бизнесменами, а также общение и обратную связь от своей целевой аудитории.\n\nКонечно, не всё так гладко, как может показаться. Ведь не все водители настолько тактичны, как Гевин. Не чувствуя настроения пассажира и не видя его заинтересованности, они будут предлагать свои товары чуть ли не насильно! Но в этом случае на помощь приходит система рейтингов. Недовольные пассажиры могут поставить им низкие оценки, защищая себя от назойливых и чрезмерно разговорчивых водителей.\n\nА может быть, Гевин не один такой? Может быть, его примеру последовал кто-то ещё? Чтобы узнать это, мы опросили 89 водителей Uber и Lyft. Оказалось, что 28 из них (31.5%) тоже имеют свой бизнес. Но только двое из них размещают свои рекламные материалы в своих автомобилях. Так что, эти люди позволяют себе просто сидеть за рулем, день за днем упуская отличную возможность для развития своего бизнеса.\n\nИстория Гевина ещё раз показывает, что путь, которым человек идет к своей мечте, может иметь самые неожиданные повороты. Главное – всегда быть готовым сделать шаг вперед и не смотреть на происходящие перемены как на новые проблемы, а видеть перед собой неограниченные возможности.\n\nКраткое содержание:\nСтатья рассказывает о Гевине Эсколаре, филиппинце, который работает водителем Uber и использует свой автомобиль для продвижения ювелирного бизнеса. Он превращает свою машину в передвижной торговый павильон, размещая в ней ювелирные изделия и каталоги. Пассажиры, которые замечают товар, могут узнать больше о его продукции, а те, кто не обращает внимания, могут быть не его целевой аудиторией. Гевин зарабатывает в среднем 18 000 долларов в месяц на изготовлении драгоценностей, а с учетом заработка в Uber в год получает 252 000 долларов. Он также использует Uber для расширения своего бизнеса, покупая еще три машины и нанимая шесть водителей. Гевин сам возит пассажиров как минимум 8 часов в неделю, чтобы быть в курсе изменений на дорогах и в районах с наибольшим количеством клиентов. Его опыт работы в Uber помогает ему получать стабильную зарплату, развивать свой бизнес и устанавливать деловые контакты с другими бизнесменами.","summary":"Статья рассказывает о Гевине Эсколаре, филиппинце, который работает водителем Uber и использует свой автомобиль для продвижения ювелирного бизнеса. Он превращает свою машину в передвижной торговый павильон, размещая в ней ювелирные изделия и каталоги. Пассажиры, которые замечают товар, могут узнать больше о его продукции, а те, кто не обращает внимания, могут быть не его целевой аудиторией. Гевин зарабатывает в среднем 18 000 долларов в месяц на изготовлении драгоценностей, а с учетом заработка в Uber в год получает 252 000 долларов. Он также использует Uber для расширения своего бизнеса, покупая еще три машины и нанимая шесть водителей. Гевин сам возит пассажиров как минимум 8 часов в неделю, чтобы быть в курсе изменений на дорогах и в районах с наибольшим количеством клиентов. Его опыт работы в Uber помогает ему получать стабильную зарплату, развивать свой бизнес и устанавливать деловые контакты с другими бизнесменами."}
{"Text":"Универсальный Nixie-модуль на ИН-12 / Хабр                 \n\n04-04-2015\nОригинальные технические решения прошлого зачастую вызывают сегодня умиление и восторг, а если не имеют прямого современного аналога, то вполне могут продолжать свое существование — так рождается дикий микс из компонентов, разница в возрасте которых составляет десятки лет. В прошлый раз я с подобным чувством сооружал чиптюновый модуль на AY-3-8912. Результатом остался чрезвычайно доволен, но отмечу, что законченным изделием он не является. Как и герой данного материала, блок газоразрядных индикаторов ИН-12. Часики на ИНках на Хабре фигурировали не раз (например 1, 2, 3), поэтому поста в стиле «Yet another Nixie clock» не будет. Сосредоточусь на том, чтобы кратко и емко изложить идею блока индикации и особенности реализации. Принцип действия Индикаторы ИН представляют собой стеклянную колбу с ножками, заполненную инертным газом. В ней находится пакет электродов (катодов), выполненных в форме символов (в большинстве ИН это цифры). При приложении к выбранному катоду и общему аноду (сетка перед пакетом) напряжения в пару сотен вольт зажигается тлеющий разряд, визуальное проявление — красно-оранжевое свечение вблизи катода, повторяющее его форму. Управление Чтобы управлять индикаторами серии ИН, нужно коммутировать высокое напряжение (ток — единицы миллиампер). Жизнь сильно облегчает существование в любимой 155-й серии дешифраторов ИД1 — они непосредственно и предназначены для работы с индикаторами ИН. Можно использовать и обычные дешифраторы, но тогда на их выходы придется ставить транзисторные ключи. Вполне вариант, но имеет смысл только при недоступности К155ИД1. То есть, на каждый индикатор ставим по микросхеме. В моем случае получается 4 штуки. Это будет статическая индикация. Для упрощения схемотехники нередко применяется индикация динамическая — когда в каждый момент времени выводится одна цифра, но смена происходит быстро, и за счет инерции человеческого зрения, незаметно. Но такой подход идет вразрез с моей идеей универсального модуля, с минимальными затратами подключаемого к «голове» (микроконтроллер, компьютер, «малинка» какая-нибудь). Итерация первая, 16 ножек. Решаем проблему в лоб. 4 индикатора ИН-12. На каждый из них приходится по одному дешифратору. Аноды через ограничивающие резисторы (50 КОм 0.5 Вт) подключены к источнику высокого напряжения. В моем случае — маленький бесхозный анодно-накальный трансформатор с диодным мостом. Разведена плата, с применением фоторезиста изготовлена, компоненты установлены, можно пользоваться. 16 входов подключаются к GPIO Raspberry PI, пишется простенькая программка, отображающая время. Только вот не слишком ли жирно, отдавать такое количество ножек на отображение четырех цифр? Я уж не говорю о том, что кому-то захочется показать секунды, и GPIO «малинки» просто не хватит. И вообще, причем тут Raspberry, если для минимальных часов нам понадобятся восьминогая ATtiny и DS1307? Итерация вторая, 2 ножки. Так что, разводим еще одну маленькую платку, которая будет «вторым этажом». На ней располагаются еще две микросхемы — восьмиразрядные сдвиговые регистры 155ИР8. Они у нас будут преобразователем последовательного интерфейса в параллельный. Сдвиговый регистр работает следующим образом: по фронту импульса на тактовом входе (CLK) происходит запись в регистр бита на информационном входе (A) с одновременным сдвигом уже имеющихся битов. Восемь выходов регистра отражают его содержимое — они отправляются на входы двух дешифраторов. Два регистра объединяются в один 16-разрядный путем соединения последнего выхода одного со входом другого. Так что, от устройства, управляющего модулем индикации, требуется выдавать 16 бит (по 4 бита на разряд индикации), не забывая каждый зафиксировать тактовым импульсом. И что? И все. Есть базовая часть модуля индикации, которую можно использовать, если вам не жалко 16 пинов. Есть дополнительная плата, которая сокращает необходимое число управляющих пинов до двух. Питание — 5 вольт и что-то в районе 180-220 (по 2 мА на индикатор). Можно использовать трансформатор, можно — импульсный преобразователь. Можно — от сетевого напряжения через диод, как это рекомендуется в древних мануалах. Последнего варианта я убоялся, т.к. не люблю, когда у меня по плате свободно гуляет никак не развязанная «сеть». Ближе к готовому устройству Все наработки публикуются на Github. В настоящий момент там можно лицезреть готовые разводки плат (Sprint Layout 6), схемы (Eagle) и программу (Python) для Raspberry Pi. Последняя находится в процессе пиления. На момент публикации реализован вывод времени, эффект рандомной прокрутки цифр (надо периодически зажигать все катоды ИНки во избежание т.н. «отравления»), начата работа над будильником. Ведь я не просто так делал модуль на AY-3-8912, он здесь тоже поучаствует. Питоний код в рамках данного материала я описывать не буду, это для третьей части, посвященной сборке в один мегадевайс Raspberry Pi, Nixie-индикатора и чиптюнового модуля. Гитхаб (основной проект) Гитхаб (чиптюн) Индикатор ИН-12 Дешифратор ИД1 Сдвиговый регистр ИР8\n","metric":0.58740234380000000414,"metric_input":"Текст статьи:\nУниверсальный Nixie-модуль на ИН-12 / Хабр                 \n\n04-04-2015\nОригинальные технические решения прошлого зачастую вызывают сегодня умиление и восторг, а если не имеют прямого современного аналога, то вполне могут продолжать свое существование — так рождается дикий микс из компонентов, разница в возрасте которых составляет десятки лет. В прошлый раз я с подобным чувством сооружал чиптюновый модуль на AY-3-8912. Результатом остался чрезвычайно доволен, но отмечу, что законченным изделием он не является. Как и герой данного материала, блок газоразрядных индикаторов ИН-12. Часики на ИНках на Хабре фигурировали не раз (например 1, 2, 3), поэтому поста в стиле «Yet another Nixie clock» не будет. Сосредоточусь на том, чтобы кратко и емко изложить идею блока индикации и особенности реализации. Принцип действия Индикаторы ИН представляют собой стеклянную колбу с ножками, заполненную инертным газом. В ней находится пакет электродов (катодов), выполненных в форме символов (в большинстве ИН это цифры). При приложении к выбранному катоду и общему аноду (сетка перед пакетом) напряжения в пару сотен вольт зажигается тлеющий разряд, визуальное проявление — красно-оранжевое свечение вблизи катода, повторяющее его форму. Управление Чтобы управлять индикаторами серии ИН, нужно коммутировать высокое напряжение (ток — единицы миллиампер). Жизнь сильно облегчает существование в любимой 155-й серии дешифраторов ИД1 — они непосредственно и предназначены для работы с индикаторами ИН. Можно использовать и обычные дешифраторы, но тогда на их выходы придется ставить транзисторные ключи. Вполне вариант, но имеет смысл только при недоступности К155ИД1. То есть, на каждый индикатор ставим по микросхеме. В моем случае получается 4 штуки. Это будет статическая индикация. Для упрощения схемотехники нередко применяется индикация динамическая — когда в каждый момент времени выводится одна цифра, но смена происходит быстро, и за счет инерции человеческого зрения, незаметно. Но такой подход идет вразрез с моей идеей универсального модуля, с минимальными затратами подключаемого к «голове» (микроконтроллер, компьютер, «малинка» какая-нибудь). Итерация первая, 16 ножек. Решаем проблему в лоб. 4 индикатора ИН-12. На каждый из них приходится по одному дешифратору. Аноды через ограничивающие резисторы (50 КОм 0.5 Вт) подключены к источнику высокого напряжения. В моем случае — маленький бесхозный анодно-накальный трансформатор с диодным мостом. Разведена плата, с применением фоторезиста изготовлена, компоненты установлены, можно пользоваться. 16 входов подключаются к GPIO Raspberry PI, пишется простенькая программка, отображающая время. Только вот не слишком ли жирно, отдавать такое количество ножек на отображение четырех цифр? Я уж не говорю о том, что кому-то захочется показать секунды, и GPIO «малинки» просто не хватит. И вообще, причем тут Raspberry, если для минимальных часов нам понадобятся восьминогая ATtiny и DS1307? Итерация вторая, 2 ножки. Так что, разводим еще одну маленькую платку, которая будет «вторым этажом». На ней располагаются еще две микросхемы — восьмиразрядные сдвиговые регистры 155ИР8. Они у нас будут преобразователем последовательного интерфейса в параллельный. Сдвиговый регистр работает следующим образом: по фронту импульса на тактовом входе (CLK) происходит запись в регистр бита на информационном входе (A) с одновременным сдвигом уже имеющихся битов. Восемь выходов регистра отражают его содержимое — они отправляются на входы двух дешифраторов. Два регистра объединяются в один 16-разрядный путем соединения последнего выхода одного со входом другого. Так что, от устройства, управляющего модулем индикации, требуется выдавать 16 бит (по 4 бита на разряд индикации), не забывая каждый зафиксировать тактовым импульсом. И что? И все. Есть базовая часть модуля индикации, которую можно использовать, если вам не жалко 16 пинов. Есть дополнительная плата, которая сокращает необходимое число управляющих пинов до двух. Питание — 5 вольт и что-то в районе 180-220 (по 2 мА на индикатор). Можно использовать трансформатор, можно — импульсный преобразователь. Можно — от сетевого напряжения через диод, как это рекомендуется в древних мануалах. Последнего варианта я убоялся, т.к. не люблю, когда у меня по плате свободно гуляет никак не развязанная «сеть». Ближе к готовому устройству Все наработки публикуются на Github. В настоящий момент там можно лицезреть готовые разводки плат (Sprint Layout 6), схемы (Eagle) и программу (Python) для Raspberry Pi. Последняя находится в процессе пиления. На момент публикации реализован вывод времени, эффект рандомной прокрутки цифр (надо периодически зажигать все катоды ИНки во избежание т.н. «отравления»), начата работа над будильником. Ведь я не просто так делал модуль на AY-3-8912, он здесь тоже поучаствует. Питоний код в рамках данного материала я описывать не буду, это для третьей части, посвященной сборке в один мегадевайс Raspberry Pi, Nixie-индикатора и чиптюнового модуля. Гитхаб (основной проект) Гитхаб (чиптюн) Индикатор ИН-12 Дешифратор ИД1 Сдвиговый регистр ИР8\n\n\nКраткое содержание:\nВ статье рассматривается идея создания универсального Nixie-модуля на ИН-12, который может быть использован для отображения времени. Для управления индикаторами ИН-12 используются дешифраторы серии 155ИД1. Модуль состоит из основной платы, на которой располагаются индикаторы и дешифраторы, и дополнительной платы, которая преобразует последовательный интерфейс в параллельный для управления индикаторами. Все наработки опубликованы на Github, где можно найти готовые разводки плат, схемы и программу для Raspberry Pi.","summary":"В статье рассматривается идея создания универсального Nixie-модуля на ИН-12, который может быть использован для отображения времени. Для управления индикаторами ИН-12 используются дешифраторы серии 155ИД1. Модуль состоит из основной платы, на которой располагаются индикаторы и дешифраторы, и дополнительной платы, которая преобразует последовательный интерфейс в параллельный для управления индикаторами. Все наработки опубликованы на Github, где можно найти готовые разводки плат, схемы и программу для Raspberry Pi."}
{"Text":"Передача данных в системе мониторинга транспорта — технология DDF / Habr                       \n\n27-12-2013\nРедкий пример того, что патент в России может реально работать в сфере применения GPS. Технология DDF — это аббревиатура Dynamic Data Flow. Она запатентована как изобретение в России и США (патент РФ 2010127419, патент США 8,498,774 B2) и используется в системе мониторинга LOCARUS. С её помощью сокращается GPRS-трафик от приборов мониторинга, потребный объём памяти в устройстве, время сеанса связи, а также для DDF практически нет ограничений на количество контролируемых параметров транспортного средства, навесного оборудования и груза. Первоначальной целью был, конечно же, трафик — когда разрабатывался DDF, цены на передачу данных были существенно выше сегодняшних. Так как разработка шла параллельно с разработкой нового прибора мониторинга — то было решено так же максимально расширить возможности по подключению датчиков. Как обычно работают приборы спутникового мониторинга? Периодически или по событию (выход за пределы виртуального «круга», прохождение заданного расстояния, изменение курса, выход за пределы геозоны, сработка датчика, приход SMS и т.д.) прибор формирует так называемую «точку»: набор данных, включающий координаты, время, иногда курс и скорость, показания датчиков. По мере развития системы, с появлением новых приборов/прошивок такая посылка может менять формат, например, один формат служит для передачи показаний датчиков уровня топлива, другой — для передачи показаний температуры от микросети термодатчиков. В любом случае — формат данных и количество их источников, жёстко определяется на этапе разработки протокола. Изменить в нём что-либо — означает обречь все группы прикладных разработчиков на мучения, а себя — на проклятия. Если разработчик решил, что погружных датчиков уровня топлива может быть максимум два — то третий и четвёртый датчик интегратор поставить уже не сможет никогда. Если разработчик решил когда-то, что данные о давлении воздуха в пневмоподвеске тягача не нужны — то они никогда и не будут переданы в прикладное ПО. Изменения «обычного» протокола — эпохальны для массового производителя и болезненны для потребителя. В протоколе DDF принципиально отказались от записи «точек» трека. Вопрос: «А сколько точек помещается в памяти прибора LOCARUS?» не имеет однозначного ответа, всё зависит от условий работы прибора. Итак, сжатие данных. Но сжимать данные обычными методами математической компрессии — крайне нежелательно. Потому что в таком случае будет потеряна важнейшее качество системы мониторинга — оперативность! Математические методы эффективно работают на достаточно большом массиве данных, а нам нужно передавать (иногда) практически непрерывно! Таким образом, разработчики пришли к идее разделения опорных и промежуточных кадров. В опорных кадрах, периодичность передачи которых настраивается в соответствии с задачей, но обычно составляет 300 секунд, передаются абсолютные значения всех контролируемых параметров. Координаты, дата-время, курс, количество видимых спутников, напряжения питания и резервной батареи, и все заявленные датчики. А заявлять можно сколько угодно большое количество датчиков, ограничивать пользователя могут лишь соображения пропускной способности и платы за трафик. В промежуточных кадрах — передаются приращения величин. Приращение времени от последнего опорного кадра и приращение показания любой из величин. Так как максимально возможный период между опорными кадрами ограничен принудительно, то исходя из реальных величин скорости транспортных средств можно ограничить и размерность поля записи для приращения координат. Размерность полей записи времени также ограничена, а размерность полей записи остальных контролируемых параметров (мы называем их «логические каналы») — выбирается пользователем при настройке. Логические каналы универсальны, и внешне отличаются исключительно размерностью. Прикладная программа использует конфигурационный файл, сформированный при настройке прибора для того, чтобы понять — в каком канале приходит скорость, а в каком — уровень топлива. Формирование промежуточного кадра происходит в соответствии с настройками. Они предлагают пользователю определить минимальный период передачи и величину изменения, необходимую для формирования кадра. Для передачи координат также введена зависимость от изменения курса на заданную величину. То есть — получаем предельно гибкую систему. При правильных настройках стоящий на месте автомобиль с выключенным двигателем передаёт только опорные кадры каждые 5 минут. Диспетчер не теряет связь с автомобилем, но при этом имеем мизерный трафик GPRS. При движении передаются опорные кадры, плюс (для автомобиля, оснащённого датчиками уровня топлива и температуры в рефрижераторе): — нажатия тревожной кнопки или сработки тревожных датчиков, мгновенно — приращения координат, при изменении курса (обычно на 3-5 град.) — изменения уровня топлива по мере расходования — изменения температуры, если они произошли и превысили установленный порог чувствительности к изменениям По сути, реализован узкоспециализированный алгоритм сжатия данных, применительно к конкретной области — спутниковому мониторингу наземного и водного транспорта. В авиации DDF работать «as is» не будет — как минимум нужно пересматривать диапазон скоростей. Безусловно, реализация расшифровки протокола DDF на стороне сервера значительно сложнее, чем любого другого. Отсюда малая распространённость приборов LOCARUS — их технологически трудно адаптировать к стороннему ПО. Однако, потребители высоко оценивают гибкость настройки и качество работы в слабых сетях. Качество это происходит от того, что терминалу требуется гораздо меньше времени на передачу данных после выхода в зону покрытия GSM, либо при высокой загруженности сети.\n","metric":0.60888671880000000414,"metric_input":"Текст статьи:\nПередача данных в системе мониторинга транспорта — технология DDF / Habr                       \n\n27-12-2013\nРедкий пример того, что патент в России может реально работать в сфере применения GPS. Технология DDF — это аббревиатура Dynamic Data Flow. Она запатентована как изобретение в России и США (патент РФ 2010127419, патент США 8,498,774 B2) и используется в системе мониторинга LOCARUS. С её помощью сокращается GPRS-трафик от приборов мониторинга, потребный объём памяти в устройстве, время сеанса связи, а также для DDF практически нет ограничений на количество контролируемых параметров транспортного средства, навесного оборудования и груза. Первоначальной целью был, конечно же, трафик — когда разрабатывался DDF, цены на передачу данных были существенно выше сегодняшних. Так как разработка шла параллельно с разработкой нового прибора мониторинга — то было решено так же максимально расширить возможности по подключению датчиков. Как обычно работают приборы спутникового мониторинга? Периодически или по событию (выход за пределы виртуального «круга», прохождение заданного расстояния, изменение курса, выход за пределы геозоны, сработка датчика, приход SMS и т.д.) прибор формирует так называемую «точку»: набор данных, включающий координаты, время, иногда курс и скорость, показания датчиков. По мере развития системы, с появлением новых приборов/прошивок такая посылка может менять формат, например, один формат служит для передачи показаний датчиков уровня топлива, другой — для передачи показаний температуры от микросети термодатчиков. В любом случае — формат данных и количество их источников, жёстко определяется на этапе разработки протокола. Изменить в нём что-либо — означает обречь все группы прикладных разработчиков на мучения, а себя — на проклятия. Если разработчик решил, что погружных датчиков уровня топлива может быть максимум два — то третий и четвёртый датчик интегратор поставить уже не сможет никогда. Если разработчик решил когда-то, что данные о давлении воздуха в пневмоподвеске тягача не нужны — то они никогда и не будут переданы в прикладное ПО. Изменения «обычного» протокола — эпохальны для массового производителя и болезненны для потребителя. В протоколе DDF принципиально отказались от записи «точек» трека. Вопрос: «А сколько точек помещается в памяти прибора LOCARUS?» не имеет однозначного ответа, всё зависит от условий работы прибора. Итак, сжатие данных. Но сжимать данные обычными методами математической компрессии — крайне нежелательно. Потому что в таком случае будет потеряна важнейшее качество системы мониторинга — оперативность! Математические методы эффективно работают на достаточно большом массиве данных, а нам нужно передавать (иногда) практически непрерывно! Таким образом, разработчики пришли к идее разделения опорных и промежуточных кадров. В опорных кадрах, периодичность передачи которых настраивается в соответствии с задачей, но обычно составляет 300 секунд, передаются абсолютные значения всех контролируемых параметров. Координаты, дата-время, курс, количество видимых спутников, напряжения питания и резервной батареи, и все заявленные датчики. А заявлять можно сколько угодно большое количество датчиков, ограничивать пользователя могут лишь соображения пропускной способности и платы за трафик. В промежуточных кадрах — передаются приращения величин. Приращение времени от последнего опорного кадра и приращение показания любой из величин. Так как максимально возможный период между опорными кадрами ограничен принудительно, то исходя из реальных величин скорости транспортных средств можно ограничить и размерность поля записи для приращения координат. Размерность полей записи времени также ограничена, а размерность полей записи остальных контролируемых параметров (мы называем их «логические каналы») — выбирается пользователем при настройке. Логические каналы универсальны, и внешне отличаются исключительно размерностью. Прикладная программа использует конфигурационный файл, сформированный при настройке прибора для того, чтобы понять — в каком канале приходит скорость, а в каком — уровень топлива. Формирование промежуточного кадра происходит в соответствии с настройками. Они предлагают пользователю определить минимальный период передачи и величину изменения, необходимую для формирования кадра. Для передачи координат также введена зависимость от изменения курса на заданную величину. То есть — получаем предельно гибкую систему. При правильных настройках стоящий на месте автомобиль с выключенным двигателем передаёт только опорные кадры каждые 5 минут. Диспетчер не теряет связь с автомобилем, но при этом имеем мизерный трафик GPRS. При движении передаются опорные кадры, плюс (для автомобиля, оснащённого датчиками уровня топлива и температуры в рефрижераторе): — нажатия тревожной кнопки или сработки тревожных датчиков, мгновенно — приращения координат, при изменении курса (обычно на 3-5 град.) — изменения уровня топлива по мере расходования — изменения температуры, если они произошли и превысили установленный порог чувствительности к изменениям По сути, реализован узкоспециализированный алгоритм сжатия данных, применительно к конкретной области — спутниковому мониторингу наземного и водного транспорта. В авиации DDF работать «as is» не будет — как минимум нужно пересматривать диапазон скоростей. Безусловно, реализация расшифровки протокола DDF на стороне сервера значительно сложнее, чем любого другого. Отсюда малая распространённость приборов LOCARUS — их технологически трудно адаптировать к стороннему ПО. Однако, потребители высоко оценивают гибкость настройки и качество работы в слабых сетях. Качество это происходит от того, что терминалу требуется гораздо меньше времени на передачу данных после выхода в зону покрытия GSM, либо при высокой загруженности сети.\n\n\nКраткое содержание:\nСтатья описывает технологию DDF, которая используется в системе мониторинга транспорта LOCARUS. В отличие от обычных протоколов передачи данных, в DDF отказались от записи \"точек трека\", а данные сжимаются с помощью разделения опорных и промежуточных кадров. Это позволяет сократить GPRS-трафик, объем памяти в устройстве и время сеанса связи. Технология запатентована в России и США.","summary":"Статья описывает технологию DDF, которая используется в системе мониторинга транспорта LOCARUS. В отличие от обычных протоколов передачи данных, в DDF отказались от записи \"точек трека\", а данные сжимаются с помощью разделения опорных и промежуточных кадров. Это позволяет сократить GPRS-трафик, объем памяти в устройстве и время сеанса связи. Технология запатентована в России и США."}
{"Text":"[Разбор] Что еще предлагают банки: карты рассрочки vs кредитки / Хабр                                                           \n\n23-06-2021\nНа днях столкнулся с предложением открыть карту рассрочки – мне это было не особо актуально, но я сходу не понял, чем такая карта будет отличаться от кредитной. По заявлению менеджера – основной ее плюс в том, что расплачиваться за товар можно будет несколько месяцев и все это без процентов. Зачем банкам предлагать столь невыгодные для них условия при наличии кредитных карт? Я решил копнуть глубже и разобраться, как это работает на самом деле, и что важно учитывать тем, кто все же решился оформить такую карту. Кому выгодна карта рассрочки? Карты рассрочки выгодны для трех сторон: банка, потребителя и партнеров банка. Суть карты в том, что приобрести товары в рассрочку пользователь может только у продавцов-партнеров банка (впрочем, их список достаточно обширен и постоянно пополняется – например, у карты Халва сейчас более 230 тысяч магазинов-партнеров, а у карты Свобода – более 130 тысяч). Получается, что проценты за отложенный платеж банку оплачивают партнеры, заинтересованные в дополнительном притоке клиентов – то есть это по сути B2B-монетизация. Вот как это работает: С помощью карты рассрочки вы покупаете товар у магазина-партнера банка, даже если у вас сейчас нет полной суммы для его оплаты. Затем вы выплачиваете эту сумму банку без процентов равными платежами в течение указанного срока. Партнер банка получает дополнительных клиентов, ведь вы не пошли к конкурентам, а приобрели товар у него. И, как следствие, повышает свою прибыль. Банк получает комиссию от продаж магазина-партнера. Точный размер комиссии не раскрывается, но, по сведениям РБК, он составляет 3-7%. Но есть же кредитные карты, зачем нужно что-то еще? У кредиток и карт рассрочки есть некоторые отличия. Чем кредитная карта отличается от карты рассрочки Обе карты имеют лимит средств. Если вы не выплачиваете долг в срок, рассрочка превращается в кредит с процентами. А долг по кредитной карте в случае задержки придется выплачивать с более высокой процентной ставкой. В этом принцип действия карт похож. А вот в чем отличия: С кредитной картой вы покупаете все что хотите и где хотите. С картой рассрочки приобрести товар можно только у продавцов-партнеров банка. Потраченную сумму с карты рассрочки вы возвращаете в том же объеме (если выплачиваете долг вовремя). Потратив средства на кредитной карте, вы дополнительно выплачиваете проценты (за исключением грейс-периода). С кредитной карты вы можете снять наличные, заплатив комиссию за «обнал». С карты рассрочки снять наличные, как правило, нельзя. Карты рассрочки не предусматривают плату за обслуживание. За обслуживание кредитной карты списывают ежегодную плату. Кстати, некоторые банки вместо выпуска отдельных карт рассрочки предлагают опцию беспроцентной рассрочки как часть функционала кредитных карт – то есть клиент может либо приобрести товар в рассрочку у партнеров банка, либо воспользоваться кредитом. Но может, все-таки не заморачиваться с открытием карты рассрочки, а оформить рассрочку на нужный товар? Давайте разбираться. Оформить рассрочку или приобрести карту рассрочки? Банки не работают бесплатно, поэтому за рассрочкой часто кроется обыкновенный кредит. Банк предоставляет кредит и начисляет проценты, а магазин дает скидку на сумму этих процентов. Скидка не всегда возвращается сразу. Для возврата процентов необходимо платить вовремя в течение всего срока, а также написать заявление на возврат. При оформлении рассрочки нужно внести первоначальный взнос в размере 20-30% от суммы товара. Рассрочка обычно выдается на более длительный срок, в отличие от карты рассрочки. Карта рассрочки опасна тем, что вместо одного товара вы можете импульсивно купить много ненужных вещей. Если вы недостаточно дисциплинированны, то обычная рассрочка вам подойдет больше. Приведем пример. Допустим, вы планируете приобрести кухонный гарнитур или, может, последнюю модель Macbook Pro. Но выделить на покупку сразу всю сумму вы по каким-то причинам сейчас не можете, а вот внести оплату равными платежами за три месяца – вполне. При этом вам не хочется платить проценты банку и потом заморачиваться с их возвратом. В этом случае вам подойдет карта рассрочки. Что учесть при оформлении карты рассрочки В заключение, еще раз перечислим важные факторы, которые нужно принимать во внимание тем, кто собрался оформить карту рассрочки: С картой рассрочки вы не сможете приобрести любые товары – только те, что есть у партнера-ретейлера банка. Перед открытием карты изучите список партнеров банка и цены на товары. Может быть, в эти магазины вы не ходите? А может, цены в них значительно выше и рассрочка невыгодна? Вносить ежемесячные выплаты необходимо вовремя, иначе банк начислит штраф, а также введет проценты за обслуживание рассрочки, которые могут достигать 20% годовых и даже больше. Так вы теряете главное преимущество карты рассрочки и портите кредитную историю. Не каждый товар можно приобрести со скидкой с помощью карты рассрочки. В некоторых случаях выгоднее совершить покупку по скидке, пусть и без рассрочки. Например, если вы оплачиваете товар кредитной картой и гасите сумму овердрафта в течение беспроцентного периода. Уточните срок, на который можно распределить выплату рассрочки. Не во всех магазинах есть возможность растянуть оплату на год. Лимит суммы на карте рассрочки обычно составляет 300-350 тысяч рублей. Но вряд ли эту сумму предложат сразу. Сначала сумма будет в 3-10 раз меньше, но если вы выплачиваете долги в срок, лимит могут повысить. Ниже я собрал таблицу для сравнения условий по кредиткам, картам рассрочки и просто покупкам в рассрочку: Кредитная карта Рассрочка Карта рассрочки Варианты использования Везде Оформляется на конкретный товар в конкретном магазине Только в магазинах-партнерах банка Беспроцентный период 2-3 месяца До 12 месяцев, необходимо офрмлять возврат процентов после выплаты рассрочки До 18-24 месяцев Обналичивание Да Не применимо Как правило, нет* Лимит средств До 700 тыс-1 млн рублей Не применимо До 300-350 тыс рублей Плата за обслуживание Да Нет, но необходимо внести первоначальный взнос 20-30% от стоимости покупки Нет * В последнее время стали появляться карты рассрочки с возможностью обналичивания средств, при этом взимается комиссия. Что в итоге Карты рассрочки – сравнительно новый, но достаточно интересный инструмент, который позволяет экономить на обслуживании кредитов. Подойдет в первую очередь тем, кто планирует совершать крупные покупки в магазинах-партнерах банка и при этом точно уверен, что сможет вносить ежемесячные платежи по плану. Посмотреть предложения банков по картам рассрочки и сравнить условия можно на различных сайтах-агрегаторах.\n","metric":0.57568359380000000414,"metric_input":"Текст статьи:\n[Разбор] Что еще предлагают банки: карты рассрочки vs кредитки / Хабр                                                           \n\n23-06-2021\nНа днях столкнулся с предложением открыть карту рассрочки – мне это было не особо актуально, но я сходу не понял, чем такая карта будет отличаться от кредитной. По заявлению менеджера – основной ее плюс в том, что расплачиваться за товар можно будет несколько месяцев и все это без процентов. Зачем банкам предлагать столь невыгодные для них условия при наличии кредитных карт? Я решил копнуть глубже и разобраться, как это работает на самом деле, и что важно учитывать тем, кто все же решился оформить такую карту. Кому выгодна карта рассрочки? Карты рассрочки выгодны для трех сторон: банка, потребителя и партнеров банка. Суть карты в том, что приобрести товары в рассрочку пользователь может только у продавцов-партнеров банка (впрочем, их список достаточно обширен и постоянно пополняется – например, у карты Халва сейчас более 230 тысяч магазинов-партнеров, а у карты Свобода – более 130 тысяч). Получается, что проценты за отложенный платеж банку оплачивают партнеры, заинтересованные в дополнительном притоке клиентов – то есть это по сути B2B-монетизация. Вот как это работает: С помощью карты рассрочки вы покупаете товар у магазина-партнера банка, даже если у вас сейчас нет полной суммы для его оплаты. Затем вы выплачиваете эту сумму банку без процентов равными платежами в течение указанного срока. Партнер банка получает дополнительных клиентов, ведь вы не пошли к конкурентам, а приобрели товар у него. И, как следствие, повышает свою прибыль. Банк получает комиссию от продаж магазина-партнера. Точный размер комиссии не раскрывается, но, по сведениям РБК, он составляет 3-7%. Но есть же кредитные карты, зачем нужно что-то еще? У кредиток и карт рассрочки есть некоторые отличия. Чем кредитная карта отличается от карты рассрочки Обе карты имеют лимит средств. Если вы не выплачиваете долг в срок, рассрочка превращается в кредит с процентами. А долг по кредитной карте в случае задержки придется выплачивать с более высокой процентной ставкой. В этом принцип действия карт похож. А вот в чем отличия: С кредитной картой вы покупаете все что хотите и где хотите. С картой рассрочки приобрести товар можно только у продавцов-партнеров банка. Потраченную сумму с карты рассрочки вы возвращаете в том же объеме (если выплачиваете долг вовремя). Потратив средства на кредитной карте, вы дополнительно выплачиваете проценты (за исключением грейс-периода). С кредитной карты вы можете снять наличные, заплатив комиссию за «обнал». С карты рассрочки снять наличные, как правило, нельзя. Карты рассрочки не предусматривают плату за обслуживание. За обслуживание кредитной карты списывают ежегодную плату. Кстати, некоторые банки вместо выпуска отдельных карт рассрочки предлагают опцию беспроцентной рассрочки как часть функционала кредитных карт – то есть клиент может либо приобрести товар в рассрочку у партнеров банка, либо воспользоваться кредитом. Но может, все-таки не заморачиваться с открытием карты рассрочки, а оформить рассрочку на нужный товар? Давайте разбираться. Оформить рассрочку или приобрести карту рассрочки? Банки не работают бесплатно, поэтому за рассрочкой часто кроется обыкновенный кредит. Банк предоставляет кредит и начисляет проценты, а магазин дает скидку на сумму этих процентов. Скидка не всегда возвращается сразу. Для возврата процентов необходимо платить вовремя в течение всего срока, а также написать заявление на возврат. При оформлении рассрочки нужно внести первоначальный взнос в размере 20-30% от суммы товара. Рассрочка обычно выдается на более длительный срок, в отличие от карты рассрочки. Карта рассрочки опасна тем, что вместо одного товара вы можете импульсивно купить много ненужных вещей. Если вы недостаточно дисциплинированны, то обычная рассрочка вам подойдет больше. Приведем пример. Допустим, вы планируете приобрести кухонный гарнитур или, может, последнюю модель Macbook Pro. Но выделить на покупку сразу всю сумму вы по каким-то причинам сейчас не можете, а вот внести оплату равными платежами за три месяца – вполне. При этом вам не хочется платить проценты банку и потом заморачиваться с их возвратом. В этом случае вам подойдет карта рассрочки. Что учесть при оформлении карты рассрочки В заключение, еще раз перечислим важные факторы, которые нужно принимать во внимание тем, кто собрался оформить карту рассрочки: С картой рассрочки вы не сможете приобрести любые товары – только те, что есть у партнера-ретейлера банка. Перед открытием карты изучите список партнеров банка и цены на товары. Может быть, в эти магазины вы не ходите? А может, цены в них значительно выше и рассрочка невыгодна? Вносить ежемесячные выплаты необходимо вовремя, иначе банк начислит штраф, а также введет проценты за обслуживание рассрочки, которые могут достигать 20% годовых и даже больше. Так вы теряете главное преимущество карты рассрочки и портите кредитную историю. Не каждый товар можно приобрести со скидкой с помощью карты рассрочки. В некоторых случаях выгоднее совершить покупку по скидке, пусть и без рассрочки. Например, если вы оплачиваете товар кредитной картой и гасите сумму овердрафта в течение беспроцентного периода. Уточните срок, на который можно распределить выплату рассрочки. Не во всех магазинах есть возможность растянуть оплату на год. Лимит суммы на карте рассрочки обычно составляет 300-350 тысяч рублей. Но вряд ли эту сумму предложат сразу. Сначала сумма будет в 3-10 раз меньше, но если вы выплачиваете долги в срок, лимит могут повысить. Ниже я собрал таблицу для сравнения условий по кредиткам, картам рассрочки и просто покупкам в рассрочку: Кредитная карта Рассрочка Карта рассрочки Варианты использования Везде Оформляется на конкретный товар в конкретном магазине Только в магазинах-партнерах банка Беспроцентный период 2-3 месяца До 12 месяцев, необходимо офрмлять возврат процентов после выплаты рассрочки До 18-24 месяцев Обналичивание Да Не применимо Как правило, нет* Лимит средств До 700 тыс-1 млн рублей Не применимо До 300-350 тыс рублей Плата за обслуживание Да Нет, но необходимо внести первоначальный взнос 20-30% от стоимости покупки Нет * В последнее время стали появляться карты рассрочки с возможностью обналичивания средств, при этом взимается комиссия. Что в итоге Карты рассрочки – сравнительно новый, но достаточно интересный инструмент, который позволяет экономить на обслуживании кредитов. Подойдет в первую очередь тем, кто планирует совершать крупные покупки в магазинах-партнерах банка и при этом точно уверен, что сможет вносить ежемесячные платежи по плану. Посмотреть предложения банков по картам рассрочки и сравнить условия можно на различных сайтах-агрегаторах.\n\n\nКраткое содержание:\nСтатья рассматривает карты рассрочки как новый инструмент для экономии на обслуживании кредитов. Они позволяют совершать покупки в магазинах-партнерах банка без процентов, но с определенными условиями и ограничениями.\n\nКарты рассрочки выгодны для банка, потребителя и партнеров, так как они получают комиссию от продаж и проценты за отложенный платеж. Однако, вместо одного товара можно импульсивно купить много ненужных вещей.\n\nПри оформлении карты рассрочки важно учитывать, что не все товары доступны, а ежемесячные выплаты необходимо вносить вовремя. Также стоит проверить цены и сроки рассрочки, прежде чем оформлять карту.\n\nВ итоге, карты рассрочки могут быть полезны для тех, кто планирует совершать крупные покупки в магазинах-партнерах банка и уверен в своих финансовых возможностях.","summary":"Статья рассматривает карты рассрочки как новый инструмент для экономии на обслуживании кредитов. Они позволяют совершать покупки в магазинах-партнерах банка без процентов, но с определенными условиями и ограничениями.\n\nКарты рассрочки выгодны для банка, потребителя и партнеров, так как они получают комиссию от продаж и проценты за отложенный платеж. Однако, вместо одного товара можно импульсивно купить много ненужных вещей.\n\nПри оформлении карты рассрочки важно учитывать, что не все товары доступны, а ежемесячные выплаты необходимо вносить вовремя. Также стоит проверить цены и сроки рассрочки, прежде чем оформлять карту.\n\nВ итоге, карты рассрочки могут быть полезны для тех, кто планирует совершать крупные покупки в магазинах-партнерах банка и уверен в своих финансовых возможностях."}
{"Text":"Работаем с ExtJS на языке РНР — библиотека PHP-Ext / Хабр                                                            \n\n02-05-2008\nЯ вряд ли ошибусь, если предположу, что язык РНР достаточно популярен, если не самый популярный в веб-разработке. Мы не будем анализировать причины этого (а тем более, следствия), а сконцентрируемся на одном небольшом аспекте, а именно — как на РНР разрабатывать сложные AJAX веб-приложения. И не просто так разрабатывать, а использовать в своей работе библиотеку ExtJS, которая позволяет создавать интерфейсы для этих веб-приложений. Конечно, все это можно разнести и серверная сторона, на РНР или на любом другом языке, совершенно ничего не будет знать о клиентской части и AJAX-библиотеке, просто оперируя JSON данными и обычным HTML. Но можно сделать и по-другому — этот подход, аналогичен популярной сегодня технологии Google Web Toolkit. Мы ничего не разделяем, а просто пишем приложение, используя одну среду, один язык и все возможности (и языка и среды), а уже сервер самостоятельно генерирует код для клиента, полностью автоматически. Таким образом можно совсем (ну или почти) не знать и не разбираться в верстке, JavaScript и ExtJS, но писать приложения, которые будут использовать этот фреймворк. Для языка Java подобные решения существуют, а недавно такой проект, EXT GWT, даже перешел под крыло самой компании-разработчика ExtJS, превратившись в вполне серьезное профессиональное решение. Ну а как в других языках? Java, конечно, хорошо и даже отлично, но душа и тело хотят разнообразия, или просто не хотят переучиваться. Для таких случаев есть свои решения. И одно из них, для РНР, так и называется — PHP-Ext. Эта библиотека представляет из себя набор классов, частично повторяющий классы из ExtJS, которые переписаны на PHP, при этом добавлен механизм генерации выходного JS-кода, которые, собственно и отсылается браузеру для исполнения. Разработчик оперирует только РНР — подключаем библиотеку, инициализируем ее, а потом создаем нужные нам обьекты в виде РНР-классов, задаем им необходимые свойства, а в конце кода просто вызываем генерацию JavaScript и отсылаем ее браузеру. Конечно же, такой подход позволяет использовать все возможности ваших любимых сред разработки — автодополнение синтаксиса, подсветка кода, встроенные отладчики, другие средства проверки кода. — не секрет, что для РНР таких средств намного больше, чем для JavaScript. Код самой библиотеки достаточно большой, но текущая версия полностью совместима с РНР 5, в частности, использует все доступные возможности ООП. Сам код достаточно хорошо документирован (хоть и не идеально), а его структура соответствует стандарту Zend Framework PHP Coding Standard, что само по себе уже не так часто встретишь в развивающихся любительских проектах, здесь это показатель серьезности разработки и, наверное, надежд на ее развитие (а вдруг и компания-разработчик ExtJS примет участие в проекте, так же, как это сделано с ExtGWT, в таком случае можно реально говорить о проекте-стартапе, причем, весьма успешном). Для разработчиков есть несколько примеров работы с типовыми обьектами, например, формами — не придумывая собственные примеры, авторы библиотеки просто переделали те, что идут в стандартной поставке ExtJS, переписав их с использованием своей библиотеки. Это отличный способ посмотреть на ее реальные возможности и даже сравнить генерируемый код. Ну а для углубленного изучения есть справка по API, генерируемая по комментариям в коде при помощи phpDocumentator. Пока, правда, не все еще возможности реализованы, хотя тех, что есть, вполне хватит для построения относительно сложных приложений. В некоторые недостатки, с оговоркой на новизну проекта, я бы записал и использование предыдущей версии ExtJS — 2.0.2 (хотя и с 2.0.1 работает), тогда как сейчас стоит уже ориентироваться на 2.1. Но это будет поправлено, ведь сейчас только второй релиз под номером 0.8.2, поэтому пока считаем, что просто совершенствуем технологию. Потенциал развития библиотеки очень большой, в частности, как по пути более полного переноса всего ExtJS API на сторону РНР, так и в сторону совершенствования самой платформы. Например, логично было бы иметь в составе генератора кода средства для сжатия кода, удаление неиспользуемых модулей (исходя из используемых классов и их зависимостей, сервер может на лету строить набор необходимых классов и генерировать сборку ExtJS под ваше приложение, пока же это можно сделать вручную при помощи он-лайнового конструктора). Пока проект молодой, но развивается достаточно быстро и, если за его разработкой будут стоять профессионалы и серьезная архитектура, он сможет стать отличным инструментом для разработки современных AJAX-приложений. На мой же взгляд, вариант все же несколько сложный, вернее, слишком много кода и работы с ним для генерации интерфейса, при этом всеравно необходимо четко себе представлять, как страница генерируется и как будет работать этот JavaScript код, необходимо, мне кажется, даже лучше знать ExtJS и его возможности, чем при обычном варианте разработки, когда серверная сторона совершенно не знает о клиенте. А вот какой-то проект, совмещающий шаблонный движок, например, Smarty или Template Lite (мой любимый) был бы просто фантастикой, если упросить создание и наполнение сложных шаблонов. В то же время, так как все последние версии ExtJS умеют генерировать визуальную часть по JSON-разметке, например, формы или окна, то я вижу очень много путей упрощения серверной части, которая бы генерировала только необходимый минимум JS кода, а основную часть разметки страницы — в виде JSON-данных. Но это уже совсем другая архитектура и подход. А на последок приведем один пример, для того, чтобы наглядно понять механизм работы PHP-Ext. Следующий пример выводит обычную панель, которая может сворачиваться и разворачиваться, один из самых применяемых и универсальных виджетов библиотеки. 1. Так панель выглядит в браузере 2. Так выглядит код на РНР для генерации этой панели и задания ее параметров. setTitle(\"My Panel\") ->setCollapsible(true) ->setRenderTo(PhpExt_Javascript::variable(\"Ext.get('centercolumn')\")) ->setWidth(400) ->setHtml(PhpExt_Javascript::variable(\"Ext.example.bogusMarkup\")); echo PhpExt_Ext::OnReady( $p->getJavascript(false, \"p\") ); ?> 3. Это код, которые РНР-Ext сгенерировал для отображения этой формы (без кода самой страницы и подключения библитек). Ext.onReady(function(){ var p = new Ext.Panel({title: 'My Panel',collapsible: true, renderTo: Ext.get('centercolumn'),width: 400,html: Ext.example.bogusMarkup}); }); 4. А это оригинальный код с примеров ExtJS Ext.onReady(function(){ var p = new Ext.Panel({ title: 'My Panel', collapsible:true, renderTo: 'container', width:400, html: Ext.example.bogusMarkup }); }); P.S. Кросс-пост с моего блога.\n","metric":0.47045898440000000207,"metric_input":"Текст статьи:\nРаботаем с ExtJS на языке РНР — библиотека PHP-Ext / Хабр                                                            \n\n02-05-2008\nЯ вряд ли ошибусь, если предположу, что язык РНР достаточно популярен, если не самый популярный в веб-разработке. Мы не будем анализировать причины этого (а тем более, следствия), а сконцентрируемся на одном небольшом аспекте, а именно — как на РНР разрабатывать сложные AJAX веб-приложения. И не просто так разрабатывать, а использовать в своей работе библиотеку ExtJS, которая позволяет создавать интерфейсы для этих веб-приложений. Конечно, все это можно разнести и серверная сторона, на РНР или на любом другом языке, совершенно ничего не будет знать о клиентской части и AJAX-библиотеке, просто оперируя JSON данными и обычным HTML. Но можно сделать и по-другому — этот подход, аналогичен популярной сегодня технологии Google Web Toolkit. Мы ничего не разделяем, а просто пишем приложение, используя одну среду, один язык и все возможности (и языка и среды), а уже сервер самостоятельно генерирует код для клиента, полностью автоматически. Таким образом можно совсем (ну или почти) не знать и не разбираться в верстке, JavaScript и ExtJS, но писать приложения, которые будут использовать этот фреймворк. Для языка Java подобные решения существуют, а недавно такой проект, EXT GWT, даже перешел под крыло самой компании-разработчика ExtJS, превратившись в вполне серьезное профессиональное решение. Ну а как в других языках? Java, конечно, хорошо и даже отлично, но душа и тело хотят разнообразия, или просто не хотят переучиваться. Для таких случаев есть свои решения. И одно из них, для РНР, так и называется — PHP-Ext. Эта библиотека представляет из себя набор классов, частично повторяющий классы из ExtJS, которые переписаны на PHP, при этом добавлен механизм генерации выходного JS-кода, которые, собственно и отсылается браузеру для исполнения. Разработчик оперирует только РНР — подключаем библиотеку, инициализируем ее, а потом создаем нужные нам обьекты в виде РНР-классов, задаем им необходимые свойства, а в конце кода просто вызываем генерацию JavaScript и отсылаем ее браузеру. Конечно же, такой подход позволяет использовать все возможности ваших любимых сред разработки — автодополнение синтаксиса, подсветка кода, встроенные отладчики, другие средства проверки кода. — не секрет, что для РНР таких средств намного больше, чем для JavaScript. Код самой библиотеки достаточно большой, но текущая версия полностью совместима с РНР 5, в частности, использует все доступные возможности ООП. Сам код достаточно хорошо документирован (хоть и не идеально), а его структура соответствует стандарту Zend Framework PHP Coding Standard, что само по себе уже не так часто встретишь в развивающихся любительских проектах, здесь это показатель серьезности разработки и, наверное, надежд на ее развитие (а вдруг и компания-разработчик ExtJS примет участие в проекте, так же, как это сделано с ExtGWT, в таком случае можно реально говорить о проекте-стартапе, причем, весьма успешном). Для разработчиков есть несколько примеров работы с типовыми обьектами, например, формами — не придумывая собственные примеры, авторы библиотеки просто переделали те, что идут в стандартной поставке ExtJS, переписав их с использованием своей библиотеки. Это отличный способ посмотреть на ее реальные возможности и даже сравнить генерируемый код. Ну а для углубленного изучения есть справка по API, генерируемая по комментариям в коде при помощи phpDocumentator. Пока, правда, не все еще возможности реализованы, хотя тех, что есть, вполне хватит для построения относительно сложных приложений. В некоторые недостатки, с оговоркой на новизну проекта, я бы записал и использование предыдущей версии ExtJS — 2.0.2 (хотя и с 2.0.1 работает), тогда как сейчас стоит уже ориентироваться на 2.1. Но это будет поправлено, ведь сейчас только второй релиз под номером 0.8.2, поэтому пока считаем, что просто совершенствуем технологию. Потенциал развития библиотеки очень большой, в частности, как по пути более полного переноса всего ExtJS API на сторону РНР, так и в сторону совершенствования самой платформы. Например, логично было бы иметь в составе генератора кода средства для сжатия кода, удаление неиспользуемых модулей (исходя из используемых классов и их зависимостей, сервер может на лету строить набор необходимых классов и генерировать сборку ExtJS под ваше приложение, пока же это можно сделать вручную при помощи он-лайнового конструктора). Пока проект молодой, но развивается достаточно быстро и, если за его разработкой будут стоять профессионалы и серьезная архитектура, он сможет стать отличным инструментом для разработки современных AJAX-приложений. На мой же взгляд, вариант все же несколько сложный, вернее, слишком много кода и работы с ним для генерации интерфейса, при этом всеравно необходимо четко себе представлять, как страница генерируется и как будет работать этот JavaScript код, необходимо, мне кажется, даже лучше знать ExtJS и его возможности, чем при обычном варианте разработки, когда серверная сторона совершенно не знает о клиенте. А вот какой-то проект, совмещающий шаблонный движок, например, Smarty или Template Lite (мой любимый) был бы просто фантастикой, если упросить создание и наполнение сложных шаблонов. В то же время, так как все последние версии ExtJS умеют генерировать визуальную часть по JSON-разметке, например, формы или окна, то я вижу очень много путей упрощения серверной части, которая бы генерировала только необходимый минимум JS кода, а основную часть разметки страницы — в виде JSON-данных. Но это уже совсем другая архитектура и подход. А на последок приведем один пример, для того, чтобы наглядно понять механизм работы PHP-Ext. Следующий пример выводит обычную панель, которая может сворачиваться и разворачиваться, один из самых применяемых и универсальных виджетов библиотеки. 1. Так панель выглядит в браузере 2. Так выглядит код на РНР для генерации этой панели и задания ее параметров. setTitle(\"My Panel\") ->setCollapsible(true) ->setRenderTo(PhpExt_Javascript::variable(\"Ext.get('centercolumn')\")) ->setWidth(400) ->setHtml(PhpExt_Javascript::variable(\"Ext.example.bogusMarkup\")); echo PhpExt_Ext::OnReady( $p->getJavascript(false, \"p\") ); ?> 3. Это код, которые РНР-Ext сгенерировал для отображения этой формы (без кода самой страницы и подключения библитек). Ext.onReady(function(){ var p = new Ext.Panel({title: 'My Panel',collapsible: true, renderTo: Ext.get('centercolumn'),width: 400,html: Ext.example.bogusMarkup}); }); 4. А это оригинальный код с примеров ExtJS Ext.onReady(function(){ var p = new Ext.Panel({ title: 'My Panel', collapsible:true, renderTo: 'container', width:400, html: Ext.example.bogusMarkup }); }); P.S. Кросс-пост с моего блога.\n\n\nКраткое содержание:\nСтатья рассказывает о библиотеке PHP-Ext, которая позволяет разработчикам на языке РНР использовать фреймворк ExtJS для создания сложных AJAX веб-приложений. PHP-Ext предоставляет набор классов, частично повторяющих классы ExtJS, и механизм генерации выходного JavaScript-кода, который отсылается браузеру для исполнения. Библиотека достаточно хорошо документирована и совместима с РНР 5. Для углубленного изучения есть справка по API.","summary":"Статья рассказывает о библиотеке PHP-Ext, которая позволяет разработчикам на языке РНР использовать фреймворк ExtJS для создания сложных AJAX веб-приложений. PHP-Ext предоставляет набор классов, частично повторяющих классы ExtJS, и механизм генерации выходного JavaScript-кода, который отсылается браузеру для исполнения. Библиотека достаточно хорошо документирована и совместима с РНР 5. Для углубленного изучения есть справка по API."}
